  I0428 16:35:08.411058      19 e2e.go:117] Starting e2e run "e19fda74-19df-4e20-970a-6122324dd6fc" on Ginkgo node 1
  Apr 28 16:35:08.553: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1682699708 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Apr 28 16:35:08.760: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 16:35:08.761: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Apr 28 16:35:08.981: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Apr 28 16:35:08.985: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'svclb-traefik-0a77677b' (0 seconds elapsed)
  Apr 28 16:35:08.985: INFO: e2e test version: v1.27.1
  Apr 28 16:35:08.986: INFO: kube-apiserver version: v1.27.1+k3s1
  Apr 28 16:35:08.986: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 16:35:08.991: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.231 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 04/28/23 16:35:09.289
  Apr 28 16:35:09.289: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 16:35:09.29
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:35:09.66
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:35:09.664
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 16:35:09.667
  STEP: Saw pod success @ 04/28/23 16:35:13.694
  Apr 28 16:35:13.699: INFO: Trying to get logs from node ip-172-31-1-213 pod downwardapi-volume-08ab285b-2daa-4016-907e-4878a709e444 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 16:35:13.721
  Apr 28 16:35:13.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7393" for this suite. @ 04/28/23 16:35:13.749
• [4.472 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 04/28/23 16:35:13.761
  Apr 28 16:35:13.762: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 16:35:13.763
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:35:13.809
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:35:13.814
  STEP: Creating a pod to test downward api env vars @ 04/28/23 16:35:13.82
  STEP: Saw pod success @ 04/28/23 16:35:18.015
  Apr 28 16:35:18.018: INFO: Trying to get logs from node ip-172-31-1-213 pod downward-api-d9780aff-5f5a-4e3e-a065-502fbe1a2353 container dapi-container: <nil>
  STEP: delete the pod @ 04/28/23 16:35:18.026
  Apr 28 16:35:18.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1297" for this suite. @ 04/28/23 16:35:18.048
• [4.294 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 04/28/23 16:35:18.056
  Apr 28 16:35:18.056: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename daemonsets @ 04/28/23 16:35:18.057
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:35:18.079
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:35:18.082
  Apr 28 16:35:18.111: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 04/28/23 16:35:18.119
  Apr 28 16:35:18.126: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:35:18.126: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 04/28/23 16:35:18.126
  Apr 28 16:35:18.148: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:35:18.148: INFO: Node ip-172-31-5-174 is running 0 daemon pod, expected 1
  Apr 28 16:35:19.152: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:35:19.152: INFO: Node ip-172-31-5-174 is running 0 daemon pod, expected 1
  Apr 28 16:35:20.152: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 28 16:35:20.152: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 04/28/23 16:35:20.155
  Apr 28 16:35:20.461: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:35:20.461: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 04/28/23 16:35:20.461
  Apr 28 16:35:20.481: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:35:20.481: INFO: Node ip-172-31-5-174 is running 0 daemon pod, expected 1
  Apr 28 16:35:21.486: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:35:21.486: INFO: Node ip-172-31-5-174 is running 0 daemon pod, expected 1
  Apr 28 16:35:22.484: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:35:22.484: INFO: Node ip-172-31-5-174 is running 0 daemon pod, expected 1
  Apr 28 16:35:23.486: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:35:23.486: INFO: Node ip-172-31-5-174 is running 0 daemon pod, expected 1
  Apr 28 16:35:24.485: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 28 16:35:24.485: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/28/23 16:35:24.491
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1414, will wait for the garbage collector to delete the pods @ 04/28/23 16:35:24.491
  Apr 28 16:35:24.549: INFO: Deleting DaemonSet.extensions daemon-set took: 5.379987ms
  Apr 28 16:35:24.650: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.784436ms
  Apr 28 16:35:25.753: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:35:25.753: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 28 16:35:25.758: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"236750"},"items":null}

  Apr 28 16:35:25.762: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"236750"},"items":null}

  Apr 28 16:35:25.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1414" for this suite. @ 04/28/23 16:35:25.8
• [7.755 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 04/28/23 16:35:25.811
  Apr 28 16:35:25.811: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/28/23 16:35:25.812
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:35:25.838
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:35:25.845
  STEP: set up a multi version CRD @ 04/28/23 16:35:25.848
  Apr 28 16:35:25.849: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: rename a version @ 04/28/23 16:35:29.743
  STEP: check the new version name is served @ 04/28/23 16:35:29.756
  STEP: check the old version name is removed @ 04/28/23 16:35:31.279
  STEP: check the other version is not changed @ 04/28/23 16:35:32.104
  Apr 28 16:35:35.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8698" for this suite. @ 04/28/23 16:35:35.197
• [9.390 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 04/28/23 16:35:35.203
  Apr 28 16:35:35.203: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 16:35:35.204
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:35:35.219
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:35:35.224
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-25dcf640-8281-408d-be44-fc603404cfb7 @ 04/28/23 16:35:35.232
  STEP: Creating the pod @ 04/28/23 16:35:35.237
  STEP: Updating configmap projected-configmap-test-upd-25dcf640-8281-408d-be44-fc603404cfb7 @ 04/28/23 16:35:37.28
  STEP: waiting to observe update in volume @ 04/28/23 16:35:37.286
  Apr 28 16:35:39.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-64" for this suite. @ 04/28/23 16:35:39.304
• [4.107 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 04/28/23 16:35:39.31
  Apr 28 16:35:39.310: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename subpath @ 04/28/23 16:35:39.311
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:35:39.327
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:35:39.333
  STEP: Setting up data @ 04/28/23 16:35:39.336
  STEP: Creating pod pod-subpath-test-configmap-5vs6 @ 04/28/23 16:35:39.354
  STEP: Creating a pod to test atomic-volume-subpath @ 04/28/23 16:35:39.354
  STEP: Saw pod success @ 04/28/23 16:36:03.485
  Apr 28 16:36:03.488: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-subpath-test-configmap-5vs6 container test-container-subpath-configmap-5vs6: <nil>
  STEP: delete the pod @ 04/28/23 16:36:03.494
  STEP: Deleting pod pod-subpath-test-configmap-5vs6 @ 04/28/23 16:36:03.515
  Apr 28 16:36:03.516: INFO: Deleting pod "pod-subpath-test-configmap-5vs6" in namespace "subpath-3434"
  Apr 28 16:36:03.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-3434" for this suite. @ 04/28/23 16:36:03.523
• [24.221 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 04/28/23 16:36:03.531
  Apr 28 16:36:03.531: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 16:36:03.532
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:36:03.595
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:36:03.598
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 04/28/23 16:36:03.6
  STEP: Saw pod success @ 04/28/23 16:36:07.933
  Apr 28 16:36:07.937: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-66fac328-de49-432b-8bc1-571c3d970190 container test-container: <nil>
  STEP: delete the pod @ 04/28/23 16:36:07.946
  Apr 28 16:36:07.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4285" for this suite. @ 04/28/23 16:36:07.973
• [4.455 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 04/28/23 16:36:07.988
  Apr 28 16:36:07.988: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename sched-pred @ 04/28/23 16:36:07.99
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:36:08.045
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:36:08.049
  Apr 28 16:36:08.056: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 28 16:36:08.071: INFO: Waiting for terminating namespaces to be deleted...
  Apr 28 16:36:08.074: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-1-213 before test
  Apr 28 16:36:08.081: INFO: svclb-traefik-0a77677b-p7w4w from kube-system started at 2023-04-28 05:45:24 +0000 UTC (2 container statuses recorded)
  Apr 28 16:36:08.082: INFO: 	Container lb-tcp-443 ready: true, restart count 0
  Apr 28 16:36:08.082: INFO: 	Container lb-tcp-80 ready: true, restart count 0
  Apr 28 16:36:08.082: INFO: sonobuoy-systemd-logs-daemon-set-d17888d62c134fe7-v6wcc from sonobuoy started at 2023-04-28 16:35:07 +0000 UTC (2 container statuses recorded)
  Apr 28 16:36:08.082: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 16:36:08.082: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 16:36:08.082: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-10-53 before test
  Apr 28 16:36:08.088: INFO: svclb-traefik-0a77677b-d6qnt from kube-system started at 2023-04-28 03:31:43 +0000 UTC (2 container statuses recorded)
  Apr 28 16:36:08.088: INFO: 	Container lb-tcp-443 ready: true, restart count 0
  Apr 28 16:36:08.088: INFO: 	Container lb-tcp-80 ready: true, restart count 0
  Apr 28 16:36:08.088: INFO: sonobuoy from sonobuoy started at 2023-04-28 16:35:06 +0000 UTC (1 container statuses recorded)
  Apr 28 16:36:08.088: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 28 16:36:08.088: INFO: sonobuoy-systemd-logs-daemon-set-d17888d62c134fe7-clf8z from sonobuoy started at 2023-04-28 16:35:07 +0000 UTC (2 container statuses recorded)
  Apr 28 16:36:08.088: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 16:36:08.088: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 16:36:08.088: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-12-59 before test
  Apr 28 16:36:08.093: INFO: coredns-77ccd57875-lg57c from kube-system started at 2023-04-28 03:29:34 +0000 UTC (1 container statuses recorded)
  Apr 28 16:36:08.093: INFO: 	Container coredns ready: true, restart count 0
  Apr 28 16:36:08.093: INFO: helm-install-traefik-crd-rxpjj from kube-system started at 2023-04-28 03:29:34 +0000 UTC (1 container statuses recorded)
  Apr 28 16:36:08.093: INFO: 	Container helm ready: false, restart count 0
  Apr 28 16:36:08.093: INFO: helm-install-traefik-q2g86 from kube-system started at 2023-04-28 03:29:34 +0000 UTC (1 container statuses recorded)
  Apr 28 16:36:08.093: INFO: 	Container helm ready: false, restart count 1
  Apr 28 16:36:08.093: INFO: local-path-provisioner-957fdf8bc-jc5qz from kube-system started at 2023-04-28 03:29:34 +0000 UTC (1 container statuses recorded)
  Apr 28 16:36:08.093: INFO: 	Container local-path-provisioner ready: true, restart count 0
  Apr 28 16:36:08.093: INFO: metrics-server-54dc485875-gbbbl from kube-system started at 2023-04-28 03:29:34 +0000 UTC (1 container statuses recorded)
  Apr 28 16:36:08.093: INFO: 	Container metrics-server ready: true, restart count 0
  Apr 28 16:36:08.093: INFO: svclb-traefik-0a77677b-jtrmg from kube-system started at 2023-04-28 03:29:54 +0000 UTC (2 container statuses recorded)
  Apr 28 16:36:08.093: INFO: 	Container lb-tcp-443 ready: true, restart count 0
  Apr 28 16:36:08.093: INFO: 	Container lb-tcp-80 ready: true, restart count 0
  Apr 28 16:36:08.093: INFO: traefik-84745cf649-mwthv from kube-system started at 2023-04-28 03:29:54 +0000 UTC (1 container statuses recorded)
  Apr 28 16:36:08.093: INFO: 	Container traefik ready: true, restart count 0
  Apr 28 16:36:08.093: INFO: sonobuoy-systemd-logs-daemon-set-d17888d62c134fe7-4jl87 from sonobuoy started at 2023-04-28 16:35:07 +0000 UTC (2 container statuses recorded)
  Apr 28 16:36:08.093: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 16:36:08.093: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 16:36:08.093: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-5-174 before test
  Apr 28 16:36:08.098: INFO: svclb-traefik-0a77677b-x527f from kube-system started at 2023-04-28 03:33:13 +0000 UTC (2 container statuses recorded)
  Apr 28 16:36:08.098: INFO: 	Container lb-tcp-443 ready: true, restart count 0
  Apr 28 16:36:08.098: INFO: 	Container lb-tcp-80 ready: true, restart count 0
  Apr 28 16:36:08.098: INFO: sonobuoy-e2e-job-4c96b6c95eb94b68 from sonobuoy started at 2023-04-28 16:35:07 +0000 UTC (2 container statuses recorded)
  Apr 28 16:36:08.098: INFO: 	Container e2e ready: true, restart count 0
  Apr 28 16:36:08.098: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 16:36:08.099: INFO: sonobuoy-systemd-logs-daemon-set-d17888d62c134fe7-wkkfr from sonobuoy started at 2023-04-28 16:35:07 +0000 UTC (2 container statuses recorded)
  Apr 28 16:36:08.099: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 16:36:08.099: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 04/28/23 16:36:08.099
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.175a26861644d59f], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match Pod's node affinity/selector. preemption: 0/4 nodes are available: 4 Preemption is not helpful for scheduling..] @ 04/28/23 16:36:08.128
  Apr 28 16:36:09.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-674" for this suite. @ 04/28/23 16:36:09.124
• [1.141 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 04/28/23 16:36:09.129
  Apr 28 16:36:09.129: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename deployment @ 04/28/23 16:36:09.13
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:36:09.145
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:36:09.148
  STEP: creating a Deployment @ 04/28/23 16:36:09.155
  STEP: waiting for Deployment to be created @ 04/28/23 16:36:09.161
  STEP: waiting for all Replicas to be Ready @ 04/28/23 16:36:09.163
  Apr 28 16:36:09.164: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 28 16:36:09.165: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 28 16:36:09.185: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 28 16:36:09.185: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 28 16:36:09.197: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 28 16:36:09.197: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 28 16:36:09.267: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 28 16:36:09.267: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 28 16:36:10.219: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Apr 28 16:36:10.220: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Apr 28 16:36:11.130: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 04/28/23 16:36:11.13
  W0428 16:36:11.155222      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 28 16:36:11.156: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 04/28/23 16:36:11.157
  Apr 28 16:36:11.158: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 0
  Apr 28 16:36:11.158: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 0
  Apr 28 16:36:11.158: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 0
  Apr 28 16:36:11.158: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 0
  Apr 28 16:36:11.158: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 0
  Apr 28 16:36:11.158: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 0
  Apr 28 16:36:11.158: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 0
  Apr 28 16:36:11.158: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 0
  Apr 28 16:36:11.158: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 1
  Apr 28 16:36:11.158: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 1
  Apr 28 16:36:11.158: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 2
  Apr 28 16:36:11.158: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 2
  Apr 28 16:36:11.158: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 2
  Apr 28 16:36:11.158: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 2
  Apr 28 16:36:11.189: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 2
  Apr 28 16:36:11.189: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 2
  Apr 28 16:36:11.234: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 2
  Apr 28 16:36:11.234: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 2
  Apr 28 16:36:11.252: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 1
  Apr 28 16:36:11.252: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 1
  Apr 28 16:36:11.288: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 1
  Apr 28 16:36:11.288: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 1
  Apr 28 16:36:12.077: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 2
  Apr 28 16:36:12.077: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 2
  Apr 28 16:36:12.118: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 1
  STEP: listing Deployments @ 04/28/23 16:36:12.118
  Apr 28 16:36:12.125: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 04/28/23 16:36:12.125
  Apr 28 16:36:12.138: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 04/28/23 16:36:12.138
  Apr 28 16:36:12.147: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 28 16:36:12.151: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 28 16:36:12.173: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 28 16:36:12.220: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 28 16:36:12.220: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 28 16:36:13.329: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 28 16:36:13.389: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 28 16:36:13.394: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 28 16:36:13.410: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 28 16:36:13.424: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 28 16:36:15.062: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 04/28/23 16:36:15.071
  STEP: fetching the DeploymentStatus @ 04/28/23 16:36:15.091
  Apr 28 16:36:15.102: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 1
  Apr 28 16:36:15.102: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 1
  Apr 28 16:36:15.102: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 1
  Apr 28 16:36:15.102: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 1
  Apr 28 16:36:15.102: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 1
  Apr 28 16:36:15.102: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 2
  Apr 28 16:36:15.102: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 2
  Apr 28 16:36:15.102: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 2
  Apr 28 16:36:15.102: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 2
  Apr 28 16:36:15.102: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 2
  Apr 28 16:36:15.102: INFO: observed Deployment test-deployment in namespace deployment-7227 with ReadyReplicas 3
  STEP: deleting the Deployment @ 04/28/23 16:36:15.102
  Apr 28 16:36:15.115: INFO: observed event type MODIFIED
  Apr 28 16:36:15.115: INFO: observed event type MODIFIED
  Apr 28 16:36:15.115: INFO: observed event type MODIFIED
  Apr 28 16:36:15.115: INFO: observed event type MODIFIED
  Apr 28 16:36:15.116: INFO: observed event type MODIFIED
  Apr 28 16:36:15.116: INFO: observed event type MODIFIED
  Apr 28 16:36:15.116: INFO: observed event type MODIFIED
  Apr 28 16:36:15.116: INFO: observed event type MODIFIED
  Apr 28 16:36:15.116: INFO: observed event type MODIFIED
  Apr 28 16:36:15.116: INFO: observed event type MODIFIED
  Apr 28 16:36:15.116: INFO: observed event type MODIFIED
  Apr 28 16:36:15.116: INFO: observed event type MODIFIED
  Apr 28 16:36:15.125: INFO: Log out all the ReplicaSets if there is no deployment created
  Apr 28 16:36:15.134: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-7227  96d0f5b2-3279-4b9e-a7ac-b8ac6a20ec88 237244 4 2023-04-28 16:36:11 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 59e79f8a-b470-4be6-ad5e-0675a5a88b04 0xc00539f0a7 0xc00539f0a8}] [] [{k3s Update apps/v1 2023-04-28 16:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"59e79f8a-b470-4be6-ad5e-0675a5a88b04\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2023-04-28 16:36:15 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00539f140 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Apr 28 16:36:15.142: INFO: pod: "test-deployment-5b5dcbcd95-9m4xv":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-9m4xv test-deployment-5b5dcbcd95- deployment-7227  15357feb-d3d3-40cf-bae6-29bb399d7095 237240 0 2023-04-28 16:36:11 +0000 UTC 2023-04-28 16:36:16 +0000 UTC 0xc00540d828 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 96d0f5b2-3279-4b9e-a7ac-b8ac6a20ec88 0xc00540d857 0xc00540d858}] [] [{k3s Update v1 2023-04-28 16:36:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96d0f5b2-3279-4b9e-a7ac-b8ac6a20ec88\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 16:36:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-465x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-465x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-213,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 16:36:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 16:36:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 16:36:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 16:36:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.213,PodIP:10.42.2.138,StartTime:2023-04-28 16:36:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 16:36:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://7998e76f4d34bf6e35cd98a1d87cb589d6c54b80e5b1ac612314235805d5174b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.138,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 28 16:36:15.142: INFO: pod: "test-deployment-5b5dcbcd95-dkftk":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-dkftk test-deployment-5b5dcbcd95- deployment-7227  159ee246-6e0b-4bcd-9637-38152876bd18 237247 0 2023-04-28 16:36:12 +0000 UTC 2023-04-28 16:36:14 +0000 UTC 0xc00540da20 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 96d0f5b2-3279-4b9e-a7ac-b8ac6a20ec88 0xc00540da57 0xc00540da58}] [] [{k3s Update v1 2023-04-28 16:36:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"96d0f5b2-3279-4b9e-a7ac-b8ac6a20ec88\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 16:36:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n8f8w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n8f8w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-5-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Succeeded,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 16:36:12 +0000 UTC,Reason:PodCompleted,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 16:36:14 +0000 UTC,Reason:PodCompleted,Message:,},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 16:36:14 +0000 UTC,Reason:PodCompleted,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 16:36:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.5.174,PodIP:,StartTime:2023-04-28 16:36:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:nil,Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2023-04-28 16:36:13 +0000 UTC,FinishedAt:2023-04-28 16:36:14 +0000 UTC,ContainerID:containerd://65fbbfd286eeebe803ed624844162b8b9cc3af8b756cbaaec21901557976038a,},},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://65fbbfd286eeebe803ed624844162b8b9cc3af8b756cbaaec21901557976038a,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 28 16:36:15.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7227" for this suite. @ 04/28/23 16:36:15.161
• [6.039 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 04/28/23 16:36:15.168
  Apr 28 16:36:15.168: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename pods @ 04/28/23 16:36:15.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:36:15.187
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:36:15.191
  STEP: creating a Pod with a static label @ 04/28/23 16:36:15.203
  STEP: watching for Pod to be ready @ 04/28/23 16:36:15.219
  Apr 28 16:36:15.245: INFO: observed Pod pod-test in namespace pods-8381 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Apr 28 16:36:15.245: INFO: observed Pod pod-test in namespace pods-8381 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:36:15 +0000 UTC  }]
  Apr 28 16:36:15.381: INFO: observed Pod pod-test in namespace pods-8381 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:36:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:36:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:36:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:36:15 +0000 UTC  }]
  Apr 28 16:36:16.280: INFO: Found Pod pod-test in namespace pods-8381 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:36:15 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:36:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:36:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:36:15 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 04/28/23 16:36:16.295
  STEP: getting the Pod and ensuring that it's patched @ 04/28/23 16:36:16.312
  STEP: replacing the Pod's status Ready condition to False @ 04/28/23 16:36:16.315
  STEP: check the Pod again to ensure its Ready conditions are False @ 04/28/23 16:36:16.335
  STEP: deleting the Pod via a Collection with a LabelSelector @ 04/28/23 16:36:16.335
  STEP: watching for the Pod to be deleted @ 04/28/23 16:36:16.346
  Apr 28 16:36:16.348: INFO: observed event type MODIFIED
  Apr 28 16:36:18.291: INFO: observed event type MODIFIED
  Apr 28 16:36:18.384: INFO: observed event type MODIFIED
  Apr 28 16:36:19.299: INFO: observed event type MODIFIED
  Apr 28 16:36:19.311: INFO: observed event type MODIFIED
  Apr 28 16:36:19.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8381" for this suite. @ 04/28/23 16:36:19.324
• [4.170 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 04/28/23 16:36:19.34
  Apr 28 16:36:19.340: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename events @ 04/28/23 16:36:19.341
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:36:19.358
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:36:19.361
  STEP: creating a test event @ 04/28/23 16:36:19.366
  STEP: listing events in all namespaces @ 04/28/23 16:36:19.376
  STEP: listing events in test namespace @ 04/28/23 16:36:19.38
  STEP: listing events with field selection filtering on source @ 04/28/23 16:36:19.383
  STEP: listing events with field selection filtering on reportingController @ 04/28/23 16:36:19.387
  STEP: getting the test event @ 04/28/23 16:36:19.389
  STEP: patching the test event @ 04/28/23 16:36:19.392
  STEP: getting the test event @ 04/28/23 16:36:19.399
  STEP: updating the test event @ 04/28/23 16:36:19.401
  STEP: getting the test event @ 04/28/23 16:36:19.409
  STEP: deleting the test event @ 04/28/23 16:36:19.412
  STEP: listing events in all namespaces @ 04/28/23 16:36:19.418
  STEP: listing events in test namespace @ 04/28/23 16:36:19.422
  Apr 28 16:36:19.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-213" for this suite. @ 04/28/23 16:36:19.427
• [0.097 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 04/28/23 16:36:19.438
  Apr 28 16:36:19.438: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename podtemplate @ 04/28/23 16:36:19.439
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:36:19.454
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:36:19.457
  STEP: Create set of pod templates @ 04/28/23 16:36:19.46
  Apr 28 16:36:19.466: INFO: created test-podtemplate-1
  Apr 28 16:36:19.474: INFO: created test-podtemplate-2
  Apr 28 16:36:19.479: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 04/28/23 16:36:19.48
  STEP: delete collection of pod templates @ 04/28/23 16:36:19.483
  Apr 28 16:36:19.483: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 04/28/23 16:36:19.505
  Apr 28 16:36:19.505: INFO: requesting list of pod templates to confirm quantity
  Apr 28 16:36:19.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-8102" for this suite. @ 04/28/23 16:36:19.516
• [0.086 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 04/28/23 16:36:19.524
  Apr 28 16:36:19.524: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename services @ 04/28/23 16:36:19.525
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:36:19.54
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:36:19.543
  STEP: creating service in namespace services-2235 @ 04/28/23 16:36:19.551
  STEP: creating service affinity-nodeport in namespace services-2235 @ 04/28/23 16:36:19.551
  STEP: creating replication controller affinity-nodeport in namespace services-2235 @ 04/28/23 16:36:19.58
  I0428 16:36:19.604408      19 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-2235, replica count: 3
  I0428 16:36:22.655481      19 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0428 16:36:25.657103      19 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 28 16:36:25.666: INFO: Creating new exec pod
  Apr 28 16:36:28.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-2235 exec execpod-affinityxz9qm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Apr 28 16:36:28.982: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Apr 28 16:36:28.982: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 16:36:28.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-2235 exec execpod-affinityxz9qm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.248.71 80'
  Apr 28 16:36:29.137: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.248.71 80\nConnection to 10.43.248.71 80 port [tcp/http] succeeded!\n"
  Apr 28 16:36:29.137: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 16:36:29.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-2235 exec execpod-affinityxz9qm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.12.59 31781'
  Apr 28 16:36:29.290: INFO: stderr: "+ nc -v -t -w 2 172.31.12.59 31781\n+ echo hostName\nConnection to 172.31.12.59 31781 port [tcp/*] succeeded!\n"
  Apr 28 16:36:29.290: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 16:36:29.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-2235 exec execpod-affinityxz9qm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.10.53 31781'
  Apr 28 16:36:31.436: INFO: rc: 1
  Apr 28 16:36:31.436: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-2235 exec execpod-affinityxz9qm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.10.53 31781:
  Command stdout:

  stderr:
  + nc -v -t -w 2 172.31.10.53 31781
  + echo hostName
  nc: connect to 172.31.10.53 port 31781 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  Apr 28 16:36:32.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-2235 exec execpod-affinityxz9qm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.10.53 31781'
  Apr 28 16:36:34.573: INFO: rc: 1
  Apr 28 16:36:34.573: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-2235 exec execpod-affinityxz9qm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.10.53 31781:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 172.31.10.53 31781
  nc: connect to 172.31.10.53 port 31781 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  Apr 28 16:36:35.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-2235 exec execpod-affinityxz9qm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.10.53 31781'
  Apr 28 16:36:37.604: INFO: rc: 1
  Apr 28 16:36:37.605: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-2235 exec execpod-affinityxz9qm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.10.53 31781:
  Command stdout:

  stderr:
  + nc -v -t -w 2 172.31.10.53 31781
  + echo hostName
  nc: connect to 172.31.10.53 port 31781 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  Apr 28 16:36:38.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-2235 exec execpod-affinityxz9qm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.10.53 31781'
  Apr 28 16:36:38.550: INFO: stderr: "+ nc -v -t -w 2 172.31.10.53 31781\n+ echo hostName\nConnection to 172.31.10.53 31781 port [tcp/*] succeeded!\n"
  Apr 28 16:36:38.550: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 16:36:38.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-2235 exec execpod-affinityxz9qm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.1.213:31781/ ; done'
  Apr 28 16:36:38.774: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:31781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:31781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:31781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:31781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:31781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:31781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:31781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:31781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:31781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:31781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:31781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:31781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:31781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:31781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:31781/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:31781/\n"
  Apr 28 16:36:38.774: INFO: stdout: "\naffinity-nodeport-bsbvj\naffinity-nodeport-bsbvj\naffinity-nodeport-bsbvj\naffinity-nodeport-bsbvj\naffinity-nodeport-bsbvj\naffinity-nodeport-bsbvj\naffinity-nodeport-bsbvj\naffinity-nodeport-bsbvj\naffinity-nodeport-bsbvj\naffinity-nodeport-bsbvj\naffinity-nodeport-bsbvj\naffinity-nodeport-bsbvj\naffinity-nodeport-bsbvj\naffinity-nodeport-bsbvj\naffinity-nodeport-bsbvj\naffinity-nodeport-bsbvj"
  Apr 28 16:36:38.774: INFO: Received response from host: affinity-nodeport-bsbvj
  Apr 28 16:36:38.774: INFO: Received response from host: affinity-nodeport-bsbvj
  Apr 28 16:36:38.774: INFO: Received response from host: affinity-nodeport-bsbvj
  Apr 28 16:36:38.774: INFO: Received response from host: affinity-nodeport-bsbvj
  Apr 28 16:36:38.774: INFO: Received response from host: affinity-nodeport-bsbvj
  Apr 28 16:36:38.775: INFO: Received response from host: affinity-nodeport-bsbvj
  Apr 28 16:36:38.775: INFO: Received response from host: affinity-nodeport-bsbvj
  Apr 28 16:36:38.775: INFO: Received response from host: affinity-nodeport-bsbvj
  Apr 28 16:36:38.775: INFO: Received response from host: affinity-nodeport-bsbvj
  Apr 28 16:36:38.775: INFO: Received response from host: affinity-nodeport-bsbvj
  Apr 28 16:36:38.775: INFO: Received response from host: affinity-nodeport-bsbvj
  Apr 28 16:36:38.775: INFO: Received response from host: affinity-nodeport-bsbvj
  Apr 28 16:36:38.775: INFO: Received response from host: affinity-nodeport-bsbvj
  Apr 28 16:36:38.775: INFO: Received response from host: affinity-nodeport-bsbvj
  Apr 28 16:36:38.775: INFO: Received response from host: affinity-nodeport-bsbvj
  Apr 28 16:36:38.775: INFO: Received response from host: affinity-nodeport-bsbvj
  Apr 28 16:36:38.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 16:36:38.779: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-2235, will wait for the garbage collector to delete the pods @ 04/28/23 16:36:38.79
  Apr 28 16:36:38.857: INFO: Deleting ReplicationController affinity-nodeport took: 7.391095ms
  Apr 28 16:36:38.958: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.113363ms
  STEP: Destroying namespace "services-2235" for this suite. @ 04/28/23 16:36:40.999
• [21.482 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 04/28/23 16:36:41.006
  Apr 28 16:36:41.006: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename subpath @ 04/28/23 16:36:41.007
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:36:41.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:36:41.037
  STEP: Setting up data @ 04/28/23 16:36:41.039
  STEP: Creating pod pod-subpath-test-secret-2lbs @ 04/28/23 16:36:41.05
  STEP: Creating a pod to test atomic-volume-subpath @ 04/28/23 16:36:41.05
  STEP: Saw pod success @ 04/28/23 16:37:05.149
  Apr 28 16:37:05.153: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-subpath-test-secret-2lbs container test-container-subpath-secret-2lbs: <nil>
  STEP: delete the pod @ 04/28/23 16:37:05.16
  STEP: Deleting pod pod-subpath-test-secret-2lbs @ 04/28/23 16:37:05.184
  Apr 28 16:37:05.184: INFO: Deleting pod "pod-subpath-test-secret-2lbs" in namespace "subpath-6174"
  Apr 28 16:37:05.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-6174" for this suite. @ 04/28/23 16:37:05.212
• [24.219 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 04/28/23 16:37:05.226
  Apr 28 16:37:05.226: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename csiinlinevolumes @ 04/28/23 16:37:05.227
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:37:05.271
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:37:05.28
  STEP: creating @ 04/28/23 16:37:05.291
  STEP: getting @ 04/28/23 16:37:05.339
  STEP: listing in namespace @ 04/28/23 16:37:05.35
  STEP: patching @ 04/28/23 16:37:05.365
  STEP: deleting @ 04/28/23 16:37:05.387
  Apr 28 16:37:05.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-9070" for this suite. @ 04/28/23 16:37:05.429
• [0.221 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 04/28/23 16:37:05.448
  Apr 28 16:37:05.448: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename runtimeclass @ 04/28/23 16:37:05.449
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:37:05.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:37:05.904
  Apr 28 16:37:05.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-2064" for this suite. @ 04/28/23 16:37:05.992
• [0.551 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 04/28/23 16:37:06
  Apr 28 16:37:06.000: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename container-runtime @ 04/28/23 16:37:06.001
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:37:06.03
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:37:06.035
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 04/28/23 16:37:06.092
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 04/28/23 16:37:21.286
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 04/28/23 16:37:21.289
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 04/28/23 16:37:21.294
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 04/28/23 16:37:21.294
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 04/28/23 16:37:21.318
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 04/28/23 16:37:23.348
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 04/28/23 16:37:25.462
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 04/28/23 16:37:25.483
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 04/28/23 16:37:25.483
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 04/28/23 16:37:25.533
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 04/28/23 16:37:26.54
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 04/28/23 16:37:27.551
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 04/28/23 16:37:27.557
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 04/28/23 16:37:27.557
  Apr 28 16:37:27.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-4955" for this suite. @ 04/28/23 16:37:27.606
• [21.621 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 04/28/23 16:37:27.623
  Apr 28 16:37:27.623: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename pods @ 04/28/23 16:37:27.624
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:37:27.661
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:37:27.664
  STEP: Create a pod @ 04/28/23 16:37:27.672
  STEP: patching /status @ 04/28/23 16:37:29.695
  Apr 28 16:37:29.702: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Apr 28 16:37:29.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9627" for this suite. @ 04/28/23 16:37:29.712
• [2.095 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 04/28/23 16:37:29.719
  Apr 28 16:37:29.719: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename replication-controller @ 04/28/23 16:37:29.719
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:37:29.786
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:37:29.79
  STEP: Creating ReplicationController "e2e-rc-brms8" @ 04/28/23 16:37:29.794
  Apr 28 16:37:29.799: INFO: Get Replication Controller "e2e-rc-brms8" to confirm replicas
  Apr 28 16:37:30.803: INFO: Get Replication Controller "e2e-rc-brms8" to confirm replicas
  Apr 28 16:37:30.807: INFO: Found 1 replicas for "e2e-rc-brms8" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-brms8" @ 04/28/23 16:37:30.807
  STEP: Updating a scale subresource @ 04/28/23 16:37:30.809
  STEP: Verifying replicas where modified for replication controller "e2e-rc-brms8" @ 04/28/23 16:37:30.817
  Apr 28 16:37:30.817: INFO: Get Replication Controller "e2e-rc-brms8" to confirm replicas
  Apr 28 16:37:31.827: INFO: Get Replication Controller "e2e-rc-brms8" to confirm replicas
  Apr 28 16:37:31.831: INFO: Found 2 replicas for "e2e-rc-brms8" replication controller
  Apr 28 16:37:31.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-6824" for this suite. @ 04/28/23 16:37:31.837
• [2.125 seconds]
------------------------------
SS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 04/28/23 16:37:31.844
  Apr 28 16:37:31.844: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename runtimeclass @ 04/28/23 16:37:31.847
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:37:31.875
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:37:31.88
  STEP: getting /apis @ 04/28/23 16:37:31.885
  STEP: getting /apis/node.k8s.io @ 04/28/23 16:37:31.892
  STEP: getting /apis/node.k8s.io/v1 @ 04/28/23 16:37:31.893
  STEP: creating @ 04/28/23 16:37:31.894
  STEP: watching @ 04/28/23 16:37:31.947
  Apr 28 16:37:31.947: INFO: starting watch
  STEP: getting @ 04/28/23 16:37:31.969
  STEP: listing @ 04/28/23 16:37:31.972
  STEP: patching @ 04/28/23 16:37:31.977
  STEP: updating @ 04/28/23 16:37:31.994
  Apr 28 16:37:32.006: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 04/28/23 16:37:32.006
  STEP: deleting a collection @ 04/28/23 16:37:32.042
  Apr 28 16:37:32.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5598" for this suite. @ 04/28/23 16:37:32.079
• [0.252 seconds]
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 04/28/23 16:37:32.096
  Apr 28 16:37:32.097: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename subpath @ 04/28/23 16:37:32.097
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:37:32.126
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:37:32.132
  STEP: Setting up data @ 04/28/23 16:37:32.137
  STEP: Creating pod pod-subpath-test-downwardapi-sgm4 @ 04/28/23 16:37:32.154
  STEP: Creating a pod to test atomic-volume-subpath @ 04/28/23 16:37:32.154
  STEP: Saw pod success @ 04/28/23 16:37:56.375
  Apr 28 16:37:56.378: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-subpath-test-downwardapi-sgm4 container test-container-subpath-downwardapi-sgm4: <nil>
  STEP: delete the pod @ 04/28/23 16:37:56.383
  STEP: Deleting pod pod-subpath-test-downwardapi-sgm4 @ 04/28/23 16:37:56.399
  Apr 28 16:37:56.399: INFO: Deleting pod "pod-subpath-test-downwardapi-sgm4" in namespace "subpath-961"
  Apr 28 16:37:56.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-961" for this suite. @ 04/28/23 16:37:56.408
• [24.321 seconds]
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 04/28/23 16:37:56.418
  Apr 28 16:37:56.418: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename pods @ 04/28/23 16:37:56.419
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:37:56.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:37:56.445
  STEP: Create set of pods @ 04/28/23 16:37:56.456
  Apr 28 16:37:56.472: INFO: created test-pod-1
  Apr 28 16:37:56.488: INFO: created test-pod-2
  Apr 28 16:37:56.521: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 04/28/23 16:37:56.521
  STEP: waiting for all pods to be deleted @ 04/28/23 16:37:58.62
  Apr 28 16:37:58.631: INFO: Pod quantity 3 is different from expected quantity 0
  Apr 28 16:37:59.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9959" for this suite. @ 04/28/23 16:37:59.642
• [3.231 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 04/28/23 16:37:59.649
  Apr 28 16:37:59.649: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename webhook @ 04/28/23 16:37:59.649
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:37:59.668
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:37:59.671
  STEP: Setting up server cert @ 04/28/23 16:37:59.72
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 16:38:00.179
  STEP: Deploying the webhook pod @ 04/28/23 16:38:00.188
  STEP: Wait for the deployment to be ready @ 04/28/23 16:38:00.205
  Apr 28 16:38:00.220: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/28/23 16:38:02.244
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 16:38:02.265
  Apr 28 16:38:03.265: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 04/28/23 16:38:03.326
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/28/23 16:38:03.426
  STEP: Deleting the collection of validation webhooks @ 04/28/23 16:38:03.511
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/28/23 16:38:04.052
  Apr 28 16:38:04.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2684" for this suite. @ 04/28/23 16:38:04.149
  STEP: Destroying namespace "webhook-markers-4468" for this suite. @ 04/28/23 16:38:04.158
• [4.533 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 04/28/23 16:38:04.187
  Apr 28 16:38:04.187: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename svcaccounts @ 04/28/23 16:38:04.187
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:38:04.218
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:38:04.221
  Apr 28 16:38:04.229: INFO: Got root ca configmap in namespace "svcaccounts-621"
  Apr 28 16:38:04.235: INFO: Deleted root ca configmap in namespace "svcaccounts-621"
  STEP: waiting for a new root ca configmap created @ 04/28/23 16:38:04.736
  Apr 28 16:38:04.739: INFO: Recreated root ca configmap in namespace "svcaccounts-621"
  Apr 28 16:38:04.749: INFO: Updated root ca configmap in namespace "svcaccounts-621"
  STEP: waiting for the root ca configmap reconciled @ 04/28/23 16:38:05.25
  Apr 28 16:38:05.255: INFO: Reconciled root ca configmap in namespace "svcaccounts-621"
  Apr 28 16:38:05.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-621" for this suite. @ 04/28/23 16:38:05.263
• [1.091 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 04/28/23 16:38:05.278
  Apr 28 16:38:05.278: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename pods @ 04/28/23 16:38:05.279
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:38:05.307
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:38:05.32
  STEP: creating the pod @ 04/28/23 16:38:05.333
  STEP: setting up watch @ 04/28/23 16:38:05.333
  STEP: submitting the pod to kubernetes @ 04/28/23 16:38:05.441
  STEP: verifying the pod is in kubernetes @ 04/28/23 16:38:05.453
  STEP: verifying pod creation was observed @ 04/28/23 16:38:05.457
  STEP: deleting the pod gracefully @ 04/28/23 16:38:07.477
  STEP: verifying pod deletion was observed @ 04/28/23 16:38:07.489
  Apr 28 16:38:08.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-564" for this suite. @ 04/28/23 16:38:08.427
• [3.156 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 04/28/23 16:38:08.435
  Apr 28 16:38:08.435: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 16:38:08.436
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:38:08.458
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:38:08.464
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 16:38:08.471
  STEP: Saw pod success @ 04/28/23 16:38:12.882
  Apr 28 16:38:12.886: INFO: Trying to get logs from node ip-172-31-1-213 pod downwardapi-volume-36c68a3a-f8ec-4ded-a6e5-66c9cc46f752 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 16:38:12.895
  Apr 28 16:38:12.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1414" for this suite. @ 04/28/23 16:38:12.915
• [4.491 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 04/28/23 16:38:12.93
  Apr 28 16:38:12.930: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 16:38:12.93
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:38:12.959
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:38:12.963
  STEP: creating a replication controller @ 04/28/23 16:38:12.968
  Apr 28 16:38:12.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-6644 create -f -'
  Apr 28 16:38:13.372: INFO: stderr: ""
  Apr 28 16:38:13.372: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/28/23 16:38:13.372
  Apr 28 16:38:13.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-6644 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 28 16:38:13.516: INFO: stderr: ""
  Apr 28 16:38:13.516: INFO: stdout: "update-demo-nautilus-68b2d update-demo-nautilus-gwpm4 "
  Apr 28 16:38:13.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-6644 get pods update-demo-nautilus-68b2d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 28 16:38:13.597: INFO: stderr: ""
  Apr 28 16:38:13.597: INFO: stdout: ""
  Apr 28 16:38:13.597: INFO: update-demo-nautilus-68b2d is created but not running
  Apr 28 16:38:18.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-6644 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 28 16:38:18.727: INFO: stderr: ""
  Apr 28 16:38:18.727: INFO: stdout: "update-demo-nautilus-68b2d update-demo-nautilus-gwpm4 "
  Apr 28 16:38:18.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-6644 get pods update-demo-nautilus-68b2d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 28 16:38:18.808: INFO: stderr: ""
  Apr 28 16:38:18.809: INFO: stdout: "true"
  Apr 28 16:38:18.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-6644 get pods update-demo-nautilus-68b2d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 28 16:38:18.886: INFO: stderr: ""
  Apr 28 16:38:18.886: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 28 16:38:18.886: INFO: validating pod update-demo-nautilus-68b2d
  Apr 28 16:38:18.896: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 28 16:38:18.896: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 28 16:38:18.896: INFO: update-demo-nautilus-68b2d is verified up and running
  Apr 28 16:38:18.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-6644 get pods update-demo-nautilus-gwpm4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 28 16:38:18.987: INFO: stderr: ""
  Apr 28 16:38:18.987: INFO: stdout: "true"
  Apr 28 16:38:18.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-6644 get pods update-demo-nautilus-gwpm4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 28 16:38:19.058: INFO: stderr: ""
  Apr 28 16:38:19.058: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 28 16:38:19.058: INFO: validating pod update-demo-nautilus-gwpm4
  Apr 28 16:38:19.064: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 28 16:38:19.064: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 28 16:38:19.064: INFO: update-demo-nautilus-gwpm4 is verified up and running
  STEP: using delete to clean up resources @ 04/28/23 16:38:19.064
  Apr 28 16:38:19.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-6644 delete --grace-period=0 --force -f -'
  Apr 28 16:38:19.143: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 28 16:38:19.143: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Apr 28 16:38:19.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-6644 get rc,svc -l name=update-demo --no-headers'
  Apr 28 16:38:19.230: INFO: stderr: "No resources found in kubectl-6644 namespace.\n"
  Apr 28 16:38:19.230: INFO: stdout: ""
  Apr 28 16:38:19.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-6644 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 28 16:38:19.336: INFO: stderr: ""
  Apr 28 16:38:19.336: INFO: stdout: ""
  Apr 28 16:38:19.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6644" for this suite. @ 04/28/23 16:38:19.343
• [6.420 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 04/28/23 16:38:19.351
  Apr 28 16:38:19.351: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 16:38:19.352
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:38:19.444
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:38:19.447
  Apr 28 16:38:19.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-5678 create -f -'
  Apr 28 16:38:19.727: INFO: stderr: ""
  Apr 28 16:38:19.727: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Apr 28 16:38:19.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-5678 create -f -'
  Apr 28 16:38:20.618: INFO: stderr: ""
  Apr 28 16:38:20.618: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/28/23 16:38:20.618
  Apr 28 16:38:21.621: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 28 16:38:21.621: INFO: Found 1 / 1
  Apr 28 16:38:21.622: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Apr 28 16:38:21.625: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 28 16:38:21.625: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 28 16:38:21.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-5678 describe pod agnhost-primary-n6fn9'
  Apr 28 16:38:21.798: INFO: stderr: ""
  Apr 28 16:38:21.798: INFO: stdout: "Name:             agnhost-primary-n6fn9\nNamespace:        kubectl-5678\nPriority:         0\nService Account:  default\nNode:             ip-172-31-1-213/172.31.1.213\nStart Time:       Fri, 28 Apr 2023 16:38:19 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.42.2.152\nIPs:\n  IP:           10.42.2.152\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://157b830edd9d85fa39caa23ac496611bee2c483d89b608832f213aae6ab5e188\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 28 Apr 2023 16:38:20 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fmqm5 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-fmqm5:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-5678/agnhost-primary-n6fn9 to ip-172-31-1-213\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  Apr 28 16:38:21.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-5678 describe rc agnhost-primary'
  Apr 28 16:38:21.996: INFO: stderr: ""
  Apr 28 16:38:21.996: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5678\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-n6fn9\n"
  Apr 28 16:38:21.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-5678 describe service agnhost-primary'
  Apr 28 16:38:22.170: INFO: stderr: ""
  Apr 28 16:38:22.170: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5678\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.43.28.133\nIPs:               10.43.28.133\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.42.2.152:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Apr 28 16:38:22.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-5678 describe node ip-172-31-1-213'
  Apr 28 16:38:22.378: INFO: stderr: ""
  Apr 28 16:38:22.378: INFO: stdout: "Name:               ip-172-31-1-213\nRoles:              control-plane,etcd,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=k3s\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-1-213\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=true\n                    node-role.kubernetes.io/etcd=true\n                    node-role.kubernetes.io/master=true\n                    node.kubernetes.io/instance-type=k3s\nAnnotations:        etcd.k3s.cattle.io/node-address: 172.31.1.213\n                    etcd.k3s.cattle.io/node-name: ip-172-31-1-213-a723e18c\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"d2:03:88:6b:e6:2c\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.31.1.213\n                    k3s.io/external-ip: 18.117.10.20\n                    k3s.io/hostname: ip-172-31-1-213\n                    k3s.io/internal-ip: 172.31.1.213\n                    k3s.io/node-args:\n                      [\"server\",\"--write-kubeconfig-mode\",\"0644\",\"--tls-san\",\"3.141.7.10\",\"--token\",\"********\",\"--server\",\"https://3.141.7.10:6443\",\"--node-exte...\n                    k3s.io/node-config-hash: JGY6WYJSKK5PLYU4LTRYCPAB77HOPGIDEOFMXUFOCL22254ZCEUA====\n                    k3s.io/node-env: {\"K3S_DATA_DIR\":\"/var/lib/rancher/k3s/data/8795d26a4229dc5b9f0e7e8b177c8deeecbe5d66c31cecf1dbe4e32fd33f97f9\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 28 Apr 2023 03:31:56 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-1-213\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 28 Apr 2023 16:38:20 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Fri, 28 Apr 2023 16:34:04 +0000   Fri, 28 Apr 2023 03:31:56 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 28 Apr 2023 16:34:04 +0000   Fri, 28 Apr 2023 03:31:56 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 28 Apr 2023 16:34:04 +0000   Fri, 28 Apr 2023 03:31:56 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 28 Apr 2023 16:34:04 +0000   Fri, 28 Apr 2023 03:31:57 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.1.213\n  ExternalIP:  18.117.10.20\n  Hostname:    ip-172-31-1-213\nCapacity:\n  cpu:                  2\n  ephemeral-storage:    20937708Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               3949560Ki\n  pods:                 110\nAllocatable:\n  cpu:                  2\n  ephemeral-storage:    20368202327\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               3949560Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 ec27e4cefd04d40027af44ee8634f13f\n  System UUID:                ec27e4ce-fd04-d400-27af-44ee8634f13f\n  Boot ID:                    8065dbf0-7150-4652-9112-b309d6a801df\n  Kernel Version:             5.14.21-150400.22-default\n  OS Image:                   SUSE Linux Enterprise Server 15 SP4\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.19-k3s1\n  Kubelet Version:            v1.27.1+k3s1\n  Kube-Proxy Version:         v1.27.1+k3s1\nPodCIDR:                      10.42.2.0/24\nPodCIDRs:                     10.42.2.0/24\nProviderID:                   k3s://ip-172-31-1-213\nNon-terminated Pods:          (4 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  csiinlinevolumes-9070       pod-csi-inline-volumes                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         77s\n  kube-system                 svclb-traefik-0a77677b-p7w4w                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         10h\n  kubectl-5678                agnhost-primary-n6fn9                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-d17888d62c134fe7-v6wcc    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m15s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests  Limits\n  --------             --------  ------\n  cpu                  0 (0%)    0 (0%)\n  memory               0 (0%)    0 (0%)\n  ephemeral-storage    0 (0%)    0 (0%)\n  hugepages-1Gi        0 (0%)    0 (0%)\n  hugepages-2Mi        0 (0%)    0 (0%)\n  example.com/fakecpu  0         0\nEvents:                <none>\n"
  Apr 28 16:38:22.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-5678 describe namespace kubectl-5678'
  Apr 28 16:38:22.503: INFO: stderr: ""
  Apr 28 16:38:22.503: INFO: stdout: "Name:         kubectl-5678\nLabels:       e2e-framework=kubectl\n              e2e-run=e19fda74-19df-4e20-970a-6122324dd6fc\n              kubernetes.io/metadata.name=kubectl-5678\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Apr 28 16:38:22.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5678" for this suite. @ 04/28/23 16:38:22.508
• [3.168 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 04/28/23 16:38:22.519
  Apr 28 16:38:22.519: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/28/23 16:38:22.52
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:38:22.544
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:38:22.548
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 04/28/23 16:38:22.552
  Apr 28 16:38:22.552: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 04/28/23 16:38:29.779
  Apr 28 16:38:29.780: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 16:38:31.444: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 16:38:38.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8989" for this suite. @ 04/28/23 16:38:38.22
• [15.707 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 04/28/23 16:38:38.229
  Apr 28 16:38:38.229: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 04/28/23 16:38:38.229
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:38:38.255
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:38:38.258
  STEP: Setting up the test @ 04/28/23 16:38:38.261
  STEP: Creating hostNetwork=false pod @ 04/28/23 16:38:38.261
  STEP: Creating hostNetwork=true pod @ 04/28/23 16:38:40.291
  STEP: Running the test @ 04/28/23 16:38:42.316
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 04/28/23 16:38:42.316
  Apr 28 16:38:42.316: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2307 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 16:38:42.316: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 16:38:42.317: INFO: ExecWithOptions: Clientset creation
  Apr 28 16:38:42.317: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2307/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 28 16:38:42.394: INFO: Exec stderr: ""
  Apr 28 16:38:42.394: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2307 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 16:38:42.394: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 16:38:42.394: INFO: ExecWithOptions: Clientset creation
  Apr 28 16:38:42.394: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2307/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 28 16:38:42.451: INFO: Exec stderr: ""
  Apr 28 16:38:42.451: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2307 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 16:38:42.451: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 16:38:42.451: INFO: ExecWithOptions: Clientset creation
  Apr 28 16:38:42.451: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2307/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 28 16:38:42.506: INFO: Exec stderr: ""
  Apr 28 16:38:42.506: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2307 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 16:38:42.506: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 16:38:42.507: INFO: ExecWithOptions: Clientset creation
  Apr 28 16:38:42.507: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2307/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 28 16:38:42.569: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 04/28/23 16:38:42.57
  Apr 28 16:38:42.570: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2307 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 16:38:42.570: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 16:38:42.570: INFO: ExecWithOptions: Clientset creation
  Apr 28 16:38:42.570: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2307/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Apr 28 16:38:42.628: INFO: Exec stderr: ""
  Apr 28 16:38:42.628: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2307 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 16:38:42.628: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 16:38:42.629: INFO: ExecWithOptions: Clientset creation
  Apr 28 16:38:42.629: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2307/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Apr 28 16:38:42.700: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 04/28/23 16:38:42.7
  Apr 28 16:38:42.700: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2307 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 16:38:42.700: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 16:38:42.701: INFO: ExecWithOptions: Clientset creation
  Apr 28 16:38:42.701: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2307/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 28 16:38:42.770: INFO: Exec stderr: ""
  Apr 28 16:38:42.770: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2307 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 16:38:42.770: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 16:38:42.771: INFO: ExecWithOptions: Clientset creation
  Apr 28 16:38:42.771: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2307/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 28 16:38:42.849: INFO: Exec stderr: ""
  Apr 28 16:38:42.849: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2307 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 16:38:42.849: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 16:38:42.850: INFO: ExecWithOptions: Clientset creation
  Apr 28 16:38:42.850: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2307/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 28 16:38:42.909: INFO: Exec stderr: ""
  Apr 28 16:38:42.909: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2307 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 16:38:42.909: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 16:38:42.910: INFO: ExecWithOptions: Clientset creation
  Apr 28 16:38:42.910: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2307/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 28 16:38:42.961: INFO: Exec stderr: ""
  Apr 28 16:38:42.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-2307" for this suite. @ 04/28/23 16:38:42.965
• [4.746 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 04/28/23 16:38:42.978
  Apr 28 16:38:42.978: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename gc @ 04/28/23 16:38:42.979
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:38:42.993
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:38:42.996
  STEP: create the rc @ 04/28/23 16:38:43.008
  W0428 16:38:43.015253      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 04/28/23 16:38:49.048
  STEP: wait for the rc to be deleted @ 04/28/23 16:38:49.14
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 04/28/23 16:38:54.143
  STEP: Gathering metrics @ 04/28/23 16:39:24.158
  W0428 16:39:24.175103      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Apr 28 16:39:24.175: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 28 16:39:24.175: INFO: Deleting pod "simpletest.rc-28jj8" in namespace "gc-4840"
  Apr 28 16:39:24.196: INFO: Deleting pod "simpletest.rc-2bpv4" in namespace "gc-4840"
  Apr 28 16:39:24.229: INFO: Deleting pod "simpletest.rc-2kv97" in namespace "gc-4840"
  Apr 28 16:39:24.293: INFO: Deleting pod "simpletest.rc-2w9q5" in namespace "gc-4840"
  Apr 28 16:39:24.325: INFO: Deleting pod "simpletest.rc-44jnd" in namespace "gc-4840"
  Apr 28 16:39:24.346: INFO: Deleting pod "simpletest.rc-456xh" in namespace "gc-4840"
  Apr 28 16:39:24.386: INFO: Deleting pod "simpletest.rc-4bcbh" in namespace "gc-4840"
  Apr 28 16:39:24.414: INFO: Deleting pod "simpletest.rc-4frsd" in namespace "gc-4840"
  Apr 28 16:39:24.430: INFO: Deleting pod "simpletest.rc-4hftx" in namespace "gc-4840"
  Apr 28 16:39:24.470: INFO: Deleting pod "simpletest.rc-4ntqq" in namespace "gc-4840"
  Apr 28 16:39:24.539: INFO: Deleting pod "simpletest.rc-4q9zw" in namespace "gc-4840"
  Apr 28 16:39:24.578: INFO: Deleting pod "simpletest.rc-56xp7" in namespace "gc-4840"
  Apr 28 16:39:24.667: INFO: Deleting pod "simpletest.rc-64rsm" in namespace "gc-4840"
  Apr 28 16:39:24.735: INFO: Deleting pod "simpletest.rc-6dltk" in namespace "gc-4840"
  Apr 28 16:39:24.805: INFO: Deleting pod "simpletest.rc-6f4kq" in namespace "gc-4840"
  Apr 28 16:39:24.832: INFO: Deleting pod "simpletest.rc-6js5b" in namespace "gc-4840"
  Apr 28 16:39:24.862: INFO: Deleting pod "simpletest.rc-6wzkm" in namespace "gc-4840"
  Apr 28 16:39:24.923: INFO: Deleting pod "simpletest.rc-7r2gf" in namespace "gc-4840"
  Apr 28 16:39:24.954: INFO: Deleting pod "simpletest.rc-8c6xh" in namespace "gc-4840"
  Apr 28 16:39:24.973: INFO: Deleting pod "simpletest.rc-8vk4n" in namespace "gc-4840"
  Apr 28 16:39:25.024: INFO: Deleting pod "simpletest.rc-8x7kp" in namespace "gc-4840"
  Apr 28 16:39:25.096: INFO: Deleting pod "simpletest.rc-97slw" in namespace "gc-4840"
  Apr 28 16:39:25.136: INFO: Deleting pod "simpletest.rc-99ngl" in namespace "gc-4840"
  Apr 28 16:39:25.181: INFO: Deleting pod "simpletest.rc-9rhtn" in namespace "gc-4840"
  Apr 28 16:39:25.294: INFO: Deleting pod "simpletest.rc-9vrgz" in namespace "gc-4840"
  Apr 28 16:39:25.350: INFO: Deleting pod "simpletest.rc-bn2tq" in namespace "gc-4840"
  Apr 28 16:39:25.390: INFO: Deleting pod "simpletest.rc-bp2dq" in namespace "gc-4840"
  Apr 28 16:39:25.442: INFO: Deleting pod "simpletest.rc-btgcw" in namespace "gc-4840"
  Apr 28 16:39:25.491: INFO: Deleting pod "simpletest.rc-bwtmz" in namespace "gc-4840"
  Apr 28 16:39:25.544: INFO: Deleting pod "simpletest.rc-bzh28" in namespace "gc-4840"
  Apr 28 16:39:25.607: INFO: Deleting pod "simpletest.rc-cjtxs" in namespace "gc-4840"
  Apr 28 16:39:25.642: INFO: Deleting pod "simpletest.rc-cpzr5" in namespace "gc-4840"
  Apr 28 16:39:25.848: INFO: Deleting pod "simpletest.rc-d4gqm" in namespace "gc-4840"
  Apr 28 16:39:26.098: INFO: Deleting pod "simpletest.rc-dkk7p" in namespace "gc-4840"
  Apr 28 16:39:26.222: INFO: Deleting pod "simpletest.rc-dr5ft" in namespace "gc-4840"
  Apr 28 16:39:26.338: INFO: Deleting pod "simpletest.rc-f544n" in namespace "gc-4840"
  Apr 28 16:39:26.380: INFO: Deleting pod "simpletest.rc-fcd4d" in namespace "gc-4840"
  Apr 28 16:39:26.412: INFO: Deleting pod "simpletest.rc-gglhj" in namespace "gc-4840"
  Apr 28 16:39:26.474: INFO: Deleting pod "simpletest.rc-gs4rq" in namespace "gc-4840"
  Apr 28 16:39:26.508: INFO: Deleting pod "simpletest.rc-gzhhh" in namespace "gc-4840"
  Apr 28 16:39:26.540: INFO: Deleting pod "simpletest.rc-h5s8w" in namespace "gc-4840"
  Apr 28 16:39:26.570: INFO: Deleting pod "simpletest.rc-hc49k" in namespace "gc-4840"
  Apr 28 16:39:26.612: INFO: Deleting pod "simpletest.rc-hf5vg" in namespace "gc-4840"
  Apr 28 16:39:26.653: INFO: Deleting pod "simpletest.rc-hgthj" in namespace "gc-4840"
  Apr 28 16:39:26.696: INFO: Deleting pod "simpletest.rc-hl54g" in namespace "gc-4840"
  Apr 28 16:39:26.737: INFO: Deleting pod "simpletest.rc-hn55g" in namespace "gc-4840"
  Apr 28 16:39:26.787: INFO: Deleting pod "simpletest.rc-hsqhj" in namespace "gc-4840"
  Apr 28 16:39:26.820: INFO: Deleting pod "simpletest.rc-jhctp" in namespace "gc-4840"
  Apr 28 16:39:26.873: INFO: Deleting pod "simpletest.rc-jw28b" in namespace "gc-4840"
  Apr 28 16:39:26.904: INFO: Deleting pod "simpletest.rc-jwqt5" in namespace "gc-4840"
  Apr 28 16:39:26.921: INFO: Deleting pod "simpletest.rc-k499r" in namespace "gc-4840"
  Apr 28 16:39:26.945: INFO: Deleting pod "simpletest.rc-khbst" in namespace "gc-4840"
  Apr 28 16:39:26.958: INFO: Deleting pod "simpletest.rc-ld9lr" in namespace "gc-4840"
  Apr 28 16:39:26.977: INFO: Deleting pod "simpletest.rc-lg2gf" in namespace "gc-4840"
  Apr 28 16:39:27.001: INFO: Deleting pod "simpletest.rc-ll889" in namespace "gc-4840"
  Apr 28 16:39:27.033: INFO: Deleting pod "simpletest.rc-mkxg5" in namespace "gc-4840"
  Apr 28 16:39:27.058: INFO: Deleting pod "simpletest.rc-mrrnt" in namespace "gc-4840"
  Apr 28 16:39:27.102: INFO: Deleting pod "simpletest.rc-msfjq" in namespace "gc-4840"
  Apr 28 16:39:27.130: INFO: Deleting pod "simpletest.rc-mvrh4" in namespace "gc-4840"
  Apr 28 16:39:27.162: INFO: Deleting pod "simpletest.rc-nchg5" in namespace "gc-4840"
  Apr 28 16:39:27.215: INFO: Deleting pod "simpletest.rc-nmrzx" in namespace "gc-4840"
  Apr 28 16:39:27.240: INFO: Deleting pod "simpletest.rc-ntmbw" in namespace "gc-4840"
  Apr 28 16:39:27.270: INFO: Deleting pod "simpletest.rc-p47mh" in namespace "gc-4840"
  Apr 28 16:39:27.323: INFO: Deleting pod "simpletest.rc-p79gp" in namespace "gc-4840"
  Apr 28 16:39:27.410: INFO: Deleting pod "simpletest.rc-q8wgm" in namespace "gc-4840"
  Apr 28 16:39:27.452: INFO: Deleting pod "simpletest.rc-qlpjk" in namespace "gc-4840"
  Apr 28 16:39:27.528: INFO: Deleting pod "simpletest.rc-qltf9" in namespace "gc-4840"
  Apr 28 16:39:27.564: INFO: Deleting pod "simpletest.rc-qxr6p" in namespace "gc-4840"
  Apr 28 16:39:27.627: INFO: Deleting pod "simpletest.rc-r8sgm" in namespace "gc-4840"
  Apr 28 16:39:27.656: INFO: Deleting pod "simpletest.rc-rdtjp" in namespace "gc-4840"
  Apr 28 16:39:27.732: INFO: Deleting pod "simpletest.rc-rghl7" in namespace "gc-4840"
  Apr 28 16:39:27.796: INFO: Deleting pod "simpletest.rc-rtw62" in namespace "gc-4840"
  Apr 28 16:39:27.842: INFO: Deleting pod "simpletest.rc-rxq7z" in namespace "gc-4840"
  Apr 28 16:39:27.876: INFO: Deleting pod "simpletest.rc-s42xt" in namespace "gc-4840"
  Apr 28 16:39:27.926: INFO: Deleting pod "simpletest.rc-s8k52" in namespace "gc-4840"
  Apr 28 16:39:28.033: INFO: Deleting pod "simpletest.rc-shjfn" in namespace "gc-4840"
  Apr 28 16:39:28.076: INFO: Deleting pod "simpletest.rc-sj849" in namespace "gc-4840"
  Apr 28 16:39:28.144: INFO: Deleting pod "simpletest.rc-szbkq" in namespace "gc-4840"
  Apr 28 16:39:28.188: INFO: Deleting pod "simpletest.rc-t2ghw" in namespace "gc-4840"
  Apr 28 16:39:28.233: INFO: Deleting pod "simpletest.rc-t4728" in namespace "gc-4840"
  Apr 28 16:39:28.268: INFO: Deleting pod "simpletest.rc-t47lp" in namespace "gc-4840"
  Apr 28 16:39:28.309: INFO: Deleting pod "simpletest.rc-t9fkg" in namespace "gc-4840"
  Apr 28 16:39:28.352: INFO: Deleting pod "simpletest.rc-tcnmf" in namespace "gc-4840"
  Apr 28 16:39:28.446: INFO: Deleting pod "simpletest.rc-tjv9c" in namespace "gc-4840"
  Apr 28 16:39:28.495: INFO: Deleting pod "simpletest.rc-tjxx2" in namespace "gc-4840"
  Apr 28 16:39:28.530: INFO: Deleting pod "simpletest.rc-tncdz" in namespace "gc-4840"
  Apr 28 16:39:28.596: INFO: Deleting pod "simpletest.rc-tr4f7" in namespace "gc-4840"
  Apr 28 16:39:28.638: INFO: Deleting pod "simpletest.rc-tzxxg" in namespace "gc-4840"
  Apr 28 16:39:28.683: INFO: Deleting pod "simpletest.rc-v82qr" in namespace "gc-4840"
  Apr 28 16:39:28.725: INFO: Deleting pod "simpletest.rc-w68dx" in namespace "gc-4840"
  Apr 28 16:39:28.812: INFO: Deleting pod "simpletest.rc-wgq5l" in namespace "gc-4840"
  Apr 28 16:39:28.864: INFO: Deleting pod "simpletest.rc-wv8j8" in namespace "gc-4840"
  Apr 28 16:39:28.926: INFO: Deleting pod "simpletest.rc-x68jq" in namespace "gc-4840"
  Apr 28 16:39:28.986: INFO: Deleting pod "simpletest.rc-xfxqj" in namespace "gc-4840"
  Apr 28 16:39:29.013: INFO: Deleting pod "simpletest.rc-xhfhq" in namespace "gc-4840"
  Apr 28 16:39:29.041: INFO: Deleting pod "simpletest.rc-xklj5" in namespace "gc-4840"
  Apr 28 16:39:29.103: INFO: Deleting pod "simpletest.rc-xqphk" in namespace "gc-4840"
  Apr 28 16:39:29.136: INFO: Deleting pod "simpletest.rc-zqdpd" in namespace "gc-4840"
  Apr 28 16:39:29.209: INFO: Deleting pod "simpletest.rc-ztgbk" in namespace "gc-4840"
  Apr 28 16:39:29.244: INFO: Deleting pod "simpletest.rc-zts2g" in namespace "gc-4840"
  Apr 28 16:39:29.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4840" for this suite. @ 04/28/23 16:39:29.307
• [46.363 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 04/28/23 16:39:29.343
  Apr 28 16:39:29.343: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename container-probe @ 04/28/23 16:39:29.344
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:39:29.379
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:39:29.382
  STEP: Creating pod test-grpc-c1252576-ed29-4af0-bbf2-c7d12ac5936f in namespace container-probe-9635 @ 04/28/23 16:39:29.385
  Apr 28 16:39:31.406: INFO: Started pod test-grpc-c1252576-ed29-4af0-bbf2-c7d12ac5936f in namespace container-probe-9635
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/28/23 16:39:31.406
  Apr 28 16:39:31.410: INFO: Initial restart count of pod test-grpc-c1252576-ed29-4af0-bbf2-c7d12ac5936f is 0
  Apr 28 16:40:35.966: INFO: Restart count of pod container-probe-9635/test-grpc-c1252576-ed29-4af0-bbf2-c7d12ac5936f is now 1 (1m4.556198369s elapsed)
  Apr 28 16:40:35.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 16:40:35.971
  STEP: Destroying namespace "container-probe-9635" for this suite. @ 04/28/23 16:40:35.982
• [66.655 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 04/28/23 16:40:36.012
  Apr 28 16:40:36.012: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 16:40:36.014
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:40:36.047
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:40:36.055
  STEP: Creating a pod to test downward api env vars @ 04/28/23 16:40:36.062
  STEP: Saw pod success @ 04/28/23 16:40:40.177
  Apr 28 16:40:40.180: INFO: Trying to get logs from node ip-172-31-1-213 pod downward-api-564ebfe9-51f1-4e47-9f1b-e64a01512338 container dapi-container: <nil>
  STEP: delete the pod @ 04/28/23 16:40:40.215
  Apr 28 16:40:40.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2429" for this suite. @ 04/28/23 16:40:40.282
• [4.289 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 04/28/23 16:40:40.303
  Apr 28 16:40:40.303: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename cronjob @ 04/28/23 16:40:40.306
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:40:40.394
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:40:40.397
  STEP: Creating a cronjob @ 04/28/23 16:40:40.399
  STEP: creating @ 04/28/23 16:40:40.399
  STEP: getting @ 04/28/23 16:40:40.41
  STEP: listing @ 04/28/23 16:40:40.414
  STEP: watching @ 04/28/23 16:40:40.432
  Apr 28 16:40:40.432: INFO: starting watch
  STEP: cluster-wide listing @ 04/28/23 16:40:40.433
  STEP: cluster-wide watching @ 04/28/23 16:40:40.438
  Apr 28 16:40:40.438: INFO: starting watch
  STEP: patching @ 04/28/23 16:40:40.439
  STEP: updating @ 04/28/23 16:40:40.444
  Apr 28 16:40:40.454: INFO: waiting for watch events with expected annotations
  Apr 28 16:40:40.454: INFO: saw patched and updated annotations
  STEP: patching /status @ 04/28/23 16:40:40.454
  STEP: updating /status @ 04/28/23 16:40:40.465
  STEP: get /status @ 04/28/23 16:40:40.478
  STEP: deleting @ 04/28/23 16:40:40.49
  STEP: deleting a collection @ 04/28/23 16:40:40.51
  Apr 28 16:40:40.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-7584" for this suite. @ 04/28/23 16:40:40.523
• [0.226 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 04/28/23 16:40:40.53
  Apr 28 16:40:40.530: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename secrets @ 04/28/23 16:40:40.531
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:40:40.546
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:40:40.549
  STEP: Creating projection with secret that has name secret-emptykey-test-255cabec-6ef4-472e-8de3-5bad02444baf @ 04/28/23 16:40:40.552
  Apr 28 16:40:40.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-158" for this suite. @ 04/28/23 16:40:40.561
• [0.040 seconds]
------------------------------
SSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 04/28/23 16:40:40.57
  Apr 28 16:40:40.570: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename podtemplate @ 04/28/23 16:40:40.571
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:40:40.588
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:40:40.592
  STEP: Create a pod template @ 04/28/23 16:40:40.595
  STEP: Replace a pod template @ 04/28/23 16:40:40.601
  Apr 28 16:40:40.617: INFO: Found updated podtemplate annotation: "true"

  Apr 28 16:40:40.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-6317" for this suite. @ 04/28/23 16:40:40.621
• [0.069 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 04/28/23 16:40:40.64
  Apr 28 16:40:40.640: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename field-validation @ 04/28/23 16:40:40.641
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:40:40.674
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:40:40.678
  Apr 28 16:40:40.685: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  W0428 16:40:43.226125      19 warnings.go:70] unknown field "alpha"
  W0428 16:40:43.226151      19 warnings.go:70] unknown field "beta"
  W0428 16:40:43.226156      19 warnings.go:70] unknown field "delta"
  W0428 16:40:43.226163      19 warnings.go:70] unknown field "epsilon"
  W0428 16:40:43.226169      19 warnings.go:70] unknown field "gamma"
  Apr 28 16:40:43.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-3132" for this suite. @ 04/28/23 16:40:43.252
• [2.618 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 04/28/23 16:40:43.258
  Apr 28 16:40:43.258: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename taint-single-pod @ 04/28/23 16:40:43.259
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:40:43.275
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:40:43.297
  Apr 28 16:40:43.315: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 28 16:41:43.352: INFO: Waiting for terminating namespaces to be deleted...
  Apr 28 16:41:43.357: INFO: Starting informer...
  STEP: Starting pod... @ 04/28/23 16:41:43.357
  Apr 28 16:41:43.581: INFO: Pod is running on ip-172-31-1-213. Tainting Node
  STEP: Trying to apply a taint on the Node @ 04/28/23 16:41:43.581
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/28/23 16:41:43.603
  STEP: Waiting short time to make sure Pod is queued for deletion @ 04/28/23 16:41:43.609
  Apr 28 16:41:43.610: INFO: Pod wasn't evicted. Proceeding
  Apr 28 16:41:43.610: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/28/23 16:41:43.641
  STEP: Waiting some time to make sure that toleration time passed. @ 04/28/23 16:41:43.656
  Apr 28 16:42:58.657: INFO: Pod wasn't evicted. Test successful
  Apr 28 16:42:58.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-2050" for this suite. @ 04/28/23 16:42:58.902
• [135.649 seconds]
------------------------------
S
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 04/28/23 16:42:58.907
  Apr 28 16:42:58.907: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename containers @ 04/28/23 16:42:58.908
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:42:58.928
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:42:58.931
  STEP: Creating a pod to test override command @ 04/28/23 16:42:58.935
  STEP: Saw pod success @ 04/28/23 16:43:02.954
  Apr 28 16:43:02.957: INFO: Trying to get logs from node ip-172-31-10-53 pod client-containers-67ebd6e6-a01c-438c-897d-c3a3b6241671 container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 16:43:02.979
  Apr 28 16:43:03.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-9619" for this suite. @ 04/28/23 16:43:03.006
• [4.121 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 04/28/23 16:43:03.03
  Apr 28 16:43:03.030: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename services @ 04/28/23 16:43:03.031
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:43:03.046
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:43:03.06
  STEP: creating an Endpoint @ 04/28/23 16:43:03.066
  STEP: waiting for available Endpoint @ 04/28/23 16:43:03.08
  STEP: listing all Endpoints @ 04/28/23 16:43:03.082
  STEP: updating the Endpoint @ 04/28/23 16:43:03.087
  STEP: fetching the Endpoint @ 04/28/23 16:43:03.094
  STEP: patching the Endpoint @ 04/28/23 16:43:03.098
  STEP: fetching the Endpoint @ 04/28/23 16:43:03.114
  STEP: deleting the Endpoint by Collection @ 04/28/23 16:43:03.117
  STEP: waiting for Endpoint deletion @ 04/28/23 16:43:03.124
  STEP: fetching the Endpoint @ 04/28/23 16:43:03.126
  Apr 28 16:43:03.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5989" for this suite. @ 04/28/23 16:43:03.135
• [0.113 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 04/28/23 16:43:03.149
  Apr 28 16:43:03.149: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename dns @ 04/28/23 16:43:03.15
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:43:03.171
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:43:03.186
  STEP: Creating a test headless service @ 04/28/23 16:43:03.198
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5811 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5811;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5811 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5811;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5811.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5811.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5811.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5811.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5811.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5811.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5811.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5811.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5811.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5811.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5811.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5811.svc;check="$$(dig +notcp +noall +answer +search 89.59.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.59.89_udp@PTR;check="$$(dig +tcp +noall +answer +search 89.59.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.59.89_tcp@PTR;sleep 1; done
   @ 04/28/23 16:43:03.249
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5811 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5811;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5811 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5811;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5811.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5811.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5811.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5811.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5811.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5811.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5811.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5811.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5811.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5811.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5811.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5811.svc;check="$$(dig +notcp +noall +answer +search 89.59.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.59.89_udp@PTR;check="$$(dig +tcp +noall +answer +search 89.59.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.59.89_tcp@PTR;sleep 1; done
   @ 04/28/23 16:43:03.249
  STEP: creating a pod to probe DNS @ 04/28/23 16:43:03.249
  STEP: submitting the pod to kubernetes @ 04/28/23 16:43:03.249
  STEP: retrieving the pod @ 04/28/23 16:43:05.308
  STEP: looking for the results for each expected name from probers @ 04/28/23 16:43:05.315
  Apr 28 16:43:05.321: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5811/dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f: the server could not find the requested resource (get pods dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f)
  Apr 28 16:43:05.324: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5811/dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f: the server could not find the requested resource (get pods dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f)
  Apr 28 16:43:05.327: INFO: Unable to read wheezy_udp@dns-test-service.dns-5811 from pod dns-5811/dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f: the server could not find the requested resource (get pods dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f)
  Apr 28 16:43:05.330: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5811 from pod dns-5811/dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f: the server could not find the requested resource (get pods dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f)
  Apr 28 16:43:05.333: INFO: Unable to read wheezy_udp@dns-test-service.dns-5811.svc from pod dns-5811/dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f: the server could not find the requested resource (get pods dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f)
  Apr 28 16:43:05.335: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5811.svc from pod dns-5811/dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f: the server could not find the requested resource (get pods dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f)
  Apr 28 16:43:05.338: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5811.svc from pod dns-5811/dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f: the server could not find the requested resource (get pods dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f)
  Apr 28 16:43:05.341: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5811.svc from pod dns-5811/dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f: the server could not find the requested resource (get pods dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f)
  Apr 28 16:43:05.355: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5811/dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f: the server could not find the requested resource (get pods dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f)
  Apr 28 16:43:05.358: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5811/dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f: the server could not find the requested resource (get pods dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f)
  Apr 28 16:43:05.361: INFO: Unable to read jessie_udp@dns-test-service.dns-5811 from pod dns-5811/dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f: the server could not find the requested resource (get pods dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f)
  Apr 28 16:43:05.364: INFO: Unable to read jessie_tcp@dns-test-service.dns-5811 from pod dns-5811/dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f: the server could not find the requested resource (get pods dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f)
  Apr 28 16:43:05.366: INFO: Unable to read jessie_udp@dns-test-service.dns-5811.svc from pod dns-5811/dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f: the server could not find the requested resource (get pods dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f)
  Apr 28 16:43:05.369: INFO: Unable to read jessie_tcp@dns-test-service.dns-5811.svc from pod dns-5811/dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f: the server could not find the requested resource (get pods dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f)
  Apr 28 16:43:05.372: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5811.svc from pod dns-5811/dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f: the server could not find the requested resource (get pods dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f)
  Apr 28 16:43:05.374: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5811.svc from pod dns-5811/dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f: the server could not find the requested resource (get pods dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f)
  Apr 28 16:43:05.385: INFO: Lookups using dns-5811/dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5811 wheezy_tcp@dns-test-service.dns-5811 wheezy_udp@dns-test-service.dns-5811.svc wheezy_tcp@dns-test-service.dns-5811.svc wheezy_udp@_http._tcp.dns-test-service.dns-5811.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5811.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5811 jessie_tcp@dns-test-service.dns-5811 jessie_udp@dns-test-service.dns-5811.svc jessie_tcp@dns-test-service.dns-5811.svc jessie_udp@_http._tcp.dns-test-service.dns-5811.svc jessie_tcp@_http._tcp.dns-test-service.dns-5811.svc]

  Apr 28 16:43:10.493: INFO: DNS probes using dns-5811/dns-test-9ec09796-f185-4ade-8fd8-1fc006f51b9f succeeded

  Apr 28 16:43:10.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 16:43:10.497
  STEP: deleting the test service @ 04/28/23 16:43:10.536
  STEP: deleting the test headless service @ 04/28/23 16:43:10.615
  STEP: Destroying namespace "dns-5811" for this suite. @ 04/28/23 16:43:10.658
• [7.536 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 04/28/23 16:43:10.685
  Apr 28 16:43:10.685: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename sysctl @ 04/28/23 16:43:10.686
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:43:10.76
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:43:10.763
  STEP: Creating a pod with one valid and two invalid sysctls @ 04/28/23 16:43:10.765
  Apr 28 16:43:10.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-2398" for this suite. @ 04/28/23 16:43:10.781
• [0.106 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 04/28/23 16:43:10.791
  Apr 28 16:43:10.791: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename pods @ 04/28/23 16:43:10.792
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:43:10.819
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:43:10.826
  STEP: creating pod @ 04/28/23 16:43:10.83
  Apr 28 16:43:12.851: INFO: Pod pod-hostip-e1d4a1eb-fb7d-4491-9195-aba552ade8fc has hostIP: 172.31.1.213
  Apr 28 16:43:12.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2309" for this suite. @ 04/28/23 16:43:12.855
• [2.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 04/28/23 16:43:12.864
  Apr 28 16:43:12.864: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename container-probe @ 04/28/23 16:43:12.864
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:43:12.917
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:43:12.927
  STEP: Creating pod test-webserver-de910b54-f69e-4fb3-ae82-af940bc00de6 in namespace container-probe-8181 @ 04/28/23 16:43:12.941
  Apr 28 16:43:14.968: INFO: Started pod test-webserver-de910b54-f69e-4fb3-ae82-af940bc00de6 in namespace container-probe-8181
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/28/23 16:43:14.968
  Apr 28 16:43:14.971: INFO: Initial restart count of pod test-webserver-de910b54-f69e-4fb3-ae82-af940bc00de6 is 0
  Apr 28 16:47:16.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 16:47:16.188
  STEP: Destroying namespace "container-probe-8181" for this suite. @ 04/28/23 16:47:16.202
• [243.356 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 04/28/23 16:47:16.221
  Apr 28 16:47:16.221: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename webhook @ 04/28/23 16:47:16.222
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:47:16.242
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:47:16.245
  STEP: Setting up server cert @ 04/28/23 16:47:16.295
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 16:47:16.758
  STEP: Deploying the webhook pod @ 04/28/23 16:47:16.768
  STEP: Wait for the deployment to be ready @ 04/28/23 16:47:16.784
  Apr 28 16:47:16.791: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/28/23 16:47:18.803
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 16:47:18.815
  Apr 28 16:47:19.815: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 04/28/23 16:47:19.819
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/28/23 16:47:19.845
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 04/28/23 16:47:19.856
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/28/23 16:47:19.865
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 04/28/23 16:47:19.874
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/28/23 16:47:19.881
  Apr 28 16:47:19.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3384" for this suite. @ 04/28/23 16:47:20.112
  STEP: Destroying namespace "webhook-markers-6199" for this suite. @ 04/28/23 16:47:20.128
• [3.942 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 04/28/23 16:47:20.166
  Apr 28 16:47:20.166: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename var-expansion @ 04/28/23 16:47:20.167
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:47:20.435
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:47:20.439
  STEP: creating the pod @ 04/28/23 16:47:20.443
  STEP: waiting for pod running @ 04/28/23 16:47:20.453
  STEP: creating a file in subpath @ 04/28/23 16:47:22.466
  Apr 28 16:47:22.469: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4877 PodName:var-expansion-cd2e9f67-9ddd-488e-a294-90aec5f472a4 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 16:47:22.469: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 16:47:22.470: INFO: ExecWithOptions: Clientset creation
  Apr 28 16:47:22.470: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/var-expansion-4877/pods/var-expansion-cd2e9f67-9ddd-488e-a294-90aec5f472a4/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 04/28/23 16:47:22.525
  Apr 28 16:47:22.530: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4877 PodName:var-expansion-cd2e9f67-9ddd-488e-a294-90aec5f472a4 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 16:47:22.530: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 16:47:22.530: INFO: ExecWithOptions: Clientset creation
  Apr 28 16:47:22.530: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/var-expansion-4877/pods/var-expansion-cd2e9f67-9ddd-488e-a294-90aec5f472a4/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 04/28/23 16:47:22.579
  Apr 28 16:47:23.099: INFO: Successfully updated pod "var-expansion-cd2e9f67-9ddd-488e-a294-90aec5f472a4"
  STEP: waiting for annotated pod running @ 04/28/23 16:47:23.101
  STEP: deleting the pod gracefully @ 04/28/23 16:47:23.106
  Apr 28 16:47:23.107: INFO: Deleting pod "var-expansion-cd2e9f67-9ddd-488e-a294-90aec5f472a4" in namespace "var-expansion-4877"
  Apr 28 16:47:23.118: INFO: Wait up to 5m0s for pod "var-expansion-cd2e9f67-9ddd-488e-a294-90aec5f472a4" to be fully deleted
  Apr 28 16:47:55.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4877" for this suite. @ 04/28/23 16:47:55.274
• [35.114 seconds]
------------------------------
S
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 04/28/23 16:47:55.28
  Apr 28 16:47:55.280: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename deployment @ 04/28/23 16:47:55.281
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:47:55.3
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:47:55.304
  Apr 28 16:47:55.307: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Apr 28 16:47:55.317: INFO: Pod name sample-pod: Found 0 pods out of 1
  Apr 28 16:48:00.323: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/28/23 16:48:00.324
  Apr 28 16:48:00.324: INFO: Creating deployment "test-rolling-update-deployment"
  Apr 28 16:48:00.335: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Apr 28 16:48:00.349: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  Apr 28 16:48:02.359: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Apr 28 16:48:02.363: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Apr 28 16:48:02.383: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3092  eb59e9e5-f672-4f9e-af70-952758e14bf4 243549 1 2023-04-28 16:48:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-04-28 16:48:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2023-04-28 16:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c2e428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-28 16:48:00 +0000 UTC,LastTransitionTime:2023-04-28 16:48:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-04-28 16:48:01 +0000 UTC,LastTransitionTime:2023-04-28 16:48:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 28 16:48:02.390: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-3092  7effbe85-a1c8-470a-8ce3-0e5faacedc50 243539 1 2023-04-28 16:48:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment eb59e9e5-f672-4f9e-af70-952758e14bf4 0xc007725cf7 0xc007725cf8}] [] [{k3s Update apps/v1 2023-04-28 16:48:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eb59e9e5-f672-4f9e-af70-952758e14bf4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2023-04-28 16:48:01 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007725db8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 16:48:02.390: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Apr 28 16:48:02.390: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3092  f1bd5311-2ef6-48dc-bd5b-4bacd84077c3 243548 2 2023-04-28 16:47:55 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment eb59e9e5-f672-4f9e-af70-952758e14bf4 0xc007725b97 0xc007725b98}] [] [{e2e.test Update apps/v1 2023-04-28 16:47:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2023-04-28 16:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eb59e9e5-f672-4f9e-af70-952758e14bf4\"}":{}}},"f:spec":{"f:replicas":{}}} } {k3s Update apps/v1 2023-04-28 16:48:01 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc007725c68 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 16:48:02.406: INFO: Pod "test-rolling-update-deployment-656d657cd8-cftb4" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-cftb4 test-rolling-update-deployment-656d657cd8- deployment-3092  f20fcb7a-9586-4e32-9ef3-748c0fbba6dc 243538 0 2023-04-28 16:48:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 7effbe85-a1c8-470a-8ce3-0e5faacedc50 0xc004c2e7e7 0xc004c2e7e8}] [] [{k3s Update v1 2023-04-28 16:48:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7effbe85-a1c8-470a-8ce3-0e5faacedc50\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 16:48:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lv2pt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lv2pt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-213,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 16:48:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 16:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 16:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 16:48:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.213,PodIP:10.42.2.188,StartTime:2023-04-28 16:48:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 16:48:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://775ec9b5378e5637124e3177929dd5f55e283cd389593897937bf33f27a44a7a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.188,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 16:48:02.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3092" for this suite. @ 04/28/23 16:48:02.412
• [7.156 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 04/28/23 16:48:02.437
  Apr 28 16:48:02.437: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename webhook @ 04/28/23 16:48:02.438
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:48:02.477
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:48:02.484
  STEP: Setting up server cert @ 04/28/23 16:48:02.583
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 16:48:03.074
  STEP: Deploying the webhook pod @ 04/28/23 16:48:03.1
  STEP: Wait for the deployment to be ready @ 04/28/23 16:48:03.114
  Apr 28 16:48:03.133: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/28/23 16:48:05.158
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 16:48:05.181
  Apr 28 16:48:06.181: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 28 16:48:06.185: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1205-crds.webhook.example.com via the AdmissionRegistration API @ 04/28/23 16:48:06.705
  STEP: Creating a custom resource that should be mutated by the webhook @ 04/28/23 16:48:06.723
  Apr 28 16:48:08.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8511" for this suite. @ 04/28/23 16:48:09.734
  STEP: Destroying namespace "webhook-markers-4165" for this suite. @ 04/28/23 16:48:09.748
• [7.317 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 04/28/23 16:48:09.754
  Apr 28 16:48:09.754: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename webhook @ 04/28/23 16:48:09.755
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:48:09.771
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:48:09.774
  STEP: Setting up server cert @ 04/28/23 16:48:09.807
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 16:48:10.35
  STEP: Deploying the webhook pod @ 04/28/23 16:48:10.681
  STEP: Wait for the deployment to be ready @ 04/28/23 16:48:10.693
  Apr 28 16:48:10.701: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/28/23 16:48:12.712
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 16:48:12.723
  Apr 28 16:48:13.723: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 04/28/23 16:48:13.726
  STEP: create a pod @ 04/28/23 16:48:13.744
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 04/28/23 16:48:15.773
  Apr 28 16:48:15.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=webhook-5146 attach --namespace=webhook-5146 to-be-attached-pod -i -c=container1'
  Apr 28 16:48:15.864: INFO: rc: 1
  Apr 28 16:48:15.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5146" for this suite. @ 04/28/23 16:48:16.018
  STEP: Destroying namespace "webhook-markers-1241" for this suite. @ 04/28/23 16:48:16.037
• [6.298 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 04/28/23 16:48:16.053
  Apr 28 16:48:16.053: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename webhook @ 04/28/23 16:48:16.054
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:48:16.126
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:48:16.133
  STEP: Setting up server cert @ 04/28/23 16:48:16.172
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 16:48:16.453
  STEP: Deploying the webhook pod @ 04/28/23 16:48:16.458
  STEP: Wait for the deployment to be ready @ 04/28/23 16:48:16.467
  Apr 28 16:48:16.473: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/28/23 16:48:18.482
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 16:48:18.491
  Apr 28 16:48:19.492: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 04/28/23 16:48:19.495
  STEP: create a namespace for the webhook @ 04/28/23 16:48:19.514
  STEP: create a configmap should be unconditionally rejected by the webhook @ 04/28/23 16:48:19.536
  Apr 28 16:48:19.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-596" for this suite. @ 04/28/23 16:48:19.673
  STEP: Destroying namespace "webhook-markers-1649" for this suite. @ 04/28/23 16:48:19.688
  STEP: Destroying namespace "fail-closed-namespace-8448" for this suite. @ 04/28/23 16:48:19.7
• [3.664 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 04/28/23 16:48:19.718
  Apr 28 16:48:19.718: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename job @ 04/28/23 16:48:19.718
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:48:19.754
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:48:19.771
  STEP: Creating Indexed job @ 04/28/23 16:48:19.783
  STEP: Ensuring job reaches completions @ 04/28/23 16:48:19.844
  STEP: Ensuring pods with index for job exist @ 04/28/23 16:48:27.849
  Apr 28 16:48:27.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9073" for this suite. @ 04/28/23 16:48:27.857
• [8.153 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 04/28/23 16:48:27.871
  Apr 28 16:48:27.871: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename watch @ 04/28/23 16:48:27.872
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:48:27.895
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:48:27.9
  STEP: creating a new configmap @ 04/28/23 16:48:27.903
  STEP: modifying the configmap once @ 04/28/23 16:48:27.912
  STEP: modifying the configmap a second time @ 04/28/23 16:48:27.924
  STEP: deleting the configmap @ 04/28/23 16:48:27.937
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 04/28/23 16:48:27.946
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 04/28/23 16:48:27.947
  Apr 28 16:48:27.947: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3179  3aaa294a-f272-4421-83b2-cc0358699c42 243986 0 2023-04-28 16:48:27 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-28 16:48:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 16:48:27.948: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3179  3aaa294a-f272-4421-83b2-cc0358699c42 243987 0 2023-04-28 16:48:27 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-28 16:48:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 16:48:27.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-3179" for this suite. @ 04/28/23 16:48:27.956
• [0.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 04/28/23 16:48:27.975
  Apr 28 16:48:27.975: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 16:48:27.978
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:48:28.015
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:48:28.024
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 16:48:28.035
  STEP: Saw pod success @ 04/28/23 16:48:32.07
  Apr 28 16:48:32.073: INFO: Trying to get logs from node ip-172-31-1-213 pod downwardapi-volume-3a2b5aaa-81ff-497e-a8f5-85020551f932 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 16:48:32.085
  Apr 28 16:48:32.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4844" for this suite. @ 04/28/23 16:48:32.114
• [4.149 seconds]
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 04/28/23 16:48:32.124
  Apr 28 16:48:32.124: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename disruption @ 04/28/23 16:48:32.125
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:48:32.174
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:48:32.179
  STEP: Creating a pdb that targets all three pods in a test replica set @ 04/28/23 16:48:32.185
  STEP: Waiting for the pdb to be processed @ 04/28/23 16:48:32.203
  STEP: First trying to evict a pod which shouldn't be evictable @ 04/28/23 16:48:34.219
  STEP: Waiting for all pods to be running @ 04/28/23 16:48:34.219
  Apr 28 16:48:34.224: INFO: pods: 1 < 3
  Apr 28 16:48:36.228: INFO: running pods: 2 < 3
  STEP: locating a running pod @ 04/28/23 16:48:38.382
  STEP: Updating the pdb to allow a pod to be evicted @ 04/28/23 16:48:38.392
  STEP: Waiting for the pdb to be processed @ 04/28/23 16:48:38.404
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 04/28/23 16:48:38.408
  STEP: Waiting for all pods to be running @ 04/28/23 16:48:38.408
  STEP: Waiting for the pdb to observed all healthy pods @ 04/28/23 16:48:38.415
  STEP: Patching the pdb to disallow a pod to be evicted @ 04/28/23 16:48:38.446
  STEP: Waiting for the pdb to be processed @ 04/28/23 16:48:38.471
  STEP: Waiting for all pods to be running @ 04/28/23 16:48:38.49
  Apr 28 16:48:38.503: INFO: running pods: 2 < 3
  STEP: locating a running pod @ 04/28/23 16:48:40.509
  STEP: Deleting the pdb to allow a pod to be evicted @ 04/28/23 16:48:40.515
  STEP: Waiting for the pdb to be deleted @ 04/28/23 16:48:40.52
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 04/28/23 16:48:40.524
  STEP: Waiting for all pods to be running @ 04/28/23 16:48:40.524
  Apr 28 16:48:40.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2639" for this suite. @ 04/28/23 16:48:40.558
• [8.444 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 04/28/23 16:48:40.57
  Apr 28 16:48:40.570: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 16:48:40.573
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:48:40.591
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:48:40.594
  STEP: Creating projection with secret that has name projected-secret-test-map-588c779c-ca83-40fc-b041-d471904e820c @ 04/28/23 16:48:40.602
  STEP: Creating a pod to test consume secrets @ 04/28/23 16:48:40.61
  STEP: Saw pod success @ 04/28/23 16:48:42.873
  Apr 28 16:48:42.877: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-projected-secrets-b5ef9699-57bc-4ba0-b940-e3ced16355dc container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 16:48:42.887
  Apr 28 16:48:42.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3264" for this suite. @ 04/28/23 16:48:42.913
• [2.351 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 04/28/23 16:48:42.922
  Apr 28 16:48:42.922: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 16:48:42.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:48:43.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:48:43.421
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 04/28/23 16:48:43.425
  STEP: Saw pod success @ 04/28/23 16:48:47.451
  Apr 28 16:48:47.454: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-9be4d1ba-0ae5-40d5-99fb-1deb070dc681 container test-container: <nil>
  STEP: delete the pod @ 04/28/23 16:48:47.468
  Apr 28 16:48:47.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1203" for this suite. @ 04/28/23 16:48:47.499
• [4.587 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 04/28/23 16:48:47.517
  Apr 28 16:48:47.517: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename security-context-test @ 04/28/23 16:48:47.52
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:48:47.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:48:47.552
  Apr 28 16:48:51.590: INFO: Got logs for pod "busybox-privileged-false-90821db5-b2ff-4a38-ac80-359f9cfadaf1": "ip: RTNETLINK answers: Operation not permitted\n"
  Apr 28 16:48:51.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-6102" for this suite. @ 04/28/23 16:48:51.594
• [4.083 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 04/28/23 16:48:51.6
  Apr 28 16:48:51.600: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 16:48:51.601
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:48:51.618
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:48:51.629
  STEP: Creating the pod @ 04/28/23 16:48:51.637
  Apr 28 16:48:54.197: INFO: Successfully updated pod "annotationupdate3c7b74e6-64c0-4889-9a71-43e6bc5b6800"
  Apr 28 16:48:56.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9077" for this suite. @ 04/28/23 16:48:56.238
• [4.646 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 04/28/23 16:48:56.248
  Apr 28 16:48:56.248: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename certificates @ 04/28/23 16:48:56.249
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:48:56.286
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:48:56.288
  STEP: getting /apis @ 04/28/23 16:48:57.231
  STEP: getting /apis/certificates.k8s.io @ 04/28/23 16:48:57.235
  STEP: getting /apis/certificates.k8s.io/v1 @ 04/28/23 16:48:57.236
  STEP: creating @ 04/28/23 16:48:57.238
  STEP: getting @ 04/28/23 16:48:57.26
  STEP: listing @ 04/28/23 16:48:57.263
  STEP: watching @ 04/28/23 16:48:57.266
  Apr 28 16:48:57.266: INFO: starting watch
  STEP: patching @ 04/28/23 16:48:57.272
  STEP: updating @ 04/28/23 16:48:57.281
  Apr 28 16:48:57.305: INFO: waiting for watch events with expected annotations
  Apr 28 16:48:57.305: INFO: saw patched and updated annotations
  STEP: getting /approval @ 04/28/23 16:48:57.305
  STEP: patching /approval @ 04/28/23 16:48:57.311
  STEP: updating /approval @ 04/28/23 16:48:57.32
  STEP: getting /status @ 04/28/23 16:48:57.325
  STEP: patching /status @ 04/28/23 16:48:57.328
  STEP: updating /status @ 04/28/23 16:48:57.337
  STEP: deleting @ 04/28/23 16:48:57.344
  STEP: deleting a collection @ 04/28/23 16:48:57.354
  Apr 28 16:48:57.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-6675" for this suite. @ 04/28/23 16:48:57.381
• [1.140 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 04/28/23 16:48:57.392
  Apr 28 16:48:57.392: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename configmap @ 04/28/23 16:48:57.393
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:48:57.413
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:48:57.417
  STEP: Creating configMap with name configmap-test-upd-98080706-610d-42a7-b4ee-36a729b3c911 @ 04/28/23 16:48:57.433
  STEP: Creating the pod @ 04/28/23 16:48:57.438
  STEP: Updating configmap configmap-test-upd-98080706-610d-42a7-b4ee-36a729b3c911 @ 04/28/23 16:48:59.469
  STEP: waiting to observe update in volume @ 04/28/23 16:48:59.475
  Apr 28 16:50:31.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6008" for this suite. @ 04/28/23 16:50:31.679
• [94.296 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 04/28/23 16:50:31.689
  Apr 28 16:50:31.689: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename init-container @ 04/28/23 16:50:31.69
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:50:31.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:50:31.71
  STEP: creating the pod @ 04/28/23 16:50:31.722
  Apr 28 16:50:31.722: INFO: PodSpec: initContainers in spec.initContainers
  Apr 28 16:50:35.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-965" for this suite. @ 04/28/23 16:50:35.339
• [3.658 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 04/28/23 16:50:35.35
  Apr 28 16:50:35.350: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename container-probe @ 04/28/23 16:50:35.351
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:50:35.836
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:50:35.84
  STEP: Creating pod busybox-638ca8ba-3f23-48e8-aac8-d72c568a4db1 in namespace container-probe-5738 @ 04/28/23 16:50:35.843
  Apr 28 16:50:37.871: INFO: Started pod busybox-638ca8ba-3f23-48e8-aac8-d72c568a4db1 in namespace container-probe-5738
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/28/23 16:50:37.871
  Apr 28 16:50:37.874: INFO: Initial restart count of pod busybox-638ca8ba-3f23-48e8-aac8-d72c568a4db1 is 0
  Apr 28 16:54:38.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 16:54:38.135
  STEP: Destroying namespace "container-probe-5738" for this suite. @ 04/28/23 16:54:38.15
• [242.822 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 04/28/23 16:54:38.176
  Apr 28 16:54:38.176: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename job @ 04/28/23 16:54:38.176
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:54:38.202
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:54:38.209
  STEP: Creating a suspended job @ 04/28/23 16:54:38.227
  STEP: Patching the Job @ 04/28/23 16:54:38.242
  STEP: Watching for Job to be patched @ 04/28/23 16:54:38.272
  Apr 28 16:54:38.280: INFO: Event ADDED observed for Job e2e-dl4d5 in namespace job-6414 with labels: map[e2e-job-label:e2e-dl4d5] and annotations: map[batch.kubernetes.io/job-tracking:]
  Apr 28 16:54:38.280: INFO: Event MODIFIED observed for Job e2e-dl4d5 in namespace job-6414 with labels: map[e2e-job-label:e2e-dl4d5] and annotations: map[batch.kubernetes.io/job-tracking:]
  Apr 28 16:54:38.280: INFO: Event MODIFIED found for Job e2e-dl4d5 in namespace job-6414 with labels: map[e2e-dl4d5:patched e2e-job-label:e2e-dl4d5] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 04/28/23 16:54:38.28
  STEP: Watching for Job to be updated @ 04/28/23 16:54:38.303
  Apr 28 16:54:38.306: INFO: Event MODIFIED found for Job e2e-dl4d5 in namespace job-6414 with labels: map[e2e-dl4d5:patched e2e-job-label:e2e-dl4d5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 28 16:54:38.306: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 04/28/23 16:54:38.306
  Apr 28 16:54:38.356: INFO: Job: e2e-dl4d5 as labels: map[e2e-dl4d5:patched e2e-job-label:e2e-dl4d5]
  STEP: Waiting for job to complete @ 04/28/23 16:54:38.357
  STEP: Delete a job collection with a labelselector @ 04/28/23 16:54:46.363
  STEP: Watching for Job to be deleted @ 04/28/23 16:54:46.374
  Apr 28 16:54:46.377: INFO: Event MODIFIED observed for Job e2e-dl4d5 in namespace job-6414 with labels: map[e2e-dl4d5:patched e2e-job-label:e2e-dl4d5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 28 16:54:46.377: INFO: Event MODIFIED observed for Job e2e-dl4d5 in namespace job-6414 with labels: map[e2e-dl4d5:patched e2e-job-label:e2e-dl4d5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 28 16:54:46.377: INFO: Event MODIFIED observed for Job e2e-dl4d5 in namespace job-6414 with labels: map[e2e-dl4d5:patched e2e-job-label:e2e-dl4d5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 28 16:54:46.377: INFO: Event MODIFIED observed for Job e2e-dl4d5 in namespace job-6414 with labels: map[e2e-dl4d5:patched e2e-job-label:e2e-dl4d5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 28 16:54:46.378: INFO: Event MODIFIED observed for Job e2e-dl4d5 in namespace job-6414 with labels: map[e2e-dl4d5:patched e2e-job-label:e2e-dl4d5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 28 16:54:46.378: INFO: Event DELETED found for Job e2e-dl4d5 in namespace job-6414 with labels: map[e2e-dl4d5:patched e2e-job-label:e2e-dl4d5] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 04/28/23 16:54:46.378
  Apr 28 16:54:46.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6414" for this suite. @ 04/28/23 16:54:46.391
• [8.682 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 04/28/23 16:54:46.859
  Apr 28 16:54:46.859: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename namespaces @ 04/28/23 16:54:46.859
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:54:46.939
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:54:46.964
  STEP: creating a Namespace @ 04/28/23 16:54:46.979
  STEP: patching the Namespace @ 04/28/23 16:54:47.027
  STEP: get the Namespace and ensuring it has the label @ 04/28/23 16:54:47.038
  Apr 28 16:54:47.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5878" for this suite. @ 04/28/23 16:54:47.059
  STEP: Destroying namespace "nspatchtest-6df1c0ba-d72e-4c40-b889-01a5be1d81a1-2217" for this suite. @ 04/28/23 16:54:47.079
• [0.241 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 04/28/23 16:54:47.101
  Apr 28 16:54:47.101: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename secrets @ 04/28/23 16:54:47.102
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:54:47.179
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:54:47.191
  STEP: Creating secret with name secret-test-0995f4f6-2a78-4160-afca-aa40cffd1d11 @ 04/28/23 16:54:47.252
  STEP: Creating a pod to test consume secrets @ 04/28/23 16:54:47.32
  STEP: Saw pod success @ 04/28/23 16:54:51.388
  Apr 28 16:54:51.391: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-secrets-0dfe06d5-d9dc-41ab-8ed3-faf2478cbb21 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 16:54:51.403
  Apr 28 16:54:51.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5154" for this suite. @ 04/28/23 16:54:51.428
• [4.335 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 04/28/23 16:54:51.436
  Apr 28 16:54:51.436: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename replication-controller @ 04/28/23 16:54:51.437
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:54:51.457
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:54:51.462
  Apr 28 16:54:51.468: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 04/28/23 16:54:51.483
  STEP: Checking rc "condition-test" has the desired failure condition set @ 04/28/23 16:54:51.496
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 04/28/23 16:54:52.508
  Apr 28 16:54:52.518: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 04/28/23 16:54:52.518
  Apr 28 16:54:52.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9904" for this suite. @ 04/28/23 16:54:52.53
• [1.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 04/28/23 16:54:52.542
  Apr 28 16:54:52.542: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename gc @ 04/28/23 16:54:52.544
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:54:52.565
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:54:52.568
  STEP: create the rc1 @ 04/28/23 16:54:52.574
  STEP: create the rc2 @ 04/28/23 16:54:52.581
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 04/28/23 16:54:58.636
  STEP: delete the rc simpletest-rc-to-be-deleted @ 04/28/23 16:54:59.81
  STEP: wait for the rc to be deleted @ 04/28/23 16:54:59.825
  Apr 28 16:55:04.839: INFO: 70 pods remaining
  Apr 28 16:55:04.839: INFO: 70 pods has nil DeletionTimestamp
  Apr 28 16:55:04.839: INFO: 
  STEP: Gathering metrics @ 04/28/23 16:55:09.836
  W0428 16:55:09.842580      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Apr 28 16:55:09.842: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 28 16:55:09.842: INFO: Deleting pod "simpletest-rc-to-be-deleted-29t7m" in namespace "gc-3260"
  Apr 28 16:55:09.858: INFO: Deleting pod "simpletest-rc-to-be-deleted-2m2bv" in namespace "gc-3260"
  Apr 28 16:55:09.879: INFO: Deleting pod "simpletest-rc-to-be-deleted-2m9fw" in namespace "gc-3260"
  Apr 28 16:55:09.935: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mmld" in namespace "gc-3260"
  Apr 28 16:55:09.970: INFO: Deleting pod "simpletest-rc-to-be-deleted-2n4xd" in namespace "gc-3260"
  Apr 28 16:55:10.034: INFO: Deleting pod "simpletest-rc-to-be-deleted-466q7" in namespace "gc-3260"
  Apr 28 16:55:10.567: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ccqz" in namespace "gc-3260"
  Apr 28 16:55:10.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ntpn" in namespace "gc-3260"
  Apr 28 16:55:10.721: INFO: Deleting pod "simpletest-rc-to-be-deleted-4tcx9" in namespace "gc-3260"
  Apr 28 16:55:10.789: INFO: Deleting pod "simpletest-rc-to-be-deleted-52zln" in namespace "gc-3260"
  Apr 28 16:55:10.833: INFO: Deleting pod "simpletest-rc-to-be-deleted-587kf" in namespace "gc-3260"
  Apr 28 16:55:10.943: INFO: Deleting pod "simpletest-rc-to-be-deleted-5nktn" in namespace "gc-3260"
  Apr 28 16:55:10.993: INFO: Deleting pod "simpletest-rc-to-be-deleted-5v8qd" in namespace "gc-3260"
  Apr 28 16:55:11.122: INFO: Deleting pod "simpletest-rc-to-be-deleted-67hbw" in namespace "gc-3260"
  Apr 28 16:55:11.210: INFO: Deleting pod "simpletest-rc-to-be-deleted-68pfc" in namespace "gc-3260"
  Apr 28 16:55:11.267: INFO: Deleting pod "simpletest-rc-to-be-deleted-6bltw" in namespace "gc-3260"
  Apr 28 16:55:11.360: INFO: Deleting pod "simpletest-rc-to-be-deleted-6n8pv" in namespace "gc-3260"
  Apr 28 16:55:11.402: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vh6v" in namespace "gc-3260"
  Apr 28 16:55:11.429: INFO: Deleting pod "simpletest-rc-to-be-deleted-6wbqg" in namespace "gc-3260"
  Apr 28 16:55:11.491: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dm6l" in namespace "gc-3260"
  Apr 28 16:55:11.529: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kgqf" in namespace "gc-3260"
  Apr 28 16:55:11.587: INFO: Deleting pod "simpletest-rc-to-be-deleted-7l27n" in namespace "gc-3260"
  Apr 28 16:55:11.624: INFO: Deleting pod "simpletest-rc-to-be-deleted-7rbwt" in namespace "gc-3260"
  Apr 28 16:55:11.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bpp7" in namespace "gc-3260"
  Apr 28 16:55:11.689: INFO: Deleting pod "simpletest-rc-to-be-deleted-8gk6f" in namespace "gc-3260"
  Apr 28 16:55:11.721: INFO: Deleting pod "simpletest-rc-to-be-deleted-8kvw5" in namespace "gc-3260"
  Apr 28 16:55:11.764: INFO: Deleting pod "simpletest-rc-to-be-deleted-9445p" in namespace "gc-3260"
  Apr 28 16:55:11.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-9wxcf" in namespace "gc-3260"
  Apr 28 16:55:11.841: INFO: Deleting pod "simpletest-rc-to-be-deleted-9x8ms" in namespace "gc-3260"
  Apr 28 16:55:11.873: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdcv2" in namespace "gc-3260"
  Apr 28 16:55:11.910: INFO: Deleting pod "simpletest-rc-to-be-deleted-bfhhb" in namespace "gc-3260"
  Apr 28 16:55:11.945: INFO: Deleting pod "simpletest-rc-to-be-deleted-bhd5l" in namespace "gc-3260"
  Apr 28 16:55:12.021: INFO: Deleting pod "simpletest-rc-to-be-deleted-bshzn" in namespace "gc-3260"
  Apr 28 16:55:12.046: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvqpk" in namespace "gc-3260"
  Apr 28 16:55:12.092: INFO: Deleting pod "simpletest-rc-to-be-deleted-cbc7w" in namespace "gc-3260"
  Apr 28 16:55:12.129: INFO: Deleting pod "simpletest-rc-to-be-deleted-cgjsj" in namespace "gc-3260"
  Apr 28 16:55:12.150: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnwnm" in namespace "gc-3260"
  Apr 28 16:55:12.200: INFO: Deleting pod "simpletest-rc-to-be-deleted-cpd5x" in namespace "gc-3260"
  Apr 28 16:55:12.241: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnl4t" in namespace "gc-3260"
  Apr 28 16:55:12.267: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvmxq" in namespace "gc-3260"
  Apr 28 16:55:12.327: INFO: Deleting pod "simpletest-rc-to-be-deleted-fdh6z" in namespace "gc-3260"
  Apr 28 16:55:12.364: INFO: Deleting pod "simpletest-rc-to-be-deleted-fdqf6" in namespace "gc-3260"
  Apr 28 16:55:12.436: INFO: Deleting pod "simpletest-rc-to-be-deleted-g7pw6" in namespace "gc-3260"
  Apr 28 16:55:12.468: INFO: Deleting pod "simpletest-rc-to-be-deleted-gm8xb" in namespace "gc-3260"
  Apr 28 16:55:12.518: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2tsg" in namespace "gc-3260"
  Apr 28 16:55:12.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9857" in namespace "gc-3260"
  Apr 28 16:55:12.568: INFO: Deleting pod "simpletest-rc-to-be-deleted-hghh6" in namespace "gc-3260"
  Apr 28 16:55:12.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-hj5hb" in namespace "gc-3260"
  Apr 28 16:55:12.620: INFO: Deleting pod "simpletest-rc-to-be-deleted-hpf7v" in namespace "gc-3260"
  Apr 28 16:55:12.657: INFO: Deleting pod "simpletest-rc-to-be-deleted-hpm64" in namespace "gc-3260"
  Apr 28 16:55:12.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3260" for this suite. @ 04/28/23 16:55:12.698
• [20.182 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 04/28/23 16:55:12.729
  Apr 28 16:55:12.729: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename job @ 04/28/23 16:55:12.73
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:55:12.789
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:55:12.795
  STEP: Creating a job @ 04/28/23 16:55:12.803
  STEP: Ensure pods equal to parallelism count is attached to the job @ 04/28/23 16:55:12.816
  STEP: patching /status @ 04/28/23 16:55:14.82
  STEP: updating /status @ 04/28/23 16:55:14.829
  STEP: get /status @ 04/28/23 16:55:14.862
  Apr 28 16:55:14.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1281" for this suite. @ 04/28/23 16:55:14.869
• [2.146 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 04/28/23 16:55:14.88
  Apr 28 16:55:14.880: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename job @ 04/28/23 16:55:14.882
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:55:14.9
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:55:14.905
  STEP: Creating a job @ 04/28/23 16:55:14.908
  STEP: Ensuring active pods == parallelism @ 04/28/23 16:55:14.918
  STEP: Orphaning one of the Job's Pods @ 04/28/23 16:55:16.922
  Apr 28 16:55:17.437: INFO: Successfully updated pod "adopt-release-5czpv"
  STEP: Checking that the Job readopts the Pod @ 04/28/23 16:55:17.437
  STEP: Removing the labels from the Job's Pod @ 04/28/23 16:55:19.445
  Apr 28 16:55:19.962: INFO: Successfully updated pod "adopt-release-5czpv"
  STEP: Checking that the Job releases the Pod @ 04/28/23 16:55:19.962
  Apr 28 16:55:21.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7159" for this suite. @ 04/28/23 16:55:21.975
• [7.101 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 04/28/23 16:55:21.982
  Apr 28 16:55:21.982: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename secrets @ 04/28/23 16:55:21.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:55:21.998
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:55:22.01
  STEP: Creating secret with name secret-test-3ddf1328-6faf-4d36-8ad4-4f99a7ebfea7 @ 04/28/23 16:55:22.015
  STEP: Creating a pod to test consume secrets @ 04/28/23 16:55:22.026
  STEP: Saw pod success @ 04/28/23 16:55:26.05
  Apr 28 16:55:26.053: INFO: Trying to get logs from node ip-172-31-10-53 pod pod-secrets-9a4ab634-b5d7-431f-aa2b-a2046d50c2c2 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 16:55:26.063
  Apr 28 16:55:26.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5732" for this suite. @ 04/28/23 16:55:26.805
• [4.830 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 04/28/23 16:55:26.813
  Apr 28 16:55:26.813: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename container-runtime @ 04/28/23 16:55:26.814
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:55:26.84
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:55:26.845
  STEP: create the container @ 04/28/23 16:55:26.85
  W0428 16:55:27.098551      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 04/28/23 16:55:27.098
  STEP: get the container status @ 04/28/23 16:55:30.138
  STEP: the container should be terminated @ 04/28/23 16:55:30.143
  STEP: the termination message should be set @ 04/28/23 16:55:30.143
  Apr 28 16:55:30.143: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 04/28/23 16:55:30.143
  Apr 28 16:55:30.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-1821" for this suite. @ 04/28/23 16:55:30.176
• [3.371 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 04/28/23 16:55:30.185
  Apr 28 16:55:30.185: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename configmap @ 04/28/23 16:55:30.186
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:55:30.207
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:55:30.211
  STEP: Creating configMap with name cm-test-opt-del-94867045-a538-42cf-adc5-79634787d906 @ 04/28/23 16:55:30.219
  STEP: Creating configMap with name cm-test-opt-upd-a052334a-bafc-4c18-a863-e5c6028b6316 @ 04/28/23 16:55:30.228
  STEP: Creating the pod @ 04/28/23 16:55:30.233
  STEP: Deleting configmap cm-test-opt-del-94867045-a538-42cf-adc5-79634787d906 @ 04/28/23 16:55:32.93
  STEP: Updating configmap cm-test-opt-upd-a052334a-bafc-4c18-a863-e5c6028b6316 @ 04/28/23 16:55:32.977
  STEP: Creating configMap with name cm-test-opt-create-6cff4990-5c69-4c18-9c17-6de039320d4e @ 04/28/23 16:55:32.988
  STEP: waiting to observe update in volume @ 04/28/23 16:55:32.994
  Apr 28 16:56:50.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7772" for this suite. @ 04/28/23 16:56:50.065
• [79.887 seconds]
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 04/28/23 16:56:50.073
  Apr 28 16:56:50.073: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename configmap @ 04/28/23 16:56:50.073
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:56:50.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:56:50.116
  STEP: Creating configMap with name configmap-test-volume-975315c2-c63c-4e20-88fa-05e96c231e56 @ 04/28/23 16:56:50.12
  STEP: Creating a pod to test consume configMaps @ 04/28/23 16:56:50.132
  STEP: Saw pod success @ 04/28/23 16:56:54.171
  Apr 28 16:56:54.174: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-configmaps-b55350e0-2b63-42dd-a0b6-c40d52fb8a98 container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 16:56:54.184
  Apr 28 16:56:54.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5933" for this suite. @ 04/28/23 16:56:54.212
• [4.157 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 04/28/23 16:56:54.23
  Apr 28 16:56:54.230: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename endpointslice @ 04/28/23 16:56:54.231
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:56:54.261
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:56:54.264
  STEP: getting /apis @ 04/28/23 16:56:54.267
  STEP: getting /apis/discovery.k8s.io @ 04/28/23 16:56:54.271
  STEP: getting /apis/discovery.k8s.iov1 @ 04/28/23 16:56:54.272
  STEP: creating @ 04/28/23 16:56:54.273
  STEP: getting @ 04/28/23 16:56:54.294
  STEP: listing @ 04/28/23 16:56:54.298
  STEP: watching @ 04/28/23 16:56:54.301
  Apr 28 16:56:54.301: INFO: starting watch
  STEP: cluster-wide listing @ 04/28/23 16:56:54.302
  STEP: cluster-wide watching @ 04/28/23 16:56:54.305
  Apr 28 16:56:54.305: INFO: starting watch
  STEP: patching @ 04/28/23 16:56:54.306
  STEP: updating @ 04/28/23 16:56:54.311
  Apr 28 16:56:54.320: INFO: waiting for watch events with expected annotations
  Apr 28 16:56:54.320: INFO: saw patched and updated annotations
  STEP: deleting @ 04/28/23 16:56:54.321
  STEP: deleting a collection @ 04/28/23 16:56:54.34
  Apr 28 16:56:54.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-3294" for this suite. @ 04/28/23 16:56:54.373
• [0.151 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 04/28/23 16:56:54.383
  Apr 28 16:56:54.383: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 16:56:54.384
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:56:54.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:56:54.417
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 04/28/23 16:56:54.42
  STEP: Saw pod success @ 04/28/23 16:56:58.441
  Apr 28 16:56:58.446: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-0cbdc9c8-cb87-4821-bfcc-d7d12c0d6ed0 container test-container: <nil>
  STEP: delete the pod @ 04/28/23 16:56:58.454
  Apr 28 16:56:58.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9190" for this suite. @ 04/28/23 16:56:58.475
• [4.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 04/28/23 16:56:58.483
  Apr 28 16:56:58.483: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename kubelet-test @ 04/28/23 16:56:58.484
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:56:58.51
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:56:58.514
  Apr 28 16:57:00.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-813" for this suite. @ 04/28/23 16:57:00.565
• [2.092 seconds]
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 04/28/23 16:57:00.575
  Apr 28 16:57:00.575: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename init-container @ 04/28/23 16:57:00.576
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:57:00.603
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:57:00.606
  STEP: creating the pod @ 04/28/23 16:57:00.609
  Apr 28 16:57:00.609: INFO: PodSpec: initContainers in spec.initContainers
  Apr 28 16:57:05.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-8602" for this suite. @ 04/28/23 16:57:05.08
• [4.525 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 04/28/23 16:57:05.101
  Apr 28 16:57:05.101: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename dns @ 04/28/23 16:57:05.102
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:57:05.139
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:57:05.143
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 04/28/23 16:57:05.146
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 04/28/23 16:57:05.146
  STEP: creating a pod to probe DNS @ 04/28/23 16:57:05.146
  STEP: submitting the pod to kubernetes @ 04/28/23 16:57:05.146
  STEP: retrieving the pod @ 04/28/23 16:57:07.191
  STEP: looking for the results for each expected name from probers @ 04/28/23 16:57:07.194
  Apr 28 16:57:07.216: INFO: DNS probes using dns-2690/dns-test-4a587d63-6383-4e2a-bd67-7f311e46027a succeeded

  Apr 28 16:57:07.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 16:57:07.226
  STEP: Destroying namespace "dns-2690" for this suite. @ 04/28/23 16:57:07.238
• [2.200 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 04/28/23 16:57:07.307
  Apr 28 16:57:07.307: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename replicaset @ 04/28/23 16:57:07.308
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:57:07.383
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:57:07.389
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 04/28/23 16:57:07.397
  STEP: When a replicaset with a matching selector is created @ 04/28/23 16:57:09.437
  STEP: Then the orphan pod is adopted @ 04/28/23 16:57:09.445
  STEP: When the matched label of one of its pods change @ 04/28/23 16:57:10.452
  Apr 28 16:57:10.455: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 04/28/23 16:57:10.464
  Apr 28 16:57:10.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1677" for this suite. @ 04/28/23 16:57:10.489
• [3.202 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 04/28/23 16:57:10.51
  Apr 28 16:57:10.510: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 16:57:10.511
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:57:10.531
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:57:10.535
  STEP: Creating a pod to test downward api env vars @ 04/28/23 16:57:10.538
  STEP: Saw pod success @ 04/28/23 16:57:14.684
  Apr 28 16:57:14.686: INFO: Trying to get logs from node ip-172-31-10-53 pod downward-api-8ad2a6de-31e3-4902-8a2b-a317ed5344a5 container dapi-container: <nil>
  STEP: delete the pod @ 04/28/23 16:57:14.692
  Apr 28 16:57:14.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5706" for this suite. @ 04/28/23 16:57:14.715
• [4.210 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 04/28/23 16:57:14.721
  Apr 28 16:57:14.721: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename proxy @ 04/28/23 16:57:14.721
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:57:14.745
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:57:14.748
  Apr 28 16:57:14.752: INFO: Creating pod...
  Apr 28 16:57:16.772: INFO: Creating service...
  Apr 28 16:57:16.782: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4785/pods/agnhost/proxy?method=DELETE
  Apr 28 16:57:16.804: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 28 16:57:16.805: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4785/pods/agnhost/proxy?method=OPTIONS
  Apr 28 16:57:16.809: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 28 16:57:16.809: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4785/pods/agnhost/proxy?method=PATCH
  Apr 28 16:57:16.816: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 28 16:57:16.816: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4785/pods/agnhost/proxy?method=POST
  Apr 28 16:57:16.821: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 28 16:57:16.821: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4785/pods/agnhost/proxy?method=PUT
  Apr 28 16:57:16.823: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 28 16:57:16.823: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4785/services/e2e-proxy-test-service/proxy?method=DELETE
  Apr 28 16:57:16.827: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 28 16:57:16.827: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4785/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Apr 28 16:57:16.831: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 28 16:57:16.831: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4785/services/e2e-proxy-test-service/proxy?method=PATCH
  Apr 28 16:57:16.835: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 28 16:57:16.835: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4785/services/e2e-proxy-test-service/proxy?method=POST
  Apr 28 16:57:16.839: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 28 16:57:16.839: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4785/services/e2e-proxy-test-service/proxy?method=PUT
  Apr 28 16:57:16.843: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 28 16:57:16.843: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4785/pods/agnhost/proxy?method=GET
  Apr 28 16:57:16.845: INFO: http.Client request:GET StatusCode:301
  Apr 28 16:57:16.845: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4785/services/e2e-proxy-test-service/proxy?method=GET
  Apr 28 16:57:16.849: INFO: http.Client request:GET StatusCode:301
  Apr 28 16:57:16.849: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4785/pods/agnhost/proxy?method=HEAD
  Apr 28 16:57:16.851: INFO: http.Client request:HEAD StatusCode:301
  Apr 28 16:57:16.851: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-4785/services/e2e-proxy-test-service/proxy?method=HEAD
  Apr 28 16:57:16.854: INFO: http.Client request:HEAD StatusCode:301
  Apr 28 16:57:16.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-4785" for this suite. @ 04/28/23 16:57:16.858
• [2.144 seconds]
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 04/28/23 16:57:16.865
  Apr 28 16:57:16.865: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 16:57:16.866
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:57:16.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:57:16.884
  STEP: Creating configMap with name configmap-projected-all-test-volume-39e495a0-3187-47cf-906b-8920d57bf589 @ 04/28/23 16:57:16.887
  STEP: Creating secret with name secret-projected-all-test-volume-7f24fff0-57a0-4be6-8682-8bb09f8880a1 @ 04/28/23 16:57:16.894
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 04/28/23 16:57:16.902
  STEP: Saw pod success @ 04/28/23 16:57:19.227
  Apr 28 16:57:19.231: INFO: Trying to get logs from node ip-172-31-1-213 pod projected-volume-dd9b0ab0-438a-475c-92ad-730a3f2eb310 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 16:57:19.24
  Apr 28 16:57:19.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5284" for this suite. @ 04/28/23 16:57:19.263
• [2.406 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 04/28/23 16:57:19.271
  Apr 28 16:57:19.271: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename sched-preemption @ 04/28/23 16:57:19.272
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:57:19.292
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:57:19.295
  Apr 28 16:57:19.317: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 28 16:58:19.347: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 04/28/23 16:58:19.351
  Apr 28 16:58:19.380: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Apr 28 16:58:19.386: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Apr 28 16:58:19.416: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Apr 28 16:58:19.430: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Apr 28 16:58:19.479: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Apr 28 16:58:19.497: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  Apr 28 16:58:19.546: INFO: Created pod: pod3-0-sched-preemption-medium-priority
  Apr 28 16:58:19.571: INFO: Created pod: pod3-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 04/28/23 16:58:19.571
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 04/28/23 16:58:21.636
  Apr 28 16:58:25.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-3324" for this suite. @ 04/28/23 16:58:25.81
• [66.544 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 04/28/23 16:58:25.817
  Apr 28 16:58:25.817: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename disruption @ 04/28/23 16:58:25.818
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:58:25.838
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:58:25.843
  STEP: creating the pdb @ 04/28/23 16:58:25.846
  STEP: Waiting for the pdb to be processed @ 04/28/23 16:58:25.863
  STEP: updating the pdb @ 04/28/23 16:58:25.891
  STEP: Waiting for the pdb to be processed @ 04/28/23 16:58:25.915
  STEP: patching the pdb @ 04/28/23 16:58:27.935
  STEP: Waiting for the pdb to be processed @ 04/28/23 16:58:27.944
  STEP: Waiting for the pdb to be deleted @ 04/28/23 16:58:29.958
  Apr 28 16:58:29.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-3973" for this suite. @ 04/28/23 16:58:29.966
• [4.157 seconds]
------------------------------
SSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 04/28/23 16:58:29.974
  Apr 28 16:58:29.974: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename containers @ 04/28/23 16:58:29.975
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:58:30.036
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:58:30.044
  Apr 28 16:58:32.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-6884" for this suite. @ 04/28/23 16:58:32.125
• [2.160 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 04/28/23 16:58:32.136
  Apr 28 16:58:32.136: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename replication-controller @ 04/28/23 16:58:32.136
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:58:32.154
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:58:32.157
  STEP: Given a ReplicationController is created @ 04/28/23 16:58:32.16
  STEP: When the matched label of one of its pods change @ 04/28/23 16:58:32.166
  Apr 28 16:58:32.170: INFO: Pod name pod-release: Found 0 pods out of 1
  Apr 28 16:58:37.176: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 04/28/23 16:58:37.189
  Apr 28 16:58:37.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-1269" for this suite. @ 04/28/23 16:58:37.217
• [5.113 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 04/28/23 16:58:37.25
  Apr 28 16:58:37.250: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename replication-controller @ 04/28/23 16:58:37.251
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:58:37.293
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:58:37.3
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 04/28/23 16:58:37.305
  STEP: When a replication controller with a matching selector is created @ 04/28/23 16:58:39.331
  STEP: Then the orphan pod is adopted @ 04/28/23 16:58:39.335
  Apr 28 16:58:40.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-2557" for this suite. @ 04/28/23 16:58:40.349
• [3.107 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 04/28/23 16:58:40.357
  Apr 28 16:58:40.357: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename secrets @ 04/28/23 16:58:40.358
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:58:40.385
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:58:40.389
  STEP: Creating secret with name secret-test-map-973b5146-c09c-4ca0-a39c-27ae0cc71796 @ 04/28/23 16:58:40.392
  STEP: Creating a pod to test consume secrets @ 04/28/23 16:58:40.399
  STEP: Saw pod success @ 04/28/23 16:58:42.43
  Apr 28 16:58:42.433: INFO: Trying to get logs from node ip-172-31-5-174 pod pod-secrets-2f1695c0-451d-4216-a536-de26ded2b050 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 16:58:42.457
  Apr 28 16:58:42.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-520" for this suite. @ 04/28/23 16:58:42.489
• [2.142 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 04/28/23 16:58:42.506
  Apr 28 16:58:42.506: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 16:58:42.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:58:42.546
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:58:42.551
  STEP: Counting existing ResourceQuota @ 04/28/23 16:58:59.558
  STEP: Creating a ResourceQuota @ 04/28/23 16:59:04.565
  STEP: Ensuring resource quota status is calculated @ 04/28/23 16:59:04.57
  STEP: Creating a ConfigMap @ 04/28/23 16:59:07.001
  STEP: Ensuring resource quota status captures configMap creation @ 04/28/23 16:59:07.03
  STEP: Deleting a ConfigMap @ 04/28/23 16:59:09.034
  STEP: Ensuring resource quota status released usage @ 04/28/23 16:59:09.045
  Apr 28 16:59:11.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9123" for this suite. @ 04/28/23 16:59:11.053
• [28.552 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 04/28/23 16:59:11.06
  Apr 28 16:59:11.060: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename replication-controller @ 04/28/23 16:59:11.061
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:59:11.095
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:59:11.099
  STEP: creating a ReplicationController @ 04/28/23 16:59:11.106
  STEP: waiting for RC to be added @ 04/28/23 16:59:11.124
  STEP: waiting for available Replicas @ 04/28/23 16:59:11.124
  STEP: patching ReplicationController @ 04/28/23 16:59:12.078
  STEP: waiting for RC to be modified @ 04/28/23 16:59:12.114
  STEP: patching ReplicationController status @ 04/28/23 16:59:12.114
  STEP: waiting for RC to be modified @ 04/28/23 16:59:12.143
  STEP: waiting for available Replicas @ 04/28/23 16:59:12.143
  STEP: fetching ReplicationController status @ 04/28/23 16:59:12.147
  STEP: patching ReplicationController scale @ 04/28/23 16:59:12.158
  STEP: waiting for RC to be modified @ 04/28/23 16:59:12.202
  STEP: waiting for ReplicationController's scale to be the max amount @ 04/28/23 16:59:12.202
  STEP: fetching ReplicationController; ensuring that it's patched @ 04/28/23 16:59:13.349
  STEP: updating ReplicationController status @ 04/28/23 16:59:13.353
  STEP: waiting for RC to be modified @ 04/28/23 16:59:13.36
  STEP: listing all ReplicationControllers @ 04/28/23 16:59:13.36
  STEP: checking that ReplicationController has expected values @ 04/28/23 16:59:13.365
  STEP: deleting ReplicationControllers by collection @ 04/28/23 16:59:13.365
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 04/28/23 16:59:13.373
  Apr 28 16:59:13.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0428 16:59:13.431764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-8784" for this suite. @ 04/28/23 16:59:13.434
• [2.379 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 04/28/23 16:59:13.439
  Apr 28 16:59:13.439: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename daemonsets @ 04/28/23 16:59:13.44
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:59:13.454
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:59:13.457
  STEP: Creating simple DaemonSet "daemon-set" @ 04/28/23 16:59:13.507
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/28/23 16:59:13.515
  Apr 28 16:59:13.523: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:59:13.523: INFO: Node ip-172-31-1-213 is running 0 daemon pod, expected 1
  E0428 16:59:14.431909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 16:59:14.530: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 28 16:59:14.530: INFO: Node ip-172-31-1-213 is running 0 daemon pod, expected 1
  E0428 16:59:15.432020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 16:59:15.533: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
  Apr 28 16:59:15.533: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: listing all DaemonSets @ 04/28/23 16:59:15.538
  STEP: DeleteCollection of the DaemonSets @ 04/28/23 16:59:15.542
  STEP: Verify that ReplicaSets have been deleted @ 04/28/23 16:59:15.549
  Apr 28 16:59:15.559: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"250289"},"items":null}

  Apr 28 16:59:15.571: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"250290"},"items":[{"metadata":{"name":"daemon-set-g7mrk","generateName":"daemon-set-","namespace":"daemonsets-6399","uid":"55e21700-c666-43f6-a8ea-02bd490780b4","resourceVersion":"250283","creationTimestamp":"2023-04-28T16:59:13Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c8fb49ff-491d-4ee7-897b-0037f877819d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"k3s","operation":"Update","apiVersion":"v1","time":"2023-04-28T16:59:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8fb49ff-491d-4ee7-897b-0037f877819d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"k3s","operation":"Update","apiVersion":"v1","time":"2023-04-28T16:59:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.245\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-n95pf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-n95pf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-1-213","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-1-213"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T16:59:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T16:59:15Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T16:59:15Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T16:59:13Z"}],"hostIP":"172.31.1.213","podIP":"10.42.2.245","podIPs":[{"ip":"10.42.2.245"}],"startTime":"2023-04-28T16:59:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-28T16:59:14Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://ed6124c1d8305fb436e00c37b4ce4352b87ed963a372841d4a89b1469ef92a29","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-hwth6","generateName":"daemon-set-","namespace":"daemonsets-6399","uid":"5713759b-947c-4308-a750-ba9fd81336df","resourceVersion":"250285","creationTimestamp":"2023-04-28T16:59:13Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c8fb49ff-491d-4ee7-897b-0037f877819d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"k3s","operation":"Update","apiVersion":"v1","time":"2023-04-28T16:59:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8fb49ff-491d-4ee7-897b-0037f877819d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"k3s","operation":"Update","apiVersion":"v1","time":"2023-04-28T16:59:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.210\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5cmxn","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5cmxn","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-5-174","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-5-174"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T16:59:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T16:59:15Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T16:59:15Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T16:59:13Z"}],"hostIP":"172.31.5.174","podIP":"10.42.3.210","podIPs":[{"ip":"10.42.3.210"}],"startTime":"2023-04-28T16:59:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-28T16:59:14Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://1243b672e04b3cc7aa715b21ae0a433c6f29c2962dc4373018629b77485b45c1","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-mn988","generateName":"daemon-set-","namespace":"daemonsets-6399","uid":"3ebbab7f-12c6-408d-b2f7-3723d4eae7ac","resourceVersion":"250280","creationTimestamp":"2023-04-28T16:59:13Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c8fb49ff-491d-4ee7-897b-0037f877819d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"k3s","operation":"Update","apiVersion":"v1","time":"2023-04-28T16:59:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8fb49ff-491d-4ee7-897b-0037f877819d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"k3s","operation":"Update","apiVersion":"v1","time":"2023-04-28T16:59:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.165\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-t8kn5","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-t8kn5","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-12-59","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-12-59"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T16:59:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T16:59:14Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T16:59:14Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T16:59:13Z"}],"hostIP":"172.31.12.59","podIP":"10.42.0.165","podIPs":[{"ip":"10.42.0.165"}],"startTime":"2023-04-28T16:59:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-28T16:59:14Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://7769285b105d6d3f6fd6dc327e2307b2b0e76963cd95bbb3c24eb89686183554","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vsg5t","generateName":"daemon-set-","namespace":"daemonsets-6399","uid":"20c6d74e-574d-48c7-ad50-34029f7b3bb6","resourceVersion":"250290","creationTimestamp":"2023-04-28T16:59:13Z","deletionTimestamp":"2023-04-28T16:59:45Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c8fb49ff-491d-4ee7-897b-0037f877819d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"k3s","operation":"Update","apiVersion":"v1","time":"2023-04-28T16:59:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8fb49ff-491d-4ee7-897b-0037f877819d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"k3s","operation":"Update","apiVersion":"v1","time":"2023-04-28T16:59:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-vjpsv","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-vjpsv","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-10-53","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-10-53"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T16:59:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T16:59:14Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T16:59:14Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T16:59:13Z"}],"hostIP":"172.31.10.53","podIP":"10.42.1.14","podIPs":[{"ip":"10.42.1.14"}],"startTime":"2023-04-28T16:59:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-28T16:59:14Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://736d7814fbccefb65c21eb5be532ccce240a429a0267cc0708fd8d4d55748b30","started":true}],"qosClass":"BestEffort"}}]}

  Apr 28 16:59:15.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6399" for this suite. @ 04/28/23 16:59:15.592
• [2.159 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 04/28/23 16:59:15.599
  Apr 28 16:59:15.599: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename container-probe @ 04/28/23 16:59:15.6
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:59:15.62
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:59:15.623
  E0428 16:59:16.432139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:17.432261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:18.433119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:19.433219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:20.433366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:21.433692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:22.434398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:23.434666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:24.434784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:25.434879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:26.435022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:27.435124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:28.436177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:29.436936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:30.437068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:31.437208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:32.437350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:33.437407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:34.437534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:35.437611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:36.438090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:37.438242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:38.439207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:39.439671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:40.439730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:41.440190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:42.441150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:43.441867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:44.441950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:45.442921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:46.443697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:47.444684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:48.445207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:49.445367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:50.445528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:51.445638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:52.446222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:53.446760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:54.447652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:55.448575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:56.448734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:57.449597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:58.450360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 16:59:59.451202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:00.451350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:01.452376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:02.453101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:03.453408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:04.453458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:05.454040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:06.454562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:07.455217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:08.456214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:09.456285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:10.456702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:11.457352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:12.457769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:13.458347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:14.459084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:15.459271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:00:15.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-673" for this suite. @ 04/28/23 17:00:15.967
• [60.380 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 04/28/23 17:00:15.983
  Apr 28 17:00:15.983: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename tables @ 04/28/23 17:00:15.984
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:00:16.046
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:00:16.066
  Apr 28 17:00:16.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-4856" for this suite. @ 04/28/23 17:00:16.101
• [0.141 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 04/28/23 17:00:16.125
  Apr 28 17:00:16.125: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 17:00:16.126
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:00:16.165
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:00:16.172
  STEP: Counting existing ResourceQuota @ 04/28/23 17:00:16.189
  E0428 17:00:16.459747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:17.460368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:18.460908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:19.461732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:20.462177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/28/23 17:00:21.197
  STEP: Ensuring resource quota status is calculated @ 04/28/23 17:00:21.207
  E0428 17:00:21.463263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:22.463844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:00:23.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-769" for this suite. @ 04/28/23 17:00:23.219
• [7.101 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 04/28/23 17:00:23.229
  Apr 28 17:00:23.229: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename server-version @ 04/28/23 17:00:23.231
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:00:23.28
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:00:23.284
  STEP: Request ServerVersion @ 04/28/23 17:00:23.287
  STEP: Confirm major version @ 04/28/23 17:00:23.288
  Apr 28 17:00:23.288: INFO: Major version: 1
  STEP: Confirm minor version @ 04/28/23 17:00:23.288
  Apr 28 17:00:23.288: INFO: cleanMinorVersion: 27
  Apr 28 17:00:23.288: INFO: Minor version: 27
  Apr 28 17:00:23.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-7284" for this suite. @ 04/28/23 17:00:23.295
• [0.075 seconds]
------------------------------
S
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 04/28/23 17:00:23.305
  Apr 28 17:00:23.305: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 04/28/23 17:00:23.306
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:00:23.385
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:00:23.387
  STEP: creating a target pod @ 04/28/23 17:00:23.389
  E0428 17:00:23.464159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:24.464391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding an ephemeral container @ 04/28/23 17:00:25.415
  E0428 17:00:25.465198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:26.465318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking pod container endpoints @ 04/28/23 17:00:27.443
  Apr 28 17:00:27.443: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4220 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:00:27.443: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 17:00:27.443: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:00:27.443: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/ephemeral-containers-test-4220/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  E0428 17:00:27.465960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:00:27.551: INFO: Exec stderr: ""
  Apr 28 17:00:27.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-4220" for this suite. @ 04/28/23 17:00:27.575
• [4.277 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 04/28/23 17:00:27.583
  Apr 28 17:00:27.583: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename webhook @ 04/28/23 17:00:27.584
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:00:27.605
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:00:27.609
  STEP: Setting up server cert @ 04/28/23 17:00:27.642
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 17:00:28.396
  STEP: Deploying the webhook pod @ 04/28/23 17:00:28.41
  STEP: Wait for the deployment to be ready @ 04/28/23 17:00:28.428
  Apr 28 17:00:28.442: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0428 17:00:28.466600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:29.466839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 17:00:30.45
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 17:00:30.46
  E0428 17:00:30.467867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:00:31.460: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 28 17:00:31.464: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 17:00:31.468476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8072-crds.webhook.example.com via the AdmissionRegistration API @ 04/28/23 17:00:31.979
  STEP: Creating a custom resource that should be mutated by the webhook @ 04/28/23 17:00:32.003
  E0428 17:00:32.468691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:33.469460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:00:34.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0428 17:00:34.470197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-2936" for this suite. @ 04/28/23 17:00:34.77
  STEP: Destroying namespace "webhook-markers-6600" for this suite. @ 04/28/23 17:00:34.778
• [7.201 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 04/28/23 17:00:34.786
  Apr 28 17:00:34.786: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename kubectl-logs @ 04/28/23 17:00:34.786
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:00:34.803
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:00:34.807
  STEP: creating an pod @ 04/28/23 17:00:34.81
  Apr 28 17:00:34.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-logs-7619 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Apr 28 17:00:34.917: INFO: stderr: ""
  Apr 28 17:00:34.917: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 04/28/23 17:00:34.917
  Apr 28 17:00:34.917: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E0428 17:00:35.471082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:36.471222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:00:36.929: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 04/28/23 17:00:36.929
  Apr 28 17:00:36.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-logs-7619 logs logs-generator logs-generator'
  Apr 28 17:00:37.001: INFO: stderr: ""
  Apr 28 17:00:37.001: INFO: stdout: "I0428 17:00:35.532072       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/lst 256\nI0428 17:00:35.732160       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/bn7q 272\nI0428 17:00:35.932706       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/rjg8 538\nI0428 17:00:36.133016       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/d6lf 580\nI0428 17:00:36.332259       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/vn57 307\nI0428 17:00:36.532573       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/tknd 209\nI0428 17:00:36.733796       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/5dtg 516\nI0428 17:00:36.933116       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/fmmr 514\n"
  STEP: limiting log lines @ 04/28/23 17:00:37.001
  Apr 28 17:00:37.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-logs-7619 logs logs-generator logs-generator --tail=1'
  Apr 28 17:00:37.104: INFO: stderr: ""
  Apr 28 17:00:37.104: INFO: stdout: "I0428 17:00:36.933116       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/fmmr 514\n"
  Apr 28 17:00:37.104: INFO: got output "I0428 17:00:36.933116       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/fmmr 514\n"
  STEP: limiting log bytes @ 04/28/23 17:00:37.104
  Apr 28 17:00:37.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-logs-7619 logs logs-generator logs-generator --limit-bytes=1'
  Apr 28 17:00:37.228: INFO: stderr: ""
  Apr 28 17:00:37.228: INFO: stdout: "I"
  Apr 28 17:00:37.228: INFO: got output "I"
  STEP: exposing timestamps @ 04/28/23 17:00:37.228
  Apr 28 17:00:37.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-logs-7619 logs logs-generator logs-generator --tail=1 --timestamps'
  Apr 28 17:00:37.318: INFO: stderr: ""
  Apr 28 17:00:37.318: INFO: stdout: "2023-04-28T17:00:37.132534405Z I0428 17:00:37.132407       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/xbw 309\n"
  Apr 28 17:00:37.318: INFO: got output "2023-04-28T17:00:37.132534405Z I0428 17:00:37.132407       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/xbw 309\n"
  STEP: restricting to a time range @ 04/28/23 17:00:37.319
  E0428 17:00:37.472207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:38.472797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:39.472930      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:00:39.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-logs-7619 logs logs-generator logs-generator --since=1s'
  Apr 28 17:00:39.962: INFO: stderr: ""
  Apr 28 17:00:39.962: INFO: stdout: "I0428 17:00:39.132381       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/96bd 501\nI0428 17:00:39.332710       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/hq9k 220\nI0428 17:00:39.533095       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/54nd 241\nI0428 17:00:39.732426       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/r9mv 569\nI0428 17:00:39.932747       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/kube-system/pods/vpv 275\n"
  Apr 28 17:00:39.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-logs-7619 logs logs-generator logs-generator --since=24h'
  Apr 28 17:00:40.096: INFO: stderr: ""
  Apr 28 17:00:40.096: INFO: stdout: "I0428 17:00:35.532072       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/lst 256\nI0428 17:00:35.732160       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/bn7q 272\nI0428 17:00:35.932706       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/rjg8 538\nI0428 17:00:36.133016       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/d6lf 580\nI0428 17:00:36.332259       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/vn57 307\nI0428 17:00:36.532573       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/tknd 209\nI0428 17:00:36.733796       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/5dtg 516\nI0428 17:00:36.933116       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/fmmr 514\nI0428 17:00:37.132407       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/xbw 309\nI0428 17:00:37.332699       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/w2b 403\nI0428 17:00:37.533005       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/6ppz 526\nI0428 17:00:37.732159       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/7rt 538\nI0428 17:00:37.932601       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/9kfw 474\nI0428 17:00:38.132900       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/lfb 365\nI0428 17:00:38.332150       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/gg8q 436\nI0428 17:00:38.532455       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/bbs 339\nI0428 17:00:38.732767       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/h7t 557\nI0428 17:00:38.933082       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/jnjb 244\nI0428 17:00:39.132381       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/96bd 501\nI0428 17:00:39.332710       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/hq9k 220\nI0428 17:00:39.533095       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/54nd 241\nI0428 17:00:39.732426       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/r9mv 569\nI0428 17:00:39.932747       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/kube-system/pods/vpv 275\n"
  Apr 28 17:00:40.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-logs-7619 delete pod logs-generator'
  E0428 17:00:40.473679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:00:41.330: INFO: stderr: ""
  Apr 28 17:00:41.330: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Apr 28 17:00:41.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-7619" for this suite. @ 04/28/23 17:00:41.336
• [6.560 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 04/28/23 17:00:41.345
  Apr 28 17:00:41.345: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename statefulset @ 04/28/23 17:00:41.346
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:00:41.381
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:00:41.384
  STEP: Creating service test in namespace statefulset-2318 @ 04/28/23 17:00:41.387
  STEP: Looking for a node to schedule stateful set and pod @ 04/28/23 17:00:41.405
  STEP: Creating pod with conflicting port in namespace statefulset-2318 @ 04/28/23 17:00:41.424
  STEP: Waiting until pod test-pod will start running in namespace statefulset-2318 @ 04/28/23 17:00:41.433
  E0428 17:00:41.478754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:42.479411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating statefulset with conflicting port in namespace statefulset-2318 @ 04/28/23 17:00:43.445
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2318 @ 04/28/23 17:00:43.451
  Apr 28 17:00:43.472: INFO: Observed stateful pod in namespace: statefulset-2318, name: ss-0, uid: 15aa0ddd-634e-41d5-b08a-1d1c87b9f16f, status phase: Pending. Waiting for statefulset controller to delete.
  E0428 17:00:43.480271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:00:43.487: INFO: Observed stateful pod in namespace: statefulset-2318, name: ss-0, uid: 15aa0ddd-634e-41d5-b08a-1d1c87b9f16f, status phase: Failed. Waiting for statefulset controller to delete.
  Apr 28 17:00:43.495: INFO: Observed stateful pod in namespace: statefulset-2318, name: ss-0, uid: 15aa0ddd-634e-41d5-b08a-1d1c87b9f16f, status phase: Failed. Waiting for statefulset controller to delete.
  Apr 28 17:00:43.507: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2318
  STEP: Removing pod with conflicting port in namespace statefulset-2318 @ 04/28/23 17:00:43.507
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2318 and will be in running state @ 04/28/23 17:00:43.796
  E0428 17:00:44.480399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:45.480625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:00:45.807: INFO: Deleting all statefulset in ns statefulset-2318
  Apr 28 17:00:45.809: INFO: Scaling statefulset ss to 0
  E0428 17:00:46.481130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:47.481244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:48.481746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:49.481902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:50.482224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:51.482462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:52.482586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:53.482815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:54.483380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:55.483485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:00:55.824: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 17:00:55.827: INFO: Deleting statefulset ss
  Apr 28 17:00:55.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2318" for this suite. @ 04/28/23 17:00:55.846
• [14.505 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 04/28/23 17:00:55.851
  Apr 28 17:00:55.851: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename job @ 04/28/23 17:00:55.852
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:00:55.884
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:00:55.894
  STEP: Creating a job @ 04/28/23 17:00:55.898
  STEP: Ensuring active pods == parallelism @ 04/28/23 17:00:55.907
  E0428 17:00:56.483634      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:57.484023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete a job @ 04/28/23 17:00:57.912
  STEP: deleting Job.batch foo in namespace job-8515, will wait for the garbage collector to delete the pods @ 04/28/23 17:00:57.912
  Apr 28 17:00:57.971: INFO: Deleting Job.batch foo took: 6.251024ms
  Apr 28 17:00:58.073: INFO: Terminating Job.batch foo pods took: 101.127409ms
  E0428 17:00:58.484920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:00:59.486011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:00.486763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:01.487521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:02.488246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:03.488719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:04.489571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:05.490395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:06.491268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:07.491947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:08.493020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:09.493796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:10.494117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:11.494438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:12.495267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:13.495358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:14.496262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:15.496924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:16.497177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:17.497594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:18.497701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:19.498333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:20.498711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:21.499664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:22.500462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:23.500777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:24.500975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:25.501896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:26.502489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:27.503072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:28.503340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:29.504239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:30.504731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring job was deleted @ 04/28/23 17:01:30.773
  Apr 28 17:01:30.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8515" for this suite. @ 04/28/23 17:01:30.787
• [34.943 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 04/28/23 17:01:30.795
  Apr 28 17:01:30.795: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename lease-test @ 04/28/23 17:01:30.796
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:01:30.828
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:01:30.833
  Apr 28 17:01:30.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-50" for this suite. @ 04/28/23 17:01:30.907
• [0.119 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 04/28/23 17:01:30.914
  Apr 28 17:01:30.914: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename secrets @ 04/28/23 17:01:30.915
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:01:30.943
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:01:30.949
  STEP: Creating secret with name secret-test-a805c784-e87a-4943-ae58-8d8b1fb6872c @ 04/28/23 17:01:30.953
  STEP: Creating a pod to test consume secrets @ 04/28/23 17:01:30.964
  E0428 17:01:31.504856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:32.505084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:33.506040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:34.506165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:01:34.992
  Apr 28 17:01:34.995: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-secrets-07115e1b-77e2-4d12-a771-bc06abfa4257 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 17:01:35.006
  Apr 28 17:01:35.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4093" for this suite. @ 04/28/23 17:01:35.045
• [4.156 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 04/28/23 17:01:35.077
  Apr 28 17:01:35.077: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename deployment @ 04/28/23 17:01:35.08
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:01:35.106
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:01:35.117
  Apr 28 17:01:35.142: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0428 17:01:35.507213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:36.507406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:37.507549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:38.508191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:39.508292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:01:40.146: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/28/23 17:01:40.146
  Apr 28 17:01:40.146: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 04/28/23 17:01:40.154
  Apr 28 17:01:40.172: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5392  cc532791-3b13-4f3b-a331-ba42e01a8e8d 251401 1 2023-04-28 17:01:40 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-04-28 17:01:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004610f08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Apr 28 17:01:40.182: INFO: New ReplicaSet "test-cleanup-deployment-68b75d69f8" of Deployment "test-cleanup-deployment":
  &ReplicaSet{ObjectMeta:{test-cleanup-deployment-68b75d69f8  deployment-5392  ab80c6cf-bf77-4d00-963b-a2f28874aaea 251403 1 2023-04-28 17:01:40 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment cc532791-3b13-4f3b-a331-ba42e01a8e8d 0xc004611397 0xc004611398}] [] [{k3s Update apps/v1 2023-04-28 17:01:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc532791-3b13-4f3b-a331-ba42e01a8e8d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 68b75d69f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004611438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 17:01:40.182: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Apr 28 17:01:40.182: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-5392  886dfef0-4a18-4f29-b896-73ac1848bd9b 251402 1 2023-04-28 17:01:35 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment cc532791-3b13-4f3b-a331-ba42e01a8e8d 0xc004611267 0xc004611268}] [] [{e2e.test Update apps/v1 2023-04-28 17:01:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2023-04-28 17:01:36 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {k3s Update apps/v1 2023-04-28 17:01:40 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"cc532791-3b13-4f3b-a331-ba42e01a8e8d\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004611328 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 17:01:40.193: INFO: Pod "test-cleanup-controller-n9zkd" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-n9zkd test-cleanup-controller- deployment-5392  e72df643-31d3-41a6-8553-07deb5f19799 251377 0 2023-04-28 17:01:35 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 886dfef0-4a18-4f29-b896-73ac1848bd9b 0xc004611887 0xc004611888}] [] [{k3s Update v1 2023-04-28 17:01:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"886dfef0-4a18-4f29-b896-73ac1848bd9b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:01:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r6fw2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r6fw2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-213,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:01:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:01:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:01:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:01:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.213,PodIP:10.42.2.252,StartTime:2023-04-28 17:01:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:01:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6442005cd2362dea236884fd9ca2af22438d9cf3f58df32dae4b7bbeb4f46d67,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.252,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:01:40.194: INFO: Pod "test-cleanup-deployment-68b75d69f8-5sxx6" is not available:
  &Pod{ObjectMeta:{test-cleanup-deployment-68b75d69f8-5sxx6 test-cleanup-deployment-68b75d69f8- deployment-5392  5970cf12-dc92-4309-9082-94e33c5aba9b 251407 0 2023-04-28 17:01:40 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-68b75d69f8 ab80c6cf-bf77-4d00-963b-a2f28874aaea 0xc004611a87 0xc004611a88}] [] [{k3s Update v1 2023-04-28 17:01:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ab80c6cf-bf77-4d00-963b-a2f28874aaea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ttpgw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ttpgw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-213,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:01:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:01:40.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5392" for this suite. @ 04/28/23 17:01:40.201
• [5.142 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 04/28/23 17:01:40.216
  Apr 28 17:01:40.216: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 17:01:40.22
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:01:40.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:01:40.249
  Apr 28 17:01:40.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4072 version'
  Apr 28 17:01:40.503: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Apr 28 17:01:40.503: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:21:19Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1+k3s1\", GitCommit:\"bc5b42c27908ab430101eff0db0a0b22f870bd7a\", GitTreeState:\"clean\", BuildDate:\"2023-04-27T21:48:42Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Apr 28 17:01:40.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0428 17:01:40.511635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "kubectl-4072" for this suite. @ 04/28/23 17:01:40.512
• [0.304 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 04/28/23 17:01:40.526
  Apr 28 17:01:40.526: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 17:01:40.528
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:01:40.55
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:01:40.557
  STEP: creating a replication controller @ 04/28/23 17:01:40.567
  Apr 28 17:01:40.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 create -f -'
  E0428 17:01:41.512356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:01:41.544: INFO: stderr: ""
  Apr 28 17:01:41.544: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/28/23 17:01:41.544
  Apr 28 17:01:41.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 28 17:01:41.618: INFO: stderr: ""
  Apr 28 17:01:41.618: INFO: stdout: "update-demo-nautilus-gdc9p update-demo-nautilus-tvff8 "
  Apr 28 17:01:41.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 get pods update-demo-nautilus-gdc9p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 28 17:01:41.699: INFO: stderr: ""
  Apr 28 17:01:41.699: INFO: stdout: ""
  Apr 28 17:01:41.699: INFO: update-demo-nautilus-gdc9p is created but not running
  E0428 17:01:42.512672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:43.513540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:44.513786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:45.514147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:46.514150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:01:46.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 28 17:01:46.778: INFO: stderr: ""
  Apr 28 17:01:46.778: INFO: stdout: "update-demo-nautilus-gdc9p update-demo-nautilus-tvff8 "
  Apr 28 17:01:46.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 get pods update-demo-nautilus-gdc9p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 28 17:01:46.875: INFO: stderr: ""
  Apr 28 17:01:46.875: INFO: stdout: "true"
  Apr 28 17:01:46.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 get pods update-demo-nautilus-gdc9p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 28 17:01:46.949: INFO: stderr: ""
  Apr 28 17:01:46.949: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 28 17:01:46.949: INFO: validating pod update-demo-nautilus-gdc9p
  Apr 28 17:01:46.955: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 28 17:01:46.955: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 28 17:01:46.955: INFO: update-demo-nautilus-gdc9p is verified up and running
  Apr 28 17:01:46.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 get pods update-demo-nautilus-tvff8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 28 17:01:47.020: INFO: stderr: ""
  Apr 28 17:01:47.020: INFO: stdout: "true"
  Apr 28 17:01:47.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 get pods update-demo-nautilus-tvff8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 28 17:01:47.108: INFO: stderr: ""
  Apr 28 17:01:47.108: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 28 17:01:47.108: INFO: validating pod update-demo-nautilus-tvff8
  Apr 28 17:01:47.116: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 28 17:01:47.116: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 28 17:01:47.116: INFO: update-demo-nautilus-tvff8 is verified up and running
  STEP: scaling down the replication controller @ 04/28/23 17:01:47.116
  Apr 28 17:01:47.118: INFO: scanned /root for discovery docs: <nil>
  Apr 28 17:01:47.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  E0428 17:01:47.514817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:01:48.222: INFO: stderr: ""
  Apr 28 17:01:48.222: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/28/23 17:01:48.222
  Apr 28 17:01:48.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 28 17:01:48.285: INFO: stderr: ""
  Apr 28 17:01:48.285: INFO: stdout: "update-demo-nautilus-gdc9p "
  Apr 28 17:01:48.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 get pods update-demo-nautilus-gdc9p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 28 17:01:48.347: INFO: stderr: ""
  Apr 28 17:01:48.347: INFO: stdout: "true"
  Apr 28 17:01:48.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 get pods update-demo-nautilus-gdc9p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 28 17:01:48.423: INFO: stderr: ""
  Apr 28 17:01:48.423: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 28 17:01:48.423: INFO: validating pod update-demo-nautilus-gdc9p
  Apr 28 17:01:48.427: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 28 17:01:48.427: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 28 17:01:48.427: INFO: update-demo-nautilus-gdc9p is verified up and running
  STEP: scaling up the replication controller @ 04/28/23 17:01:48.427
  Apr 28 17:01:48.428: INFO: scanned /root for discovery docs: <nil>
  Apr 28 17:01:48.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  E0428 17:01:48.515226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:49.515824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:01:49.515: INFO: stderr: ""
  Apr 28 17:01:49.516: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/28/23 17:01:49.516
  Apr 28 17:01:49.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 28 17:01:49.580: INFO: stderr: ""
  Apr 28 17:01:49.580: INFO: stdout: "update-demo-nautilus-gdc9p update-demo-nautilus-kgbkv "
  Apr 28 17:01:49.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 get pods update-demo-nautilus-gdc9p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 28 17:01:49.643: INFO: stderr: ""
  Apr 28 17:01:49.643: INFO: stdout: "true"
  Apr 28 17:01:49.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 get pods update-demo-nautilus-gdc9p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 28 17:01:49.719: INFO: stderr: ""
  Apr 28 17:01:49.719: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 28 17:01:49.719: INFO: validating pod update-demo-nautilus-gdc9p
  Apr 28 17:01:49.722: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 28 17:01:49.722: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 28 17:01:49.722: INFO: update-demo-nautilus-gdc9p is verified up and running
  Apr 28 17:01:49.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 get pods update-demo-nautilus-kgbkv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 28 17:01:49.794: INFO: stderr: ""
  Apr 28 17:01:49.794: INFO: stdout: ""
  Apr 28 17:01:49.794: INFO: update-demo-nautilus-kgbkv is created but not running
  E0428 17:01:50.516386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:51.516718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:52.516863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:53.517216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:54.517581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:01:54.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 28 17:01:54.861: INFO: stderr: ""
  Apr 28 17:01:54.861: INFO: stdout: "update-demo-nautilus-gdc9p update-demo-nautilus-kgbkv "
  Apr 28 17:01:54.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 get pods update-demo-nautilus-gdc9p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 28 17:01:54.927: INFO: stderr: ""
  Apr 28 17:01:54.927: INFO: stdout: "true"
  Apr 28 17:01:54.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 get pods update-demo-nautilus-gdc9p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 28 17:01:54.997: INFO: stderr: ""
  Apr 28 17:01:54.997: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 28 17:01:54.997: INFO: validating pod update-demo-nautilus-gdc9p
  Apr 28 17:01:55.000: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 28 17:01:55.000: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 28 17:01:55.000: INFO: update-demo-nautilus-gdc9p is verified up and running
  Apr 28 17:01:55.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 get pods update-demo-nautilus-kgbkv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 28 17:01:55.130: INFO: stderr: ""
  Apr 28 17:01:55.130: INFO: stdout: "true"
  Apr 28 17:01:55.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 get pods update-demo-nautilus-kgbkv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 28 17:01:55.286: INFO: stderr: ""
  Apr 28 17:01:55.286: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 28 17:01:55.286: INFO: validating pod update-demo-nautilus-kgbkv
  Apr 28 17:01:55.293: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 28 17:01:55.293: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 28 17:01:55.293: INFO: update-demo-nautilus-kgbkv is verified up and running
  STEP: using delete to clean up resources @ 04/28/23 17:01:55.293
  Apr 28 17:01:55.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 delete --grace-period=0 --force -f -'
  Apr 28 17:01:55.377: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 28 17:01:55.377: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Apr 28 17:01:55.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 get rc,svc -l name=update-demo --no-headers'
  Apr 28 17:01:55.470: INFO: stderr: "No resources found in kubectl-4711 namespace.\n"
  Apr 28 17:01:55.470: INFO: stdout: ""
  Apr 28 17:01:55.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4711 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  E0428 17:01:55.517899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:01:55.535: INFO: stderr: ""
  Apr 28 17:01:55.535: INFO: stdout: ""
  Apr 28 17:01:55.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4711" for this suite. @ 04/28/23 17:01:55.55
• [15.038 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 04/28/23 17:01:55.564
  Apr 28 17:01:55.565: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename subjectreview @ 04/28/23 17:01:55.565
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:01:55.59
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:01:55.593
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-1902" @ 04/28/23 17:01:55.598
  Apr 28 17:01:55.605: INFO: saUsername: "system:serviceaccount:subjectreview-1902:e2e"
  Apr 28 17:01:55.605: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-1902"}
  Apr 28 17:01:55.605: INFO: saUID: "a631244f-8c21-4611-9ef0-83ac0a7d2a48"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-1902:e2e" @ 04/28/23 17:01:55.605
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-1902:e2e" @ 04/28/23 17:01:55.605
  Apr 28 17:01:55.608: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-1902:e2e" api 'list' configmaps in "subjectreview-1902" namespace @ 04/28/23 17:01:55.608
  Apr 28 17:01:55.610: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-1902:e2e" @ 04/28/23 17:01:55.61
  Apr 28 17:01:55.613: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Apr 28 17:01:55.613: INFO: LocalSubjectAccessReview has been verified
  Apr 28 17:01:55.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-1902" for this suite. @ 04/28/23 17:01:55.621
• [0.064 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 04/28/23 17:01:55.629
  Apr 28 17:01:55.629: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename services @ 04/28/23 17:01:55.63
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:01:55.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:01:55.709
  STEP: creating service in namespace services-7682 @ 04/28/23 17:01:55.712
  STEP: creating service affinity-clusterip-transition in namespace services-7682 @ 04/28/23 17:01:55.712
  STEP: creating replication controller affinity-clusterip-transition in namespace services-7682 @ 04/28/23 17:01:55.745
  I0428 17:01:55.756625      19 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-7682, replica count: 3
  E0428 17:01:56.518850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:57.519925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:58.520389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 17:01:58.808134      19 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 28 17:01:58.825: INFO: Creating new exec pod
  E0428 17:01:59.520459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:00.520557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:01.521179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:02:01.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-7682 exec execpod-affinitytp26w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Apr 28 17:02:02.017: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Apr 28 17:02:02.017: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:02:02.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-7682 exec execpod-affinitytp26w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.113.81 80'
  Apr 28 17:02:02.276: INFO: stderr: "+ nc -v -t -w 2 10.43.113.81 80\nConnection to 10.43.113.81 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Apr 28 17:02:02.276: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:02:02.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-7682 exec execpod-affinitytp26w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.113.81:80/ ; done'
  E0428 17:02:02.521558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:02:02.730: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n"
  Apr 28 17:02:02.730: INFO: stdout: "\naffinity-clusterip-transition-qcwpl\naffinity-clusterip-transition-29h6c\naffinity-clusterip-transition-hp6cq\naffinity-clusterip-transition-29h6c\naffinity-clusterip-transition-29h6c\naffinity-clusterip-transition-qcwpl\naffinity-clusterip-transition-29h6c\naffinity-clusterip-transition-hp6cq\naffinity-clusterip-transition-29h6c\naffinity-clusterip-transition-qcwpl\naffinity-clusterip-transition-hp6cq\naffinity-clusterip-transition-qcwpl\naffinity-clusterip-transition-hp6cq\naffinity-clusterip-transition-qcwpl\naffinity-clusterip-transition-qcwpl\naffinity-clusterip-transition-qcwpl"
  Apr 28 17:02:02.730: INFO: Received response from host: affinity-clusterip-transition-qcwpl
  Apr 28 17:02:02.730: INFO: Received response from host: affinity-clusterip-transition-29h6c
  Apr 28 17:02:02.730: INFO: Received response from host: affinity-clusterip-transition-hp6cq
  Apr 28 17:02:02.730: INFO: Received response from host: affinity-clusterip-transition-29h6c
  Apr 28 17:02:02.730: INFO: Received response from host: affinity-clusterip-transition-29h6c
  Apr 28 17:02:02.730: INFO: Received response from host: affinity-clusterip-transition-qcwpl
  Apr 28 17:02:02.730: INFO: Received response from host: affinity-clusterip-transition-29h6c
  Apr 28 17:02:02.730: INFO: Received response from host: affinity-clusterip-transition-hp6cq
  Apr 28 17:02:02.730: INFO: Received response from host: affinity-clusterip-transition-29h6c
  Apr 28 17:02:02.730: INFO: Received response from host: affinity-clusterip-transition-qcwpl
  Apr 28 17:02:02.730: INFO: Received response from host: affinity-clusterip-transition-hp6cq
  Apr 28 17:02:02.730: INFO: Received response from host: affinity-clusterip-transition-qcwpl
  Apr 28 17:02:02.730: INFO: Received response from host: affinity-clusterip-transition-hp6cq
  Apr 28 17:02:02.730: INFO: Received response from host: affinity-clusterip-transition-qcwpl
  Apr 28 17:02:02.730: INFO: Received response from host: affinity-clusterip-transition-qcwpl
  Apr 28 17:02:02.730: INFO: Received response from host: affinity-clusterip-transition-qcwpl
  Apr 28 17:02:02.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-7682 exec execpod-affinitytp26w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.113.81:80/ ; done'
  Apr 28 17:02:03.085: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.113.81:80/\n"
  Apr 28 17:02:03.085: INFO: stdout: "\naffinity-clusterip-transition-hp6cq\naffinity-clusterip-transition-hp6cq\naffinity-clusterip-transition-hp6cq\naffinity-clusterip-transition-hp6cq\naffinity-clusterip-transition-hp6cq\naffinity-clusterip-transition-hp6cq\naffinity-clusterip-transition-hp6cq\naffinity-clusterip-transition-hp6cq\naffinity-clusterip-transition-hp6cq\naffinity-clusterip-transition-hp6cq\naffinity-clusterip-transition-hp6cq\naffinity-clusterip-transition-hp6cq\naffinity-clusterip-transition-hp6cq\naffinity-clusterip-transition-hp6cq\naffinity-clusterip-transition-hp6cq\naffinity-clusterip-transition-hp6cq"
  Apr 28 17:02:03.086: INFO: Received response from host: affinity-clusterip-transition-hp6cq
  Apr 28 17:02:03.086: INFO: Received response from host: affinity-clusterip-transition-hp6cq
  Apr 28 17:02:03.086: INFO: Received response from host: affinity-clusterip-transition-hp6cq
  Apr 28 17:02:03.086: INFO: Received response from host: affinity-clusterip-transition-hp6cq
  Apr 28 17:02:03.086: INFO: Received response from host: affinity-clusterip-transition-hp6cq
  Apr 28 17:02:03.086: INFO: Received response from host: affinity-clusterip-transition-hp6cq
  Apr 28 17:02:03.086: INFO: Received response from host: affinity-clusterip-transition-hp6cq
  Apr 28 17:02:03.086: INFO: Received response from host: affinity-clusterip-transition-hp6cq
  Apr 28 17:02:03.086: INFO: Received response from host: affinity-clusterip-transition-hp6cq
  Apr 28 17:02:03.086: INFO: Received response from host: affinity-clusterip-transition-hp6cq
  Apr 28 17:02:03.086: INFO: Received response from host: affinity-clusterip-transition-hp6cq
  Apr 28 17:02:03.086: INFO: Received response from host: affinity-clusterip-transition-hp6cq
  Apr 28 17:02:03.086: INFO: Received response from host: affinity-clusterip-transition-hp6cq
  Apr 28 17:02:03.086: INFO: Received response from host: affinity-clusterip-transition-hp6cq
  Apr 28 17:02:03.086: INFO: Received response from host: affinity-clusterip-transition-hp6cq
  Apr 28 17:02:03.086: INFO: Received response from host: affinity-clusterip-transition-hp6cq
  Apr 28 17:02:03.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 17:02:03.090: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7682, will wait for the garbage collector to delete the pods @ 04/28/23 17:02:03.108
  Apr 28 17:02:03.214: INFO: Deleting ReplicationController affinity-clusterip-transition took: 37.789404ms
  Apr 28 17:02:03.314: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.424966ms
  E0428 17:02:03.521629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:04.522503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:05.523521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-7682" for this suite. @ 04/28/23 17:02:05.758
• [10.148 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 04/28/23 17:02:05.778
  Apr 28 17:02:05.778: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 17:02:05.778
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:02:05.815
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:02:05.82
  STEP: Creating Pod @ 04/28/23 17:02:05.827
  E0428 17:02:06.524367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:07.525447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 04/28/23 17:02:07.88
  Apr 28 17:02:07.880: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7981 PodName:pod-sharedvolume-1a54b759-7f20-49db-8ff5-c9fce644ed2e ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:02:07.880: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 17:02:07.880: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:02:07.880: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/emptydir-7981/pods/pod-sharedvolume-1a54b759-7f20-49db-8ff5-c9fce644ed2e/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Apr 28 17:02:07.993: INFO: Exec stderr: ""
  Apr 28 17:02:07.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7981" for this suite. @ 04/28/23 17:02:08
• [2.239 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 04/28/23 17:02:08.018
  Apr 28 17:02:08.018: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename field-validation @ 04/28/23 17:02:08.018
  E0428 17:02:08.526192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:02:08.694
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:02:08.701
  Apr 28 17:02:08.704: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 17:02:09.526746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:10.526949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:02:11.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2132" for this suite. @ 04/28/23 17:02:11.298
• [3.286 seconds]
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 04/28/23 17:02:11.304
  Apr 28 17:02:11.304: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename var-expansion @ 04/28/23 17:02:11.305
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:02:11.321
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:02:11.324
  STEP: Creating a pod to test env composition @ 04/28/23 17:02:11.328
  E0428 17:02:11.527348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:12.528368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:13.529404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:14.529531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:02:15.36
  Apr 28 17:02:15.363: INFO: Trying to get logs from node ip-172-31-10-53 pod var-expansion-e91a6ada-7a08-4034-9af9-4b6a7c160624 container dapi-container: <nil>
  STEP: delete the pod @ 04/28/23 17:02:15.374
  Apr 28 17:02:15.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4524" for this suite. @ 04/28/23 17:02:15.397
• [4.101 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 04/28/23 17:02:15.406
  Apr 28 17:02:15.406: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 17:02:15.407
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:02:15.429
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:02:15.432
  STEP: Counting existing ResourceQuota @ 04/28/23 17:02:15.435
  E0428 17:02:15.529955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:16.530040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:17.530281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:18.530340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:19.531283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/28/23 17:02:20.439
  STEP: Ensuring resource quota status is calculated @ 04/28/23 17:02:20.451
  E0428 17:02:20.532110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:21.532258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicaSet @ 04/28/23 17:02:22.455
  STEP: Ensuring resource quota status captures replicaset creation @ 04/28/23 17:02:22.466
  E0428 17:02:22.533016      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:23.533346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicaSet @ 04/28/23 17:02:24.47
  STEP: Ensuring resource quota status released usage @ 04/28/23 17:02:24.478
  E0428 17:02:24.533759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:25.533962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:02:26.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8490" for this suite. @ 04/28/23 17:02:26.487
• [11.086 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 04/28/23 17:02:26.494
  Apr 28 17:02:26.494: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename statefulset @ 04/28/23 17:02:26.495
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:02:26.524
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:02:26.53
  STEP: Creating service test in namespace statefulset-7907 @ 04/28/23 17:02:26.533
  E0428 17:02:26.536599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating stateful set ss in namespace statefulset-7907 @ 04/28/23 17:02:26.548
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7907 @ 04/28/23 17:02:26.738
  Apr 28 17:02:26.751: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
  E0428 17:02:27.536723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:28.537544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:29.537766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:30.537900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:31.538026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:32.538160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:33.538522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:34.540986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:35.541062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:36.541224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:02:36.756: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 04/28/23 17:02:36.756
  Apr 28 17:02:36.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=statefulset-7907 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 28 17:02:37.030: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 28 17:02:37.030: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 28 17:02:37.030: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 28 17:02:37.034: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0428 17:02:37.541564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:38.542281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:39.542494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:40.542680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:41.543623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:42.543745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:43.544112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:44.544309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:45.544499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:46.544701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:02:47.039: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 28 17:02:47.039: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 17:02:47.057: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
  Apr 28 17:02:47.057: INFO: ss-0  ip-172-31-1-213  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:26 +0000 UTC  }]
  Apr 28 17:02:47.057: INFO: 
  Apr 28 17:02:47.057: INFO: StatefulSet ss has not reached scale 3, at 1
  E0428 17:02:47.545208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:02:48.065: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995459737s
  E0428 17:02:48.546345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:02:49.069: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987297447s
  E0428 17:02:49.546426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:02:50.260: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983263432s
  E0428 17:02:50.547281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:02:51.264: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.792244948s
  E0428 17:02:51.547963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:02:52.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.788404313s
  E0428 17:02:52.548963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:02:53.272: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.784531341s
  E0428 17:02:53.549891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:02:54.275: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.780732002s
  E0428 17:02:54.550168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:02:55.286: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.777263125s
  E0428 17:02:55.550855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:02:56.291: INFO: Verifying statefulset ss doesn't scale past 3 for another 766.47875ms
  E0428 17:02:56.551320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7907 @ 04/28/23 17:02:57.291
  Apr 28 17:02:57.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=statefulset-7907 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 28 17:02:57.486: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 28 17:02:57.486: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 28 17:02:57.486: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 28 17:02:57.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=statefulset-7907 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0428 17:02:57.552353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:02:57.860: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Apr 28 17:02:57.860: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 28 17:02:57.860: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 28 17:02:57.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=statefulset-7907 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 28 17:02:58.055: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Apr 28 17:02:58.056: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 28 17:02:58.056: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 28 17:02:58.059: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 17:02:58.059: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 17:02:58.059: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 04/28/23 17:02:58.059
  Apr 28 17:02:58.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=statefulset-7907 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 28 17:02:58.209: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 28 17:02:58.209: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 28 17:02:58.209: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 28 17:02:58.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=statefulset-7907 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 28 17:02:58.354: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 28 17:02:58.354: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 28 17:02:58.354: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 28 17:02:58.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=statefulset-7907 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0428 17:02:58.553879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:02:58.748: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 28 17:02:58.748: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 28 17:02:58.748: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 28 17:02:58.748: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 17:02:58.752: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
  E0428 17:02:59.552947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:00.553144      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:01.553310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:02.553445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:03.553523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:04.553670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:05.553799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:06.553931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:07.554126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:08.554655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:08.759: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 28 17:03:08.759: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Apr 28 17:03:08.759: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Apr 28 17:03:08.773: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
  Apr 28 17:03:08.773: INFO: ss-0  ip-172-31-1-213  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:26 +0000 UTC  }]
  Apr 28 17:03:08.773: INFO: ss-1  ip-172-31-10-53  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:47 +0000 UTC  }]
  Apr 28 17:03:08.773: INFO: ss-2  ip-172-31-5-174  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:47 +0000 UTC  }]
  Apr 28 17:03:08.773: INFO: 
  Apr 28 17:03:08.773: INFO: StatefulSet ss has not reached scale 0, at 3
  E0428 17:03:09.555114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:09.777: INFO: POD   NODE             PHASE      GRACE  CONDITIONS
  Apr 28 17:03:09.777: INFO: ss-0  ip-172-31-1-213  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:26 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:58 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:58 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:26 +0000 UTC  }]
  Apr 28 17:03:09.777: INFO: ss-1  ip-172-31-10-53  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:47 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:58 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:58 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 17:02:47 +0000 UTC  }]
  Apr 28 17:03:09.777: INFO: 
  Apr 28 17:03:09.777: INFO: StatefulSet ss has not reached scale 0, at 2
  E0428 17:03:10.555834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:10.780: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.990477939s
  E0428 17:03:11.555959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:11.815: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.987348585s
  E0428 17:03:12.556821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:12.819: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.951402588s
  E0428 17:03:13.557579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:14.096: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.947658879s
  E0428 17:03:14.558280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:15.101: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.6706978s
  E0428 17:03:15.558894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:16.104: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.666356683s
  E0428 17:03:16.559951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:17.108: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.663002659s
  E0428 17:03:17.560351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:18.117: INFO: Verifying statefulset ss doesn't scale past 0 for another 658.71775ms
  E0428 17:03:18.560441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7907 @ 04/28/23 17:03:19.118
  Apr 28 17:03:19.121: INFO: Scaling statefulset ss to 0
  Apr 28 17:03:19.132: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 17:03:19.135: INFO: Deleting all statefulset in ns statefulset-7907
  Apr 28 17:03:19.137: INFO: Scaling statefulset ss to 0
  Apr 28 17:03:19.149: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 17:03:19.151: INFO: Deleting statefulset ss
  Apr 28 17:03:19.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7907" for this suite. @ 04/28/23 17:03:19.171
• [52.683 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 04/28/23 17:03:19.179
  Apr 28 17:03:19.179: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/28/23 17:03:19.18
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:03:19.2
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:03:19.211
  Apr 28 17:03:19.217: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 17:03:19.561153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:20.561702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/28/23 17:03:20.88
  Apr 28 17:03:20.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-9225 --namespace=crd-publish-openapi-9225 create -f -'
  E0428 17:03:21.562622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:21.933: INFO: stderr: ""
  Apr 28 17:03:21.933: INFO: stdout: "e2e-test-crd-publish-openapi-5772-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Apr 28 17:03:21.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-9225 --namespace=crd-publish-openapi-9225 delete e2e-test-crd-publish-openapi-5772-crds test-cr'
  E0428 17:03:22.562713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:23.120: INFO: stderr: ""
  Apr 28 17:03:23.121: INFO: stdout: "e2e-test-crd-publish-openapi-5772-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Apr 28 17:03:23.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-9225 --namespace=crd-publish-openapi-9225 apply -f -'
  E0428 17:03:23.563080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:24.060: INFO: stderr: ""
  Apr 28 17:03:24.060: INFO: stdout: "e2e-test-crd-publish-openapi-5772-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Apr 28 17:03:24.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-9225 --namespace=crd-publish-openapi-9225 delete e2e-test-crd-publish-openapi-5772-crds test-cr'
  Apr 28 17:03:24.129: INFO: stderr: ""
  Apr 28 17:03:24.130: INFO: stdout: "e2e-test-crd-publish-openapi-5772-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 04/28/23 17:03:24.13
  Apr 28 17:03:24.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-9225 explain e2e-test-crd-publish-openapi-5772-crds'
  E0428 17:03:24.563989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:25.075: INFO: stderr: ""
  Apr 28 17:03:25.075: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-5772-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0428 17:03:25.564968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:26.565731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:26.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9225" for this suite. @ 04/28/23 17:03:26.685
• [7.510 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 04/28/23 17:03:26.693
  Apr 28 17:03:26.693: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename configmap @ 04/28/23 17:03:26.693
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:03:26.711
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:03:26.716
  STEP: Creating configMap with name configmap-test-volume-map-a0701e7a-c4ec-4a5d-9322-58356d53121c @ 04/28/23 17:03:26.719
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:03:26.724
  E0428 17:03:27.565788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:28.566419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:29.567448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:30.567806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:03:30.75
  Apr 28 17:03:30.752: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-configmaps-e1a3ff9c-9971-49cb-be88-965c938fa129 container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 17:03:30.761
  Apr 28 17:03:30.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5467" for this suite. @ 04/28/23 17:03:30.784
• [4.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 04/28/23 17:03:30.794
  Apr 28 17:03:30.794: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename statefulset @ 04/28/23 17:03:30.796
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:03:30.819
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:03:30.824
  STEP: Creating service test in namespace statefulset-7261 @ 04/28/23 17:03:30.826
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 04/28/23 17:03:30.838
  STEP: Creating stateful set ss in namespace statefulset-7261 @ 04/28/23 17:03:30.847
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7261 @ 04/28/23 17:03:30.855
  Apr 28 17:03:30.863: INFO: Found 0 stateful pods, waiting for 1
  E0428 17:03:31.568577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:32.568731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:33.569637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:34.569822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:35.569927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:36.570032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:37.570099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:38.570160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:39.570384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:40.570455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:40.867: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 04/28/23 17:03:40.867
  Apr 28 17:03:40.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=statefulset-7261 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 28 17:03:41.002: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 28 17:03:41.002: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 28 17:03:41.002: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 28 17:03:41.009: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0428 17:03:41.571102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:42.571270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:43.571672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:44.571814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:45.572032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:46.572322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:47.572571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:48.573266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:49.573477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:50.573610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:51.013: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 28 17:03:51.013: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 17:03:51.025: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999705s
  E0428 17:03:51.573769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:52.030: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996459555s
  E0428 17:03:52.573870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:53.034: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992404675s
  E0428 17:03:53.574336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:54.037: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988221114s
  E0428 17:03:54.575345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:55.040: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.984971097s
  E0428 17:03:55.576363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:56.043: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.981940066s
  E0428 17:03:56.577395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:57.048: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.978893737s
  E0428 17:03:57.577543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:58.052: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.973645866s
  E0428 17:03:58.578281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:59.055: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.969821367s
  E0428 17:03:59.578473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:00.059: INFO: Verifying statefulset ss doesn't scale past 1 for another 966.815756ms
  E0428 17:04:00.578610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7261 @ 04/28/23 17:04:01.06
  Apr 28 17:04:01.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=statefulset-7261 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 28 17:04:01.208: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 28 17:04:01.208: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 28 17:04:01.208: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 28 17:04:01.212: INFO: Found 1 stateful pods, waiting for 3
  E0428 17:04:01.580330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:02.580526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:03.581256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:04.581404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:05.581535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:06.581767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:07.581919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:08.582353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:09.582565      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:10.582696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:11.219: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 17:04:11.219: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 17:04:11.219: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 04/28/23 17:04:11.22
  STEP: Scale down will halt with unhealthy stateful pod @ 04/28/23 17:04:11.22
  Apr 28 17:04:11.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=statefulset-7261 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 28 17:04:11.359: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 28 17:04:11.359: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 28 17:04:11.359: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 28 17:04:11.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=statefulset-7261 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0428 17:04:11.583704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:11.698: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 28 17:04:11.698: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 28 17:04:11.698: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 28 17:04:11.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=statefulset-7261 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 28 17:04:11.818: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 28 17:04:11.818: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 28 17:04:11.818: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 28 17:04:11.818: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 17:04:11.820: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0428 17:04:12.584405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:13.584436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:14.584638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:15.584802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:16.584929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:17.585084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:18.585201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:19.585294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:20.585432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:21.585612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:21.828: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 28 17:04:21.828: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Apr 28 17:04:21.828: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Apr 28 17:04:21.841: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999721s
  E0428 17:04:22.585766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:22.845: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997120093s
  E0428 17:04:23.586191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:23.849: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992656475s
  E0428 17:04:24.586256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:24.854: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989630376s
  E0428 17:04:25.586284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:25.857: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984554526s
  E0428 17:04:26.586418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:26.862: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980419497s
  E0428 17:04:27.586528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:27.865: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.976284907s
  E0428 17:04:28.587056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:28.870: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.972678599s
  E0428 17:04:29.587282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:29.874: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.968535839s
  E0428 17:04:30.587729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:30.877: INFO: Verifying statefulset ss doesn't scale past 3 for another 965.104584ms
  E0428 17:04:31.588185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7261 @ 04/28/23 17:04:31.877
  Apr 28 17:04:31.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=statefulset-7261 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 28 17:04:32.128: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 28 17:04:32.128: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 28 17:04:32.128: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 28 17:04:32.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=statefulset-7261 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 28 17:04:32.342: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 28 17:04:32.342: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 28 17:04:32.342: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 28 17:04:32.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=statefulset-7261 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 28 17:04:32.536: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 28 17:04:32.536: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 28 17:04:32.536: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 28 17:04:32.536: INFO: Scaling statefulset ss to 0
  E0428 17:04:32.589242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:33.589980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:34.590517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:35.591073      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:36.591959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:37.592063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:38.592284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:39.592473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:40.592691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:41.592850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 04/28/23 17:04:42.551
  Apr 28 17:04:42.551: INFO: Deleting all statefulset in ns statefulset-7261
  Apr 28 17:04:42.554: INFO: Scaling statefulset ss to 0
  Apr 28 17:04:42.562: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 17:04:42.566: INFO: Deleting statefulset ss
  Apr 28 17:04:42.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0428 17:04:42.593183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "statefulset-7261" for this suite. @ 04/28/23 17:04:42.595
• [71.808 seconds]
------------------------------
SSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 04/28/23 17:04:42.604
  Apr 28 17:04:42.604: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename limitrange @ 04/28/23 17:04:42.605
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:04:42.628
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:04:42.632
  STEP: Creating LimitRange "e2e-limitrange-b95mt" in namespace "limitrange-5701" @ 04/28/23 17:04:42.637
  STEP: Creating another limitRange in another namespace @ 04/28/23 17:04:42.647
  Apr 28 17:04:42.667: INFO: Namespace "e2e-limitrange-b95mt-6256" created
  Apr 28 17:04:42.667: INFO: Creating LimitRange "e2e-limitrange-b95mt" in namespace "e2e-limitrange-b95mt-6256"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-b95mt" @ 04/28/23 17:04:42.675
  Apr 28 17:04:42.680: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-b95mt" in "limitrange-5701" namespace @ 04/28/23 17:04:42.68
  Apr 28 17:04:42.689: INFO: LimitRange "e2e-limitrange-b95mt" has been patched
  STEP: Delete LimitRange "e2e-limitrange-b95mt" by Collection with labelSelector: "e2e-limitrange-b95mt=patched" @ 04/28/23 17:04:42.689
  STEP: Confirm that the limitRange "e2e-limitrange-b95mt" has been deleted @ 04/28/23 17:04:42.708
  Apr 28 17:04:42.709: INFO: Requesting list of LimitRange to confirm quantity
  Apr 28 17:04:42.714: INFO: Found 0 LimitRange with label "e2e-limitrange-b95mt=patched"
  Apr 28 17:04:42.714: INFO: LimitRange "e2e-limitrange-b95mt" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-b95mt" @ 04/28/23 17:04:42.714
  Apr 28 17:04:42.720: INFO: Found 1 limitRange
  Apr 28 17:04:42.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-5701" for this suite. @ 04/28/23 17:04:42.728
  STEP: Destroying namespace "e2e-limitrange-b95mt-6256" for this suite. @ 04/28/23 17:04:42.737
• [0.155 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 04/28/23 17:04:42.759
  Apr 28 17:04:42.759: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename watch @ 04/28/23 17:04:42.76
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:04:42.793
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:04:42.8
  STEP: creating a watch on configmaps with a certain label @ 04/28/23 17:04:42.804
  STEP: creating a new configmap @ 04/28/23 17:04:42.807
  STEP: modifying the configmap once @ 04/28/23 17:04:42.818
  STEP: changing the label value of the configmap @ 04/28/23 17:04:42.831
  STEP: Expecting to observe a delete notification for the watched object @ 04/28/23 17:04:42.846
  Apr 28 17:04:42.846: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1070  dc9f0a81-a869-4fe5-a60a-7e266e3794b1 253014 0 2023-04-28 17:04:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-28 17:04:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 17:04:42.846: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1070  dc9f0a81-a869-4fe5-a60a-7e266e3794b1 253015 0 2023-04-28 17:04:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-28 17:04:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 17:04:42.847: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1070  dc9f0a81-a869-4fe5-a60a-7e266e3794b1 253016 0 2023-04-28 17:04:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-28 17:04:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 04/28/23 17:04:42.847
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 04/28/23 17:04:42.859
  E0428 17:04:43.594081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:44.594355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:45.595396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:46.595559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:47.595713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:48.596297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:49.596493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:50.596738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:51.596951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:52.597130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 04/28/23 17:04:52.859
  STEP: modifying the configmap a third time @ 04/28/23 17:04:52.866
  STEP: deleting the configmap @ 04/28/23 17:04:52.872
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 04/28/23 17:04:52.876
  Apr 28 17:04:52.876: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1070  dc9f0a81-a869-4fe5-a60a-7e266e3794b1 253104 0 2023-04-28 17:04:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-28 17:04:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 17:04:52.877: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1070  dc9f0a81-a869-4fe5-a60a-7e266e3794b1 253105 0 2023-04-28 17:04:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-28 17:04:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 17:04:52.877: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1070  dc9f0a81-a869-4fe5-a60a-7e266e3794b1 253106 0 2023-04-28 17:04:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-28 17:04:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 17:04:52.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-1070" for this suite. @ 04/28/23 17:04:52.88
• [10.126 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 04/28/23 17:04:52.886
  Apr 28 17:04:52.886: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename webhook @ 04/28/23 17:04:52.886
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:04:52.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:04:52.912
  STEP: Setting up server cert @ 04/28/23 17:04:52.941
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 17:04:53.366
  STEP: Deploying the webhook pod @ 04/28/23 17:04:53.372
  STEP: Wait for the deployment to be ready @ 04/28/23 17:04:53.383
  Apr 28 17:04:53.394: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0428 17:04:53.597959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:54.598045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:55.407: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 4, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 4, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 4, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 4, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:04:55.598474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:56.598556      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:57.410: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 4, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 4, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 4, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 4, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:04:57.599242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:58.599682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:59.410: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 4, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 4, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 4, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 4, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:04:59.600279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:00.600389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:05:01.411: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 4, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 4, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 4, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 4, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:05:01.600521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:02.600624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 17:05:03.411
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 17:05:03.421
  E0428 17:05:03.600770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:05:04.421: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 28 17:05:04.431: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 17:05:04.600938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 04/28/23 17:05:04.961
  STEP: Creating a custom resource that should be denied by the webhook @ 04/28/23 17:05:04.99
  E0428 17:05:05.601311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:06.601777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 04/28/23 17:05:07.014
  STEP: Updating the custom resource with disallowed data should be denied @ 04/28/23 17:05:07.02
  STEP: Deleting the custom resource should be denied @ 04/28/23 17:05:07.029
  STEP: Remove the offending key and value from the custom resource data @ 04/28/23 17:05:07.034
  STEP: Deleting the updated custom resource should be successful @ 04/28/23 17:05:07.044
  Apr 28 17:05:07.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0428 17:05:07.601864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-6437" for this suite. @ 04/28/23 17:05:07.677
  STEP: Destroying namespace "webhook-markers-4578" for this suite. @ 04/28/23 17:05:07.681
• [14.800 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 04/28/23 17:05:07.686
  Apr 28 17:05:07.687: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/28/23 17:05:07.688
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:05:07.751
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:05:07.755
  STEP: set up a multi version CRD @ 04/28/23 17:05:07.759
  Apr 28 17:05:07.759: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 17:05:08.602237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:09.602679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:10.603281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mark a version not serverd @ 04/28/23 17:05:11.423
  STEP: check the unserved version gets removed @ 04/28/23 17:05:11.441
  E0428 17:05:11.603634      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:12.604384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:13.604720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 04/28/23 17:05:13.688
  E0428 17:05:14.605242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:15.606280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:16.606375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:05:16.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4241" for this suite. @ 04/28/23 17:05:16.758
• [9.079 seconds]
------------------------------
S
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 04/28/23 17:05:16.766
  Apr 28 17:05:16.766: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename security-context @ 04/28/23 17:05:16.767
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:05:16.791
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:05:16.794
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 04/28/23 17:05:16.796
  E0428 17:05:17.607075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:18.607754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:19.607865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:20.607933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:05:20.82
  Apr 28 17:05:20.827: INFO: Trying to get logs from node ip-172-31-1-213 pod security-context-ca437503-18b6-448a-bdd3-f66de0e17789 container test-container: <nil>
  STEP: delete the pod @ 04/28/23 17:05:20.847
  Apr 28 17:05:20.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-325" for this suite. @ 04/28/23 17:05:20.879
• [4.144 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 04/28/23 17:05:20.91
  Apr 28 17:05:20.910: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename proxy @ 04/28/23 17:05:20.911
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:05:20.935
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:05:20.939
  Apr 28 17:05:20.942: INFO: Creating pod...
  E0428 17:05:21.608106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:22.608151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:05:22.955: INFO: Creating service...
  Apr 28 17:05:22.990: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7956/pods/agnhost/proxy/some/path/with/DELETE
  Apr 28 17:05:23.038: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 28 17:05:23.039: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7956/pods/agnhost/proxy/some/path/with/GET
  Apr 28 17:05:23.051: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Apr 28 17:05:23.051: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7956/pods/agnhost/proxy/some/path/with/HEAD
  Apr 28 17:05:23.062: INFO: http.Client request:HEAD | StatusCode:200
  Apr 28 17:05:23.062: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7956/pods/agnhost/proxy/some/path/with/OPTIONS
  Apr 28 17:05:23.067: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 28 17:05:23.067: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7956/pods/agnhost/proxy/some/path/with/PATCH
  Apr 28 17:05:23.071: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 28 17:05:23.071: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7956/pods/agnhost/proxy/some/path/with/POST
  Apr 28 17:05:23.085: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 28 17:05:23.085: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7956/pods/agnhost/proxy/some/path/with/PUT
  Apr 28 17:05:23.094: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 28 17:05:23.094: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7956/services/test-service/proxy/some/path/with/DELETE
  Apr 28 17:05:23.104: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 28 17:05:23.104: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7956/services/test-service/proxy/some/path/with/GET
  Apr 28 17:05:23.109: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Apr 28 17:05:23.109: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7956/services/test-service/proxy/some/path/with/HEAD
  Apr 28 17:05:23.114: INFO: http.Client request:HEAD | StatusCode:200
  Apr 28 17:05:23.114: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7956/services/test-service/proxy/some/path/with/OPTIONS
  Apr 28 17:05:23.129: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 28 17:05:23.129: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7956/services/test-service/proxy/some/path/with/PATCH
  Apr 28 17:05:23.133: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 28 17:05:23.133: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7956/services/test-service/proxy/some/path/with/POST
  Apr 28 17:05:23.138: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 28 17:05:23.138: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7956/services/test-service/proxy/some/path/with/PUT
  Apr 28 17:05:23.143: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 28 17:05:23.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-7956" for this suite. @ 04/28/23 17:05:23.148
• [2.248 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 04/28/23 17:05:23.158
  Apr 28 17:05:23.158: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename svcaccounts @ 04/28/23 17:05:23.159
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:05:23.189
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:05:23.197
  STEP: Creating a pod to test service account token:  @ 04/28/23 17:05:23.204
  E0428 17:05:23.608964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:24.609057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:25.609214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:26.609453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:05:27.273
  Apr 28 17:05:27.276: INFO: Trying to get logs from node ip-172-31-10-53 pod test-pod-f72d1970-b666-4f3b-a466-186b12850138 container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 17:05:27.286
  Apr 28 17:05:27.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-571" for this suite. @ 04/28/23 17:05:27.311
• [4.161 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 04/28/23 17:05:27.32
  Apr 28 17:05:27.320: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename services @ 04/28/23 17:05:27.321
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:05:27.334
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:05:27.337
  STEP: creating a collection of services @ 04/28/23 17:05:27.34
  Apr 28 17:05:27.340: INFO: Creating e2e-svc-a-hfsmx
  Apr 28 17:05:27.350: INFO: Creating e2e-svc-b-jpld8
  Apr 28 17:05:27.361: INFO: Creating e2e-svc-c-98j24
  STEP: deleting service collection @ 04/28/23 17:05:27.374
  Apr 28 17:05:27.403: INFO: Collection of services has been deleted
  Apr 28 17:05:27.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8722" for this suite. @ 04/28/23 17:05:27.406
• [0.093 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 04/28/23 17:05:27.413
  Apr 28 17:05:27.413: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename statefulset @ 04/28/23 17:05:27.414
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:05:27.43
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:05:27.433
  STEP: Creating service test in namespace statefulset-7494 @ 04/28/23 17:05:27.437
  STEP: Creating a new StatefulSet @ 04/28/23 17:05:27.442
  Apr 28 17:05:27.461: INFO: Found 0 stateful pods, waiting for 3
  E0428 17:05:27.610501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:28.611106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:29.611296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:30.611372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:31.611469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:32.612381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:33.612723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:34.612948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:35.613230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:36.613442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:05:37.464: INFO: Found 2 stateful pods, waiting for 3
  E0428 17:05:37.613846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:38.614334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:39.614462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:40.614673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:41.615156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:42.615345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:43.615764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:44.616121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:45.616320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:46.616524      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:05:47.471: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 17:05:47.471: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 17:05:47.471: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 17:05:47.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=statefulset-7494 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0428 17:05:47.617441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:05:47.669: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 28 17:05:47.669: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 28 17:05:47.669: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0428 17:05:48.618348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:49.618418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:50.618527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:51.618628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:52.619103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:53.619542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:54.619646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:55.619842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:56.620362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:57.620590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 04/28/23 17:05:57.681
  Apr 28 17:05:57.700: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 04/28/23 17:05:57.7
  E0428 17:05:58.621376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:59.621520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:00.621760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:01.621966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:02.622173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:03.622660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:04.622801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:05.622910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:06.623156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:07.623359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating Pods in reverse ordinal order @ 04/28/23 17:06:07.72
  Apr 28 17:06:07.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=statefulset-7494 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 28 17:06:07.920: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 28 17:06:07.920: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 28 17:06:07.920: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0428 17:06:08.624365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:09.624517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:10.624579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:11.624723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:12.624911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:13.625518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:14.625775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:15.625886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:16.626094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:17.626352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back to a previous revision @ 04/28/23 17:06:17.946
  Apr 28 17:06:17.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=statefulset-7494 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 28 17:06:18.188: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 28 17:06:18.188: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 28 17:06:18.188: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0428 17:06:18.626985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:19.627110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:20.627270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:21.627396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:22.628356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:23.628736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:24.628993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:25.629212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:26.629409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:27.629651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:06:28.220: INFO: Updating stateful set ss2
  E0428 17:06:28.630326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:29.630457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:30.630645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:31.631294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:32.631434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:33.631972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:34.632121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:35.632314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:36.633277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:37.633392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back update in reverse ordinal order @ 04/28/23 17:06:38.233
  Apr 28 17:06:38.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=statefulset-7494 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 28 17:06:38.385: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 28 17:06:38.385: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 28 17:06:38.385: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0428 17:06:38.634236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:39.634574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:40.634720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:41.634949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:42.635078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:43.635534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:44.635640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:45.635833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:46.635934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:47.636174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:06:48.404: INFO: Deleting all statefulset in ns statefulset-7494
  Apr 28 17:06:48.406: INFO: Scaling statefulset ss2 to 0
  E0428 17:06:48.636992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:49.637122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:50.637376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:51.637510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:52.637576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:53.637987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:54.638109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:55.638205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:56.638394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:57.638545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:06:58.423: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 17:06:58.425: INFO: Deleting statefulset ss2
  Apr 28 17:06:58.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7494" for this suite. @ 04/28/23 17:06:58.444
• [91.039 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 04/28/23 17:06:58.452
  Apr 28 17:06:58.452: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename conformance-tests @ 04/28/23 17:06:58.453
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:06:58.488
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:06:58.502
  STEP: Getting node addresses @ 04/28/23 17:06:58.509
  Apr 28 17:06:58.509: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Apr 28 17:06:58.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-623" for this suite. @ 04/28/23 17:06:58.522
• [0.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 04/28/23 17:06:58.533
  Apr 28 17:06:58.533: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename var-expansion @ 04/28/23 17:06:58.533
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:06:58.554
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:06:58.564
  E0428 17:06:58.638595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:59.639089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:07:00.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 17:07:00.591: INFO: Deleting pod "var-expansion-07e1edf6-2963-4cc8-a7a3-2001267ec298" in namespace "var-expansion-4529"
  Apr 28 17:07:00.606: INFO: Wait up to 5m0s for pod "var-expansion-07e1edf6-2963-4cc8-a7a3-2001267ec298" to be fully deleted
  E0428 17:07:00.639785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:01.640377      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-4529" for this suite. @ 04/28/23 17:07:02.616
• [4.094 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 04/28/23 17:07:02.628
  Apr 28 17:07:02.628: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:07:02.629
  E0428 17:07:02.640695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:07:02.692
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:07:02.706
  STEP: Creating projection with secret that has name projected-secret-test-map-552b3b99-502c-4e5e-a137-57ac7a9a5953 @ 04/28/23 17:07:02.711
  STEP: Creating a pod to test consume secrets @ 04/28/23 17:07:02.732
  E0428 17:07:03.640805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:04.641639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:05.642553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:06.642932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:07:06.795
  Apr 28 17:07:06.798: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-projected-secrets-a40c12c0-24d8-4810-b3cd-9653a7613d46 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 17:07:06.809
  Apr 28 17:07:06.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9650" for this suite. @ 04/28/23 17:07:06.825
• [4.204 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 04/28/23 17:07:06.833
  Apr 28 17:07:06.833: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename svcaccounts @ 04/28/23 17:07:06.834
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:07:06.855
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:07:06.857
  Apr 28 17:07:06.874: INFO: created pod
  E0428 17:07:07.643296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:08.643756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:09.643908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:10.644465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:07:10.893
  E0428 17:07:11.644892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:12.645079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:13.645542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:14.645886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:15.645990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:16.646756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:17.647202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:18.647451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:19.648167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:20.648390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:21.648638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:22.649145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:23.649297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:24.649412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:25.649594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:26.649706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:27.649990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:28.650514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:29.650721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:30.650922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:31.651128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:32.651319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:33.651562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:34.653819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:35.654001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:36.654188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:37.654385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:38.655003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:39.655234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:40.655447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:07:40.894: INFO: polling logs
  Apr 28 17:07:40.903: INFO: Pod logs: 
  I0428 17:07:07.500451       1 log.go:198] OK: Got token
  I0428 17:07:07.500494       1 log.go:198] validating with in-cluster discovery
  I0428 17:07:07.500832       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
  I0428 17:07:07.500868       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5832:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682702227, NotBefore:1682701627, IssuedAt:1682701627, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5832", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"186a4101-22cd-4693-bf1b-f3fbc794d0d1"}}}
  I0428 17:07:07.509801       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I0428 17:07:07.513537       1 log.go:198] OK: Validated signature on JWT
  I0428 17:07:07.513638       1 log.go:198] OK: Got valid claims from token!
  I0428 17:07:07.513669       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5832:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682702227, NotBefore:1682701627, IssuedAt:1682701627, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5832", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"186a4101-22cd-4693-bf1b-f3fbc794d0d1"}}}

  Apr 28 17:07:40.903: INFO: completed pod
  Apr 28 17:07:40.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5832" for this suite. @ 04/28/23 17:07:40.913
• [34.088 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 04/28/23 17:07:40.922
  Apr 28 17:07:40.922: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:07:40.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:07:40.938
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:07:40.941
  STEP: Creating projection with secret that has name projected-secret-test-647a9447-b3d1-44b2-b772-c70196b5b0c4 @ 04/28/23 17:07:40.944
  STEP: Creating a pod to test consume secrets @ 04/28/23 17:07:40.953
  E0428 17:07:41.656335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:42.656536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:43.657690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:44.657824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:07:44.978
  Apr 28 17:07:44.982: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-projected-secrets-3a3ba5a8-aea9-4504-b173-d375b203474a container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 17:07:44.99
  Apr 28 17:07:45.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9714" for this suite. @ 04/28/23 17:07:45.021
• [4.107 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 04/28/23 17:07:45.042
  Apr 28 17:07:45.042: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename container-runtime @ 04/28/23 17:07:45.044
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:07:45.061
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:07:45.066
  STEP: create the container @ 04/28/23 17:07:45.071
  W0428 17:07:45.096032      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/28/23 17:07:45.096
  E0428 17:07:45.657944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:46.658050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:47.658355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 04/28/23 17:07:48.117
  STEP: the container should be terminated @ 04/28/23 17:07:48.12
  STEP: the termination message should be set @ 04/28/23 17:07:48.12
  Apr 28 17:07:48.120: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 04/28/23 17:07:48.12
  Apr 28 17:07:48.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-9469" for this suite. @ 04/28/23 17:07:48.15
• [3.118 seconds]
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 04/28/23 17:07:48.161
  Apr 28 17:07:48.161: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename configmap @ 04/28/23 17:07:48.162
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:07:48.239
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:07:48.241
  Apr 28 17:07:48.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3582" for this suite. @ 04/28/23 17:07:48.275
• [0.118 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 04/28/23 17:07:48.281
  Apr 28 17:07:48.281: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename replicaset @ 04/28/23 17:07:48.282
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:07:48.294
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:07:48.297
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 04/28/23 17:07:48.3
  Apr 28 17:07:48.312: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0428 17:07:48.658706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:49.658918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:50.659011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:51.659229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:52.659306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:07:53.316: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/28/23 17:07:53.316
  STEP: getting scale subresource @ 04/28/23 17:07:53.316
  STEP: updating a scale subresource @ 04/28/23 17:07:53.321
  STEP: verifying the replicaset Spec.Replicas was modified @ 04/28/23 17:07:53.328
  STEP: Patch a scale subresource @ 04/28/23 17:07:53.332
  Apr 28 17:07:53.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-5946" for this suite. @ 04/28/23 17:07:53.395
• [5.165 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 04/28/23 17:07:53.446
  Apr 28 17:07:53.446: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename secrets @ 04/28/23 17:07:53.447
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:07:53.535
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:07:53.54
  Apr 28 17:07:53.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4294" for this suite. @ 04/28/23 17:07:53.649
  E0428 17:07:53.660190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
• [0.218 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 04/28/23 17:07:53.667
  Apr 28 17:07:53.667: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 17:07:53.668
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:07:53.693
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:07:53.698
  STEP: creating Agnhost RC @ 04/28/23 17:07:53.704
  Apr 28 17:07:53.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-5381 create -f -'
  Apr 28 17:07:54.531: INFO: stderr: ""
  Apr 28 17:07:54.532: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/28/23 17:07:54.532
  E0428 17:07:54.661227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:07:55.535: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 28 17:07:55.535: INFO: Found 0 / 1
  E0428 17:07:55.662317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:07:56.535: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 28 17:07:56.535: INFO: Found 1 / 1
  Apr 28 17:07:56.535: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 04/28/23 17:07:56.535
  Apr 28 17:07:56.537: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 28 17:07:56.537: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 28 17:07:56.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-5381 patch pod agnhost-primary-ccmwq -p {"metadata":{"annotations":{"x":"y"}}}'
  Apr 28 17:07:56.645: INFO: stderr: ""
  Apr 28 17:07:56.645: INFO: stdout: "pod/agnhost-primary-ccmwq patched\n"
  STEP: checking annotations @ 04/28/23 17:07:56.645
  Apr 28 17:07:56.647: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 28 17:07:56.647: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 28 17:07:56.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5381" for this suite. @ 04/28/23 17:07:56.651
• [2.989 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 04/28/23 17:07:56.66
  Apr 28 17:07:56.662: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 17:07:56.662626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:07:56.669
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:07:56.685
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:07:56.69
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 17:07:56.694
  E0428 17:07:57.662723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:58.663366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:59.663484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:00.664377      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:08:00.769
  Apr 28 17:08:00.773: INFO: Trying to get logs from node ip-172-31-5-174 pod downwardapi-volume-16894d14-dfb6-46de-a1ae-2ea50249487d container client-container: <nil>
  STEP: delete the pod @ 04/28/23 17:08:00.791
  Apr 28 17:08:00.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2135" for this suite. @ 04/28/23 17:08:00.813
• [4.168 seconds]
------------------------------
SS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 04/28/23 17:08:00.828
  Apr 28 17:08:00.828: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename deployment @ 04/28/23 17:08:00.828
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:08:00.899
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:08:00.905
  Apr 28 17:08:00.912: INFO: Creating deployment "webserver-deployment"
  Apr 28 17:08:00.917: INFO: Waiting for observed generation 1
  E0428 17:08:01.664469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:02.664624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:08:02.935: INFO: Waiting for all required pods to come up
  Apr 28 17:08:02.939: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 04/28/23 17:08:02.939
  E0428 17:08:03.665523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:04.665650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:08:04.949: INFO: Waiting for deployment "webserver-deployment" to complete
  Apr 28 17:08:04.954: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Apr 28 17:08:04.967: INFO: Updating deployment webserver-deployment
  Apr 28 17:08:04.967: INFO: Waiting for observed generation 2
  E0428 17:08:05.666030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:06.666134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:08:06.976: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Apr 28 17:08:06.978: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Apr 28 17:08:06.980: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Apr 28 17:08:06.987: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Apr 28 17:08:06.987: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Apr 28 17:08:06.991: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Apr 28 17:08:06.995: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Apr 28 17:08:06.995: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Apr 28 17:08:07.004: INFO: Updating deployment webserver-deployment
  Apr 28 17:08:07.004: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Apr 28 17:08:07.015: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  E0428 17:08:07.670098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:08.670279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:08:09.026: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Apr 28 17:08:09.030: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-9504  a60777ed-98bd-4c96-a80d-8d02543cc904 255301 3 2023-04-28 17:08:00 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2023-04-28 17:08:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00539f9f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:17,UnavailableReplicas:16,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-28 17:08:07 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-04-28 17:08:08 +0000 UTC,LastTransitionTime:2023-04-28 17:08:00 +0000 UTC,},},ReadyReplicas:17,CollisionCount:nil,},}

  Apr 28 17:08:09.032: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-9504  f67b956b-d63b-4938-a1b4-c8744fe705df 255156 3 2023-04-28 17:08:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment a60777ed-98bd-4c96-a80d-8d02543cc904 0xc0056e3547 0xc0056e3548}] [] [{k3s Update apps/v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a60777ed-98bd-4c96-a80d-8d02543cc904\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0056e3608 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 17:08:09.032: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Apr 28 17:08:09.032: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-9504  d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8 255300 3 2023-04-28 17:08:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment a60777ed-98bd-4c96-a80d-8d02543cc904 0xc0056e3437 0xc0056e3438}] [] [{k3s Update apps/v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a60777ed-98bd-4c96-a80d-8d02543cc904\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2023-04-28 17:08:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0056e34d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:17,AvailableReplicas:17,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 17:08:09.037: INFO: Pod "webserver-deployment-67bd4bf6dc-4b9jd" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-4b9jd webserver-deployment-67bd4bf6dc- deployment-9504  a480d410-f617-4076-bde2-ad2ddab0eac0 254934 0 2023-04-28 17:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8 0xc0056e3b37 0xc0056e3b38}] [] [{k3s Update v1 2023-04-28 17:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.29\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7dn55,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7dn55,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-10-53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.10.53,PodIP:10.42.1.29,StartTime:2023-04-28 17:08:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:08:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6bd643052d7cb14641bbaa4f33c5f492677c03a7b240453e3451ada4b802209f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.29,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.037: INFO: Pod "webserver-deployment-67bd4bf6dc-5tpp4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-5tpp4 webserver-deployment-67bd4bf6dc- deployment-9504  c008af46-0f66-4727-839b-8a7e5714d2b6 255165 0 2023-04-28 17:08:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8 0xc0056e3d20 0xc0056e3d21}] [] [{k3s Update v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tq4k2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tq4k2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-59,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.59,PodIP:,StartTime:2023-04-28 17:08:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.037: INFO: Pod "webserver-deployment-67bd4bf6dc-c5j2h" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-c5j2h webserver-deployment-67bd4bf6dc- deployment-9504  c275d91a-eaae-42bd-80ff-9702a7ebf0ad 255288 0 2023-04-28 17:08:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8 0xc0056e3ee7 0xc0056e3ee8}] [] [{k3s Update v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.25\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5r7b9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5r7b9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-213,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.213,PodIP:10.42.2.25,StartTime:2023-04-28 17:08:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:08:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3ed79a36cba6d2bbd64af4c348c58eb9aa2895b880bdd5d752b6ec6df1d687eb,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.25,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.037: INFO: Pod "webserver-deployment-67bd4bf6dc-cksnz" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-cksnz webserver-deployment-67bd4bf6dc- deployment-9504  3a79edfe-3eec-487c-b9e0-30eb2304e2b2 255273 0 2023-04-28 17:08:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8 0xc003d120d0 0xc003d120d1}] [] [{k3s Update v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.35\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nwfhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nwfhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-10-53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.10.53,PodIP:10.42.1.35,StartTime:2023-04-28 17:08:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:08:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c29a615d6d95458305f927179f20bf1358686fc5510639d3bbd28ddcc666aa9e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.35,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.038: INFO: Pod "webserver-deployment-67bd4bf6dc-d6rbm" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-d6rbm webserver-deployment-67bd4bf6dc- deployment-9504  e75b1696-522f-4641-8edd-764e49e85b54 255246 0 2023-04-28 17:08:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8 0xc003d122b0 0xc003d122b1}] [] [{k3s Update v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.172\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g6n48,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g6n48,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-59,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.59,PodIP:10.42.0.172,StartTime:2023-04-28 17:08:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:08:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://32fb2a9a3c171ee371e133f58935e1dec72237bc4648ae899ea56ccb80c91279,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.172,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.038: INFO: Pod "webserver-deployment-67bd4bf6dc-dkmpq" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-dkmpq webserver-deployment-67bd4bf6dc- deployment-9504  49f41454-c8b5-44b1-9f8b-b944871e7509 255292 0 2023-04-28 17:08:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8 0xc003d12497 0xc003d12498}] [] [{k3s Update v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.29\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k8crd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k8crd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-213,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.213,PodIP:10.42.2.29,StartTime:2023-04-28 17:08:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:08:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b2db886d0857bafbcf9aa1391b335030a52c9b54423018da662336bdec471442,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.29,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.038: INFO: Pod "webserver-deployment-67bd4bf6dc-g4n76" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-g4n76 webserver-deployment-67bd4bf6dc- deployment-9504  0279dc38-9a25-49e0-8c03-70b1cd06abb2 255296 0 2023-04-28 17:08:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8 0xc003d126a0 0xc003d126a1}] [] [{k3s Update v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-247gs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-247gs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-213,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.213,PodIP:10.42.2.28,StartTime:2023-04-28 17:08:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:08:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d3f10f7e77cd833cc994a66bd88370721901f7c5a89a92dc363dba1662f03a57,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.28,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.038: INFO: Pod "webserver-deployment-67bd4bf6dc-hwp2f" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hwp2f webserver-deployment-67bd4bf6dc- deployment-9504  d8fb2216-81c9-4c9a-aafa-530f929e8272 254940 0 2023-04-28 17:08:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8 0xc003d12880 0xc003d12881}] [] [{k3s Update v1 2023-04-28 17:08:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fmlwk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fmlwk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-213,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.213,PodIP:10.42.2.20,StartTime:2023-04-28 17:08:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:08:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://54f5fd6e7fe2bf6e53435ef52ffcbf0cc26c9228969ca18cd73e8c73af08d250,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.20,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.038: INFO: Pod "webserver-deployment-67bd4bf6dc-jnwtm" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-jnwtm webserver-deployment-67bd4bf6dc- deployment-9504  6d93b4f4-9b6d-4a4b-91f0-6fc5a96babe6 254916 0 2023-04-28 17:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8 0xc003d12a80 0xc003d12a81}] [] [{k3s Update v1 2023-04-28 17:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.166\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dqsw5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqsw5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-59,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.59,PodIP:10.42.0.166,StartTime:2023-04-28 17:08:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:08:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8c027e7ee2eb661795c59eb3d86cd51381606f63ca779a32b98d8121860a6330,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.166,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.039: INFO: Pod "webserver-deployment-67bd4bf6dc-lcdx7" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-lcdx7 webserver-deployment-67bd4bf6dc- deployment-9504  7203e00d-c7f7-40a0-81e1-f9eaf48de4a4 254918 0 2023-04-28 17:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8 0xc003d12c67 0xc003d12c68}] [] [{k3s Update v1 2023-04-28 17:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mnntf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mnntf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-59,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.59,PodIP:10.42.0.167,StartTime:2023-04-28 17:08:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:08:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7f2655784ad88983a80180ed8bcfce27209d47cd84fdcc6a098b8fece5caca99,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.167,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.039: INFO: Pod "webserver-deployment-67bd4bf6dc-lx4gc" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-lx4gc webserver-deployment-67bd4bf6dc- deployment-9504  b1fcefaf-53b3-4f25-bb3a-4e000ab79f2a 255171 0 2023-04-28 17:08:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8 0xc003d12e57 0xc003d12e58}] [] [{k3s Update v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dq4w7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dq4w7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-5-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.5.174,PodIP:,StartTime:2023-04-28 17:08:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.039: INFO: Pod "webserver-deployment-67bd4bf6dc-mb965" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-mb965 webserver-deployment-67bd4bf6dc- deployment-9504  0730d1a9-f959-4c04-889c-5c3272fb1f46 255250 0 2023-04-28 17:08:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8 0xc003d13027 0xc003d13028}] [] [{k3s Update v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.169\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mhgs9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mhgs9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-59,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.59,PodIP:10.42.0.169,StartTime:2023-04-28 17:08:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:08:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ef56aecba7454a98a0eb84c3f4c943fa9426285cd61a41942c584ebb4902e11f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.169,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.039: INFO: Pod "webserver-deployment-67bd4bf6dc-nxcpt" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-nxcpt webserver-deployment-67bd4bf6dc- deployment-9504  d27e519e-facf-41a4-b5ab-c118edcb5644 255262 0 2023-04-28 17:08:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8 0xc003d13217 0xc003d13218}] [] [{k3s Update v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.222\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f6fsr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f6fsr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-5-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.5.174,PodIP:10.42.3.222,StartTime:2023-04-28 17:08:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:08:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1616e3e913eb5d8500c7bc8f843336eba7eca01275ea1f55ea559f6e95cbcf0d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.222,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.039: INFO: Pod "webserver-deployment-67bd4bf6dc-rdscn" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rdscn webserver-deployment-67bd4bf6dc- deployment-9504  a920b642-61f6-49dc-b360-61b08a5f24ea 255299 0 2023-04-28 17:08:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8 0xc003d13407 0xc003d13408}] [] [{k3s Update v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.26\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bkxs9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bkxs9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-213,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.213,PodIP:10.42.2.26,StartTime:2023-04-28 17:08:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:08:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2781e4bf618ae5e8c59ac88fd029a0a7010bdc9f0fcc718df2cabaca7ad38396,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.26,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.040: INFO: Pod "webserver-deployment-67bd4bf6dc-s2hwg" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-s2hwg webserver-deployment-67bd4bf6dc- deployment-9504  3c805b95-d41a-4c4f-b4f4-75121798a6d4 254960 0 2023-04-28 17:08:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8 0xc003d135f0 0xc003d135f1}] [] [{k3s Update v1 2023-04-28 17:08:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vwgmc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vwgmc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-5-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.5.174,PodIP:10.42.3.219,StartTime:2023-04-28 17:08:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:08:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e85fe2d4bc17b7e6b25d6e6c7206a8e25720a5da02d594ea13ed1d9f8c6aba54,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.219,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.040: INFO: Pod "webserver-deployment-67bd4bf6dc-tfjnj" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-tfjnj webserver-deployment-67bd4bf6dc- deployment-9504  c3af8904-8d8d-4384-a152-b7add05b82e1 254930 0 2023-04-28 17:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8 0xc003d137d7 0xc003d137d8}] [] [{k3s Update v1 2023-04-28 17:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rvdhd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rvdhd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-10-53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.10.53,PodIP:10.42.1.28,StartTime:2023-04-28 17:08:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:08:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://4257bd83cb5406bf1a5583da208d14c9b523d120b8f28932785c1c781ed492f6,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.28,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.040: INFO: Pod "webserver-deployment-67bd4bf6dc-w8jdq" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-w8jdq webserver-deployment-67bd4bf6dc- deployment-9504  8c6f31be-aea7-4791-b5b6-45d106b3cf3c 255282 0 2023-04-28 17:08:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8 0xc003d139e0 0xc003d139e1}] [] [{k3s Update v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.34\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-48ctt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-48ctt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-10-53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.10.53,PodIP:10.42.1.34,StartTime:2023-04-28 17:08:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:08:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ccfd06fe4730adf84364c5bb81165d6de4525aab1cf54431b6dba1d879143dd9,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.34,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.040: INFO: Pod "webserver-deployment-67bd4bf6dc-x8r29" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-x8r29 webserver-deployment-67bd4bf6dc- deployment-9504  146c593a-d519-4557-956c-6929a9211dac 255162 0 2023-04-28 17:08:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8 0xc003d13bc0 0xc003d13bc1}] [] [{k3s Update v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xp78s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xp78s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-5-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.5.174,PodIP:,StartTime:2023-04-28 17:08:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.040: INFO: Pod "webserver-deployment-67bd4bf6dc-xxgjf" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xxgjf webserver-deployment-67bd4bf6dc- deployment-9504  b3ff8b39-7f56-4190-bfe9-6719aa0431ff 254926 0 2023-04-28 17:08:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8 0xc003d13dc7 0xc003d13dc8}] [] [{k3s Update v1 2023-04-28 17:08:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.30\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z94s9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z94s9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-10-53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.10.53,PodIP:10.42.1.30,StartTime:2023-04-28 17:08:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:08:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0e0f305460b6d70ae4d59289d2549aac370f8c792150f9a4a3ec41f4a8ecfd9e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.30,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.041: INFO: Pod "webserver-deployment-67bd4bf6dc-zj85s" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-zj85s webserver-deployment-67bd4bf6dc- deployment-9504  0059c867-9ab2-4134-920b-a076609ee9d2 254953 0 2023-04-28 17:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8 0xc003d13fc0 0xc003d13fc1}] [] [{k3s Update v1 2023-04-28 17:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d6d97a51-98cd-4bbf-943a-e8d12f1cd8f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wf6dd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wf6dd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-5-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.5.174,PodIP:10.42.3.218,StartTime:2023-04-28 17:08:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:08:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://84325ee13e36194c8f7f20a3f1f9a74219d9e0217bdc2b9796926c3312e4bdd2,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.218,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.041: INFO: Pod "webserver-deployment-7b75d79cf5-6cffr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-6cffr webserver-deployment-7b75d79cf5- deployment-9504  b1332b98-1f21-42b8-a988-0e848d92d6d0 255046 0 2023-04-28 17:08:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 f67b956b-d63b-4938-a1b4-c8744fe705df 0xc00505c1a7 0xc00505c1a8}] [] [{k3s Update v1 2023-04-28 17:08:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67b956b-d63b-4938-a1b4-c8744fe705df\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.220\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k7gpm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k7gpm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-5-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.5.174,PodIP:10.42.3.220,StartTime:2023-04-28 17:08:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.220,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.041: INFO: Pod "webserver-deployment-7b75d79cf5-6mbd2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-6mbd2 webserver-deployment-7b75d79cf5- deployment-9504  29d0af27-29ff-4224-8377-d79d1b32aaaa 255285 0 2023-04-28 17:08:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 f67b956b-d63b-4938-a1b4-c8744fe705df 0xc00505c3d0 0xc00505c3d1}] [] [{k3s Update v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67b956b-d63b-4938-a1b4-c8744fe705df\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7r4hb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7r4hb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-10-53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.10.53,PodIP:10.42.1.33,StartTime:2023-04-28 17:08:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.33,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.041: INFO: Pod "webserver-deployment-7b75d79cf5-85m4m" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-85m4m webserver-deployment-7b75d79cf5- deployment-9504  88782560-03f7-4a31-848e-4fe9a08faa6f 255057 0 2023-04-28 17:08:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 f67b956b-d63b-4938-a1b4-c8744fe705df 0xc00505c5e0 0xc00505c5e1}] [] [{k3s Update v1 2023-04-28 17:08:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67b956b-d63b-4938-a1b4-c8744fe705df\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dqgn5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqgn5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-213,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.213,PodIP:10.42.2.23,StartTime:2023-04-28 17:08:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.23,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.041: INFO: Pod "webserver-deployment-7b75d79cf5-bngl4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-bngl4 webserver-deployment-7b75d79cf5- deployment-9504  c1ba5278-ffc2-48e5-bdf1-0c46648e0545 255295 0 2023-04-28 17:08:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 f67b956b-d63b-4938-a1b4-c8744fe705df 0xc00505c7f0 0xc00505c7f1}] [] [{k3s Update v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67b956b-d63b-4938-a1b4-c8744fe705df\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.27\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rfkxq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rfkxq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-213,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.213,PodIP:10.42.2.27,StartTime:2023-04-28 17:08:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.27,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.041: INFO: Pod "webserver-deployment-7b75d79cf5-hg5md" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-hg5md webserver-deployment-7b75d79cf5- deployment-9504  3e6e59a2-9e8c-4487-abbc-eebb0ddd9f9b 255238 0 2023-04-28 17:08:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 f67b956b-d63b-4938-a1b4-c8744fe705df 0xc00505ca10 0xc00505ca11}] [] [{k3s Update v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67b956b-d63b-4938-a1b4-c8744fe705df\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lj8pv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lj8pv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-59,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.59,PodIP:10.42.0.171,StartTime:2023-04-28 17:08:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.171,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.042: INFO: Pod "webserver-deployment-7b75d79cf5-hjr6h" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-hjr6h webserver-deployment-7b75d79cf5- deployment-9504  14a1a4e6-05a1-4f7d-8854-9932e32e756d 255286 0 2023-04-28 17:08:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 f67b956b-d63b-4938-a1b4-c8744fe705df 0xc00505cc40 0xc00505cc41}] [] [{k3s Update v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67b956b-d63b-4938-a1b4-c8744fe705df\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.36\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lzgpx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lzgpx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-10-53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.10.53,PodIP:10.42.1.36,StartTime:2023-04-28 17:08:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.36,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.042: INFO: Pod "webserver-deployment-7b75d79cf5-jlfmr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-jlfmr webserver-deployment-7b75d79cf5- deployment-9504  773ee5b8-dc34-4b38-bb98-ee44622b25b6 255241 0 2023-04-28 17:08:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 f67b956b-d63b-4938-a1b4-c8744fe705df 0xc00505cec0 0xc00505cec1}] [] [{k3s Update v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67b956b-d63b-4938-a1b4-c8744fe705df\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.170\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-28r6c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-28r6c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-59,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.59,PodIP:10.42.0.170,StartTime:2023-04-28 17:08:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.170,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.043: INFO: Pod "webserver-deployment-7b75d79cf5-knwxh" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-knwxh webserver-deployment-7b75d79cf5- deployment-9504  07a1d08e-cf04-4cec-af0b-a78a7532e53d 255268 0 2023-04-28 17:08:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 f67b956b-d63b-4938-a1b4-c8744fe705df 0xc00505d0d0 0xc00505d0d1}] [] [{k3s Update v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67b956b-d63b-4938-a1b4-c8744fe705df\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.224\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2j22h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2j22h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-5-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.5.174,PodIP:10.42.3.224,StartTime:2023-04-28 17:08:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.224,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.043: INFO: Pod "webserver-deployment-7b75d79cf5-n6b2r" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-n6b2r webserver-deployment-7b75d79cf5- deployment-9504  5e95db8b-b43e-44bc-a783-ae69e8c1b191 255054 0 2023-04-28 17:08:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 f67b956b-d63b-4938-a1b4-c8744fe705df 0xc00505d2e0 0xc00505d2e1}] [] [{k3s Update v1 2023-04-28 17:08:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67b956b-d63b-4938-a1b4-c8744fe705df\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ks82z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ks82z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-10-53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.10.53,PodIP:10.42.1.31,StartTime:2023-04-28 17:08:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.31,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.043: INFO: Pod "webserver-deployment-7b75d79cf5-n6j8m" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-n6j8m webserver-deployment-7b75d79cf5- deployment-9504  9ff479ce-23cb-4aaa-bb4b-0c71b3ba93ae 255257 0 2023-04-28 17:08:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 f67b956b-d63b-4938-a1b4-c8744fe705df 0xc00505d500 0xc00505d501}] [] [{k3s Update v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67b956b-d63b-4938-a1b4-c8744fe705df\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.221\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v5gvw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v5gvw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-5-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.5.174,PodIP:10.42.3.221,StartTime:2023-04-28 17:08:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.221,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.044: INFO: Pod "webserver-deployment-7b75d79cf5-nb5c2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-nb5c2 webserver-deployment-7b75d79cf5- deployment-9504  90c90f1e-d3ea-4b60-bc19-5016789d1d20 255278 0 2023-04-28 17:08:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 f67b956b-d63b-4938-a1b4-c8744fe705df 0xc00505d720 0xc00505d721}] [] [{k3s Update v1 2023-04-28 17:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67b956b-d63b-4938-a1b4-c8744fe705df\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qpdgw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qpdgw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-10-53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.10.53,PodIP:10.42.1.32,StartTime:2023-04-28 17:08:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.32,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.044: INFO: Pod "webserver-deployment-7b75d79cf5-njf9w" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-njf9w webserver-deployment-7b75d79cf5- deployment-9504  9f04807d-9521-4d2b-a209-d912ad6ea177 255049 0 2023-04-28 17:08:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 f67b956b-d63b-4938-a1b4-c8744fe705df 0xc00505d930 0xc00505d931}] [] [{k3s Update v1 2023-04-28 17:08:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67b956b-d63b-4938-a1b4-c8744fe705df\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bdggd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bdggd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-12-59,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.12.59,PodIP:10.42.0.168,StartTime:2023-04-28 17:08:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.168,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.044: INFO: Pod "webserver-deployment-7b75d79cf5-pzxv9" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-pzxv9 webserver-deployment-7b75d79cf5- deployment-9504  ab71e916-eb05-49d6-8dbd-5b99bab35400 255060 0 2023-04-28 17:08:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 f67b956b-d63b-4938-a1b4-c8744fe705df 0xc00505db50 0xc00505db51}] [] [{k3s Update v1 2023-04-28 17:08:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f67b956b-d63b-4938-a1b4-c8744fe705df\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:08:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kgdrh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kgdrh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-213,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.213,PodIP:10.42.2.24,StartTime:2023-04-28 17:08:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.24,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:09.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9504" for this suite. @ 04/28/23 17:08:09.048
• [8.225 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 04/28/23 17:08:09.054
  Apr 28 17:08:09.054: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename dns @ 04/28/23 17:08:09.055
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:08:09.069
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:08:09.071
  STEP: Creating a test headless service @ 04/28/23 17:08:09.074
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2972.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2972.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2972.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2972.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2972.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2972.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2972.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2972.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2972.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2972.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2972.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2972.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 75.254.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.254.75_udp@PTR;check="$$(dig +tcp +noall +answer +search 75.254.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.254.75_tcp@PTR;sleep 1; done
   @ 04/28/23 17:08:09.103
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2972.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2972.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2972.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2972.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2972.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2972.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2972.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2972.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2972.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2972.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2972.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2972.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 75.254.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.254.75_udp@PTR;check="$$(dig +tcp +noall +answer +search 75.254.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.254.75_tcp@PTR;sleep 1; done
   @ 04/28/23 17:08:09.103
  STEP: creating a pod to probe DNS @ 04/28/23 17:08:09.103
  STEP: submitting the pod to kubernetes @ 04/28/23 17:08:09.103
  E0428 17:08:09.670373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:10.671237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/28/23 17:08:11.135
  STEP: looking for the results for each expected name from probers @ 04/28/23 17:08:11.137
  Apr 28 17:08:11.142: INFO: Unable to read wheezy_udp@dns-test-service.dns-2972.svc.cluster.local from pod dns-2972/dns-test-f3a71825-a13a-4a48-8295-46b1f0257bea: the server could not find the requested resource (get pods dns-test-f3a71825-a13a-4a48-8295-46b1f0257bea)
  Apr 28 17:08:11.147: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2972.svc.cluster.local from pod dns-2972/dns-test-f3a71825-a13a-4a48-8295-46b1f0257bea: the server could not find the requested resource (get pods dns-test-f3a71825-a13a-4a48-8295-46b1f0257bea)
  Apr 28 17:08:11.152: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2972.svc.cluster.local from pod dns-2972/dns-test-f3a71825-a13a-4a48-8295-46b1f0257bea: the server could not find the requested resource (get pods dns-test-f3a71825-a13a-4a48-8295-46b1f0257bea)
  Apr 28 17:08:11.155: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2972.svc.cluster.local from pod dns-2972/dns-test-f3a71825-a13a-4a48-8295-46b1f0257bea: the server could not find the requested resource (get pods dns-test-f3a71825-a13a-4a48-8295-46b1f0257bea)
  Apr 28 17:08:11.225: INFO: Unable to read jessie_udp@dns-test-service.dns-2972.svc.cluster.local from pod dns-2972/dns-test-f3a71825-a13a-4a48-8295-46b1f0257bea: the server could not find the requested resource (get pods dns-test-f3a71825-a13a-4a48-8295-46b1f0257bea)
  Apr 28 17:08:11.285: INFO: Unable to read jessie_tcp@dns-test-service.dns-2972.svc.cluster.local from pod dns-2972/dns-test-f3a71825-a13a-4a48-8295-46b1f0257bea: the server could not find the requested resource (get pods dns-test-f3a71825-a13a-4a48-8295-46b1f0257bea)
  Apr 28 17:08:11.306: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2972.svc.cluster.local from pod dns-2972/dns-test-f3a71825-a13a-4a48-8295-46b1f0257bea: the server could not find the requested resource (get pods dns-test-f3a71825-a13a-4a48-8295-46b1f0257bea)
  Apr 28 17:08:11.310: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2972.svc.cluster.local from pod dns-2972/dns-test-f3a71825-a13a-4a48-8295-46b1f0257bea: the server could not find the requested resource (get pods dns-test-f3a71825-a13a-4a48-8295-46b1f0257bea)
  Apr 28 17:08:11.323: INFO: Lookups using dns-2972/dns-test-f3a71825-a13a-4a48-8295-46b1f0257bea failed for: [wheezy_udp@dns-test-service.dns-2972.svc.cluster.local wheezy_tcp@dns-test-service.dns-2972.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2972.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2972.svc.cluster.local jessie_udp@dns-test-service.dns-2972.svc.cluster.local jessie_tcp@dns-test-service.dns-2972.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2972.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2972.svc.cluster.local]

  E0428 17:08:11.671909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:12.672301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:13.672426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:14.672525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:15.672822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:08:16.467: INFO: DNS probes using dns-2972/dns-test-f3a71825-a13a-4a48-8295-46b1f0257bea succeeded

  Apr 28 17:08:16.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 17:08:16.472
  STEP: deleting the test service @ 04/28/23 17:08:16.504
  STEP: deleting the test headless service @ 04/28/23 17:08:16.57
  STEP: Destroying namespace "dns-2972" for this suite. @ 04/28/23 17:08:16.591
• [7.546 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 04/28/23 17:08:16.6
  Apr 28 17:08:16.600: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename cronjob @ 04/28/23 17:08:16.601
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:08:16.62
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:08:16.624
  STEP: Creating a cronjob @ 04/28/23 17:08:16.626
  STEP: Ensuring more than one job is running at a time @ 04/28/23 17:08:16.634
  E0428 17:08:16.673593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:17.674002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:18.674597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:19.674806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:20.675900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:21.676497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:22.677063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:23.677497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:24.678558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:25.678698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:26.678976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:27.679506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:28.679623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:29.679826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:30.680850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:31.680972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:32.681949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:33.682497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:34.683467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:35.683740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:36.684774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:37.684915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:38.685668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:39.685756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:40.686767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:41.686501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:42.687481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:43.688179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:44.688249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:45.688470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:46.689384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:47.689521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:48.690431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:49.690794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:50.691004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:51.691287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:52.692152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:53.692330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:54.693388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:55.693594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:56.694098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:57.694250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:58.694806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:59.694936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:00.695272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:01.695290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:02.696099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:03.696364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:04.696836      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:05.696991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:06.697821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:07.699403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:08.699509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:09.699622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:10.700192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:11.700273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:12.701270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:13.701621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:14.702317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:15.702577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:16.703412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:17.704227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:18.705161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:19.705275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:20.705337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:21.705464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:22.706325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:23.707119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:24.707730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:25.707865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:26.708698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:27.709380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:28.710015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:29.710244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:30.710549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:31.710753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:32.711438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:33.711589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:34.712418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:35.712552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:36.713510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:37.713722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:38.714735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:39.714856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:40.715599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:41.715753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:42.716524      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:43.716666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:44.717482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:45.717597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:46.718483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:47.718678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:48.718940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:49.719207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:50.720136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:51.720261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:52.720334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:53.720636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:54.721691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:55.722027      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:56.722862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:57.722993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:58.724058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:59.724195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 04/28/23 17:10:00.638
  STEP: Removing cronjob @ 04/28/23 17:10:00.641
  Apr 28 17:10:00.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-7124" for this suite. @ 04/28/23 17:10:00.651
• [104.059 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 04/28/23 17:10:00.663
  Apr 28 17:10:00.663: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename pod-network-test @ 04/28/23 17:10:00.665
  E0428 17:10:00.724851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:10:00.729
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:10:00.74
  STEP: Performing setup for networking test in namespace pod-network-test-9227 @ 04/28/23 17:10:00.747
  STEP: creating a selector @ 04/28/23 17:10:00.747
  STEP: Creating the service pods in kubernetes @ 04/28/23 17:10:00.747
  Apr 28 17:10:00.747: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0428 17:10:01.725229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:02.725476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:03.725570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:04.726033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:05.725954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:06.726075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:07.726183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:08.726721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:09.726910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:10.727150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:11.727269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:12.727405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:13.728203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:14.728411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 04/28/23 17:10:14.963
  E0428 17:10:15.728544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:16.728671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:17.728959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:18.729541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:19.006: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
  Apr 28 17:10:19.006: INFO: Going to poll 10.42.2.32 on port 8083 at least 0 times, with a maximum of 46 tries before failing
  Apr 28 17:10:19.011: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.2.32:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9227 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:10:19.011: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 17:10:19.013: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:10:19.013: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-9227/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.42.2.32%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 28 17:10:19.096: INFO: Found all 1 expected endpoints: [netserver-0]
  Apr 28 17:10:19.096: INFO: Going to poll 10.42.1.38 on port 8083 at least 0 times, with a maximum of 46 tries before failing
  Apr 28 17:10:19.098: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.1.38:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9227 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:10:19.098: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 17:10:19.099: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:10:19.099: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-9227/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.42.1.38%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 28 17:10:19.165: INFO: Found all 1 expected endpoints: [netserver-1]
  Apr 28 17:10:19.165: INFO: Going to poll 10.42.0.174 on port 8083 at least 0 times, with a maximum of 46 tries before failing
  Apr 28 17:10:19.168: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.0.174:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9227 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:10:19.168: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 17:10:19.169: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:10:19.169: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-9227/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.42.0.174%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 28 17:10:19.251: INFO: Found all 1 expected endpoints: [netserver-2]
  Apr 28 17:10:19.251: INFO: Going to poll 10.42.3.226 on port 8083 at least 0 times, with a maximum of 46 tries before failing
  Apr 28 17:10:19.257: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.3.226:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9227 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:10:19.257: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 17:10:19.257: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:10:19.257: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-9227/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.42.3.226%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 28 17:10:19.328: INFO: Found all 1 expected endpoints: [netserver-3]
  Apr 28 17:10:19.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-9227" for this suite. @ 04/28/23 17:10:19.333
• [18.677 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 04/28/23 17:10:19.34
  Apr 28 17:10:19.340: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 17:10:19.341
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:10:19.36
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:10:19.364
  STEP: Counting existing ResourceQuota @ 04/28/23 17:10:19.368
  E0428 17:10:19.729673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:20.729888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:21.730262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:22.730371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:23.730443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/28/23 17:10:24.373
  STEP: Ensuring resource quota status is calculated @ 04/28/23 17:10:24.38
  E0428 17:10:24.732436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:25.732760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 04/28/23 17:10:26.383
  STEP: Creating a NodePort Service @ 04/28/23 17:10:26.396
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 04/28/23 17:10:26.418
  STEP: Ensuring resource quota status captures service creation @ 04/28/23 17:10:26.436
  E0428 17:10:26.732802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:27.732958      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 04/28/23 17:10:28.44
  STEP: Ensuring resource quota status released usage @ 04/28/23 17:10:28.492
  E0428 17:10:28.733093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:29.733217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:30.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4685" for this suite. @ 04/28/23 17:10:30.499
• [11.166 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 04/28/23 17:10:30.506
  Apr 28 17:10:30.506: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:10:30.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:10:30.528
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:10:30.531
  STEP: Creating the pod @ 04/28/23 17:10:30.534
  E0428 17:10:30.734252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:31.734485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:32.734617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:33.082: INFO: Successfully updated pod "labelsupdatee87a4bcb-ccb7-4327-b7e9-f87ca2080725"
  E0428 17:10:33.735458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:34.735647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:35.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6374" for this suite. @ 04/28/23 17:10:35.113
• [4.611 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 04/28/23 17:10:35.118
  Apr 28 17:10:35.118: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename statefulset @ 04/28/23 17:10:35.119
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:10:35.132
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:10:35.136
  STEP: Creating service test in namespace statefulset-3863 @ 04/28/23 17:10:35.15
  STEP: Creating a new StatefulSet @ 04/28/23 17:10:35.163
  Apr 28 17:10:35.189: INFO: Found 0 stateful pods, waiting for 3
  E0428 17:10:35.735761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:36.735977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:37.736094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:38.736614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:39.736706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:40.736816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:41.736931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:42.737124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:43.737238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:44.737417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:45.193: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 17:10:45.193: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 17:10:45.193: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 04/28/23 17:10:45.202
  Apr 28 17:10:45.220: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 04/28/23 17:10:45.22
  E0428 17:10:45.737757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:46.737907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:47.738039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:48.738397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:49.738499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:50.738718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:51.738834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:52.738974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:53.739353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:54.739543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not applying an update when the partition is greater than the number of replicas @ 04/28/23 17:10:55.246
  STEP: Performing a canary update @ 04/28/23 17:10:55.246
  Apr 28 17:10:55.273: INFO: Updating stateful set ss2
  Apr 28 17:10:55.283: INFO: Waiting for Pod statefulset-3863/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0428 17:10:55.739687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:56.739966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:57.740056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:58.740633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:59.740841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:00.740924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:01.741320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:02.741374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:03.741510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:04.741690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Restoring Pods to the correct revision when they are deleted @ 04/28/23 17:11:05.296
  Apr 28 17:11:05.444: INFO: Found 1 stateful pods, waiting for 3
  E0428 17:11:05.742312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:06.742435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:07.742562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:08.743590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:09.744519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:10.745057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:11.745163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:12.745296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:13.745636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:14.745951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:11:15.448: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 17:11:15.448: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 17:11:15.448: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 04/28/23 17:11:15.454
  Apr 28 17:11:15.474: INFO: Updating stateful set ss2
  Apr 28 17:11:15.482: INFO: Waiting for Pod statefulset-3863/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0428 17:11:15.746129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:16.746259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:17.746583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:18.747386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:19.748324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:20.748444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:21.748657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:22.748852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:23.749745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:24.749878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:11:25.513: INFO: Updating stateful set ss2
  Apr 28 17:11:25.532: INFO: Waiting for StatefulSet statefulset-3863/ss2 to complete update
  Apr 28 17:11:25.532: INFO: Waiting for Pod statefulset-3863/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0428 17:11:25.750653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:26.750859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:27.750991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:28.751659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:29.752393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:30.752511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:31.752761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:32.752857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:33.753659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:34.753906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:11:35.538: INFO: Deleting all statefulset in ns statefulset-3863
  Apr 28 17:11:35.540: INFO: Scaling statefulset ss2 to 0
  E0428 17:11:35.754821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:36.755052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:37.755253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:38.756157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:39.756372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:40.756652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:41.756831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:42.757057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:43.757741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:44.758854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:11:45.555: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 17:11:45.557: INFO: Deleting statefulset ss2
  Apr 28 17:11:45.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3863" for this suite. @ 04/28/23 17:11:45.6
• [70.498 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 04/28/23 17:11:45.616
  Apr 28 17:11:45.616: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/28/23 17:11:45.616
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:11:45.647
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:11:45.65
  Apr 28 17:11:45.653: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 17:11:45.759813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:46.760765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:47.760905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:48.761040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:49.761854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:50.762608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:51.763374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:11:52.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4162" for this suite. @ 04/28/23 17:11:52.037
• [6.426 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 04/28/23 17:11:52.043
  Apr 28 17:11:52.043: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename svcaccounts @ 04/28/23 17:11:52.044
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:11:52.06
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:11:52.069
  STEP: Creating ServiceAccount "e2e-sa-dvp7h"  @ 04/28/23 17:11:52.076
  Apr 28 17:11:52.082: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-dvp7h"  @ 04/28/23 17:11:52.083
  Apr 28 17:11:52.092: INFO: AutomountServiceAccountToken: true
  Apr 28 17:11:52.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1489" for this suite. @ 04/28/23 17:11:52.097
• [0.060 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 04/28/23 17:11:52.106
  Apr 28 17:11:52.106: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename services @ 04/28/23 17:11:52.106
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:11:52.121
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:11:52.127
  STEP: creating service endpoint-test2 in namespace services-3856 @ 04/28/23 17:11:52.132
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3856 to expose endpoints map[] @ 04/28/23 17:11:52.144
  Apr 28 17:11:52.163: INFO: successfully validated that service endpoint-test2 in namespace services-3856 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-3856 @ 04/28/23 17:11:52.163
  E0428 17:11:52.763597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:53.763731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3856 to expose endpoints map[pod1:[80]] @ 04/28/23 17:11:54.182
  Apr 28 17:11:54.189: INFO: successfully validated that service endpoint-test2 in namespace services-3856 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 04/28/23 17:11:54.189
  Apr 28 17:11:54.189: INFO: Creating new exec pod
  E0428 17:11:54.764320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:55.764422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:56.764868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:11:57.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-3856 exec execpod9pqj4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E0428 17:11:57.765808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:58.766085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:11:59.391: INFO: rc: 1
  Apr 28 17:11:59.391: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-3856 exec execpod9pqj4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 endpoint-test2 80
  nc: connect to endpoint-test2 port 80 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  E0428 17:11:59.766962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:12:00.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-3856 exec execpod9pqj4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E0428 17:12:00.767071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:01.768124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:12:02.527: INFO: rc: 1
  Apr 28 17:12:02.527: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-3856 exec execpod9pqj4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 endpoint-test2 80
  nc: connect to endpoint-test2 port 80 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  E0428 17:12:02.768582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:12:03.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-3856 exec execpod9pqj4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E0428 17:12:03.768993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:04.769034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:12:05.629: INFO: rc: 1
  Apr 28 17:12:05.629: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-3856 exec execpod9pqj4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 endpoint-test2 80
  nc: connect to endpoint-test2 port 80 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  E0428 17:12:05.769916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:12:06.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-3856 exec execpod9pqj4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 28 17:12:06.591: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 28 17:12:06.591: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:12:06.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-3856 exec execpod9pqj4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.128.174 80'
  Apr 28 17:12:06.762: INFO: stderr: "+ nc -v -t -w 2 10.43.128.174 80\n+ echo hostName\nConnection to 10.43.128.174 80 port [tcp/http] succeeded!\n"
  Apr 28 17:12:06.762: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-3856 @ 04/28/23 17:12:06.762
  E0428 17:12:06.770182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:07.770470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:08.771006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3856 to expose endpoints map[pod1:[80] pod2:[80]] @ 04/28/23 17:12:08.787
  Apr 28 17:12:08.798: INFO: successfully validated that service endpoint-test2 in namespace services-3856 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 04/28/23 17:12:08.798
  E0428 17:12:09.771189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:12:09.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-3856 exec execpod9pqj4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 28 17:12:09.966: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 28 17:12:09.966: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:12:09.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-3856 exec execpod9pqj4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.128.174 80'
  Apr 28 17:12:10.097: INFO: stderr: "+ nc -v -t -w 2 10.43.128.174 80\n+ echo hostName\nConnection to 10.43.128.174 80 port [tcp/http] succeeded!\n"
  Apr 28 17:12:10.097: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-3856 @ 04/28/23 17:12:10.097
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3856 to expose endpoints map[pod2:[80]] @ 04/28/23 17:12:10.112
  Apr 28 17:12:10.157: INFO: successfully validated that service endpoint-test2 in namespace services-3856 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 04/28/23 17:12:10.158
  E0428 17:12:10.771310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:12:11.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-3856 exec execpod9pqj4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 28 17:12:11.330: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 28 17:12:11.330: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:12:11.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-3856 exec execpod9pqj4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.128.174 80'
  Apr 28 17:12:11.466: INFO: stderr: "+ nc -v -t -w 2 10.43.128.174 80\nConnection to 10.43.128.174 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Apr 28 17:12:11.466: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-3856 @ 04/28/23 17:12:11.466
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3856 to expose endpoints map[] @ 04/28/23 17:12:11.498
  Apr 28 17:12:11.534: INFO: successfully validated that service endpoint-test2 in namespace services-3856 exposes endpoints map[]
  Apr 28 17:12:11.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3856" for this suite. @ 04/28/23 17:12:11.566
• [19.481 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 04/28/23 17:12:11.586
  Apr 28 17:12:11.587: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename limitrange @ 04/28/23 17:12:11.587
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:12:11.657
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:12:11.664
  STEP: Creating a LimitRange @ 04/28/23 17:12:11.668
  STEP: Setting up watch @ 04/28/23 17:12:11.668
  E0428 17:12:11.772194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Submitting a LimitRange @ 04/28/23 17:12:11.777
  STEP: Verifying LimitRange creation was observed @ 04/28/23 17:12:11.782
  STEP: Fetching the LimitRange to ensure it has proper values @ 04/28/23 17:12:11.782
  Apr 28 17:12:11.785: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Apr 28 17:12:11.785: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 04/28/23 17:12:11.785
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 04/28/23 17:12:11.789
  Apr 28 17:12:11.793: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Apr 28 17:12:11.793: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 04/28/23 17:12:11.793
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 04/28/23 17:12:11.801
  Apr 28 17:12:11.809: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Apr 28 17:12:11.809: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 04/28/23 17:12:11.809
  STEP: Failing to create a Pod with more than max resources @ 04/28/23 17:12:11.82
  STEP: Updating a LimitRange @ 04/28/23 17:12:11.822
  STEP: Verifying LimitRange updating is effective @ 04/28/23 17:12:11.828
  E0428 17:12:12.772309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:13.772463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod with less than former min resources @ 04/28/23 17:12:13.831
  STEP: Failing to create a Pod with more than max resources @ 04/28/23 17:12:13.835
  STEP: Deleting a LimitRange @ 04/28/23 17:12:13.837
  STEP: Verifying the LimitRange was deleted @ 04/28/23 17:12:13.845
  E0428 17:12:14.772632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:15.772747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:16.772876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:17.773053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:18.773623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:12:18.849: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 04/28/23 17:12:18.849
  Apr 28 17:12:18.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-9211" for this suite. @ 04/28/23 17:12:18.864
• [7.295 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 04/28/23 17:12:18.883
  Apr 28 17:12:18.883: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename configmap @ 04/28/23 17:12:18.884
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:12:18.922
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:12:18.934
  STEP: Creating configMap configmap-1657/configmap-test-8b10b07e-1e90-4272-bf5f-2c75fc43c466 @ 04/28/23 17:12:18.947
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:12:18.975
  E0428 17:12:19.775451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:20.775554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:21.775688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:22.775832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:12:23.073
  Apr 28 17:12:23.075: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-configmaps-a4136b2e-c868-4953-b8cf-8ddb166e76ba container env-test: <nil>
  STEP: delete the pod @ 04/28/23 17:12:23.087
  Apr 28 17:12:23.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1657" for this suite. @ 04/28/23 17:12:23.106
• [4.230 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 04/28/23 17:12:23.113
  Apr 28 17:12:23.113: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename field-validation @ 04/28/23 17:12:23.114
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:12:23.129
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:12:23.134
  STEP: apply creating a deployment @ 04/28/23 17:12:23.137
  Apr 28 17:12:23.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-10" for this suite. @ 04/28/23 17:12:23.155
• [0.047 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 04/28/23 17:12:23.162
  Apr 28 17:12:23.162: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 17:12:23.163
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:12:23.175
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:12:23.179
  STEP: Counting existing ResourceQuota @ 04/28/23 17:12:23.182
  E0428 17:12:23.775891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:24.776945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:25.777497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:26.778432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:27.779489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/28/23 17:12:28.188
  STEP: Ensuring resource quota status is calculated @ 04/28/23 17:12:28.194
  E0428 17:12:28.780360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:29.780587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 04/28/23 17:12:30.198
  STEP: Ensuring resource quota status captures replication controller creation @ 04/28/23 17:12:30.208
  E0428 17:12:30.780946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:31.781172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 04/28/23 17:12:32.215
  STEP: Ensuring resource quota status released usage @ 04/28/23 17:12:32.222
  E0428 17:12:32.781670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:33.782229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:12:34.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2342" for this suite. @ 04/28/23 17:12:34.237
• [11.083 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 04/28/23 17:12:34.246
  Apr 28 17:12:34.246: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:12:34.246
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:12:34.299
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:12:34.31
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 17:12:34.339
  E0428 17:12:34.782985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:35.783133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:36.783396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:37.783494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:12:38.399
  Apr 28 17:12:38.401: INFO: Trying to get logs from node ip-172-31-1-213 pod downwardapi-volume-94e59930-3659-4cd9-9fbd-a3f6ec466cb6 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 17:12:38.409
  Apr 28 17:12:38.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6701" for this suite. @ 04/28/23 17:12:38.426
• [4.187 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 04/28/23 17:12:38.436
  Apr 28 17:12:38.436: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename cronjob @ 04/28/23 17:12:38.44
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:12:38.455
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:12:38.457
  STEP: Creating a suspended cronjob @ 04/28/23 17:12:38.46
  STEP: Ensuring no jobs are scheduled @ 04/28/23 17:12:38.469
  E0428 17:12:38.783734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:39.783862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:40.783988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:41.784364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:42.784426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:43.785594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:44.786334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:45.786731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:46.787613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:47.787760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:48.787865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:49.788083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:50.788741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:51.788929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:52.789077      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:53.789447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:54.790513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:55.790653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:56.791609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:57.791822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:58.792262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:59.792499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:00.793337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:01.793547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:02.794392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:03.794944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:04.795906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:05.796054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:06.796617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:07.796736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:08.796818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:09.796962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:10.797551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:11.797695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:12.797824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:13.798177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:14.799214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:15.799326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:16.800100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:17.800555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:18.800779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:19.801711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:20.802316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:21.802447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:22.802597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:23.802728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:24.803781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:25.804007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:26.804712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:27.804843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:28.805056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:29.805171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:30.805893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:31.806022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:32.806623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:33.806705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:34.807612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:35.807750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:36.808598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:37.808723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:38.809216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:39.809351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:40.809846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:41.809955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:42.810092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:43.810444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:44.811113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:45.811257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:46.811371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:47.811565      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:48.812056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:49.812287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:50.812413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:51.812622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:52.813188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:53.813581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:54.814554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:55.814822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:56.814930      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:57.815161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:58.816220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:59.816349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:00.817210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:01.817475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:02.818552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:03.818859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:04.819611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:05.819759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:06.820258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:07.820396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:08.820468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:09.820681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:10.821469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:11.821828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:12.821957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:13.822338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:14.822473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:15.822682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:16.822904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:17.823131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:18.823292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:19.823541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:20.824555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:21.824685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:22.824819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:23.825917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:24.826042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:25.826183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:26.826321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:27.826460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:28.826588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:29.826815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:30.827738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:31.827857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:32.828795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:33.829777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:34.830332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:35.830470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:36.831340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:37.832390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:38.833444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:39.833590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:40.834638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:41.834913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:42.835040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:43.835706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:44.836414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:45.836546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:46.837043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:47.837204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:48.837915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:49.838042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:50.838187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:51.838597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:52.838617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:53.838750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:54.839451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:55.839595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:56.840374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:57.840503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:58.840619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:59.840844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:00.840982      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:01.841351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:02.842231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:03.842672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:04.843595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:05.843752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:06.843892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:07.844114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:08.844267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:09.844406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:10.844543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:11.844693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:12.844841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:13.845455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:14.846328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:15.847267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:16.847314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:17.847473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:18.848466      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:19.848577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:20.849072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:21.849220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:22.849488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:23.850419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:24.851452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:25.851540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:26.852639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:27.852770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:28.852899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:29.853109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:30.853185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:31.853394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:32.853976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:33.854333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:34.855202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:35.855302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:36.856344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:37.856584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:38.856682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:39.856995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:40.857994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:41.858134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:42.859065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:43.859259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:44.859875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:45.860084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:46.860657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:47.860943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:48.861729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:49.861952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:50.862603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:51.862736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:52.863678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:53.863949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:54.864730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:55.864981      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:56.866067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:57.866213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:58.866316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:59.866642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:00.867680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:01.867800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:02.867932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:03.867989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:04.868368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:05.868570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:06.869595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:07.869955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:08.870703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:09.870959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:10.871730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:11.871836      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:12.872772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:13.873786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:14.874619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:15.874805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:16.874976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:17.875878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:18.876341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:19.876441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:20.876646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:21.876893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:22.877947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:23.878284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:24.879154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:25.879299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:26.879482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:27.879711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:28.880871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:29.881079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:30.881893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:31.882016      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:32.882148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:33.882508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:34.883639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:35.883848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:36.884739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:37.884882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:38.885441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:39.885562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:40.885652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:41.886326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:42.886458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:43.886712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:44.886907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:45.887039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:46.887472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:47.888380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:48.888495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:49.888612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:50.888767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:51.888979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:52.889105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:53.889495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:54.889608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:55.889837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:56.889968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:57.890950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:58.891272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:59.891395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:00.891521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:01.891750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:02.891907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:03.892922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:04.893137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:05.893344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:06.894302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:07.894535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:08.894657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:09.894878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:10.897449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:11.897811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:12.897951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:13.898352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:14.899063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:15.899278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:16.899358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:17.899578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:18.900244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:19.900586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:20.901411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:21.901563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:22.902617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:23.902755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:24.903129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:25.903278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:26.904128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:27.904199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:28.904379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:29.904564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:30.905423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:31.905574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:32.906374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:33.906710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:34.906777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:35.906987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:36.907481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:37.907283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring no job exists by listing jobs explicitly @ 04/28/23 17:17:38.477
  STEP: Removing cronjob @ 04/28/23 17:17:38.479
  Apr 28 17:17:38.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-970" for this suite. @ 04/28/23 17:17:38.487
• [300.056 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 04/28/23 17:17:38.494
  Apr 28 17:17:38.494: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/28/23 17:17:38.495
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:17:38.51
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:17:38.514
  Apr 28 17:17:38.518: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 17:17:38.907412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:17:39.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-5824" for this suite. @ 04/28/23 17:17:39.061
• [0.578 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 04/28/23 17:17:39.072
  Apr 28 17:17:39.072: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename services @ 04/28/23 17:17:39.073
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:17:39.088
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:17:39.094
  STEP: creating service in namespace services-821 @ 04/28/23 17:17:39.098
  STEP: creating service affinity-nodeport-transition in namespace services-821 @ 04/28/23 17:17:39.098
  STEP: creating replication controller affinity-nodeport-transition in namespace services-821 @ 04/28/23 17:17:39.115
  I0428 17:17:39.152167      19 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-821, replica count: 3
  E0428 17:17:39.907545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:40.908558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:41.908817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 17:17:42.203460      19 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 28 17:17:42.226: INFO: Creating new exec pod
  E0428 17:17:42.909211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:43.909601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:44.909732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:17:45.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-821 exec execpod-affinityl8kl9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Apr 28 17:17:45.445: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-transition 80\n+ echo hostName\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Apr 28 17:17:45.445: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:17:45.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-821 exec execpod-affinityl8kl9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.136.77 80'
  Apr 28 17:17:45.582: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.136.77 80\nConnection to 10.43.136.77 80 port [tcp/http] succeeded!\n"
  Apr 28 17:17:45.582: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:17:45.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-821 exec execpod-affinityl8kl9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.5.174 30928'
  Apr 28 17:17:45.721: INFO: stderr: "+ nc -v -t -w 2 172.31.5.174 30928\n+ echo hostName\nConnection to 172.31.5.174 30928 port [tcp/*] succeeded!\n"
  Apr 28 17:17:45.721: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:17:45.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-821 exec execpod-affinityl8kl9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.12.59 30928'
  Apr 28 17:17:45.861: INFO: stderr: "+ nc -v -t -w 2 172.31.12.59 30928\n+ echo hostName\nConnection to 172.31.12.59 30928 port [tcp/*] succeeded!\n"
  Apr 28 17:17:45.861: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:17:45.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-821 exec execpod-affinityl8kl9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.1.213:30928/ ; done'
  E0428 17:17:45.910512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:17:46.101: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n"
  Apr 28 17:17:46.101: INFO: stdout: "\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-hwf84\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-6xplz\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-hwf84\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-hwf84\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-6xplz\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-6xplz\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-6xplz"
  Apr 28 17:17:46.101: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.101: INFO: Received response from host: affinity-nodeport-transition-hwf84
  Apr 28 17:17:46.101: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.101: INFO: Received response from host: affinity-nodeport-transition-6xplz
  Apr 28 17:17:46.101: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.101: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.101: INFO: Received response from host: affinity-nodeport-transition-hwf84
  Apr 28 17:17:46.101: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.101: INFO: Received response from host: affinity-nodeport-transition-hwf84
  Apr 28 17:17:46.101: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.101: INFO: Received response from host: affinity-nodeport-transition-6xplz
  Apr 28 17:17:46.101: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.101: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.101: INFO: Received response from host: affinity-nodeport-transition-6xplz
  Apr 28 17:17:46.101: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.101: INFO: Received response from host: affinity-nodeport-transition-6xplz
  Apr 28 17:17:46.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-821 exec execpod-affinityl8kl9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.1.213:30928/ ; done'
  Apr 28 17:17:46.304: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.1.213:30928/\n"
  Apr 28 17:17:46.304: INFO: stdout: "\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-r4mq7\naffinity-nodeport-transition-r4mq7"
  Apr 28 17:17:46.304: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.304: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.304: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.304: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.304: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.304: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.304: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.304: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.304: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.304: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.304: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.304: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.304: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.304: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.304: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.304: INFO: Received response from host: affinity-nodeport-transition-r4mq7
  Apr 28 17:17:46.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 17:17:46.309: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-821, will wait for the garbage collector to delete the pods @ 04/28/23 17:17:46.32
  Apr 28 17:17:46.381: INFO: Deleting ReplicationController affinity-nodeport-transition took: 5.606165ms
  Apr 28 17:17:46.482: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.496659ms
  E0428 17:17:46.911402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:47.911801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:48.912402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-821" for this suite. @ 04/28/23 17:17:49.202
• [10.135 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 04/28/23 17:17:49.207
  Apr 28 17:17:49.207: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename sched-preemption @ 04/28/23 17:17:49.208
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:17:49.223
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:17:49.225
  Apr 28 17:17:49.238: INFO: Waiting up to 1m0s for all nodes to be ready
  E0428 17:17:49.912925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:50.913064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:51.913430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:52.913674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:53.914684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:54.914900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:55.915682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:56.915792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:57.916030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:58.916521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:59.917500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:00.917730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:01.918700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:02.918854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:03.919593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:04.919708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:05.919828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:06.919955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:07.920116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:08.920592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:09.921564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:10.921814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:11.921950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:12.922123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:13.922236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:14.922360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:15.922596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:16.922725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:17.923353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:18.924398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:19.924545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:20.924768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:21.925341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:22.925432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:23.925643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:24.925851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:25.925973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:26.926139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:27.926270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:28.926556      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:29.926717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:30.926812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:31.927742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:32.928060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:33.928482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:34.928817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:35.929651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:36.929768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:37.930793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:38.931308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:39.931428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:40.931631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:41.932194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:42.932552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:43.933442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:44.934013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:45.934646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:46.934855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:47.934976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:48.935676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:18:49.278: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 04/28/23 17:18:49.281
  Apr 28 17:18:49.281: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename sched-preemption-path @ 04/28/23 17:18:49.283
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:18:49.307
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:18:49.314
  Apr 28 17:18:49.356: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Apr 28 17:18:49.363: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Apr 28 17:18:49.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 17:18:49.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-4861" for this suite. @ 04/28/23 17:18:49.542
  STEP: Destroying namespace "sched-preemption-9044" for this suite. @ 04/28/23 17:18:49.55
• [60.349 seconds]
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 04/28/23 17:18:49.558
  Apr 28 17:18:49.558: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename pod-network-test @ 04/28/23 17:18:49.559
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:18:49.583
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:18:49.595
  STEP: Performing setup for networking test in namespace pod-network-test-6792 @ 04/28/23 17:18:49.609
  STEP: creating a selector @ 04/28/23 17:18:49.609
  STEP: Creating the service pods in kubernetes @ 04/28/23 17:18:49.609
  Apr 28 17:18:49.609: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0428 17:18:49.936337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:50.936607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:51.937130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:52.937106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:53.937592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:54.937719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:55.938595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:56.938814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:57.939561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:58.940363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:59.940909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:00.941055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:01.941970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:02.942184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:03.942595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:04.942732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:05.943652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:06.943790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:07.944837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:08.945519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:09.946519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:10.946668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 04/28/23 17:19:11.802
  E0428 17:19:11.948145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:12.948698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:19:13.828: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
  Apr 28 17:19:13.828: INFO: Breadth first check of 10.42.2.43 on host 172.31.1.213...
  Apr 28 17:19:13.832: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.2.44:9080/dial?request=hostname&protocol=udp&host=10.42.2.43&port=8081&tries=1'] Namespace:pod-network-test-6792 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:19:13.832: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 17:19:13.833: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:19:13.833: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-6792/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.2.44%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.42.2.43%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 28 17:19:13.899: INFO: Waiting for responses: map[]
  Apr 28 17:19:13.899: INFO: reached 10.42.2.43 after 0/1 tries
  Apr 28 17:19:13.899: INFO: Breadth first check of 10.42.1.44 on host 172.31.10.53...
  Apr 28 17:19:13.912: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.2.44:9080/dial?request=hostname&protocol=udp&host=10.42.1.44&port=8081&tries=1'] Namespace:pod-network-test-6792 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:19:13.912: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 17:19:13.913: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:19:13.913: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-6792/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.2.44%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.42.1.44%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  E0428 17:19:13.949714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:19:13.968: INFO: Waiting for responses: map[]
  Apr 28 17:19:13.968: INFO: reached 10.42.1.44 after 0/1 tries
  Apr 28 17:19:13.968: INFO: Breadth first check of 10.42.0.175 on host 172.31.12.59...
  Apr 28 17:19:13.971: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.2.44:9080/dial?request=hostname&protocol=udp&host=10.42.0.175&port=8081&tries=1'] Namespace:pod-network-test-6792 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:19:13.971: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 17:19:13.972: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:19:13.972: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-6792/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.2.44%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.42.0.175%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 28 17:19:14.039: INFO: Waiting for responses: map[]
  Apr 28 17:19:14.039: INFO: reached 10.42.0.175 after 0/1 tries
  Apr 28 17:19:14.039: INFO: Breadth first check of 10.42.3.231 on host 172.31.5.174...
  Apr 28 17:19:14.042: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.2.44:9080/dial?request=hostname&protocol=udp&host=10.42.3.231&port=8081&tries=1'] Namespace:pod-network-test-6792 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:19:14.042: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 17:19:14.042: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:19:14.042: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-6792/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.2.44%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.42.3.231%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 28 17:19:14.096: INFO: Waiting for responses: map[]
  Apr 28 17:19:14.096: INFO: reached 10.42.3.231 after 0/1 tries
  Apr 28 17:19:14.096: INFO: Going to retry 0 out of 4 pods....
  Apr 28 17:19:14.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-6792" for this suite. @ 04/28/23 17:19:14.101
• [24.548 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 04/28/23 17:19:14.109
  Apr 28 17:19:14.109: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 17:19:14.11
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:19:14.125
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:19:14.129
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 17:19:14.133
  E0428 17:19:14.949811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:15.949921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:16.950062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:17.950306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:19:18.166
  Apr 28 17:19:18.174: INFO: Trying to get logs from node ip-172-31-1-213 pod downwardapi-volume-31745f7e-9cf6-4987-8a8f-b47ed3d891f9 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 17:19:18.187
  Apr 28 17:19:18.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8303" for this suite. @ 04/28/23 17:19:18.206
• [4.104 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 04/28/23 17:19:18.214
  Apr 28 17:19:18.214: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename configmap @ 04/28/23 17:19:18.215
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:19:18.238
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:19:18.242
  STEP: Creating configMap with name configmap-test-volume-map-5a771d53-52fd-4a4b-9554-0eae66216c92 @ 04/28/23 17:19:18.246
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:19:18.253
  E0428 17:19:18.950455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:19.950560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:20.950808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:21.951130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:19:22.284
  Apr 28 17:19:22.287: INFO: Trying to get logs from node ip-172-31-10-53 pod pod-configmaps-64c802a3-ba27-4ff6-b6b7-d1611e1b7b7b container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 17:19:22.307
  Apr 28 17:19:22.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7178" for this suite. @ 04/28/23 17:19:22.336
• [4.134 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 04/28/23 17:19:22.353
  Apr 28 17:19:22.353: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:19:22.355
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:19:22.371
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:19:22.375
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 17:19:22.378
  E0428 17:19:22.951257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:23.951315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:24.951406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:25.951628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:19:26.402
  Apr 28 17:19:26.405: INFO: Trying to get logs from node ip-172-31-1-213 pod downwardapi-volume-27362e36-f23f-4a8e-8d3e-16651f372084 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 17:19:26.41
  Apr 28 17:19:26.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7551" for this suite. @ 04/28/23 17:19:26.435
• [4.089 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 04/28/23 17:19:26.444
  Apr 28 17:19:26.444: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:19:26.444
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:19:26.459
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:19:26.463
  STEP: Creating projection with secret that has name projected-secret-test-284f36ea-fa9d-4cef-be73-3e22a4540a52 @ 04/28/23 17:19:26.47
  STEP: Creating a pod to test consume secrets @ 04/28/23 17:19:26.476
  E0428 17:19:26.951753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:27.952356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:28.952496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:29.952624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:19:30.495
  Apr 28 17:19:30.498: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-projected-secrets-9b1091bd-e337-44e1-997d-3a65fc18aea1 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 17:19:30.504
  Apr 28 17:19:30.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8191" for this suite. @ 04/28/23 17:19:30.521
• [4.084 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 04/28/23 17:19:30.529
  Apr 28 17:19:30.529: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:19:30.53
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:19:30.549
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:19:30.552
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 17:19:30.555
  E0428 17:19:30.953674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:31.953888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:32.954142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:33.954470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:19:34.58
  Apr 28 17:19:34.583: INFO: Trying to get logs from node ip-172-31-1-213 pod downwardapi-volume-4f867d35-7fb1-4a1f-b6c0-2add1b0c483d container client-container: <nil>
  STEP: delete the pod @ 04/28/23 17:19:34.587
  Apr 28 17:19:34.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2620" for this suite. @ 04/28/23 17:19:34.614
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 04/28/23 17:19:34.623
  Apr 28 17:19:34.623: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 17:19:34.624
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:19:34.641
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:19:34.644
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 04/28/23 17:19:34.652
  E0428 17:19:34.954951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:35.955167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:36.956054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:37.956258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:19:38.679
  Apr 28 17:19:38.682: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-991df231-1f36-4229-b644-4a6bc69e2acb container test-container: <nil>
  STEP: delete the pod @ 04/28/23 17:19:38.688
  Apr 28 17:19:38.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8124" for this suite. @ 04/28/23 17:19:38.708
• [4.091 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 04/28/23 17:19:38.715
  Apr 28 17:19:38.715: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename watch @ 04/28/23 17:19:38.717
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:19:38.746
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:19:38.754
  STEP: creating a watch on configmaps with label A @ 04/28/23 17:19:38.762
  STEP: creating a watch on configmaps with label B @ 04/28/23 17:19:38.765
  STEP: creating a watch on configmaps with label A or B @ 04/28/23 17:19:38.766
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 04/28/23 17:19:38.772
  Apr 28 17:19:38.781: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8641  17e0b1e9-4b1f-4d52-9992-32e1067d428f 259926 0 2023-04-28 17:19:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-28 17:19:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 17:19:38.782: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8641  17e0b1e9-4b1f-4d52-9992-32e1067d428f 259926 0 2023-04-28 17:19:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-28 17:19:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 04/28/23 17:19:38.782
  Apr 28 17:19:38.796: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8641  17e0b1e9-4b1f-4d52-9992-32e1067d428f 259928 0 2023-04-28 17:19:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-28 17:19:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 17:19:38.796: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8641  17e0b1e9-4b1f-4d52-9992-32e1067d428f 259928 0 2023-04-28 17:19:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-28 17:19:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 04/28/23 17:19:38.797
  Apr 28 17:19:38.810: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8641  17e0b1e9-4b1f-4d52-9992-32e1067d428f 259929 0 2023-04-28 17:19:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-28 17:19:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 17:19:38.811: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8641  17e0b1e9-4b1f-4d52-9992-32e1067d428f 259929 0 2023-04-28 17:19:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-28 17:19:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 04/28/23 17:19:38.811
  Apr 28 17:19:38.818: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8641  17e0b1e9-4b1f-4d52-9992-32e1067d428f 259930 0 2023-04-28 17:19:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-28 17:19:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 17:19:38.819: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8641  17e0b1e9-4b1f-4d52-9992-32e1067d428f 259930 0 2023-04-28 17:19:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-28 17:19:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 04/28/23 17:19:38.819
  Apr 28 17:19:38.830: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8641  345c367d-e578-419c-b224-38e43d94a6cd 259931 0 2023-04-28 17:19:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-28 17:19:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 17:19:38.831: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8641  345c367d-e578-419c-b224-38e43d94a6cd 259931 0 2023-04-28 17:19:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-28 17:19:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0428 17:19:38.956861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:39.957027      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:40.957235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:41.957379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:42.957499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:43.958441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:44.959209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:45.959427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:46.959650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:47.959852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 04/28/23 17:19:48.831
  Apr 28 17:19:48.837: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8641  345c367d-e578-419c-b224-38e43d94a6cd 259993 0 2023-04-28 17:19:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-28 17:19:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 17:19:48.837: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8641  345c367d-e578-419c-b224-38e43d94a6cd 259993 0 2023-04-28 17:19:38 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-28 17:19:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0428 17:19:48.960734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:49.960962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:50.961307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:51.961402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:52.961607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:53.961869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:54.962088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:55.962266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:56.962465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:57.962680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:19:58.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-8641" for this suite. @ 04/28/23 17:19:58.845
• [20.143 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 04/28/23 17:19:58.859
  Apr 28 17:19:58.859: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/28/23 17:19:58.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:19:58.878
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:19:58.883
  Apr 28 17:19:58.891: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 17:19:58.963707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:59.963866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/28/23 17:20:00.856
  Apr 28 17:20:00.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-28 --namespace=crd-publish-openapi-28 create -f -'
  E0428 17:20:00.964820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:01.964919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:20:02.718: INFO: stderr: ""
  Apr 28 17:20:02.718: INFO: stdout: "e2e-test-crd-publish-openapi-1249-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Apr 28 17:20:02.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-28 --namespace=crd-publish-openapi-28 delete e2e-test-crd-publish-openapi-1249-crds test-cr'
  Apr 28 17:20:02.917: INFO: stderr: ""
  Apr 28 17:20:02.917: INFO: stdout: "e2e-test-crd-publish-openapi-1249-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Apr 28 17:20:02.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-28 --namespace=crd-publish-openapi-28 apply -f -'
  E0428 17:20:02.966241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:03.966351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:20:04.336: INFO: stderr: ""
  Apr 28 17:20:04.336: INFO: stdout: "e2e-test-crd-publish-openapi-1249-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Apr 28 17:20:04.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-28 --namespace=crd-publish-openapi-28 delete e2e-test-crd-publish-openapi-1249-crds test-cr'
  Apr 28 17:20:04.416: INFO: stderr: ""
  Apr 28 17:20:04.416: INFO: stdout: "e2e-test-crd-publish-openapi-1249-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 04/28/23 17:20:04.416
  Apr 28 17:20:04.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-28 explain e2e-test-crd-publish-openapi-1249-crds'
  Apr 28 17:20:04.625: INFO: stderr: ""
  Apr 28 17:20:04.625: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-1249-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0428 17:20:04.967021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:05.973509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:20:06.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-28" for this suite. @ 04/28/23 17:20:06.341
• [7.488 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 04/28/23 17:20:06.348
  Apr 28 17:20:06.348: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename csistoragecapacity @ 04/28/23 17:20:06.349
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:20:06.39
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:20:06.409
  STEP: getting /apis @ 04/28/23 17:20:06.417
  STEP: getting /apis/storage.k8s.io @ 04/28/23 17:20:06.433
  STEP: getting /apis/storage.k8s.io/v1 @ 04/28/23 17:20:06.439
  STEP: creating @ 04/28/23 17:20:06.44
  STEP: watching @ 04/28/23 17:20:06.475
  Apr 28 17:20:06.475: INFO: starting watch
  STEP: getting @ 04/28/23 17:20:06.486
  STEP: listing in namespace @ 04/28/23 17:20:06.493
  STEP: listing across namespaces @ 04/28/23 17:20:06.504
  STEP: patching @ 04/28/23 17:20:06.515
  STEP: updating @ 04/28/23 17:20:06.52
  Apr 28 17:20:06.529: INFO: waiting for watch events with expected annotations in namespace
  Apr 28 17:20:06.529: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 04/28/23 17:20:06.529
  STEP: deleting a collection @ 04/28/23 17:20:06.548
  Apr 28 17:20:06.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-5567" for this suite. @ 04/28/23 17:20:06.568
• [0.237 seconds]
------------------------------
SSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 04/28/23 17:20:06.586
  Apr 28 17:20:06.586: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename runtimeclass @ 04/28/23 17:20:06.587
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:20:06.61
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:20:06.62
  Apr 28 17:20:06.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-4016" for this suite. @ 04/28/23 17:20:06.644
• [0.076 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 04/28/23 17:20:06.664
  Apr 28 17:20:06.664: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename events @ 04/28/23 17:20:06.668
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:20:06.748
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:20:06.75
  STEP: Create set of events @ 04/28/23 17:20:06.753
  Apr 28 17:20:06.763: INFO: created test-event-1
  Apr 28 17:20:06.769: INFO: created test-event-2
  Apr 28 17:20:06.774: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 04/28/23 17:20:06.774
  STEP: delete collection of events @ 04/28/23 17:20:06.777
  Apr 28 17:20:06.777: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 04/28/23 17:20:06.797
  Apr 28 17:20:06.797: INFO: requesting list of events to confirm quantity
  Apr 28 17:20:06.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-8425" for this suite. @ 04/28/23 17:20:06.805
• [0.149 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 04/28/23 17:20:06.812
  Apr 28 17:20:06.812: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 17:20:06.813
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:20:06.832
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:20:06.835
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 04/28/23 17:20:06.838
  E0428 17:20:06.973845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:07.974024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:08.974856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:09.974982      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:20:10.871
  Apr 28 17:20:10.875: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-7517cedb-f845-4d97-9592-afc2414f519b container test-container: <nil>
  STEP: delete the pod @ 04/28/23 17:20:10.889
  Apr 28 17:20:10.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1714" for this suite. @ 04/28/23 17:20:10.927
• [4.128 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 04/28/23 17:20:10.947
  Apr 28 17:20:10.947: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename prestop @ 04/28/23 17:20:10.948
  E0428 17:20:10.975636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:20:10.981
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:20:10.984
  STEP: Creating server pod server in namespace prestop-1454 @ 04/28/23 17:20:10.987
  STEP: Waiting for pods to come up. @ 04/28/23 17:20:11.005
  E0428 17:20:11.976106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:12.976315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating tester pod tester in namespace prestop-1454 @ 04/28/23 17:20:13.811
  E0428 17:20:13.976864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:14.977087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pre-stop pod @ 04/28/23 17:20:15.832
  E0428 17:20:15.977711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:16.977931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:17.978341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:18.978871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:19.979284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:20:20.846: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Apr 28 17:20:20.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 04/28/23 17:20:20.858
  STEP: Destroying namespace "prestop-1454" for this suite. @ 04/28/23 17:20:20.888
• [9.959 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 04/28/23 17:20:20.907
  Apr 28 17:20:20.907: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 17:20:20.909
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:20:20.937
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:20:20.944
  STEP: Creating a pod to test downward api env vars @ 04/28/23 17:20:20.948
  E0428 17:20:20.979901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:21.980041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:22.980300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:23.980462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:24.980628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:20:24.982
  Apr 28 17:20:24.985: INFO: Trying to get logs from node ip-172-31-10-53 pod downward-api-62dc8b5d-75f6-4e4e-a27a-075f752d603e container dapi-container: <nil>
  STEP: delete the pod @ 04/28/23 17:20:24.994
  Apr 28 17:20:25.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6925" for this suite. @ 04/28/23 17:20:25.016
• [4.118 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 04/28/23 17:20:25.026
  Apr 28 17:20:25.026: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/28/23 17:20:25.027
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:20:25.047
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:20:25.053
  STEP: create the container to handle the HTTPGet hook request. @ 04/28/23 17:20:25.062
  E0428 17:20:25.980743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:26.980823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 04/28/23 17:20:27.092
  E0428 17:20:27.980960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:28.981226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 04/28/23 17:20:29.132
  STEP: delete the pod with lifecycle hook @ 04/28/23 17:20:29.138
  E0428 17:20:29.981374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:30.981717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:31.982664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:32.982873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:20:33.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-7330" for this suite. @ 04/28/23 17:20:33.159
• [8.142 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 04/28/23 17:20:33.168
  Apr 28 17:20:33.169: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/28/23 17:20:33.169
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:20:33.195
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:20:33.198
  Apr 28 17:20:33.203: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 17:20:33.982959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:20:34.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-8845" for this suite. @ 04/28/23 17:20:34.24
• [1.077 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 04/28/23 17:20:34.247
  Apr 28 17:20:34.247: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename replicaset @ 04/28/23 17:20:34.248
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:20:34.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:20:34.281
  STEP: Create a Replicaset @ 04/28/23 17:20:34.305
  STEP: Verify that the required pods have come up. @ 04/28/23 17:20:34.336
  Apr 28 17:20:34.341: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0428 17:20:34.983112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:35.983293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:36.983647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:37.983843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:38.984693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:20:39.347: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/28/23 17:20:39.347
  STEP: Getting /status @ 04/28/23 17:20:39.347
  Apr 28 17:20:39.350: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 04/28/23 17:20:39.35
  Apr 28 17:20:39.359: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 04/28/23 17:20:39.359
  Apr 28 17:20:39.361: INFO: Observed &ReplicaSet event: ADDED
  Apr 28 17:20:39.361: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 28 17:20:39.361: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 28 17:20:39.362: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 28 17:20:39.362: INFO: Found replicaset test-rs in namespace replicaset-8970 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 28 17:20:39.362: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 04/28/23 17:20:39.362
  Apr 28 17:20:39.362: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 28 17:20:39.369: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 04/28/23 17:20:39.369
  Apr 28 17:20:39.371: INFO: Observed &ReplicaSet event: ADDED
  Apr 28 17:20:39.371: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 28 17:20:39.372: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 28 17:20:39.372: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 28 17:20:39.372: INFO: Observed replicaset test-rs in namespace replicaset-8970 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 28 17:20:39.372: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 28 17:20:39.372: INFO: Found replicaset test-rs in namespace replicaset-8970 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Apr 28 17:20:39.372: INFO: Replicaset test-rs has a patched status
  Apr 28 17:20:39.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-8970" for this suite. @ 04/28/23 17:20:39.376
• [5.136 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 04/28/23 17:20:39.385
  Apr 28 17:20:39.385: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename daemonsets @ 04/28/23 17:20:39.386
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:20:39.411
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:20:39.414
  STEP: Creating simple DaemonSet "daemon-set" @ 04/28/23 17:20:39.442
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/28/23 17:20:39.449
  Apr 28 17:20:39.457: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 17:20:39.458: INFO: Node ip-172-31-1-213 is running 0 daemon pod, expected 1
  E0428 17:20:39.985280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:20:40.476: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 28 17:20:40.476: INFO: Node ip-172-31-1-213 is running 0 daemon pod, expected 1
  E0428 17:20:40.986057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:20:41.478: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 28 17:20:41.478: INFO: Node ip-172-31-10-53 is running 0 daemon pod, expected 1
  E0428 17:20:41.986907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:20:42.466: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 28 17:20:42.466: INFO: Node ip-172-31-10-53 is running 0 daemon pod, expected 1
  E0428 17:20:42.987066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:20:43.476: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
  Apr 28 17:20:43.476: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: Getting /status @ 04/28/23 17:20:43.484
  Apr 28 17:20:43.490: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 04/28/23 17:20:43.49
  Apr 28 17:20:43.503: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 04/28/23 17:20:43.503
  Apr 28 17:20:43.505: INFO: Observed &DaemonSet event: ADDED
  Apr 28 17:20:43.505: INFO: Observed &DaemonSet event: MODIFIED
  Apr 28 17:20:43.505: INFO: Observed &DaemonSet event: MODIFIED
  Apr 28 17:20:43.506: INFO: Observed &DaemonSet event: MODIFIED
  Apr 28 17:20:43.506: INFO: Observed &DaemonSet event: MODIFIED
  Apr 28 17:20:43.506: INFO: Observed &DaemonSet event: MODIFIED
  Apr 28 17:20:43.506: INFO: Found daemon set daemon-set in namespace daemonsets-6558 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 28 17:20:43.506: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 04/28/23 17:20:43.507
  STEP: watching for the daemon set status to be patched @ 04/28/23 17:20:43.513
  Apr 28 17:20:43.515: INFO: Observed &DaemonSet event: ADDED
  Apr 28 17:20:43.515: INFO: Observed &DaemonSet event: MODIFIED
  Apr 28 17:20:43.515: INFO: Observed &DaemonSet event: MODIFIED
  Apr 28 17:20:43.515: INFO: Observed &DaemonSet event: MODIFIED
  Apr 28 17:20:43.516: INFO: Observed &DaemonSet event: MODIFIED
  Apr 28 17:20:43.516: INFO: Observed &DaemonSet event: MODIFIED
  Apr 28 17:20:43.516: INFO: Observed daemon set daemon-set in namespace daemonsets-6558 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 28 17:20:43.516: INFO: Observed &DaemonSet event: MODIFIED
  Apr 28 17:20:43.516: INFO: Found daemon set daemon-set in namespace daemonsets-6558 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Apr 28 17:20:43.516: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 04/28/23 17:20:43.52
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6558, will wait for the garbage collector to delete the pods @ 04/28/23 17:20:43.52
  Apr 28 17:20:43.583: INFO: Deleting DaemonSet.extensions daemon-set took: 8.211904ms
  Apr 28 17:20:43.684: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.93838ms
  E0428 17:20:43.987528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:44.988415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:20:45.889: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 17:20:45.889: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 28 17:20:45.893: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"260566"},"items":null}

  Apr 28 17:20:45.896: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"260566"},"items":null}

  Apr 28 17:20:45.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6558" for this suite. @ 04/28/23 17:20:45.924
• [6.544 seconds]
------------------------------
SS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 04/28/23 17:20:45.93
  Apr 28 17:20:45.930: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename configmap @ 04/28/23 17:20:45.931
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:20:45.956
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:20:45.961
  STEP: Creating configMap with name configmap-test-volume-2538d40e-526b-498d-a442-5b98dffe6f28 @ 04/28/23 17:20:45.963
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:20:45.968
  E0428 17:20:45.988433      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:46.989133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:47.989411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:48.989689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:49.989786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:20:49.995
  Apr 28 17:20:50.011: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-configmaps-6388264f-afa5-4ec4-a62e-4e61b556ff76 container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 17:20:50.024
  Apr 28 17:20:50.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2012" for this suite. @ 04/28/23 17:20:50.056
• [4.140 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 04/28/23 17:20:50.071
  Apr 28 17:20:50.071: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename field-validation @ 04/28/23 17:20:50.074
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:20:50.106
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:20:50.114
  Apr 28 17:20:50.120: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 17:20:50.990556      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:51.990668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0428 17:20:52.681125      19 warnings.go:70] unknown field "alpha"
  W0428 17:20:52.681152      19 warnings.go:70] unknown field "beta"
  W0428 17:20:52.681158      19 warnings.go:70] unknown field "delta"
  W0428 17:20:52.681182      19 warnings.go:70] unknown field "epsilon"
  W0428 17:20:52.681190      19 warnings.go:70] unknown field "gamma"
  Apr 28 17:20:52.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-530" for this suite. @ 04/28/23 17:20:52.705
• [2.640 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 04/28/23 17:20:52.713
  Apr 28 17:20:52.713: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename statefulset @ 04/28/23 17:20:52.714
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:20:52.732
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:20:52.742
  STEP: Creating service test in namespace statefulset-1416 @ 04/28/23 17:20:52.746
  STEP: Creating statefulset ss in namespace statefulset-1416 @ 04/28/23 17:20:52.755
  Apr 28 17:20:52.770: INFO: Found 0 stateful pods, waiting for 1
  E0428 17:20:52.991052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:53.991273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:54.991486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:55.991771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:56.991995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:57.992964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:58.993747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:59.993951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:00.994173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:01.994401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:21:02.775: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 04/28/23 17:21:02.781
  STEP: Getting /status @ 04/28/23 17:21:02.79
  Apr 28 17:21:02.794: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 04/28/23 17:21:02.794
  Apr 28 17:21:02.805: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 04/28/23 17:21:02.805
  Apr 28 17:21:02.806: INFO: Observed &StatefulSet event: ADDED
  Apr 28 17:21:02.806: INFO: Found Statefulset ss in namespace statefulset-1416 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 28 17:21:02.806: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 04/28/23 17:21:02.806
  Apr 28 17:21:02.806: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 28 17:21:02.814: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 04/28/23 17:21:02.814
  Apr 28 17:21:02.816: INFO: Observed &StatefulSet event: ADDED
  Apr 28 17:21:02.816: INFO: Observed Statefulset ss in namespace statefulset-1416 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 28 17:21:02.816: INFO: Observed &StatefulSet event: MODIFIED
  Apr 28 17:21:02.817: INFO: Deleting all statefulset in ns statefulset-1416
  Apr 28 17:21:02.820: INFO: Scaling statefulset ss to 0
  E0428 17:21:02.995363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:03.996428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:04.996626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:05.996849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:06.997071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:07.998406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:08.998462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:09.998718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:10.998886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:11.999030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:21:12.837: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 17:21:12.840: INFO: Deleting statefulset ss
  Apr 28 17:21:12.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1416" for this suite. @ 04/28/23 17:21:12.857
• [20.156 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 04/28/23 17:21:12.9
  Apr 28 17:21:12.900: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename taint-multiple-pods @ 04/28/23 17:21:12.902
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:21:12.938
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:21:12.944
  Apr 28 17:21:12.950: INFO: Waiting up to 1m0s for all nodes to be ready
  E0428 17:21:12.999845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:14.000454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:15.000578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:16.000867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:17.001317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:18.001440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:19.002353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:20.003691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:21.004213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:22.004352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:23.004686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:24.004966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:25.005620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:26.005902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:27.006630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:28.006907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:29.007004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:30.007166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:31.007291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:32.007414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:33.007656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:34.007961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:35.008232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:36.008456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:37.008928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:38.009095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:39.009219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:40.009341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:41.009863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:42.010830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:43.011879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:44.011925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:45.012605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:46.012819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:47.013245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:48.013384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:49.014159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:50.014302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:51.015344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:52.015588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:53.016629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:54.016721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:55.017323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:56.017621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:57.017910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:58.018272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:59.018406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:00.018550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:01.018613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:02.018821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:03.019675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:04.019893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:05.020736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:06.020876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:07.021061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:08.021270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:09.021331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:10.021497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:11.021839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:12.022060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:22:12.973: INFO: Waiting for terminating namespaces to be deleted...
  Apr 28 17:22:12.976: INFO: Starting informer...
  STEP: Starting pods... @ 04/28/23 17:22:12.976
  E0428 17:22:13.022229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:22:13.192: INFO: Pod1 is running on ip-172-31-1-213. Tainting Node
  E0428 17:22:14.022819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:15.023450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:22:15.409: INFO: Pod2 is running on ip-172-31-1-213. Tainting Node
  STEP: Trying to apply a taint on the Node @ 04/28/23 17:22:15.409
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/28/23 17:22:15.425
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 04/28/23 17:22:15.481
  E0428 17:22:16.024125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:17.024277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:18.024362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:19.025203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:20.025339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:21.025743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:22:21.264: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  E0428 17:22:22.026771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:23.027115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:24.027535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:25.027863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:26.028076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:27.028494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:28.028693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:29.029191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:30.029290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:31.030330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:32.031405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:33.031553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:34.031790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:35.032110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:36.032338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:37.032521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:38.032727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:39.033488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:40.033661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:41.034503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:42.034630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:22:42.298: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Apr 28 17:22:42.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/28/23 17:22:42.331
  STEP: Destroying namespace "taint-multiple-pods-6856" for this suite. @ 04/28/23 17:22:42.348
• [89.483 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 04/28/23 17:22:42.385
  Apr 28 17:22:42.385: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename kubelet-test @ 04/28/23 17:22:42.388
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:22:42.43
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:22:42.435
  Apr 28 17:22:42.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8515" for this suite. @ 04/28/23 17:22:42.478
• [0.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 04/28/23 17:22:42.49
  Apr 28 17:22:42.490: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename disruption @ 04/28/23 17:22:42.491
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:22:42.511
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:22:42.517
  STEP: Creating a kubernetes client @ 04/28/23 17:22:42.522
  Apr 28 17:22:42.522: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename disruption-2 @ 04/28/23 17:22:42.523
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:22:42.569
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:22:42.577
  STEP: Waiting for the pdb to be processed @ 04/28/23 17:22:42.594
  E0428 17:22:43.035132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 04/28/23 17:22:43.24
  STEP: Waiting for the pdb to be processed @ 04/28/23 17:22:43.249
  STEP: listing a collection of PDBs across all namespaces @ 04/28/23 17:22:43.254
  STEP: listing a collection of PDBs in namespace disruption-251 @ 04/28/23 17:22:43.257
  STEP: deleting a collection of PDBs @ 04/28/23 17:22:43.262
  STEP: Waiting for the PDB collection to be deleted @ 04/28/23 17:22:43.273
  Apr 28 17:22:43.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 17:22:43.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-2829" for this suite. @ 04/28/23 17:22:43.283
  STEP: Destroying namespace "disruption-251" for this suite. @ 04/28/23 17:22:43.296
• [0.815 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 04/28/23 17:22:43.305
  Apr 28 17:22:43.305: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename events @ 04/28/23 17:22:43.306
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:22:43.378
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:22:43.381
  STEP: creating a test event @ 04/28/23 17:22:43.384
  STEP: listing all events in all namespaces @ 04/28/23 17:22:43.391
  STEP: patching the test event @ 04/28/23 17:22:43.396
  STEP: fetching the test event @ 04/28/23 17:22:43.403
  STEP: updating the test event @ 04/28/23 17:22:43.406
  STEP: getting the test event @ 04/28/23 17:22:43.414
  STEP: deleting the test event @ 04/28/23 17:22:43.417
  STEP: listing all events in all namespaces @ 04/28/23 17:22:43.426
  Apr 28 17:22:43.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-4922" for this suite. @ 04/28/23 17:22:43.437
• [0.140 seconds]
------------------------------
S
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 04/28/23 17:22:43.445
  Apr 28 17:22:43.445: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename pods @ 04/28/23 17:22:43.446
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:22:43.462
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:22:43.465
  STEP: creating the pod @ 04/28/23 17:22:43.469
  STEP: submitting the pod to kubernetes @ 04/28/23 17:22:43.469
  STEP: verifying QOS class is set on the pod @ 04/28/23 17:22:43.479
  Apr 28 17:22:43.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4469" for this suite. @ 04/28/23 17:22:43.496
• [0.062 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 04/28/23 17:22:43.507
  Apr 28 17:22:43.507: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename pods @ 04/28/23 17:22:43.508
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:22:43.533
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:22:43.538
  STEP: creating the pod @ 04/28/23 17:22:43.541
  STEP: submitting the pod to kubernetes @ 04/28/23 17:22:43.542
  W0428 17:22:43.557889      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0428 17:22:44.035299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:45.035386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 04/28/23 17:22:45.57
  STEP: updating the pod @ 04/28/23 17:22:45.574
  E0428 17:22:46.036081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:22:46.085: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b915a8b3-bdd8-4b1c-9e4a-956eb98a4680"
  E0428 17:22:47.036123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:48.036349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:49.037253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:50.037479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:22:50.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8821" for this suite. @ 04/28/23 17:22:50.102
• [6.600 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 04/28/23 17:22:50.107
  Apr 28 17:22:50.107: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 17:22:50.108
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:22:50.127
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:22:50.13
  STEP: Starting the proxy @ 04/28/23 17:22:50.138
  Apr 28 17:22:50.139: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-5285 proxy --unix-socket=/tmp/kubectl-proxy-unix2570753587/test'
  STEP: retrieving proxy /api/ output @ 04/28/23 17:22:50.197
  Apr 28 17:22:50.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5285" for this suite. @ 04/28/23 17:22:50.202
• [0.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 04/28/23 17:22:50.21
  Apr 28 17:22:50.210: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename containers @ 04/28/23 17:22:50.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:22:50.255
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:22:50.258
  STEP: Creating a pod to test override all @ 04/28/23 17:22:50.26
  E0428 17:22:51.037588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:52.037738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:53.037879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:54.038044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:22:54.284
  Apr 28 17:22:54.288: INFO: Trying to get logs from node ip-172-31-1-213 pod client-containers-acbbff66-2823-4843-b17b-f593e2be2288 container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 17:22:54.316
  Apr 28 17:22:54.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-7013" for this suite. @ 04/28/23 17:22:54.348
• [4.146 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 04/28/23 17:22:54.356
  Apr 28 17:22:54.356: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename secrets @ 04/28/23 17:22:54.357
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:22:54.38
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:22:54.384
  STEP: creating a secret @ 04/28/23 17:22:54.386
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 04/28/23 17:22:54.392
  STEP: patching the secret @ 04/28/23 17:22:54.397
  STEP: deleting the secret using a LabelSelector @ 04/28/23 17:22:54.407
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 04/28/23 17:22:54.416
  Apr 28 17:22:54.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7674" for this suite. @ 04/28/23 17:22:54.424
• [0.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 04/28/23 17:22:54.431
  Apr 28 17:22:54.431: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/28/23 17:22:54.432
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:22:54.445
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:22:54.448
  STEP: create the container to handle the HTTPGet hook request. @ 04/28/23 17:22:54.455
  E0428 17:22:55.039065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:56.039206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:57.039335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:58.039453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 04/28/23 17:22:58.484
  E0428 17:22:59.040349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:00.040508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 04/28/23 17:23:00.498
  E0428 17:23:01.040632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:02.040801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 04/28/23 17:23:02.51
  Apr 28 17:23:02.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-9469" for this suite. @ 04/28/23 17:23:02.532
• [8.110 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 04/28/23 17:23:02.541
  Apr 28 17:23:02.541: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 17:23:02.542
  E0428 17:23:03.041678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:23:03.264
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:23:03.273
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 04/28/23 17:23:03.277
  E0428 17:23:04.041882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:05.041944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:23:05.747
  Apr 28 17:23:05.752: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-367ab9ff-eb5d-4b8e-b4a9-300188228a46 container test-container: <nil>
  STEP: delete the pod @ 04/28/23 17:23:05.76
  Apr 28 17:23:05.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6103" for this suite. @ 04/28/23 17:23:05.793
• [3.264 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 04/28/23 17:23:05.807
  Apr 28 17:23:05.807: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename svcaccounts @ 04/28/23 17:23:05.81
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:23:05.854
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:23:05.858
  E0428 17:23:06.043012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:07.043454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 04/28/23 17:23:07.891
  Apr 28 17:23:07.891: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1880 pod-service-account-0e28ee62-4d34-4b20-8b76-6176a832c790 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 04/28/23 17:23:08.014
  Apr 28 17:23:08.014: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1880 pod-service-account-0e28ee62-4d34-4b20-8b76-6176a832c790 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  E0428 17:23:08.043683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 04/28/23 17:23:08.241
  Apr 28 17:23:08.241: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1880 pod-service-account-0e28ee62-4d34-4b20-8b76-6176a832c790 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Apr 28 17:23:08.390: INFO: Got root ca configmap in namespace "svcaccounts-1880"
  Apr 28 17:23:08.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1880" for this suite. @ 04/28/23 17:23:08.395
• [2.597 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 04/28/23 17:23:08.404
  Apr 28 17:23:08.404: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 17:23:08.405
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:23:08.419
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:23:08.421
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/28/23 17:23:08.43
  Apr 28 17:23:08.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-1019 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Apr 28 17:23:08.512: INFO: stderr: ""
  Apr 28 17:23:08.512: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 04/28/23 17:23:08.512
  E0428 17:23:09.044428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:10.044479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:11.045659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:12.045781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:13.045903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 04/28/23 17:23:13.564
  Apr 28 17:23:13.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-1019 get pod e2e-test-httpd-pod -o json'
  Apr 28 17:23:13.681: INFO: stderr: ""
  Apr 28 17:23:13.681: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-04-28T17:23:08Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1019\",\n        \"resourceVersion\": \"261671\",\n        \"uid\": \"e20eb632-c2e4-42d6-aa31-02b38ca988e8\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-bcbh4\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-1-213\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-bcbh4\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-28T17:23:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-28T17:23:09Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-28T17:23:09Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-28T17:23:08Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://1b8aef6d5e71772bef3e1e0a2c348f4ebea8426dd81218d579c3cc9b55e79aca\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-04-28T17:23:09Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.1.213\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.42.2.66\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.42.2.66\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-04-28T17:23:08Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 04/28/23 17:23:13.681
  Apr 28 17:23:13.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-1019 replace -f -'
  E0428 17:23:14.046400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:23:14.899: INFO: stderr: ""
  Apr 28 17:23:14.899: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 04/28/23 17:23:14.899
  Apr 28 17:23:14.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-1019 delete pods e2e-test-httpd-pod'
  E0428 17:23:15.046922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:16.047270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:23:16.569: INFO: stderr: ""
  Apr 28 17:23:16.569: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 28 17:23:16.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1019" for this suite. @ 04/28/23 17:23:16.575
• [8.180 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 04/28/23 17:23:16.584
  Apr 28 17:23:16.584: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 17:23:16.585
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:23:16.608
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:23:16.613
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/28/23 17:23:16.616
  Apr 28 17:23:16.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-8810 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Apr 28 17:23:16.698: INFO: stderr: ""
  Apr 28 17:23:16.698: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 04/28/23 17:23:16.698
  Apr 28 17:23:16.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-8810 delete pods e2e-test-httpd-pod'
  E0428 17:23:17.047381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:18.047517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:23:18.559: INFO: stderr: ""
  Apr 28 17:23:18.559: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 28 17:23:18.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8810" for this suite. @ 04/28/23 17:23:18.563
• [1.994 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 04/28/23 17:23:18.579
  Apr 28 17:23:18.579: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename configmap @ 04/28/23 17:23:18.58
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:23:18.613
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:23:18.616
  STEP: Creating configMap with name configmap-test-volume-map-258869ea-4eb9-4f8b-b768-161fe9f23c5f @ 04/28/23 17:23:18.624
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:23:18.635
  E0428 17:23:19.048321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:20.048536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:21.049609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:22.049753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:23:22.66
  Apr 28 17:23:22.662: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-configmaps-82b234df-c88b-4ac9-84ba-c0af942f1ecc container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 17:23:22.672
  Apr 28 17:23:22.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8976" for this suite. @ 04/28/23 17:23:22.695
• [4.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 04/28/23 17:23:22.719
  Apr 28 17:23:22.719: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename container-probe @ 04/28/23 17:23:22.719
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:23:22.749
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:23:22.753
  STEP: Creating pod liveness-af770a6e-3fb3-4c24-915e-ea2e48bcdfb9 in namespace container-probe-9005 @ 04/28/23 17:23:22.76
  E0428 17:23:23.049884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:24.049995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:23:24.792: INFO: Started pod liveness-af770a6e-3fb3-4c24-915e-ea2e48bcdfb9 in namespace container-probe-9005
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/28/23 17:23:24.792
  Apr 28 17:23:24.794: INFO: Initial restart count of pod liveness-af770a6e-3fb3-4c24-915e-ea2e48bcdfb9 is 0
  E0428 17:23:25.050033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:26.050163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:27.050645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:28.050774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:29.051506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:30.051732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:31.052383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:32.052497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:33.052545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:34.052682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:35.053611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:36.053849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:37.054291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:38.054442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:39.055016      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:40.055280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:41.056153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:42.056310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:43.057206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:44.057568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:23:44.839: INFO: Restart count of pod container-probe-9005/liveness-af770a6e-3fb3-4c24-915e-ea2e48bcdfb9 is now 1 (20.04548303s elapsed)
  Apr 28 17:23:44.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 17:23:44.843
  STEP: Destroying namespace "container-probe-9005" for this suite. @ 04/28/23 17:23:44.854
• [22.142 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 04/28/23 17:23:44.863
  Apr 28 17:23:44.863: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename endpointslicemirroring @ 04/28/23 17:23:44.864
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:23:44.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:23:44.884
  STEP: mirroring a new custom Endpoint @ 04/28/23 17:23:44.904
  Apr 28 17:23:44.930: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  E0428 17:23:45.058212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:46.058566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring an update to a custom Endpoint @ 04/28/23 17:23:46.936
  Apr 28 17:23:46.946: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  E0428 17:23:47.059214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:48.059395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring deletion of a custom Endpoint @ 04/28/23 17:23:48.951
  Apr 28 17:23:48.962: INFO: Waiting for 0 EndpointSlices to exist, got 1
  E0428 17:23:49.059517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:50.059806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:23:50.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-7546" for this suite. @ 04/28/23 17:23:50.969
• [6.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 04/28/23 17:23:50.975
  Apr 28 17:23:50.975: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename configmap @ 04/28/23 17:23:50.976
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:23:51.048
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:23:51.05
  STEP: Creating configMap that has name configmap-test-emptyKey-7ca9c32d-0231-4ee7-ad3f-2ddd3c8542b2 @ 04/28/23 17:23:51.053
  Apr 28 17:23:51.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3269" for this suite. @ 04/28/23 17:23:51.058
  E0428 17:23:51.060751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
• [0.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 04/28/23 17:23:51.067
  Apr 28 17:23:51.067: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename services @ 04/28/23 17:23:51.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:23:51.09
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:23:51.093
  STEP: creating service multi-endpoint-test in namespace services-5709 @ 04/28/23 17:23:51.096
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5709 to expose endpoints map[] @ 04/28/23 17:23:51.113
  Apr 28 17:23:51.134: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  E0428 17:23:52.061574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:23:52.147: INFO: successfully validated that service multi-endpoint-test in namespace services-5709 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-5709 @ 04/28/23 17:23:52.147
  E0428 17:23:53.061679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:54.062041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5709 to expose endpoints map[pod1:[100]] @ 04/28/23 17:23:54.166
  Apr 28 17:23:54.175: INFO: successfully validated that service multi-endpoint-test in namespace services-5709 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-5709 @ 04/28/23 17:23:54.175
  E0428 17:23:55.062203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:56.062310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5709 to expose endpoints map[pod1:[100] pod2:[101]] @ 04/28/23 17:23:56.191
  Apr 28 17:23:56.209: INFO: successfully validated that service multi-endpoint-test in namespace services-5709 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 04/28/23 17:23:56.209
  Apr 28 17:23:56.209: INFO: Creating new exec pod
  E0428 17:23:57.062457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:58.062694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:59.063331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:23:59.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-5709 exec execpodprsmx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Apr 28 17:23:59.382: INFO: stderr: "+ + ncecho -v -t hostName -w\n 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Apr 28 17:23:59.383: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:23:59.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-5709 exec execpodprsmx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.131.175 80'
  Apr 28 17:23:59.506: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.131.175 80\nConnection to 10.43.131.175 80 port [tcp/http] succeeded!\n"
  Apr 28 17:23:59.506: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:23:59.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-5709 exec execpodprsmx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Apr 28 17:23:59.698: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 81\n+ echo hostName\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Apr 28 17:23:59.698: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:23:59.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-5709 exec execpodprsmx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.131.175 81'
  Apr 28 17:23:59.893: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.131.175 81\nConnection to 10.43.131.175 81 port [tcp/*] succeeded!\n"
  Apr 28 17:23:59.893: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-5709 @ 04/28/23 17:23:59.893
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5709 to expose endpoints map[pod2:[101]] @ 04/28/23 17:23:59.914
  E0428 17:24:00.064077      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:24:00.942: INFO: successfully validated that service multi-endpoint-test in namespace services-5709 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-5709 @ 04/28/23 17:24:00.942
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5709 to expose endpoints map[] @ 04/28/23 17:24:00.957
  Apr 28 17:24:00.972: INFO: successfully validated that service multi-endpoint-test in namespace services-5709 exposes endpoints map[]
  Apr 28 17:24:00.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5709" for this suite. @ 04/28/23 17:24:01
• [9.941 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 04/28/23 17:24:01.011
  Apr 28 17:24:01.011: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename daemonsets @ 04/28/23 17:24:01.014
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:24:01.037
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:24:01.043
  E0428 17:24:01.064432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:24:01.081: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/28/23 17:24:01.097
  Apr 28 17:24:01.112: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 17:24:01.112: INFO: Node ip-172-31-1-213 is running 0 daemon pod, expected 1
  E0428 17:24:02.065061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:24:02.138: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 28 17:24:02.138: INFO: Node ip-172-31-1-213 is running 0 daemon pod, expected 1
  E0428 17:24:03.065311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:24:03.121: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 28 17:24:03.121: INFO: Node ip-172-31-10-53 is running 0 daemon pod, expected 1
  E0428 17:24:04.065329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:24:04.120: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
  Apr 28 17:24:04.120: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: Update daemon pods image. @ 04/28/23 17:24:04.133
  STEP: Check that daemon pods images are updated. @ 04/28/23 17:24:04.146
  Apr 28 17:24:04.156: INFO: Wrong image for pod: daemon-set-67hgt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 17:24:04.156: INFO: Wrong image for pod: daemon-set-dpbsk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 17:24:04.156: INFO: Wrong image for pod: daemon-set-vpl92. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 17:24:04.156: INFO: Wrong image for pod: daemon-set-zjhtw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0428 17:24:05.066409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:24:05.169: INFO: Wrong image for pod: daemon-set-67hgt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 17:24:05.169: INFO: Wrong image for pod: daemon-set-vpl92. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 17:24:05.169: INFO: Wrong image for pod: daemon-set-zjhtw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0428 17:24:06.067322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:24:06.169: INFO: Wrong image for pod: daemon-set-67hgt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 17:24:06.169: INFO: Pod daemon-set-crwwq is not available
  Apr 28 17:24:06.169: INFO: Wrong image for pod: daemon-set-vpl92. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 17:24:06.169: INFO: Wrong image for pod: daemon-set-zjhtw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0428 17:24:07.067421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:24:07.167: INFO: Wrong image for pod: daemon-set-67hgt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 17:24:07.167: INFO: Wrong image for pod: daemon-set-vpl92. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0428 17:24:08.067543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:24:08.168: INFO: Wrong image for pod: daemon-set-67hgt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 17:24:08.168: INFO: Pod daemon-set-9qkc6 is not available
  Apr 28 17:24:08.168: INFO: Wrong image for pod: daemon-set-vpl92. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0428 17:24:09.068342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:24:09.171: INFO: Wrong image for pod: daemon-set-vpl92. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0428 17:24:10.069192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:24:10.170: INFO: Pod daemon-set-722dm is not available
  Apr 28 17:24:10.170: INFO: Wrong image for pod: daemon-set-vpl92. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0428 17:24:11.069172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:12.069486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:24:12.168: INFO: Pod daemon-set-nbsnb is not available
  STEP: Check that daemon pods are still running on every node of the cluster. @ 04/28/23 17:24:12.171
  Apr 28 17:24:12.181: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 28 17:24:12.181: INFO: Node ip-172-31-10-53 is running 0 daemon pod, expected 1
  E0428 17:24:13.069673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:24:13.194: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
  Apr 28 17:24:13.194: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/28/23 17:24:13.217
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8266, will wait for the garbage collector to delete the pods @ 04/28/23 17:24:13.217
  Apr 28 17:24:13.283: INFO: Deleting DaemonSet.extensions daemon-set took: 8.001841ms
  Apr 28 17:24:13.385: INFO: Terminating DaemonSet.extensions daemon-set pods took: 102.359458ms
  E0428 17:24:14.069787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:24:14.892: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 17:24:14.892: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 28 17:24:14.897: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"262344"},"items":null}

  Apr 28 17:24:14.903: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"262344"},"items":null}

  Apr 28 17:24:14.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8266" for this suite. @ 04/28/23 17:24:14.926
• [13.924 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 04/28/23 17:24:14.936
  Apr 28 17:24:14.936: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 17:24:14.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:24:14.949
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:24:14.952
  STEP: Creating a pod to test emptydir volume type on node default medium @ 04/28/23 17:24:14.955
  E0428 17:24:15.070353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:16.070617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:24:16.969
  Apr 28 17:24:16.972: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-5a7ef56c-0f5d-4d38-bd56-a0a3a38acdf0 container test-container: <nil>
  STEP: delete the pod @ 04/28/23 17:24:16.978
  Apr 28 17:24:16.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-624" for this suite. @ 04/28/23 17:24:16.996
• [2.068 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 04/28/23 17:24:17.01
  Apr 28 17:24:17.010: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 17:24:17.011
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:24:17.032
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:24:17.037
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 04/28/23 17:24:17.042
  E0428 17:24:17.070919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:18.071009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:19.071906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:24:19.072
  Apr 28 17:24:19.079: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-0159cb0c-dfb5-41fb-9ad6-067125feee74 container test-container: <nil>
  STEP: delete the pod @ 04/28/23 17:24:19.095
  Apr 28 17:24:19.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3109" for this suite. @ 04/28/23 17:24:19.143
• [2.155 seconds]
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 04/28/23 17:24:19.165
  Apr 28 17:24:19.165: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename subpath @ 04/28/23 17:24:19.166
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:24:19.193
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:24:19.197
  STEP: Setting up data @ 04/28/23 17:24:19.2
  STEP: Creating pod pod-subpath-test-projected-mpcx @ 04/28/23 17:24:19.218
  STEP: Creating a pod to test atomic-volume-subpath @ 04/28/23 17:24:19.218
  E0428 17:24:20.072072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:21.072299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:22.072544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:23.073004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:24.073629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:25.073755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:26.073882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:27.074439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:28.074378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:29.074842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:30.074987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:31.075272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:32.075405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:33.075500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:34.076204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:35.076535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:36.077352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:37.077615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:38.077732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:39.078368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:40.079375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:41.079579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:42.080423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:43.080784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:24:43.497
  Apr 28 17:24:43.499: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-subpath-test-projected-mpcx container test-container-subpath-projected-mpcx: <nil>
  STEP: delete the pod @ 04/28/23 17:24:43.508
  STEP: Deleting pod pod-subpath-test-projected-mpcx @ 04/28/23 17:24:43.525
  Apr 28 17:24:43.525: INFO: Deleting pod "pod-subpath-test-projected-mpcx" in namespace "subpath-4452"
  Apr 28 17:24:43.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4452" for this suite. @ 04/28/23 17:24:43.544
• [24.389 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 04/28/23 17:24:43.555
  Apr 28 17:24:43.555: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename ingressclass @ 04/28/23 17:24:43.556
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:24:43.586
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:24:43.589
  STEP: getting /apis @ 04/28/23 17:24:43.598
  STEP: getting /apis/networking.k8s.io @ 04/28/23 17:24:43.604
  STEP: getting /apis/networking.k8s.iov1 @ 04/28/23 17:24:43.606
  STEP: creating @ 04/28/23 17:24:43.607
  STEP: getting @ 04/28/23 17:24:43.633
  STEP: listing @ 04/28/23 17:24:43.644
  STEP: watching @ 04/28/23 17:24:43.649
  Apr 28 17:24:43.649: INFO: starting watch
  STEP: patching @ 04/28/23 17:24:43.65
  STEP: updating @ 04/28/23 17:24:43.659
  Apr 28 17:24:43.673: INFO: waiting for watch events with expected annotations
  Apr 28 17:24:43.673: INFO: saw patched and updated annotations
  STEP: deleting @ 04/28/23 17:24:43.674
  STEP: deleting a collection @ 04/28/23 17:24:43.697
  Apr 28 17:24:43.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-8222" for this suite. @ 04/28/23 17:24:43.727
• [0.180 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 04/28/23 17:24:43.735
  Apr 28 17:24:43.735: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:24:43.735
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:24:43.781
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:24:43.785
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 17:24:43.795
  E0428 17:24:44.080937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:45.081057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:46.081668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:47.081921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:24:47.821
  Apr 28 17:24:47.824: INFO: Trying to get logs from node ip-172-31-1-213 pod downwardapi-volume-f2dbca44-1436-46ec-b052-1e01e7d324a8 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 17:24:47.829
  Apr 28 17:24:47.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9923" for this suite. @ 04/28/23 17:24:47.852
• [4.124 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 04/28/23 17:24:47.86
  Apr 28 17:24:47.860: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename services @ 04/28/23 17:24:47.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:24:47.88
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:24:47.883
  STEP: creating service nodeport-test with type=NodePort in namespace services-2871 @ 04/28/23 17:24:47.886
  STEP: creating replication controller nodeport-test in namespace services-2871 @ 04/28/23 17:24:47.908
  I0428 17:24:47.924893      19 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-2871, replica count: 2
  E0428 17:24:48.082379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:49.082369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:50.082503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 17:24:50.976551      19 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 28 17:24:50.976: INFO: Creating new exec pod
  E0428 17:24:51.083037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:52.083200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:53.083931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:54.084047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:55.084479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:24:56.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-2871 exec execpod7xhdp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  E0428 17:24:56.086322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:24:56.218: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Apr 28 17:24:56.218: INFO: stdout: ""
  E0428 17:24:57.086449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:24:57.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-2871 exec execpod7xhdp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Apr 28 17:24:57.356: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Apr 28 17:24:57.356: INFO: stdout: "nodeport-test-54wc5"
  Apr 28 17:24:57.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-2871 exec execpod7xhdp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.200.93 80'
  Apr 28 17:24:57.514: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.200.93 80\nConnection to 10.43.200.93 80 port [tcp/http] succeeded!\n"
  Apr 28 17:24:57.514: INFO: stdout: "nodeport-test-54wc5"
  Apr 28 17:24:57.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-2871 exec execpod7xhdp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.10.53 30957'
  Apr 28 17:24:57.842: INFO: stderr: "+ + echonc -v hostName\n -t -w 2 172.31.10.53 30957\nConnection to 172.31.10.53 30957 port [tcp/*] succeeded!\n"
  Apr 28 17:24:57.842: INFO: stdout: "nodeport-test-strg5"
  Apr 28 17:24:57.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-2871 exec execpod7xhdp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.5.174 30957'
  Apr 28 17:24:58.006: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.5.174 30957\nConnection to 172.31.5.174 30957 port [tcp/*] succeeded!\n"
  Apr 28 17:24:58.006: INFO: stdout: "nodeport-test-54wc5"
  Apr 28 17:24:58.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2871" for this suite. @ 04/28/23 17:24:58.01
• [10.158 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 04/28/23 17:24:58.018
  Apr 28 17:24:58.018: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename security-context-test @ 04/28/23 17:24:58.019
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:24:58.056
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:24:58.059
  E0428 17:24:58.087176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:59.088193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:00.088304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:25:00.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-223" for this suite. @ 04/28/23 17:25:00.597
• [2.602 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 04/28/23 17:25:00.623
  Apr 28 17:25:00.623: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename secrets @ 04/28/23 17:25:00.623
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:25:00.703
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:25:00.717
  STEP: Creating secret with name s-test-opt-del-355c01e5-2080-49eb-9456-102f09fa442c @ 04/28/23 17:25:00.735
  STEP: Creating secret with name s-test-opt-upd-cb97cce9-4fa5-4d19-8cd6-75ba5d4074d3 @ 04/28/23 17:25:00.749
  STEP: Creating the pod @ 04/28/23 17:25:00.758
  E0428 17:25:01.089391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:02.089625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:03.090334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:04.090721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-355c01e5-2080-49eb-9456-102f09fa442c @ 04/28/23 17:25:04.859
  STEP: Updating secret s-test-opt-upd-cb97cce9-4fa5-4d19-8cd6-75ba5d4074d3 @ 04/28/23 17:25:04.868
  STEP: Creating secret with name s-test-opt-create-ce455b52-f6d9-4c44-945b-3ea822e581b4 @ 04/28/23 17:25:04.876
  STEP: waiting to observe update in volume @ 04/28/23 17:25:04.881
  E0428 17:25:05.091353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:06.091484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:07.092228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:08.092325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:09.092848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:10.092993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:11.093354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:12.093508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:13.094226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:14.094905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:15.095477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:16.095588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:17.095627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:18.095714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:19.095799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:20.095921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:21.096249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:22.096496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:23.097053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:24.097170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:25.097839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:26.098003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:27.098538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:28.098668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:29.098937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:30.099256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:31.099671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:32.099802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:33.100622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:34.100736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:35.101562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:36.101680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:37.101976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:38.102099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:39.102779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:40.102735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:41.102936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:42.103177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:43.104060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:44.104930      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:45.105153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:46.105705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:47.105848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:48.106166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:49.106686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:50.106798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:51.107005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:52.108005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:53.108109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:54.108198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:55.109229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:56.109314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:57.109415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:58.110441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:59.111271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:00.111377      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:01.112381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:02.112481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:03.112579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:04.112655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:05.112771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:06.113239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:07.113436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:08.113470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:09.113568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:10.113683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:11.113815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:12.113921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:13.114129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:14.114937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:15.115105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:16.115107      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:17.115342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:18.116108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:19.116224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:20.116412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:21.116540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:22.117586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:23.118491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:24.119444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:25.119573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:26.120225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:27.120359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:28.121350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:29.121777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:30.121909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:31.122026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:32.122153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:33.122363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:34.122904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:35.123000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:36.123707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:37.123926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:26:37.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8022" for this suite. @ 04/28/23 17:26:37.843
• [97.227 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 04/28/23 17:26:37.853
  Apr 28 17:26:37.853: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 17:26:37.854
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:26:37.889
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:26:37.892
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 17:26:37.895
  E0428 17:26:38.124565      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:39.125270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:26:39.916
  Apr 28 17:26:39.918: INFO: Trying to get logs from node ip-172-31-10-53 pod downwardapi-volume-8179c58c-bf7b-4671-ade5-b31f96de0db7 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 17:26:39.93
  Apr 28 17:26:39.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3928" for this suite. @ 04/28/23 17:26:39.95
• [2.104 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 04/28/23 17:26:39.958
  Apr 28 17:26:39.958: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 17:26:39.959
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:26:39.977
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:26:39.981
  STEP: Creating the pod @ 04/28/23 17:26:39.983
  E0428 17:26:40.125736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:41.125866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:42.126603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:26:42.544: INFO: Successfully updated pod "labelsupdate16d93181-e48f-46fe-94c1-26bb7639c121"
  E0428 17:26:43.127075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:44.127405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:45.128042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:46.128290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:26:46.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4067" for this suite. @ 04/28/23 17:26:46.569
• [6.619 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 04/28/23 17:26:46.578
  Apr 28 17:26:46.578: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 17:26:46.579
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:26:46.603
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:26:46.606
  STEP: Discovering how many secrets are in namespace by default @ 04/28/23 17:26:46.61
  E0428 17:26:47.129183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:48.129815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:49.129926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:50.130718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:51.130868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 04/28/23 17:26:51.614
  E0428 17:26:52.131367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:53.132151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:54.133021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:55.133070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:56.133094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/28/23 17:26:56.617
  STEP: Ensuring resource quota status is calculated @ 04/28/23 17:26:56.622
  E0428 17:26:57.133449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:58.133584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Secret @ 04/28/23 17:26:58.627
  STEP: Ensuring resource quota status captures secret creation @ 04/28/23 17:26:58.86
  E0428 17:26:59.133861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:00.134078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a secret @ 04/28/23 17:27:00.864
  STEP: Ensuring resource quota status released usage @ 04/28/23 17:27:00.868
  E0428 17:27:01.134938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:02.135212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:27:02.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7082" for this suite. @ 04/28/23 17:27:02.875
• [16.304 seconds]
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 04/28/23 17:27:02.881
  Apr 28 17:27:02.881: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename container-runtime @ 04/28/23 17:27:02.882
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:27:02.911
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:27:02.915
  STEP: create the container @ 04/28/23 17:27:02.918
  W0428 17:27:02.927753      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/28/23 17:27:02.928
  E0428 17:27:03.135294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:04.136343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:05.136715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:06.136857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 04/28/23 17:27:06.314
  STEP: the container should be terminated @ 04/28/23 17:27:06.32
  STEP: the termination message should be set @ 04/28/23 17:27:06.321
  Apr 28 17:27:06.321: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 04/28/23 17:27:06.321
  Apr 28 17:27:06.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-5758" for this suite. @ 04/28/23 17:27:06.344
• [3.474 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 04/28/23 17:27:06.356
  Apr 28 17:27:06.356: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:27:06.356
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:27:06.458
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:27:06.46
  STEP: Creating configMap with name cm-test-opt-del-a0c2127c-675b-4026-bd49-baa0a93344ce @ 04/28/23 17:27:06.466
  STEP: Creating configMap with name cm-test-opt-upd-3db31949-e45d-4863-9ae6-ee11d0ec99b7 @ 04/28/23 17:27:06.475
  STEP: Creating the pod @ 04/28/23 17:27:06.486
  E0428 17:27:07.137636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:08.137728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:09.137828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:10.138822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-a0c2127c-675b-4026-bd49-baa0a93344ce @ 04/28/23 17:27:10.55
  STEP: Updating configmap cm-test-opt-upd-3db31949-e45d-4863-9ae6-ee11d0ec99b7 @ 04/28/23 17:27:10.557
  STEP: Creating configMap with name cm-test-opt-create-1a9449a0-5e4a-442f-b964-d9545d428cbc @ 04/28/23 17:27:10.562
  STEP: waiting to observe update in volume @ 04/28/23 17:27:10.569
  E0428 17:27:11.138492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:12.138621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:13.138755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:14.138873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:27:14.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1917" for this suite. @ 04/28/23 17:27:14.617
• [8.266 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 04/28/23 17:27:14.624
  Apr 28 17:27:14.624: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename namespaces @ 04/28/23 17:27:14.624
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:27:14.643
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:27:14.649
  STEP: Creating namespace "e2e-ns-xtrnm" @ 04/28/23 17:27:14.655
  Apr 28 17:27:14.676: INFO: Namespace "e2e-ns-xtrnm-7737" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-xtrnm-7737" @ 04/28/23 17:27:14.676
  Apr 28 17:27:14.686: INFO: Namespace "e2e-ns-xtrnm-7737" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-xtrnm-7737" @ 04/28/23 17:27:14.686
  Apr 28 17:27:14.697: INFO: Namespace "e2e-ns-xtrnm-7737" has []v1.FinalizerName{"kubernetes"}
  Apr 28 17:27:14.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-277" for this suite. @ 04/28/23 17:27:14.701
  STEP: Destroying namespace "e2e-ns-xtrnm-7737" for this suite. @ 04/28/23 17:27:14.708
• [0.090 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 04/28/23 17:27:14.714
  Apr 28 17:27:14.714: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 17:27:14.715
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:27:14.732
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:27:14.735
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 17:27:14.741
  E0428 17:27:15.139611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:16.139947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:17.140161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:18.140388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:27:18.798
  Apr 28 17:27:18.803: INFO: Trying to get logs from node ip-172-31-10-53 pod downwardapi-volume-90dad54d-2519-405f-98d8-d4100db7900d container client-container: <nil>
  STEP: delete the pod @ 04/28/23 17:27:18.812
  Apr 28 17:27:18.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3297" for this suite. @ 04/28/23 17:27:18.859
• [4.162 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 04/28/23 17:27:18.877
  Apr 28 17:27:18.877: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename services @ 04/28/23 17:27:18.878
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:27:18.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:27:18.905
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-50 @ 04/28/23 17:27:18.907
  STEP: changing the ExternalName service to type=NodePort @ 04/28/23 17:27:18.918
  STEP: creating replication controller externalname-service in namespace services-50 @ 04/28/23 17:27:18.952
  I0428 17:27:18.979109      19 runners.go:194] Created replication controller with name: externalname-service, namespace: services-50, replica count: 2
  E0428 17:27:19.140888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:20.141148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:21.141335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 17:27:22.030286      19 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 28 17:27:22.030: INFO: Creating new exec pod
  E0428 17:27:22.141963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:23.142113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:24.142935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:27:25.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-50 exec execpodlrx99 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  E0428 17:27:25.143492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:27:25.283: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 28 17:27:25.283: INFO: stdout: ""
  E0428 17:27:26.143593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:27:26.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-50 exec execpodlrx99 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 28 17:27:26.402: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 28 17:27:26.402: INFO: stdout: ""
  E0428 17:27:27.143741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:27:27.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-50 exec execpodlrx99 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 28 17:27:27.415: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 28 17:27:27.415: INFO: stdout: "externalname-service-s2c87"
  Apr 28 17:27:27.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-50 exec execpodlrx99 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.43.185 80'
  Apr 28 17:27:27.562: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.43.185 80\nConnection to 10.43.43.185 80 port [tcp/http] succeeded!\n"
  Apr 28 17:27:27.562: INFO: stdout: ""
  E0428 17:27:28.144341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:27:28.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-50 exec execpodlrx99 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.43.185 80'
  Apr 28 17:27:28.722: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.43.185 80\nConnection to 10.43.43.185 80 port [tcp/http] succeeded!\n"
  Apr 28 17:27:28.722: INFO: stdout: "externalname-service-s2c87"
  Apr 28 17:27:28.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-50 exec execpodlrx99 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.1.213 30914'
  Apr 28 17:27:28.878: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.1.213 30914\nConnection to 172.31.1.213 30914 port [tcp/*] succeeded!\n"
  Apr 28 17:27:28.878: INFO: stdout: "externalname-service-s2c87"
  Apr 28 17:27:28.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-50 exec execpodlrx99 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.10.53 30914'
  Apr 28 17:27:29.036: INFO: stderr: "+ + ncecho -v hostName\n -t -w 2 172.31.10.53 30914\nConnection to 172.31.10.53 30914 port [tcp/*] succeeded!\n"
  Apr 28 17:27:29.036: INFO: stdout: ""
  E0428 17:27:29.144546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:27:30.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-50 exec execpodlrx99 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.10.53 30914'
  E0428 17:27:30.149460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:27:30.312: INFO: stderr: "+ + ncecho -v hostName\n -t -w 2 172.31.10.53 30914\nConnection to 172.31.10.53 30914 port [tcp/*] succeeded!\n"
  Apr 28 17:27:30.312: INFO: stdout: "externalname-service-s2c87"
  Apr 28 17:27:30.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 17:27:30.316: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-50" for this suite. @ 04/28/23 17:27:30.349
• [11.478 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 04/28/23 17:27:30.356
  Apr 28 17:27:30.356: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename field-validation @ 04/28/23 17:27:30.356
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:27:30.378
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:27:30.385
  STEP: apply creating a deployment @ 04/28/23 17:27:30.392
  Apr 28 17:27:30.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1510" for this suite. @ 04/28/23 17:27:30.416
• [0.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 04/28/23 17:27:30.425
  Apr 28 17:27:30.426: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename kubelet-test @ 04/28/23 17:27:30.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:27:30.451
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:27:30.457
  E0428 17:27:31.150346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:32.150469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:33.150722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:34.150888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:27:34.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-5740" for this suite. @ 04/28/23 17:27:34.498
• [4.079 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 04/28/23 17:27:34.505
  Apr 28 17:27:34.505: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename var-expansion @ 04/28/23 17:27:34.506
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:27:34.522
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:27:34.525
  STEP: Creating a pod to test substitution in container's args @ 04/28/23 17:27:34.528
  E0428 17:27:35.151284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:36.151416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:27:36.547
  Apr 28 17:27:36.553: INFO: Trying to get logs from node ip-172-31-1-213 pod var-expansion-0ddf070e-8173-4d4c-81f5-5880a9262ac0 container dapi-container: <nil>
  STEP: delete the pod @ 04/28/23 17:27:36.568
  Apr 28 17:27:36.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7326" for this suite. @ 04/28/23 17:27:36.591
• [2.104 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 04/28/23 17:27:36.609
  Apr 28 17:27:36.609: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename webhook @ 04/28/23 17:27:36.613
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:27:36.635
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:27:36.64
  STEP: Setting up server cert @ 04/28/23 17:27:36.695
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 17:27:37.131
  STEP: Deploying the webhook pod @ 04/28/23 17:27:37.149
  E0428 17:27:37.151439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Wait for the deployment to be ready @ 04/28/23 17:27:37.16
  Apr 28 17:27:37.173: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0428 17:27:38.151662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:39.152162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 17:27:39.182
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 17:27:39.191
  E0428 17:27:40.152268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:27:40.191: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 04/28/23 17:27:40.194
  STEP: create a pod that should be updated by the webhook @ 04/28/23 17:27:40.212
  Apr 28 17:27:40.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5816" for this suite. @ 04/28/23 17:27:40.37
  STEP: Destroying namespace "webhook-markers-7489" for this suite. @ 04/28/23 17:27:40.385
• [3.783 seconds]
------------------------------
S
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 04/28/23 17:27:40.393
  Apr 28 17:27:40.393: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename runtimeclass @ 04/28/23 17:27:40.394
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:27:40.441
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:27:40.445
  E0428 17:27:41.152425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:42.152653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:27:42.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-7538" for this suite. @ 04/28/23 17:27:42.487
• [2.099 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 04/28/23 17:27:42.492
  Apr 28 17:27:42.492: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:27:42.493
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:27:42.506
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:27:42.519
  STEP: Creating secret with name projected-secret-test-0f13f0e2-8f56-4028-81b3-f3bd0270a41f @ 04/28/23 17:27:42.521
  STEP: Creating a pod to test consume secrets @ 04/28/23 17:27:42.527
  E0428 17:27:43.152795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:44.153346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:45.154357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:46.155181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:27:47.032
  Apr 28 17:27:47.035: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-projected-secrets-30cb0dda-a460-4b73-82a9-cbe6fdea07f8 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 17:27:47.042
  Apr 28 17:27:47.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2996" for this suite. @ 04/28/23 17:27:47.065
• [4.581 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 04/28/23 17:27:47.073
  Apr 28 17:27:47.073: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename container-probe @ 04/28/23 17:27:47.074
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:27:47.099
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:27:47.103
  STEP: Creating pod liveness-64a86121-ebe4-4f7b-ac23-b74f7fdb4a0f in namespace container-probe-5386 @ 04/28/23 17:27:47.107
  E0428 17:27:47.156210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:48.156739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:27:49.123: INFO: Started pod liveness-64a86121-ebe4-4f7b-ac23-b74f7fdb4a0f in namespace container-probe-5386
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/28/23 17:27:49.123
  Apr 28 17:27:49.126: INFO: Initial restart count of pod liveness-64a86121-ebe4-4f7b-ac23-b74f7fdb4a0f is 0
  E0428 17:27:49.157822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:50.158872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:51.159747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:52.159864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:53.160826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:54.161184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:55.161778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:56.161991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:57.162422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:58.162547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:59.163263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:00.163289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:01.163905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:02.164009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:03.164340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:04.164911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:05.165287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:06.165571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:07.165569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:08.165720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:09.166239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:28:09.171: INFO: Restart count of pod container-probe-5386/liveness-64a86121-ebe4-4f7b-ac23-b74f7fdb4a0f is now 1 (20.044980203s elapsed)
  E0428 17:28:10.166389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:11.166620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:12.166732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:13.166851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:14.166990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:15.167231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:16.167364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:17.167535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:18.167620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:19.168355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:20.168489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:21.168594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:22.168702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:23.168932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:24.169542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:25.169785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:26.169874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:27.170018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:28.170152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:29.170751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:28:29.222: INFO: Restart count of pod container-probe-5386/liveness-64a86121-ebe4-4f7b-ac23-b74f7fdb4a0f is now 2 (40.095972398s elapsed)
  E0428 17:28:30.171308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:31.171393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:32.171501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:33.171616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:34.172627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:35.172833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:36.172967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:37.173199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:38.173308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:39.173837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:40.173981      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:41.174122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:42.174244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:43.174370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:44.174456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:45.174653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:46.175497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:47.175636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:48.175759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:49.176308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:28:49.996: INFO: Restart count of pod container-probe-5386/liveness-64a86121-ebe4-4f7b-ac23-b74f7fdb4a0f is now 3 (1m0.869664955s elapsed)
  E0428 17:28:50.177307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:51.177416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:52.178029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:53.178173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:54.178534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:55.178645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:56.178874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:57.179312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:58.179432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:59.180469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:00.180598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:01.181622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:02.181740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:03.181877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:04.182293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:05.183242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:06.183443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:07.184516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:08.184743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:09.185478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:10.185800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:29:10.314: INFO: Restart count of pod container-probe-5386/liveness-64a86121-ebe4-4f7b-ac23-b74f7fdb4a0f is now 4 (1m21.188009894s elapsed)
  E0428 17:29:11.185888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:12.186108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:13.186234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:14.186602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:15.187013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:16.187403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:17.187305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:18.187532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:19.187623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:20.188406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:21.188551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:22.188684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:23.188823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:24.189129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:25.189269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:26.189398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:27.189530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:28.189674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:29.190658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:30.190882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:31.191056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:32.191177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:33.191997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:34.192332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:35.192456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:36.192601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:37.192740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:38.192857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:39.192984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:40.193258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:41.193387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:42.193762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:43.194785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:44.195197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:45.195975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:46.196215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:47.196365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:48.196453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:49.196903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:50.198043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:51.198180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:52.198265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:53.198372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:54.199281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:55.200146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:56.200263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:57.201068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:58.201102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:59.202084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:00.202302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:01.202941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:02.203106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:03.203730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:04.203842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:05.204703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:06.205131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:07.205980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:08.206193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:09.207017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:10.207190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:11.207307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:12.207393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:13.207524      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:14.207642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:15.207824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:16.208247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:17.208441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:18.208578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:19.209336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:20.209887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:21.210032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:30:21.349: INFO: Restart count of pod container-probe-5386/liveness-64a86121-ebe4-4f7b-ac23-b74f7fdb4a0f is now 5 (2m32.22328742s elapsed)
  Apr 28 17:30:21.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 17:30:21.353
  STEP: Destroying namespace "container-probe-5386" for this suite. @ 04/28/23 17:30:21.367
• [154.311 seconds]
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 04/28/23 17:30:21.385
  Apr 28 17:30:21.385: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename job @ 04/28/23 17:30:21.386
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:30:21.404
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:30:21.408
  STEP: Creating a job @ 04/28/23 17:30:21.414
  STEP: Ensuring job reaches completions @ 04/28/23 17:30:21.426
  E0428 17:30:22.210130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:23.211062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:24.211987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:25.212181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:26.212294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:27.212435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:28.212553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:29.213138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:30.213304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:31.213709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:30:31.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-4405" for this suite. @ 04/28/23 17:30:31.433
• [10.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 04/28/23 17:30:31.442
  Apr 28 17:30:31.442: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename endpointslice @ 04/28/23 17:30:31.443
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:30:31.496
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:30:31.5
  Apr 28 17:30:31.526: INFO: Endpoints addresses: [18.117.10.20 18.223.125.232 3.141.7.10] , ports: [6443]
  Apr 28 17:30:31.526: INFO: EndpointSlices addresses: [18.117.10.20 18.223.125.232 3.141.7.10] , ports: [6443]
  Apr 28 17:30:31.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-4851" for this suite. @ 04/28/23 17:30:31.546
• [0.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 04/28/23 17:30:31.564
  Apr 28 17:30:31.564: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:30:31.565
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:30:31.617
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:30:31.62
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 17:30:31.629
  E0428 17:30:32.213867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:33.214002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:34.214096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:35.214318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:30:35.679
  Apr 28 17:30:35.682: INFO: Trying to get logs from node ip-172-31-1-213 pod downwardapi-volume-ea594659-c386-4950-90a8-e87b1c581356 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 17:30:35.698
  Apr 28 17:30:35.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1693" for this suite. @ 04/28/23 17:30:35.725
• [4.176 seconds]
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 04/28/23 17:30:35.741
  Apr 28 17:30:35.741: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename container-probe @ 04/28/23 17:30:35.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:30:35.762
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:30:35.765
  STEP: Creating pod liveness-71775f8b-9212-46b2-9a10-8f56379b38cf in namespace container-probe-95 @ 04/28/23 17:30:35.768
  E0428 17:30:36.214621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:37.215730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:30:37.797: INFO: Started pod liveness-71775f8b-9212-46b2-9a10-8f56379b38cf in namespace container-probe-95
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/28/23 17:30:37.798
  Apr 28 17:30:37.802: INFO: Initial restart count of pod liveness-71775f8b-9212-46b2-9a10-8f56379b38cf is 0
  E0428 17:30:38.216040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:39.216613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:40.217268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:41.217389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:42.217519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:43.217705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:44.219922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:45.218049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:46.218706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:47.219430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:48.219998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:49.220254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:50.220412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:51.220615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:52.220737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:53.220895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:54.221454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:55.221577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:56.221760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:57.221968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:58.222644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:59.223062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:00.223919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:01.224186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:02.225077      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:03.225272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:04.225590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:05.225781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:06.226360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:07.226518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:08.226912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:09.227300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:10.228019      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:11.228417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:12.229393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:13.229456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:14.230098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:15.230687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:16.231295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:17.231415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:18.231483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:19.232100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:20.232583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:21.232736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:22.233299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:23.234083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:24.234441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:25.234727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:26.234948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:27.235015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:28.235355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:29.236352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:30.236392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:31.236545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:32.236814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:33.237073      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:34.237391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:35.237506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:36.237747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:37.238635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:38.239012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:39.239757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:40.240645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:41.240776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:42.240919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:43.241004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:44.241677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:45.242527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:46.242666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:47.242771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:48.242874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:49.243002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:50.243237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:51.243261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:52.243324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:53.243458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:54.243904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:55.244826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:56.245045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:57.245502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:58.245688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:59.246442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:00.246660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:01.247645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:02.247786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:03.248468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:04.248564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:05.249242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:06.249354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:07.249538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:08.249647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:09.250375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:10.250576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:11.250624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:12.250738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:13.251530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:14.252389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:15.253130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:16.253269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:17.254188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:18.254333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:19.254990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:20.255123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:21.255254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:22.255320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:23.255656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:24.255857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:25.255895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:26.256033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:27.256579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:28.256830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:29.257802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:30.257918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:31.258631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:32.258763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:33.259232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:34.259312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:35.259665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:36.259789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:37.260160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:38.260275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:39.261201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:40.261315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:41.261857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:42.261983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:43.262625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:44.262966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:45.263322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:46.263255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:47.263843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:48.264044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:49.264377      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:50.264582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:51.265049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:52.265194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:53.266113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:54.266238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:55.266610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:56.266714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:57.267092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:58.267244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:59.267441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:00.267590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:01.267630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:02.267842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:03.268039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:04.268389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:05.268632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:06.268795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:07.269029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:08.269164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:09.269710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:10.269911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:11.269995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:12.270122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:13.270243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:14.270351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:15.270444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:16.270591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:17.270820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:18.270943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:19.272372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:20.272543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:21.272664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:22.272809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:23.273009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:24.273051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:25.273274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:26.273422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:27.273555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:28.273673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:29.274072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:30.274199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:31.274426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:32.274562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:33.274754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:34.275410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:35.275637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:36.276682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:37.276982      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:38.277104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:39.277594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:40.277721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:41.277928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:42.278078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:43.278210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:44.278499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:45.278607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:46.278728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:47.278860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:48.278972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:49.279102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:50.279295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:51.279443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:52.279618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:53.279716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:54.280396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:55.280543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:56.280705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:57.280817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:58.280932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:59.281417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:00.281566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:01.281881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:02.282031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:03.282125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:04.282392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:05.282538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:06.282804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:07.283075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:08.284150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:09.284693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:10.284819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:11.284919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:12.286016      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:13.286135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:14.286296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:15.286505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:16.287449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:17.287579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:18.288400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:19.288901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:20.289443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:21.289715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:22.290529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:23.290712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:24.290787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:25.290910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:26.291105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:27.291296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:28.292182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:29.292682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:30.293534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:31.293810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:32.293951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:33.294152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:34.294973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:35.295189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:36.295284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:37.295401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:34:38.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 17:34:38.254
  STEP: Destroying namespace "container-probe-95" for this suite. @ 04/28/23 17:34:38.268
• [242.537 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 04/28/23 17:34:38.279
  Apr 28 17:34:38.279: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename csiinlinevolumes @ 04/28/23 17:34:38.28
  E0428 17:34:38.295925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:34:38.313
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:34:38.319
  STEP: creating @ 04/28/23 17:34:38.324
  STEP: getting @ 04/28/23 17:34:38.356
  STEP: listing @ 04/28/23 17:34:38.362
  STEP: deleting @ 04/28/23 17:34:38.364
  Apr 28 17:34:38.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-3258" for this suite. @ 04/28/23 17:34:38.387
• [0.114 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 04/28/23 17:34:38.396
  Apr 28 17:34:38.396: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 17:34:38.397
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:34:38.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:34:38.422
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 04/28/23 17:34:38.424
  E0428 17:34:39.296941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:40.297070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:41.297216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:42.297359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:34:42.772
  Apr 28 17:34:42.775: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-86225867-b063-4fa1-8ca4-e3ef3d71a9f0 container test-container: <nil>
  STEP: delete the pod @ 04/28/23 17:34:42.785
  Apr 28 17:34:42.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3425" for this suite. @ 04/28/23 17:34:42.81
• [4.424 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 04/28/23 17:34:42.821
  Apr 28 17:34:42.821: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename webhook @ 04/28/23 17:34:42.822
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:34:42.844
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:34:42.847
  STEP: Setting up server cert @ 04/28/23 17:34:42.879
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 17:34:43.258
  STEP: Deploying the webhook pod @ 04/28/23 17:34:43.266
  STEP: Wait for the deployment to be ready @ 04/28/23 17:34:43.283
  Apr 28 17:34:43.295: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0428 17:34:43.297894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:44.298333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:45.299731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 17:34:45.311
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 17:34:45.325
  E0428 17:34:46.299763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:34:46.326: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 04/28/23 17:34:46.329
  STEP: Creating a custom resource definition that should be denied by the webhook @ 04/28/23 17:34:46.349
  Apr 28 17:34:46.349: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 17:34:46.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3077" for this suite. @ 04/28/23 17:34:46.449
  STEP: Destroying namespace "webhook-markers-734" for this suite. @ 04/28/23 17:34:46.456
• [3.650 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 04/28/23 17:34:46.473
  Apr 28 17:34:46.473: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename pod-network-test @ 04/28/23 17:34:46.474
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:34:46.501
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:34:46.506
  STEP: Performing setup for networking test in namespace pod-network-test-4625 @ 04/28/23 17:34:46.51
  STEP: creating a selector @ 04/28/23 17:34:46.51
  STEP: Creating the service pods in kubernetes @ 04/28/23 17:34:46.51
  Apr 28 17:34:46.510: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0428 17:34:47.300359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:48.300476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:49.301319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:50.301533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:51.301671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:52.301909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:53.302774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:54.303102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:55.303174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:56.303372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:57.303486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:58.303706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:59.304690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:00.304842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:01.305828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:02.306221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:03.307089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:04.307517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:05.307604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:06.307780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:07.307899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:08.308011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 04/28/23 17:35:08.683
  E0428 17:35:09.308402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:10.308550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:35:10.722: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
  Apr 28 17:35:10.722: INFO: Going to poll 10.42.2.96 on port 8081 at least 0 times, with a maximum of 46 tries before failing
  Apr 28 17:35:10.725: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.2.96 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4625 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:35:10.725: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 17:35:10.725: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:35:10.725: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-4625/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.42.2.96+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0428 17:35:11.308757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:35:11.787: INFO: Found all 1 expected endpoints: [netserver-0]
  Apr 28 17:35:11.787: INFO: Going to poll 10.42.1.59 on port 8081 at least 0 times, with a maximum of 46 tries before failing
  Apr 28 17:35:11.790: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.1.59 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4625 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:35:11.790: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 17:35:11.791: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:35:11.791: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-4625/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.42.1.59+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0428 17:35:12.308900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:35:12.862: INFO: Found all 1 expected endpoints: [netserver-1]
  Apr 28 17:35:12.862: INFO: Going to poll 10.42.0.179 on port 8081 at least 0 times, with a maximum of 46 tries before failing
  Apr 28 17:35:12.865: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.0.179 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4625 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:35:12.865: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 17:35:12.866: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:35:12.866: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-4625/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.42.0.179+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0428 17:35:13.309651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:35:13.970: INFO: Found all 1 expected endpoints: [netserver-2]
  Apr 28 17:35:13.970: INFO: Going to poll 10.42.3.238 on port 8081 at least 0 times, with a maximum of 46 tries before failing
  Apr 28 17:35:13.975: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.3.238 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4625 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:35:13.975: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 17:35:13.976: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:35:13.976: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-4625/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.42.3.238+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0428 17:35:14.310545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:35:15.064: INFO: Found all 1 expected endpoints: [netserver-3]
  Apr 28 17:35:15.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-4625" for this suite. @ 04/28/23 17:35:15.187
  E0428 17:35:15.310601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
• [28.867 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 04/28/23 17:35:15.34
  Apr 28 17:35:15.340: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename deployment @ 04/28/23 17:35:15.341
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:35:15.359
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:35:15.365
  Apr 28 17:35:15.368: INFO: Creating deployment "test-recreate-deployment"
  Apr 28 17:35:15.374: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Apr 28 17:35:15.395: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  E0428 17:35:16.310843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:17.310870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:35:17.401: INFO: Waiting deployment "test-recreate-deployment" to complete
  Apr 28 17:35:17.404: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Apr 28 17:35:18.032: INFO: Updating deployment test-recreate-deployment
  Apr 28 17:35:18.032: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Apr 28 17:35:18.040: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-1347  fe229b1b-379b-4028-9a34-6d0fdaa78af8 266497 2 2023-04-28 17:35:15 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-28 17:35:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2023-04-28 17:35:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c2eb08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-28 17:35:17 +0000 UTC,LastTransitionTime:2023-04-28 17:35:17 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-04-28 17:35:17 +0000 UTC,LastTransitionTime:2023-04-28 17:35:15 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Apr 28 17:35:18.043: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-1347  eefc6943-07fb-4e24-94fb-9c86f990ccfe 266495 1 2023-04-28 17:35:17 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment fe229b1b-379b-4028-9a34-6d0fdaa78af8 0xc004c1cb27 0xc004c1cb28}] [] [{k3s Update apps/v1 2023-04-28 17:35:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe229b1b-379b-4028-9a34-6d0fdaa78af8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2023-04-28 17:35:17 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c1cbe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 17:35:18.043: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Apr 28 17:35:18.043: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-1347  b8c19430-f059-4228-977a-e08eefee2aaf 266485 2 2023-04-28 17:35:15 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment fe229b1b-379b-4028-9a34-6d0fdaa78af8 0xc004c1cc57 0xc004c1cc58}] [] [{k3s Update apps/v1 2023-04-28 17:35:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe229b1b-379b-4028-9a34-6d0fdaa78af8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2023-04-28 17:35:17 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c1cd18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 17:35:18.046: INFO: Pod "test-recreate-deployment-54757ffd6c-knssb" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-knssb test-recreate-deployment-54757ffd6c- deployment-1347  a82063bc-1a27-4da9-a3c2-01697c8e784f 266498 0 2023-04-28 17:35:17 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c eefc6943-07fb-4e24-94fb-9c86f990ccfe 0xc004c2ee97 0xc004c2ee98}] [] [{k3s Update v1 2023-04-28 17:35:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eefc6943-07fb-4e24-94fb-9c86f990ccfe\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:35:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k8nh4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k8nh4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-213,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:35:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:35:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:35:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:35:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.213,PodIP:,StartTime:2023-04-28 17:35:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:35:18.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1347" for this suite. @ 04/28/23 17:35:18.06
• [2.730 seconds]
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 04/28/23 17:35:18.072
  Apr 28 17:35:18.072: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename sched-pred @ 04/28/23 17:35:18.073
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:35:18.097
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:35:18.103
  Apr 28 17:35:18.108: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 28 17:35:18.116: INFO: Waiting for terminating namespaces to be deleted...
  Apr 28 17:35:18.120: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-1-213 before test
  Apr 28 17:35:18.130: INFO: test-recreate-deployment-54757ffd6c-knssb from deployment-1347 started at 2023-04-28 17:35:17 +0000 UTC (1 container statuses recorded)
  Apr 28 17:35:18.130: INFO: 	Container httpd ready: false, restart count 0
  Apr 28 17:35:18.130: INFO: svclb-traefik-0a77677b-4z972 from kube-system started at 2023-04-28 17:22:42 +0000 UTC (2 container statuses recorded)
  Apr 28 17:35:18.130: INFO: 	Container lb-tcp-443 ready: true, restart count 0
  Apr 28 17:35:18.130: INFO: 	Container lb-tcp-80 ready: true, restart count 0
  Apr 28 17:35:18.130: INFO: netserver-0 from pod-network-test-4625 started at 2023-04-28 17:34:46 +0000 UTC (1 container statuses recorded)
  Apr 28 17:35:18.130: INFO: 	Container webserver ready: true, restart count 0
  Apr 28 17:35:18.130: INFO: test-container-pod from pod-network-test-4625 started at 2023-04-28 17:35:08 +0000 UTC (1 container statuses recorded)
  Apr 28 17:35:18.130: INFO: 	Container webserver ready: true, restart count 0
  Apr 28 17:35:18.130: INFO: sonobuoy-systemd-logs-daemon-set-d17888d62c134fe7-v6wcc from sonobuoy started at 2023-04-28 16:35:07 +0000 UTC (2 container statuses recorded)
  Apr 28 17:35:18.130: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 17:35:18.130: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 17:35:18.130: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-10-53 before test
  Apr 28 17:35:18.140: INFO: svclb-traefik-0a77677b-d6qnt from kube-system started at 2023-04-28 03:31:43 +0000 UTC (2 container statuses recorded)
  Apr 28 17:35:18.140: INFO: 	Container lb-tcp-443 ready: true, restart count 0
  Apr 28 17:35:18.140: INFO: 	Container lb-tcp-80 ready: true, restart count 0
  Apr 28 17:35:18.140: INFO: host-test-container-pod from pod-network-test-4625 started at 2023-04-28 17:35:08 +0000 UTC (1 container statuses recorded)
  Apr 28 17:35:18.140: INFO: 	Container agnhost-container ready: true, restart count 0
  Apr 28 17:35:18.140: INFO: netserver-1 from pod-network-test-4625 started at 2023-04-28 17:34:46 +0000 UTC (1 container statuses recorded)
  Apr 28 17:35:18.140: INFO: 	Container webserver ready: true, restart count 0
  Apr 28 17:35:18.140: INFO: sonobuoy from sonobuoy started at 2023-04-28 16:35:06 +0000 UTC (1 container statuses recorded)
  Apr 28 17:35:18.140: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 28 17:35:18.140: INFO: sonobuoy-systemd-logs-daemon-set-d17888d62c134fe7-clf8z from sonobuoy started at 2023-04-28 16:35:07 +0000 UTC (2 container statuses recorded)
  Apr 28 17:35:18.140: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 17:35:18.140: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 17:35:18.140: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-12-59 before test
  Apr 28 17:35:18.147: INFO: coredns-77ccd57875-lg57c from kube-system started at 2023-04-28 03:29:34 +0000 UTC (1 container statuses recorded)
  Apr 28 17:35:18.147: INFO: 	Container coredns ready: true, restart count 0
  Apr 28 17:35:18.147: INFO: helm-install-traefik-crd-rxpjj from kube-system started at 2023-04-28 03:29:34 +0000 UTC (1 container statuses recorded)
  Apr 28 17:35:18.147: INFO: 	Container helm ready: false, restart count 0
  Apr 28 17:35:18.147: INFO: helm-install-traefik-q2g86 from kube-system started at 2023-04-28 03:29:34 +0000 UTC (1 container statuses recorded)
  Apr 28 17:35:18.147: INFO: 	Container helm ready: false, restart count 1
  Apr 28 17:35:18.147: INFO: local-path-provisioner-957fdf8bc-jc5qz from kube-system started at 2023-04-28 03:29:34 +0000 UTC (1 container statuses recorded)
  Apr 28 17:35:18.147: INFO: 	Container local-path-provisioner ready: true, restart count 0
  Apr 28 17:35:18.147: INFO: metrics-server-54dc485875-gbbbl from kube-system started at 2023-04-28 03:29:34 +0000 UTC (1 container statuses recorded)
  Apr 28 17:35:18.147: INFO: 	Container metrics-server ready: true, restart count 0
  Apr 28 17:35:18.147: INFO: svclb-traefik-0a77677b-jtrmg from kube-system started at 2023-04-28 03:29:54 +0000 UTC (2 container statuses recorded)
  Apr 28 17:35:18.147: INFO: 	Container lb-tcp-443 ready: true, restart count 0
  Apr 28 17:35:18.147: INFO: 	Container lb-tcp-80 ready: true, restart count 0
  Apr 28 17:35:18.147: INFO: traefik-84745cf649-mwthv from kube-system started at 2023-04-28 03:29:54 +0000 UTC (1 container statuses recorded)
  Apr 28 17:35:18.147: INFO: 	Container traefik ready: true, restart count 0
  Apr 28 17:35:18.147: INFO: netserver-2 from pod-network-test-4625 started at 2023-04-28 17:34:46 +0000 UTC (1 container statuses recorded)
  Apr 28 17:35:18.148: INFO: 	Container webserver ready: true, restart count 0
  Apr 28 17:35:18.148: INFO: sonobuoy-systemd-logs-daemon-set-d17888d62c134fe7-4jl87 from sonobuoy started at 2023-04-28 16:35:07 +0000 UTC (2 container statuses recorded)
  Apr 28 17:35:18.148: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 17:35:18.148: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 17:35:18.148: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-5-174 before test
  Apr 28 17:35:18.154: INFO: svclb-traefik-0a77677b-x527f from kube-system started at 2023-04-28 03:33:13 +0000 UTC (2 container statuses recorded)
  Apr 28 17:35:18.154: INFO: 	Container lb-tcp-443 ready: true, restart count 0
  Apr 28 17:35:18.154: INFO: 	Container lb-tcp-80 ready: true, restart count 0
  Apr 28 17:35:18.154: INFO: netserver-3 from pod-network-test-4625 started at 2023-04-28 17:34:46 +0000 UTC (1 container statuses recorded)
  Apr 28 17:35:18.154: INFO: 	Container webserver ready: true, restart count 0
  Apr 28 17:35:18.154: INFO: sonobuoy-e2e-job-4c96b6c95eb94b68 from sonobuoy started at 2023-04-28 16:35:07 +0000 UTC (2 container statuses recorded)
  Apr 28 17:35:18.154: INFO: 	Container e2e ready: true, restart count 0
  Apr 28 17:35:18.154: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 17:35:18.154: INFO: sonobuoy-systemd-logs-daemon-set-d17888d62c134fe7-wkkfr from sonobuoy started at 2023-04-28 16:35:07 +0000 UTC (2 container statuses recorded)
  Apr 28 17:35:18.154: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 17:35:18.154: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/28/23 17:35:18.154
  E0428 17:35:18.310993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:19.311208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/28/23 17:35:20.182
  STEP: Trying to apply a random label on the found node. @ 04/28/23 17:35:20.202
  STEP: verifying the node has the label kubernetes.io/e2e-5a4b68b4-2346-467e-9678-1db9f700fd4f 42 @ 04/28/23 17:35:20.217
  STEP: Trying to relaunch the pod, now with labels. @ 04/28/23 17:35:20.226
  E0428 17:35:20.311448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:21.312466      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-5a4b68b4-2346-467e-9678-1db9f700fd4f off the node ip-172-31-5-174 @ 04/28/23 17:35:22.254
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-5a4b68b4-2346-467e-9678-1db9f700fd4f @ 04/28/23 17:35:22.262
  Apr 28 17:35:22.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-9839" for this suite. @ 04/28/23 17:35:22.269
• [4.202 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 04/28/23 17:35:22.275
  Apr 28 17:35:22.275: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename services @ 04/28/23 17:35:22.276
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:35:22.291
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:35:22.294
  STEP: creating a Service @ 04/28/23 17:35:22.304
  E0428 17:35:22.312800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: watching for the Service to be added @ 04/28/23 17:35:22.319
  Apr 28 17:35:22.321: INFO: Found Service test-service-6pkcz in namespace services-3114 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Apr 28 17:35:22.321: INFO: Service test-service-6pkcz created
  STEP: Getting /status @ 04/28/23 17:35:22.321
  Apr 28 17:35:22.325: INFO: Service test-service-6pkcz has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 04/28/23 17:35:22.325
  STEP: watching for the Service to be patched @ 04/28/23 17:35:22.331
  Apr 28 17:35:22.332: INFO: observed Service test-service-6pkcz in namespace services-3114 with annotations: map[] & LoadBalancer: {[]}
  Apr 28 17:35:22.332: INFO: Found Service test-service-6pkcz in namespace services-3114 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Apr 28 17:35:22.332: INFO: Service test-service-6pkcz has service status patched
  STEP: updating the ServiceStatus @ 04/28/23 17:35:22.332
  Apr 28 17:35:22.340: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 04/28/23 17:35:22.34
  Apr 28 17:35:22.341: INFO: Observed Service test-service-6pkcz in namespace services-3114 with annotations: map[] & Conditions: {[]}
  Apr 28 17:35:22.341: INFO: Observed event: &Service{ObjectMeta:{test-service-6pkcz  services-3114  e1d2c39e-dd71-440f-b5f1-b1b3593b9029 266611 0 2023-04-28 17:35:22 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-04-28 17:35:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-04-28 17:35:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.43.43.242,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.43.43.242],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Apr 28 17:35:22.342: INFO: Found Service test-service-6pkcz in namespace services-3114 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 28 17:35:22.342: INFO: Service test-service-6pkcz has service status updated
  STEP: patching the service @ 04/28/23 17:35:22.342
  STEP: watching for the Service to be patched @ 04/28/23 17:35:22.347
  Apr 28 17:35:22.349: INFO: observed Service test-service-6pkcz in namespace services-3114 with labels: map[test-service-static:true]
  Apr 28 17:35:22.349: INFO: observed Service test-service-6pkcz in namespace services-3114 with labels: map[test-service-static:true]
  Apr 28 17:35:22.349: INFO: observed Service test-service-6pkcz in namespace services-3114 with labels: map[test-service-static:true]
  Apr 28 17:35:22.349: INFO: Found Service test-service-6pkcz in namespace services-3114 with labels: map[test-service:patched test-service-static:true]
  Apr 28 17:35:22.349: INFO: Service test-service-6pkcz patched
  STEP: deleting the service @ 04/28/23 17:35:22.349
  STEP: watching for the Service to be deleted @ 04/28/23 17:35:22.36
  Apr 28 17:35:22.361: INFO: Observed event: ADDED
  Apr 28 17:35:22.361: INFO: Observed event: MODIFIED
  Apr 28 17:35:22.361: INFO: Observed event: MODIFIED
  Apr 28 17:35:22.361: INFO: Observed event: MODIFIED
  Apr 28 17:35:22.361: INFO: Found Service test-service-6pkcz in namespace services-3114 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Apr 28 17:35:22.361: INFO: Service test-service-6pkcz deleted
  Apr 28 17:35:22.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3114" for this suite. @ 04/28/23 17:35:22.365
• [0.094 seconds]
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 04/28/23 17:35:22.37
  Apr 28 17:35:22.370: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename subpath @ 04/28/23 17:35:22.371
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:35:22.384
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:35:22.386
  STEP: Setting up data @ 04/28/23 17:35:22.389
  STEP: Creating pod pod-subpath-test-configmap-wtzs @ 04/28/23 17:35:22.399
  STEP: Creating a pod to test atomic-volume-subpath @ 04/28/23 17:35:22.399
  E0428 17:35:23.313217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:24.313562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:25.313720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:26.313845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:27.313967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:28.314198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:29.315012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:30.315128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:31.316185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:32.316409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:33.316529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:34.317044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:35.317221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:36.317689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:37.317825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:38.317950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:39.318092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:40.318241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:41.318723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:42.319419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:43.320381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:44.321094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:45.321823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:46.321945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:35:46.472
  Apr 28 17:35:46.475: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-subpath-test-configmap-wtzs container test-container-subpath-configmap-wtzs: <nil>
  STEP: delete the pod @ 04/28/23 17:35:46.484
  STEP: Deleting pod pod-subpath-test-configmap-wtzs @ 04/28/23 17:35:46.498
  Apr 28 17:35:46.498: INFO: Deleting pod "pod-subpath-test-configmap-wtzs" in namespace "subpath-6213"
  Apr 28 17:35:46.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-6213" for this suite. @ 04/28/23 17:35:46.505
• [24.153 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 04/28/23 17:35:46.525
  Apr 28 17:35:46.525: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename dns @ 04/28/23 17:35:46.526
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:35:46.549
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:35:46.552
  STEP: Creating a test externalName service @ 04/28/23 17:35:46.555
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9414.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9414.svc.cluster.local; sleep 1; done
   @ 04/28/23 17:35:46.562
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9414.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9414.svc.cluster.local; sleep 1; done
   @ 04/28/23 17:35:46.562
  STEP: creating a pod to probe DNS @ 04/28/23 17:35:46.562
  STEP: submitting the pod to kubernetes @ 04/28/23 17:35:46.562
  E0428 17:35:47.322088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:48.322286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/28/23 17:35:48.592
  STEP: looking for the results for each expected name from probers @ 04/28/23 17:35:48.595
  Apr 28 17:35:48.609: INFO: DNS probes using dns-test-06da6ec8-0a89-480e-9561-2ca32b35e7c1 succeeded

  STEP: changing the externalName to bar.example.com @ 04/28/23 17:35:48.609
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9414.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9414.svc.cluster.local; sleep 1; done
   @ 04/28/23 17:35:48.618
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9414.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9414.svc.cluster.local; sleep 1; done
   @ 04/28/23 17:35:48.618
  STEP: creating a second pod to probe DNS @ 04/28/23 17:35:48.618
  STEP: submitting the pod to kubernetes @ 04/28/23 17:35:48.618
  E0428 17:35:49.323467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:50.323595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/28/23 17:35:50.646
  STEP: looking for the results for each expected name from probers @ 04/28/23 17:35:50.655
  Apr 28 17:35:50.663: INFO: File wheezy_udp@dns-test-service-3.dns-9414.svc.cluster.local from pod  dns-9414/dns-test-1c301a08-66e3-4528-aff1-3fc6ac7d555b contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 28 17:35:50.667: INFO: File jessie_udp@dns-test-service-3.dns-9414.svc.cluster.local from pod  dns-9414/dns-test-1c301a08-66e3-4528-aff1-3fc6ac7d555b contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 28 17:35:50.667: INFO: Lookups using dns-9414/dns-test-1c301a08-66e3-4528-aff1-3fc6ac7d555b failed for: [wheezy_udp@dns-test-service-3.dns-9414.svc.cluster.local jessie_udp@dns-test-service-3.dns-9414.svc.cluster.local]

  E0428 17:35:51.324123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:52.324343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:53.324486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:54.324938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:55.325007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:35:55.677: INFO: DNS probes using dns-test-1c301a08-66e3-4528-aff1-3fc6ac7d555b succeeded

  STEP: changing the service to type=ClusterIP @ 04/28/23 17:35:55.677
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9414.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9414.svc.cluster.local; sleep 1; done
   @ 04/28/23 17:35:55.689
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9414.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9414.svc.cluster.local; sleep 1; done
   @ 04/28/23 17:35:55.689
  STEP: creating a third pod to probe DNS @ 04/28/23 17:35:55.689
  STEP: submitting the pod to kubernetes @ 04/28/23 17:35:55.692
  E0428 17:35:56.325849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:57.325946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/28/23 17:35:57.706
  STEP: looking for the results for each expected name from probers @ 04/28/23 17:35:57.709
  Apr 28 17:35:57.717: INFO: DNS probes using dns-test-8d1f17d6-29b8-4d18-8a26-8df8f70e6e71 succeeded

  Apr 28 17:35:57.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 17:35:57.721
  STEP: deleting the pod @ 04/28/23 17:35:57.735
  STEP: deleting the pod @ 04/28/23 17:35:57.777
  STEP: deleting the test externalName service @ 04/28/23 17:35:58.303
  E0428 17:35:58.327444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "dns-9414" for this suite. @ 04/28/23 17:35:58.365
• [11.863 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 04/28/23 17:35:58.389
  Apr 28 17:35:58.389: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 17:35:58.39
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:35:58.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:35:58.422
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/28/23 17:35:58.426
  Apr 28 17:35:58.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-8631 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Apr 28 17:35:58.522: INFO: stderr: ""
  Apr 28 17:35:58.522: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 04/28/23 17:35:58.522
  Apr 28 17:35:58.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-8631 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Apr 28 17:35:58.646: INFO: stderr: ""
  Apr 28 17:35:58.646: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/28/23 17:35:58.646
  Apr 28 17:35:58.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-8631 delete pods e2e-test-httpd-pod'
  E0428 17:35:59.327354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:00.327482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:00.640: INFO: stderr: ""
  Apr 28 17:36:00.640: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 28 17:36:00.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8631" for this suite. @ 04/28/23 17:36:00.653
• [2.274 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 04/28/23 17:36:00.663
  Apr 28 17:36:00.663: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:36:00.664
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:36:00.691
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:36:00.695
  STEP: Creating configMap with name projected-configmap-test-volume-a147d965-f934-409c-a99d-674262b59144 @ 04/28/23 17:36:00.699
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:36:00.723
  E0428 17:36:01.327738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:02.327968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:03.328126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:04.328264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:36:04.926
  Apr 28 17:36:04.930: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-projected-configmaps-fd02f157-d004-4d7c-9f6b-339c30049b28 container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 17:36:04.944
  Apr 28 17:36:04.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4748" for this suite. @ 04/28/23 17:36:04.98
• [4.327 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 04/28/23 17:36:04.991
  Apr 28 17:36:04.991: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 17:36:04.991
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:36:05.021
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:36:05.028
  STEP: starting the proxy server @ 04/28/23 17:36:05.032
  Apr 28 17:36:05.032: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-4878 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 04/28/23 17:36:05.1
  Apr 28 17:36:05.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4878" for this suite. @ 04/28/23 17:36:05.114
• [0.129 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 04/28/23 17:36:05.126
  Apr 28 17:36:05.126: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename containers @ 04/28/23 17:36:05.127
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:36:05.148
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:36:05.151
  STEP: Creating a pod to test override arguments @ 04/28/23 17:36:05.154
  E0428 17:36:05.328851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:06.329092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:07.329387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:08.329537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:36:09.177
  Apr 28 17:36:09.181: INFO: Trying to get logs from node ip-172-31-1-213 pod client-containers-54aee7b2-dd7d-49a5-9986-ee7cf7df3d51 container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 17:36:09.193
  Apr 28 17:36:09.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-9095" for this suite. @ 04/28/23 17:36:09.212
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 04/28/23 17:36:09.221
  Apr 28 17:36:09.221: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:36:09.222
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:36:09.298
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:36:09.301
  STEP: Creating configMap with name projected-configmap-test-volume-map-6eab2cbf-c07f-449a-87ca-2f4672411bbd @ 04/28/23 17:36:09.304
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:36:09.308
  E0428 17:36:09.330097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:10.330574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:11.330782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:12.330927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:13.331003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:36:13.331
  Apr 28 17:36:13.334: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-projected-configmaps-aeea71bb-ee9d-4793-bc22-bf02e9cd0c28 container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 17:36:13.34
  Apr 28 17:36:13.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1465" for this suite. @ 04/28/23 17:36:13.359
• [4.147 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 04/28/23 17:36:13.369
  Apr 28 17:36:13.369: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 17:36:13.37
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:36:13.392
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:36:13.398
  STEP: create deployment with httpd image @ 04/28/23 17:36:13.401
  Apr 28 17:36:13.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-2666 create -f -'
  E0428 17:36:14.331796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:14.621: INFO: stderr: ""
  Apr 28 17:36:14.621: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 04/28/23 17:36:14.621
  Apr 28 17:36:14.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-2666 diff -f -'
  Apr 28 17:36:14.922: INFO: rc: 1
  Apr 28 17:36:14.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-2666 delete -f -'
  Apr 28 17:36:14.992: INFO: stderr: ""
  Apr 28 17:36:14.992: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Apr 28 17:36:14.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2666" for this suite. @ 04/28/23 17:36:14.995
• [1.632 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 04/28/23 17:36:15.006
  Apr 28 17:36:15.006: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename webhook @ 04/28/23 17:36:15.007
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:36:15.044
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:36:15.05
  STEP: Setting up server cert @ 04/28/23 17:36:15.098
  E0428 17:36:15.333304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 17:36:15.649
  STEP: Deploying the webhook pod @ 04/28/23 17:36:15.669
  STEP: Wait for the deployment to be ready @ 04/28/23 17:36:15.681
  Apr 28 17:36:15.698: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0428 17:36:16.334328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:17.334492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:17.710: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 36, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 36, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 36, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 36, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:36:18.334897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:19.335723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 17:36:19.716
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 17:36:19.785
  E0428 17:36:20.336535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:20.786: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 04/28/23 17:36:20.79
  STEP: create a pod that should be denied by the webhook @ 04/28/23 17:36:20.806
  STEP: create a pod that causes the webhook to hang @ 04/28/23 17:36:20.82
  E0428 17:36:21.336698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:22.337107      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:23.337324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:24.337746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:25.338040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:26.338134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:27.338242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:28.338397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:29.338504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:30.338673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 04/28/23 17:36:30.827
  STEP: create a configmap that should be admitted by the webhook @ 04/28/23 17:36:30.842
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 04/28/23 17:36:30.86
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 04/28/23 17:36:30.872
  STEP: create a namespace that bypass the webhook @ 04/28/23 17:36:30.878
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 04/28/23 17:36:30.901
  Apr 28 17:36:30.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1422" for this suite. @ 04/28/23 17:36:30.997
  STEP: Destroying namespace "webhook-markers-5798" for this suite. @ 04/28/23 17:36:31.017
  STEP: Destroying namespace "exempted-namespace-28" for this suite. @ 04/28/23 17:36:31.025
• [16.041 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 04/28/23 17:36:31.052
  Apr 28 17:36:31.052: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename namespaces @ 04/28/23 17:36:31.053
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:36:31.074
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:36:31.077
  STEP: Creating a test namespace @ 04/28/23 17:36:31.083
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:36:31.105
  STEP: Creating a pod in the namespace @ 04/28/23 17:36:31.115
  STEP: Waiting for the pod to have running status @ 04/28/23 17:36:31.129
  E0428 17:36:31.338746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:32.338994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the namespace @ 04/28/23 17:36:33.147
  STEP: Waiting for the namespace to be removed. @ 04/28/23 17:36:33.157
  E0428 17:36:33.339354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:34.339915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:35.340106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:36.341064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:37.341999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:38.342443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:39.343229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:40.343561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:41.343683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:42.344249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:43.344940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 04/28/23 17:36:44.161
  STEP: Verifying there are no pods in the namespace @ 04/28/23 17:36:44.192
  Apr 28 17:36:44.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-643" for this suite. @ 04/28/23 17:36:44.209
  STEP: Destroying namespace "nsdeletetest-8405" for this suite. @ 04/28/23 17:36:44.227
  Apr 28 17:36:44.231: INFO: Namespace nsdeletetest-8405 was already deleted
  STEP: Destroying namespace "nsdeletetest-8857" for this suite. @ 04/28/23 17:36:44.231
• [13.188 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 04/28/23 17:36:44.24
  Apr 28 17:36:44.240: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename svcaccounts @ 04/28/23 17:36:44.241
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:36:44.266
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:36:44.27
  Apr 28 17:36:44.295: INFO: created pod pod-service-account-defaultsa
  Apr 28 17:36:44.295: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Apr 28 17:36:44.304: INFO: created pod pod-service-account-mountsa
  Apr 28 17:36:44.304: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Apr 28 17:36:44.317: INFO: created pod pod-service-account-nomountsa
  Apr 28 17:36:44.317: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Apr 28 17:36:44.328: INFO: created pod pod-service-account-defaultsa-mountspec
  Apr 28 17:36:44.328: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  E0428 17:36:44.346251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:44.355: INFO: created pod pod-service-account-mountsa-mountspec
  Apr 28 17:36:44.355: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Apr 28 17:36:44.375: INFO: created pod pod-service-account-nomountsa-mountspec
  Apr 28 17:36:44.375: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Apr 28 17:36:44.398: INFO: created pod pod-service-account-defaultsa-nomountspec
  Apr 28 17:36:44.398: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Apr 28 17:36:44.426: INFO: created pod pod-service-account-mountsa-nomountspec
  Apr 28 17:36:44.426: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Apr 28 17:36:44.447: INFO: created pod pod-service-account-nomountsa-nomountspec
  Apr 28 17:36:44.447: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Apr 28 17:36:44.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7986" for this suite. @ 04/28/23 17:36:44.483
• [0.276 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 04/28/23 17:36:44.52
  Apr 28 17:36:44.520: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename services @ 04/28/23 17:36:44.521
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:36:44.569
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:36:44.575
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-8630 @ 04/28/23 17:36:44.579
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 04/28/23 17:36:44.607
  STEP: creating service externalsvc in namespace services-8630 @ 04/28/23 17:36:44.607
  STEP: creating replication controller externalsvc in namespace services-8630 @ 04/28/23 17:36:44.641
  I0428 17:36:44.671587      19 runners.go:194] Created replication controller with name: externalsvc, namespace: services-8630, replica count: 2
  E0428 17:36:45.346764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:46.347730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:47.347824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 17:36:47.722351      19 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 04/28/23 17:36:47.725
  Apr 28 17:36:47.742: INFO: Creating new exec pod
  E0428 17:36:48.347952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:49.348454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:49.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-8630 exec execpodcwk6h -- /bin/sh -x -c nslookup nodeport-service.services-8630.svc.cluster.local'
  Apr 28 17:36:50.021: INFO: stderr: "+ nslookup nodeport-service.services-8630.svc.cluster.local\n"
  Apr 28 17:36:50.021: INFO: stdout: "Server:\t\t10.43.0.10\nAddress:\t10.43.0.10#53\n\nnodeport-service.services-8630.svc.cluster.local\tcanonical name = externalsvc.services-8630.svc.cluster.local.\nName:\texternalsvc.services-8630.svc.cluster.local\nAddress: 10.43.67.128\n\n"
  Apr 28 17:36:50.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-8630, will wait for the garbage collector to delete the pods @ 04/28/23 17:36:50.042
  Apr 28 17:36:50.110: INFO: Deleting ReplicationController externalsvc took: 10.69577ms
  Apr 28 17:36:50.211: INFO: Terminating ReplicationController externalsvc pods took: 101.161129ms
  E0428 17:36:50.349474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:51.350538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:52.059: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-8630" for this suite. @ 04/28/23 17:36:52.091
• [7.598 seconds]
------------------------------
SS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 04/28/23 17:36:52.118
  Apr 28 17:36:52.118: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename podtemplate @ 04/28/23 17:36:52.119
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:36:52.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:36:52.173
  Apr 28 17:36:52.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-4976" for this suite. @ 04/28/23 17:36:52.223
• [0.116 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 04/28/23 17:36:52.234
  Apr 28 17:36:52.234: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename replicaset @ 04/28/23 17:36:52.235
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:36:52.274
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:36:52.277
  Apr 28 17:36:52.279: INFO: Creating ReplicaSet my-hostname-basic-f2f72e60-6986-4e56-9e3b-b304051a846e
  Apr 28 17:36:52.300: INFO: Pod name my-hostname-basic-f2f72e60-6986-4e56-9e3b-b304051a846e: Found 0 pods out of 1
  E0428 17:36:52.352256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:53.352653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:54.353241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:55.353311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:56.353914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:57.305: INFO: Pod name my-hostname-basic-f2f72e60-6986-4e56-9e3b-b304051a846e: Found 1 pods out of 1
  Apr 28 17:36:57.305: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f2f72e60-6986-4e56-9e3b-b304051a846e" is running
  Apr 28 17:36:57.310: INFO: Pod "my-hostname-basic-f2f72e60-6986-4e56-9e3b-b304051a846e-xpg5v" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-28 17:36:52 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-28 17:36:52 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-28 17:36:52 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-28 17:36:52 +0000 UTC Reason: Message:}])
  Apr 28 17:36:57.310: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 04/28/23 17:36:57.31
  Apr 28 17:36:57.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-8678" for this suite. @ 04/28/23 17:36:57.333
• [5.108 seconds]
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 04/28/23 17:36:57.343
  Apr 28 17:36:57.343: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename proxy @ 04/28/23 17:36:57.343
  E0428 17:36:57.354255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:36:57.378
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:36:57.384
  STEP: starting an echo server on multiple ports @ 04/28/23 17:36:57.407
  STEP: creating replication controller proxy-service-lrtv7 in namespace proxy-1515 @ 04/28/23 17:36:57.407
  I0428 17:36:57.432277      19 runners.go:194] Created replication controller with name: proxy-service-lrtv7, namespace: proxy-1515, replica count: 1
  E0428 17:36:58.355283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 17:36:58.483900      19 runners.go:194] proxy-service-lrtv7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0428 17:36:59.356332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 17:36:59.484857      19 runners.go:194] proxy-service-lrtv7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0428 17:37:00.357164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 17:37:00.485533      19 runners.go:194] proxy-service-lrtv7 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 28 17:37:00.488: INFO: setup took 3.099534965s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 04/28/23 17:37:00.488
  Apr 28 17:37:00.530: INFO: (0) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">test<... (200; 41.787535ms)
  Apr 28 17:37:00.537: INFO: (0) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">... (200; 48.778448ms)
  Apr 28 17:37:00.540: INFO: (0) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/rewriteme">test</a> (200; 51.165124ms)
  Apr 28 17:37:00.540: INFO: (0) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 51.963608ms)
  Apr 28 17:37:00.543: INFO: (0) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname2/proxy/: bar (200; 54.374607ms)
  Apr 28 17:37:00.543: INFO: (0) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 54.513784ms)
  Apr 28 17:37:00.543: INFO: (0) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 54.089971ms)
  Apr 28 17:37:00.543: INFO: (0) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:462/proxy/: tls qux (200; 54.658864ms)
  Apr 28 17:37:00.546: INFO: (0) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname2/proxy/: tls qux (200; 57.226701ms)
  Apr 28 17:37:00.547: INFO: (0) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname1/proxy/: foo (200; 58.57822ms)
  Apr 28 17:37:00.547: INFO: (0) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 58.548439ms)
  Apr 28 17:37:00.547: INFO: (0) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname2/proxy/: bar (200; 58.748557ms)
  Apr 28 17:37:00.550: INFO: (0) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/tlsrewritem... (200; 61.631943ms)
  Apr 28 17:37:00.550: INFO: (0) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname1/proxy/: foo (200; 61.799345ms)
  Apr 28 17:37:00.558: INFO: (0) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname1/proxy/: tls baz (200; 69.755981ms)
  Apr 28 17:37:00.558: INFO: (0) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:460/proxy/: tls baz (200; 69.878321ms)
  Apr 28 17:37:00.568: INFO: (1) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">... (200; 9.33343ms)
  Apr 28 17:37:00.569: INFO: (1) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 10.589482ms)
  Apr 28 17:37:00.569: INFO: (1) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/rewriteme">test</a> (200; 10.855736ms)
  Apr 28 17:37:00.569: INFO: (1) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">test<... (200; 10.579536ms)
  Apr 28 17:37:00.570: INFO: (1) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 11.388806ms)
  Apr 28 17:37:00.571: INFO: (1) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname2/proxy/: bar (200; 12.573667ms)
  Apr 28 17:37:00.571: INFO: (1) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 12.032124ms)
  Apr 28 17:37:00.571: INFO: (1) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname1/proxy/: tls baz (200; 12.372916ms)
  Apr 28 17:37:00.571: INFO: (1) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:462/proxy/: tls qux (200; 12.661126ms)
  Apr 28 17:37:00.571: INFO: (1) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 12.722991ms)
  Apr 28 17:37:00.571: INFO: (1) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/tlsrewritem... (200; 12.570766ms)
  Apr 28 17:37:00.571: INFO: (1) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:460/proxy/: tls baz (200; 12.52717ms)
  Apr 28 17:37:00.572: INFO: (1) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname2/proxy/: bar (200; 13.16977ms)
  Apr 28 17:37:00.572: INFO: (1) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname1/proxy/: foo (200; 13.611323ms)
  Apr 28 17:37:00.572: INFO: (1) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname2/proxy/: tls qux (200; 13.845672ms)
  Apr 28 17:37:00.573: INFO: (1) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname1/proxy/: foo (200; 13.624485ms)
  Apr 28 17:37:00.580: INFO: (2) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/rewriteme">test</a> (200; 6.84528ms)
  Apr 28 17:37:00.582: INFO: (2) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 8.210769ms)
  Apr 28 17:37:00.582: INFO: (2) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/tlsrewritem... (200; 9.121103ms)
  Apr 28 17:37:00.582: INFO: (2) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:462/proxy/: tls qux (200; 9.214422ms)
  Apr 28 17:37:00.584: INFO: (2) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 10.529515ms)
  Apr 28 17:37:00.584: INFO: (2) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 10.43306ms)
  Apr 28 17:37:00.584: INFO: (2) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:460/proxy/: tls baz (200; 10.680499ms)
  Apr 28 17:37:00.584: INFO: (2) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 10.783373ms)
  Apr 28 17:37:00.584: INFO: (2) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">... (200; 11.332257ms)
  Apr 28 17:37:00.585: INFO: (2) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">test<... (200; 12.247756ms)
  Apr 28 17:37:00.588: INFO: (2) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname2/proxy/: bar (200; 15.126404ms)
  Apr 28 17:37:00.588: INFO: (2) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname2/proxy/: bar (200; 15.144712ms)
  Apr 28 17:37:00.588: INFO: (2) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname1/proxy/: foo (200; 15.102183ms)
  Apr 28 17:37:00.589: INFO: (2) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname1/proxy/: tls baz (200; 15.141882ms)
  Apr 28 17:37:00.589: INFO: (2) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname1/proxy/: foo (200; 15.396856ms)
  Apr 28 17:37:00.589: INFO: (2) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname2/proxy/: tls qux (200; 15.761991ms)
  Apr 28 17:37:00.597: INFO: (3) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname1/proxy/: foo (200; 8.151849ms)
  Apr 28 17:37:00.597: INFO: (3) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:462/proxy/: tls qux (200; 7.786369ms)
  Apr 28 17:37:00.597: INFO: (3) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 7.87348ms)
  Apr 28 17:37:00.597: INFO: (3) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 7.738922ms)
  Apr 28 17:37:00.598: INFO: (3) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">test<... (200; 8.068297ms)
  Apr 28 17:37:00.598: INFO: (3) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 8.351896ms)
  Apr 28 17:37:00.598: INFO: (3) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname2/proxy/: bar (200; 8.287746ms)
  Apr 28 17:37:00.598: INFO: (3) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 8.768736ms)
  Apr 28 17:37:00.598: INFO: (3) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/tlsrewritem... (200; 8.951625ms)
  Apr 28 17:37:00.598: INFO: (3) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">... (200; 8.883882ms)
  Apr 28 17:37:00.598: INFO: (3) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/rewriteme">test</a> (200; 9.150111ms)
  Apr 28 17:37:00.598: INFO: (3) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:460/proxy/: tls baz (200; 9.147973ms)
  Apr 28 17:37:00.600: INFO: (3) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname2/proxy/: bar (200; 10.273561ms)
  Apr 28 17:37:00.600: INFO: (3) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname1/proxy/: tls baz (200; 10.399075ms)
  Apr 28 17:37:00.600: INFO: (3) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname1/proxy/: foo (200; 10.452121ms)
  Apr 28 17:37:00.600: INFO: (3) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname2/proxy/: tls qux (200; 10.974822ms)
  Apr 28 17:37:00.620: INFO: (4) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">test<... (200; 19.940777ms)
  Apr 28 17:37:00.620: INFO: (4) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/tlsrewritem... (200; 19.888308ms)
  Apr 28 17:37:00.621: INFO: (4) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:460/proxy/: tls baz (200; 20.769328ms)
  Apr 28 17:37:00.628: INFO: (4) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 28.094075ms)
  Apr 28 17:37:00.628: INFO: (4) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/rewriteme">test</a> (200; 27.892439ms)
  Apr 28 17:37:00.628: INFO: (4) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">... (200; 27.947905ms)
  Apr 28 17:37:00.629: INFO: (4) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:462/proxy/: tls qux (200; 27.934087ms)
  Apr 28 17:37:00.638: INFO: (4) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 36.987705ms)
  Apr 28 17:37:00.639: INFO: (4) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 38.196768ms)
  Apr 28 17:37:00.639: INFO: (4) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 38.256557ms)
  Apr 28 17:37:00.640: INFO: (4) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname1/proxy/: foo (200; 39.667818ms)
  Apr 28 17:37:00.640: INFO: (4) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname1/proxy/: foo (200; 39.500974ms)
  Apr 28 17:37:00.640: INFO: (4) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname2/proxy/: bar (200; 39.754545ms)
  Apr 28 17:37:00.641: INFO: (4) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname2/proxy/: bar (200; 40.236619ms)
  Apr 28 17:37:00.641: INFO: (4) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname2/proxy/: tls qux (200; 40.23069ms)
  Apr 28 17:37:00.644: INFO: (4) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname1/proxy/: tls baz (200; 43.995027ms)
  Apr 28 17:37:00.668: INFO: (5) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 23.330727ms)
  Apr 28 17:37:00.669: INFO: (5) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 23.797306ms)
  Apr 28 17:37:00.669: INFO: (5) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/tlsrewritem... (200; 24.349974ms)
  Apr 28 17:37:00.669: INFO: (5) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">... (200; 23.997795ms)
  Apr 28 17:37:00.671: INFO: (5) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:460/proxy/: tls baz (200; 26.526141ms)
  Apr 28 17:37:00.671: INFO: (5) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/rewriteme">test</a> (200; 26.650666ms)
  Apr 28 17:37:00.672: INFO: (5) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 26.919337ms)
  Apr 28 17:37:00.672: INFO: (5) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">test<... (200; 26.623123ms)
  Apr 28 17:37:00.672: INFO: (5) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 26.582084ms)
  Apr 28 17:37:00.672: INFO: (5) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:462/proxy/: tls qux (200; 26.829016ms)
  Apr 28 17:37:00.672: INFO: (5) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname2/proxy/: bar (200; 27.322427ms)
  Apr 28 17:37:00.676: INFO: (5) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname1/proxy/: foo (200; 31.223416ms)
  Apr 28 17:37:00.676: INFO: (5) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname2/proxy/: bar (200; 30.771161ms)
  Apr 28 17:37:00.676: INFO: (5) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname1/proxy/: tls baz (200; 31.517413ms)
  Apr 28 17:37:00.676: INFO: (5) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname1/proxy/: foo (200; 31.023695ms)
  Apr 28 17:37:00.678: INFO: (5) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname2/proxy/: tls qux (200; 33.127869ms)
  Apr 28 17:37:00.684: INFO: (6) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/tlsrewritem... (200; 5.296197ms)
  Apr 28 17:37:00.687: INFO: (6) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 8.454312ms)
  Apr 28 17:37:00.687: INFO: (6) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 8.216833ms)
  Apr 28 17:37:00.687: INFO: (6) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 9.133979ms)
  Apr 28 17:37:00.687: INFO: (6) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 9.060035ms)
  Apr 28 17:37:00.688: INFO: (6) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:462/proxy/: tls qux (200; 9.698251ms)
  Apr 28 17:37:00.688: INFO: (6) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">... (200; 9.619188ms)
  Apr 28 17:37:00.688: INFO: (6) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:460/proxy/: tls baz (200; 9.735616ms)
  Apr 28 17:37:00.688: INFO: (6) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">test<... (200; 9.973506ms)
  Apr 28 17:37:00.689: INFO: (6) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname1/proxy/: foo (200; 10.957404ms)
  Apr 28 17:37:00.689: INFO: (6) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/rewriteme">test</a> (200; 11.060213ms)
  Apr 28 17:37:00.690: INFO: (6) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname2/proxy/: bar (200; 11.815299ms)
  Apr 28 17:37:00.690: INFO: (6) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname2/proxy/: bar (200; 12.147812ms)
  Apr 28 17:37:00.691: INFO: (6) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname1/proxy/: tls baz (200; 12.733875ms)
  Apr 28 17:37:00.691: INFO: (6) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname2/proxy/: tls qux (200; 12.401191ms)
  Apr 28 17:37:00.691: INFO: (6) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname1/proxy/: foo (200; 12.591494ms)
  Apr 28 17:37:00.698: INFO: (7) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 6.929417ms)
  Apr 28 17:37:00.701: INFO: (7) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/tlsrewritem... (200; 9.6249ms)
  Apr 28 17:37:00.701: INFO: (7) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 9.828615ms)
  Apr 28 17:37:00.701: INFO: (7) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/rewriteme">test</a> (200; 9.854966ms)
  Apr 28 17:37:00.702: INFO: (7) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">test<... (200; 9.927762ms)
  Apr 28 17:37:00.702: INFO: (7) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 10.085007ms)
  Apr 28 17:37:00.702: INFO: (7) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:462/proxy/: tls qux (200; 10.156047ms)
  Apr 28 17:37:00.702: INFO: (7) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:460/proxy/: tls baz (200; 10.328521ms)
  Apr 28 17:37:00.703: INFO: (7) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">... (200; 10.795187ms)
  Apr 28 17:37:00.703: INFO: (7) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 10.99409ms)
  Apr 28 17:37:00.703: INFO: (7) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname1/proxy/: tls baz (200; 11.272444ms)
  Apr 28 17:37:00.704: INFO: (7) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname1/proxy/: foo (200; 12.458524ms)
  Apr 28 17:37:00.704: INFO: (7) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname2/proxy/: tls qux (200; 12.821073ms)
  Apr 28 17:37:00.704: INFO: (7) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname2/proxy/: bar (200; 12.860699ms)
  Apr 28 17:37:00.704: INFO: (7) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname1/proxy/: foo (200; 13.122567ms)
  Apr 28 17:37:00.705: INFO: (7) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname2/proxy/: bar (200; 13.007626ms)
  Apr 28 17:37:00.710: INFO: (8) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 4.776797ms)
  Apr 28 17:37:00.713: INFO: (8) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">test<... (200; 8.70843ms)
  Apr 28 17:37:00.713: INFO: (8) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 8.276603ms)
  Apr 28 17:37:00.714: INFO: (8) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">... (200; 9.210615ms)
  Apr 28 17:37:00.716: INFO: (8) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname1/proxy/: foo (200; 10.368629ms)
  Apr 28 17:37:00.716: INFO: (8) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 10.40329ms)
  Apr 28 17:37:00.716: INFO: (8) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname1/proxy/: tls baz (200; 11.109742ms)
  Apr 28 17:37:00.716: INFO: (8) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:462/proxy/: tls qux (200; 11.038591ms)
  Apr 28 17:37:00.716: INFO: (8) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:460/proxy/: tls baz (200; 10.529021ms)
  Apr 28 17:37:00.716: INFO: (8) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/tlsrewritem... (200; 10.797538ms)
  Apr 28 17:37:00.716: INFO: (8) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname1/proxy/: foo (200; 11.266872ms)
  Apr 28 17:37:00.716: INFO: (8) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 11.094507ms)
  Apr 28 17:37:00.716: INFO: (8) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/rewriteme">test</a> (200; 11.645533ms)
  Apr 28 17:37:00.720: INFO: (8) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname2/proxy/: bar (200; 14.692642ms)
  Apr 28 17:37:00.720: INFO: (8) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname2/proxy/: tls qux (200; 14.194693ms)
  Apr 28 17:37:00.722: INFO: (8) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname2/proxy/: bar (200; 17.126058ms)
  Apr 28 17:37:00.729: INFO: (9) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">test<... (200; 6.501441ms)
  Apr 28 17:37:00.731: INFO: (9) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 8.436496ms)
  Apr 28 17:37:00.731: INFO: (9) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 9.042854ms)
  Apr 28 17:37:00.732: INFO: (9) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:462/proxy/: tls qux (200; 9.09723ms)
  Apr 28 17:37:00.732: INFO: (9) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 9.342496ms)
  Apr 28 17:37:00.732: INFO: (9) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname2/proxy/: tls qux (200; 9.849049ms)
  Apr 28 17:37:00.732: INFO: (9) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 9.46235ms)
  Apr 28 17:37:00.732: INFO: (9) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">... (200; 9.259541ms)
  Apr 28 17:37:00.732: INFO: (9) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:460/proxy/: tls baz (200; 9.55098ms)
  Apr 28 17:37:00.732: INFO: (9) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/tlsrewritem... (200; 9.867344ms)
  Apr 28 17:37:00.732: INFO: (9) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/rewriteme">test</a> (200; 9.767351ms)
  Apr 28 17:37:00.733: INFO: (9) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname2/proxy/: bar (200; 10.071703ms)
  Apr 28 17:37:00.734: INFO: (9) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname1/proxy/: foo (200; 11.014742ms)
  Apr 28 17:37:00.734: INFO: (9) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname2/proxy/: bar (200; 11.012501ms)
  Apr 28 17:37:00.734: INFO: (9) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname1/proxy/: tls baz (200; 11.352085ms)
  Apr 28 17:37:00.734: INFO: (9) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname1/proxy/: foo (200; 11.794511ms)
  Apr 28 17:37:00.739: INFO: (10) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:462/proxy/: tls qux (200; 4.401203ms)
  Apr 28 17:37:00.739: INFO: (10) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">test<... (200; 5.13203ms)
  Apr 28 17:37:00.743: INFO: (10) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:460/proxy/: tls baz (200; 8.330211ms)
  Apr 28 17:37:00.743: INFO: (10) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 8.358001ms)
  Apr 28 17:37:00.743: INFO: (10) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 8.949447ms)
  Apr 28 17:37:00.743: INFO: (10) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">... (200; 8.495318ms)
  Apr 28 17:37:00.743: INFO: (10) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 8.743888ms)
  Apr 28 17:37:00.743: INFO: (10) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/rewriteme">test</a> (200; 8.804287ms)
  Apr 28 17:37:00.744: INFO: (10) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/tlsrewritem... (200; 9.105997ms)
  Apr 28 17:37:00.743: INFO: (10) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 8.175316ms)
  Apr 28 17:37:00.744: INFO: (10) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname1/proxy/: foo (200; 8.876423ms)
  Apr 28 17:37:00.749: INFO: (10) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname2/proxy/: bar (200; 13.911412ms)
  Apr 28 17:37:00.749: INFO: (10) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname1/proxy/: tls baz (200; 14.358896ms)
  Apr 28 17:37:00.749: INFO: (10) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname2/proxy/: bar (200; 14.659084ms)
  Apr 28 17:37:00.750: INFO: (10) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname1/proxy/: foo (200; 15.17522ms)
  Apr 28 17:37:00.750: INFO: (10) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname2/proxy/: tls qux (200; 15.159063ms)
  Apr 28 17:37:00.755: INFO: (11) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/tlsrewritem... (200; 4.921888ms)
  Apr 28 17:37:00.756: INFO: (11) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 5.926801ms)
  Apr 28 17:37:00.759: INFO: (11) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 8.551348ms)
  Apr 28 17:37:00.759: INFO: (11) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">test<... (200; 8.984542ms)
  Apr 28 17:37:00.759: INFO: (11) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/rewriteme">test</a> (200; 8.902612ms)
  Apr 28 17:37:00.759: INFO: (11) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">... (200; 9.106961ms)
  Apr 28 17:37:00.759: INFO: (11) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 9.403486ms)
  Apr 28 17:37:00.760: INFO: (11) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:460/proxy/: tls baz (200; 9.339055ms)
  Apr 28 17:37:00.760: INFO: (11) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:462/proxy/: tls qux (200; 9.471826ms)
  Apr 28 17:37:00.760: INFO: (11) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 9.706064ms)
  Apr 28 17:37:00.760: INFO: (11) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname1/proxy/: foo (200; 10.44692ms)
  Apr 28 17:37:00.761: INFO: (11) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname2/proxy/: tls qux (200; 10.999074ms)
  Apr 28 17:37:00.765: INFO: (11) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname2/proxy/: bar (200; 14.699973ms)
  Apr 28 17:37:00.765: INFO: (11) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname1/proxy/: foo (200; 14.885988ms)
  Apr 28 17:37:00.765: INFO: (11) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname2/proxy/: bar (200; 14.769241ms)
  Apr 28 17:37:00.765: INFO: (11) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname1/proxy/: tls baz (200; 14.862489ms)
  Apr 28 17:37:00.774: INFO: (12) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:460/proxy/: tls baz (200; 9.210606ms)
  Apr 28 17:37:00.775: INFO: (12) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">test<... (200; 9.545122ms)
  Apr 28 17:37:00.775: INFO: (12) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 8.822325ms)
  Apr 28 17:37:00.775: INFO: (12) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 8.72017ms)
  Apr 28 17:37:00.775: INFO: (12) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 8.816669ms)
  Apr 28 17:37:00.775: INFO: (12) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/tlsrewritem... (200; 9.849555ms)
  Apr 28 17:37:00.775: INFO: (12) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/rewriteme">test</a> (200; 9.719003ms)
  Apr 28 17:37:00.776: INFO: (12) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 10.056784ms)
  Apr 28 17:37:00.776: INFO: (12) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">... (200; 10.217788ms)
  Apr 28 17:37:00.776: INFO: (12) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:462/proxy/: tls qux (200; 10.147434ms)
  Apr 28 17:37:00.776: INFO: (12) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname2/proxy/: bar (200; 10.621741ms)
  Apr 28 17:37:00.781: INFO: (12) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname2/proxy/: tls qux (200; 15.098279ms)
  Apr 28 17:37:00.781: INFO: (12) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname2/proxy/: bar (200; 15.331888ms)
  Apr 28 17:37:00.781: INFO: (12) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname1/proxy/: foo (200; 15.53128ms)
  Apr 28 17:37:00.781: INFO: (12) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname1/proxy/: foo (200; 15.735539ms)
  Apr 28 17:37:00.781: INFO: (12) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname1/proxy/: tls baz (200; 15.964456ms)
  Apr 28 17:37:00.787: INFO: (13) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 5.051207ms)
  Apr 28 17:37:00.792: INFO: (13) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">... (200; 10.055203ms)
  Apr 28 17:37:00.792: INFO: (13) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 10.388937ms)
  Apr 28 17:37:00.792: INFO: (13) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 10.198128ms)
  Apr 28 17:37:00.793: INFO: (13) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:460/proxy/: tls baz (200; 10.919291ms)
  Apr 28 17:37:00.793: INFO: (13) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/tlsrewritem... (200; 11.127309ms)
  Apr 28 17:37:00.794: INFO: (13) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">test<... (200; 11.788547ms)
  Apr 28 17:37:00.794: INFO: (13) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 11.860767ms)
  Apr 28 17:37:00.794: INFO: (13) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:462/proxy/: tls qux (200; 12.18106ms)
  Apr 28 17:37:00.796: INFO: (13) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/rewriteme">test</a> (200; 13.568142ms)
  Apr 28 17:37:00.796: INFO: (13) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname1/proxy/: tls baz (200; 14.085013ms)
  Apr 28 17:37:00.797: INFO: (13) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname2/proxy/: bar (200; 14.54856ms)
  Apr 28 17:37:00.797: INFO: (13) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname1/proxy/: foo (200; 14.708542ms)
  Apr 28 17:37:00.797: INFO: (13) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname1/proxy/: foo (200; 14.427506ms)
  Apr 28 17:37:00.797: INFO: (13) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname2/proxy/: tls qux (200; 14.694223ms)
  Apr 28 17:37:00.797: INFO: (13) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname2/proxy/: bar (200; 14.482809ms)
  Apr 28 17:37:00.807: INFO: (14) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 9.633339ms)
  Apr 28 17:37:00.807: INFO: (14) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 9.876628ms)
  Apr 28 17:37:00.807: INFO: (14) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">... (200; 9.747763ms)
  Apr 28 17:37:00.807: INFO: (14) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 10.138234ms)
  Apr 28 17:37:00.807: INFO: (14) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/rewriteme">test</a> (200; 10.2799ms)
  Apr 28 17:37:00.807: INFO: (14) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">test<... (200; 10.453494ms)
  Apr 28 17:37:00.807: INFO: (14) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 10.226365ms)
  Apr 28 17:37:00.808: INFO: (14) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname1/proxy/: foo (200; 11.722703ms)
  Apr 28 17:37:00.809: INFO: (14) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:460/proxy/: tls baz (200; 11.408977ms)
  Apr 28 17:37:00.809: INFO: (14) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/tlsrewritem... (200; 12.000287ms)
  Apr 28 17:37:00.809: INFO: (14) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:462/proxy/: tls qux (200; 11.424443ms)
  Apr 28 17:37:00.809: INFO: (14) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname1/proxy/: foo (200; 11.999386ms)
  Apr 28 17:37:00.810: INFO: (14) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname2/proxy/: tls qux (200; 13.205131ms)
  Apr 28 17:37:00.811: INFO: (14) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname2/proxy/: bar (200; 13.608806ms)
  Apr 28 17:37:00.811: INFO: (14) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname1/proxy/: tls baz (200; 13.950481ms)
  Apr 28 17:37:00.811: INFO: (14) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname2/proxy/: bar (200; 13.999564ms)
  Apr 28 17:37:00.820: INFO: (15) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/tlsrewritem... (200; 8.239933ms)
  Apr 28 17:37:00.820: INFO: (15) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:460/proxy/: tls baz (200; 8.634168ms)
  Apr 28 17:37:00.820: INFO: (15) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/rewriteme">test</a> (200; 8.55653ms)
  Apr 28 17:37:00.821: INFO: (15) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 9.330799ms)
  Apr 28 17:37:00.821: INFO: (15) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 9.08623ms)
  Apr 28 17:37:00.821: INFO: (15) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:462/proxy/: tls qux (200; 9.475111ms)
  Apr 28 17:37:00.821: INFO: (15) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">... (200; 9.840786ms)
  Apr 28 17:37:00.821: INFO: (15) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 9.922829ms)
  Apr 28 17:37:00.821: INFO: (15) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 9.730961ms)
  Apr 28 17:37:00.821: INFO: (15) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">test<... (200; 9.937956ms)
  Apr 28 17:37:00.822: INFO: (15) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname1/proxy/: foo (200; 10.258011ms)
  Apr 28 17:37:00.822: INFO: (15) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname2/proxy/: tls qux (200; 10.68099ms)
  Apr 28 17:37:00.822: INFO: (15) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname2/proxy/: bar (200; 11.092979ms)
  Apr 28 17:37:00.824: INFO: (15) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname1/proxy/: tls baz (200; 11.893666ms)
  Apr 28 17:37:00.824: INFO: (15) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname1/proxy/: foo (200; 12.491118ms)
  Apr 28 17:37:00.824: INFO: (15) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname2/proxy/: bar (200; 12.547321ms)
  Apr 28 17:37:00.836: INFO: (16) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:462/proxy/: tls qux (200; 11.758294ms)
  Apr 28 17:37:00.840: INFO: (16) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 14.891129ms)
  Apr 28 17:37:00.841: INFO: (16) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 16.14747ms)
  Apr 28 17:37:00.841: INFO: (16) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/tlsrewritem... (200; 15.99704ms)
  Apr 28 17:37:00.842: INFO: (16) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/rewriteme">test</a> (200; 16.790066ms)
  Apr 28 17:37:00.842: INFO: (16) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">test<... (200; 17.2456ms)
  Apr 28 17:37:00.842: INFO: (16) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:460/proxy/: tls baz (200; 17.203484ms)
  Apr 28 17:37:00.842: INFO: (16) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">... (200; 17.477092ms)
  Apr 28 17:37:00.842: INFO: (16) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 17.464335ms)
  Apr 28 17:37:00.842: INFO: (16) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 17.551ms)
  Apr 28 17:37:00.843: INFO: (16) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname2/proxy/: bar (200; 18.084219ms)
  Apr 28 17:37:00.844: INFO: (16) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname1/proxy/: tls baz (200; 19.697343ms)
  Apr 28 17:37:00.845: INFO: (16) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname2/proxy/: tls qux (200; 19.81425ms)
  Apr 28 17:37:00.845: INFO: (16) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname2/proxy/: bar (200; 19.628208ms)
  Apr 28 17:37:00.845: INFO: (16) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname1/proxy/: foo (200; 19.675108ms)
  Apr 28 17:37:00.845: INFO: (16) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname1/proxy/: foo (200; 19.954266ms)
  Apr 28 17:37:00.854: INFO: (17) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">... (200; 9.711186ms)
  Apr 28 17:37:00.854: INFO: (17) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:462/proxy/: tls qux (200; 9.672229ms)
  Apr 28 17:37:00.854: INFO: (17) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 9.76379ms)
  Apr 28 17:37:00.854: INFO: (17) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/tlsrewritem... (200; 9.830299ms)
  Apr 28 17:37:00.858: INFO: (17) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 12.824362ms)
  Apr 28 17:37:00.858: INFO: (17) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/rewriteme">test</a> (200; 12.469537ms)
  Apr 28 17:37:00.861: INFO: (17) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 15.896782ms)
  Apr 28 17:37:00.861: INFO: (17) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 16.645858ms)
  Apr 28 17:37:00.861: INFO: (17) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">test<... (200; 16.529382ms)
  Apr 28 17:37:00.863: INFO: (17) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname1/proxy/: foo (200; 17.734647ms)
  Apr 28 17:37:00.863: INFO: (17) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname2/proxy/: bar (200; 18.04384ms)
  Apr 28 17:37:00.863: INFO: (17) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:460/proxy/: tls baz (200; 18.128772ms)
  Apr 28 17:37:00.863: INFO: (17) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname1/proxy/: tls baz (200; 17.947315ms)
  Apr 28 17:37:00.866: INFO: (17) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname1/proxy/: foo (200; 21.171787ms)
  Apr 28 17:37:00.867: INFO: (17) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname2/proxy/: bar (200; 21.517921ms)
  Apr 28 17:37:00.867: INFO: (17) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname2/proxy/: tls qux (200; 21.898238ms)
  Apr 28 17:37:00.873: INFO: (18) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/tlsrewritem... (200; 5.488994ms)
  Apr 28 17:37:00.877: INFO: (18) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">... (200; 9.623283ms)
  Apr 28 17:37:00.877: INFO: (18) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/rewriteme">test</a> (200; 9.790064ms)
  Apr 28 17:37:00.879: INFO: (18) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 10.977131ms)
  Apr 28 17:37:00.879: INFO: (18) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 11.245177ms)
  Apr 28 17:37:00.879: INFO: (18) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">test<... (200; 11.347021ms)
  Apr 28 17:37:00.879: INFO: (18) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 11.210438ms)
  Apr 28 17:37:00.879: INFO: (18) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:462/proxy/: tls qux (200; 11.117615ms)
  Apr 28 17:37:00.880: INFO: (18) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 12.148613ms)
  Apr 28 17:37:00.881: INFO: (18) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:460/proxy/: tls baz (200; 13.284005ms)
  Apr 28 17:37:00.881: INFO: (18) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname2/proxy/: bar (200; 13.690045ms)
  Apr 28 17:37:00.881: INFO: (18) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname2/proxy/: tls qux (200; 14.004801ms)
  Apr 28 17:37:00.881: INFO: (18) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname1/proxy/: foo (200; 14.01436ms)
  Apr 28 17:37:00.881: INFO: (18) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname1/proxy/: tls baz (200; 13.824389ms)
  Apr 28 17:37:00.881: INFO: (18) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname1/proxy/: foo (200; 13.751166ms)
  Apr 28 17:37:00.881: INFO: (18) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname2/proxy/: bar (200; 14.29523ms)
  Apr 28 17:37:00.888: INFO: (19) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">... (200; 5.591947ms)
  Apr 28 17:37:00.895: INFO: (19) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 11.612556ms)
  Apr 28 17:37:00.896: INFO: (19) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 12.436237ms)
  Apr 28 17:37:00.896: INFO: (19) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq/proxy/rewriteme">test</a> (200; 12.938893ms)
  Apr 28 17:37:00.897: INFO: (19) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:460/proxy/: tls baz (200; 13.040401ms)
  Apr 28 17:37:00.897: INFO: (19) /api/v1/namespaces/proxy-1515/pods/http:proxy-service-lrtv7-nbwlq:162/proxy/: bar (200; 13.882446ms)
  Apr 28 17:37:00.897: INFO: (19) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname1/proxy/: foo (200; 14.554289ms)
  Apr 28 17:37:00.899: INFO: (19) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:1080/proxy/rewriteme">test<... (200; 15.479444ms)
  Apr 28 17:37:00.899: INFO: (19) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/: <a href="/api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:443/proxy/tlsrewritem... (200; 16.584808ms)
  Apr 28 17:37:00.899: INFO: (19) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname2/proxy/: tls qux (200; 15.265309ms)
  Apr 28 17:37:00.899: INFO: (19) /api/v1/namespaces/proxy-1515/pods/https:proxy-service-lrtv7-nbwlq:462/proxy/: tls qux (200; 15.882892ms)
  Apr 28 17:37:00.900: INFO: (19) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname2/proxy/: bar (200; 16.142217ms)
  Apr 28 17:37:00.901: INFO: (19) /api/v1/namespaces/proxy-1515/services/https:proxy-service-lrtv7:tlsportname1/proxy/: tls baz (200; 18.178291ms)
  Apr 28 17:37:00.901: INFO: (19) /api/v1/namespaces/proxy-1515/pods/proxy-service-lrtv7-nbwlq:160/proxy/: foo (200; 16.76882ms)
  Apr 28 17:37:00.901: INFO: (19) /api/v1/namespaces/proxy-1515/services/proxy-service-lrtv7:portname1/proxy/: foo (200; 16.934061ms)
  Apr 28 17:37:00.901: INFO: (19) /api/v1/namespaces/proxy-1515/services/http:proxy-service-lrtv7:portname2/proxy/: bar (200; 18.403297ms)
  Apr 28 17:37:00.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-lrtv7 in namespace proxy-1515, will wait for the garbage collector to delete the pods @ 04/28/23 17:37:00.905
  Apr 28 17:37:00.964: INFO: Deleting ReplicationController proxy-service-lrtv7 took: 5.842702ms
  Apr 28 17:37:01.064: INFO: Terminating ReplicationController proxy-service-lrtv7 pods took: 100.388426ms
  E0428 17:37:01.357224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:02.357595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-1515" for this suite. @ 04/28/23 17:37:03.065
• [5.739 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 04/28/23 17:37:03.082
  Apr 28 17:37:03.082: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename secrets @ 04/28/23 17:37:03.083
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:37:03.128
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:37:03.132
  STEP: creating secret secrets-2787/secret-test-b7cdd662-0f85-4d5c-b17c-817041c4cf96 @ 04/28/23 17:37:03.139
  STEP: Creating a pod to test consume secrets @ 04/28/23 17:37:03.153
  E0428 17:37:03.357969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:04.358354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:05.358488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:06.358592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:37:07.213
  Apr 28 17:37:07.217: INFO: Trying to get logs from node ip-172-31-10-53 pod pod-configmaps-243d5a96-7849-4540-b414-c0bcf0174790 container env-test: <nil>
  STEP: delete the pod @ 04/28/23 17:37:07.229
  Apr 28 17:37:07.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2787" for this suite. @ 04/28/23 17:37:07.267
• [4.196 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 04/28/23 17:37:07.279
  Apr 28 17:37:07.279: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 17:37:07.283
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:37:07.298
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:37:07.315
  STEP: Creating a pod to test downward api env vars @ 04/28/23 17:37:07.323
  E0428 17:37:07.359983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:08.360617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:09.360817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:10.360954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:11.361076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:37:11.389
  Apr 28 17:37:11.392: INFO: Trying to get logs from node ip-172-31-10-53 pod downward-api-d07adc07-f5b7-41c7-86ca-0454568c950a container dapi-container: <nil>
  STEP: delete the pod @ 04/28/23 17:37:11.397
  Apr 28 17:37:11.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7244" for this suite. @ 04/28/23 17:37:11.415
• [4.147 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 04/28/23 17:37:11.43
  Apr 28 17:37:11.430: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 17:37:11.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:37:11.458
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:37:11.463
  STEP: Creating a ResourceQuota @ 04/28/23 17:37:11.467
  STEP: Getting a ResourceQuota @ 04/28/23 17:37:11.473
  STEP: Listing all ResourceQuotas with LabelSelector @ 04/28/23 17:37:11.489
  STEP: Patching the ResourceQuota @ 04/28/23 17:37:11.498
  STEP: Deleting a Collection of ResourceQuotas @ 04/28/23 17:37:11.508
  STEP: Verifying the deleted ResourceQuota @ 04/28/23 17:37:11.537
  Apr 28 17:37:11.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6983" for this suite. @ 04/28/23 17:37:11.55
• [0.131 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 04/28/23 17:37:11.562
  Apr 28 17:37:11.562: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename webhook @ 04/28/23 17:37:11.563
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:37:11.604
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:37:11.608
  STEP: Setting up server cert @ 04/28/23 17:37:11.647
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 17:37:12.301
  STEP: Deploying the webhook pod @ 04/28/23 17:37:12.312
  STEP: Wait for the deployment to be ready @ 04/28/23 17:37:12.324
  Apr 28 17:37:12.330: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0428 17:37:12.362045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:13.362249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 17:37:14.341
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 17:37:14.35
  E0428 17:37:14.362417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:37:15.351: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 28 17:37:15.354: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 17:37:15.363013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9251-crds.webhook.example.com via the AdmissionRegistration API @ 04/28/23 17:37:15.866
  STEP: Creating a custom resource while v1 is storage version @ 04/28/23 17:37:15.882
  E0428 17:37:16.363341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:17.363557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 04/28/23 17:37:17.906
  STEP: Patching the custom resource while v2 is storage version @ 04/28/23 17:37:17.911
  Apr 28 17:37:17.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0428 17:37:18.364104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-4421" for this suite. @ 04/28/23 17:37:18.522
  STEP: Destroying namespace "webhook-markers-4452" for this suite. @ 04/28/23 17:37:18.528
• [6.977 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 04/28/23 17:37:18.54
  Apr 28 17:37:18.540: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename gc @ 04/28/23 17:37:18.542
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:37:18.595
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:37:18.601
  STEP: create the deployment @ 04/28/23 17:37:18.605
  W0428 17:37:18.609838      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 04/28/23 17:37:18.61
  STEP: delete the deployment @ 04/28/23 17:37:19.12
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 04/28/23 17:37:19.127
  E0428 17:37:19.364553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 04/28/23 17:37:19.652
  W0428 17:37:19.656371      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Apr 28 17:37:19.656: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 28 17:37:19.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8451" for this suite. @ 04/28/23 17:37:19.659
• [1.124 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 04/28/23 17:37:19.668
  Apr 28 17:37:19.668: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:37:19.669
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:37:19.697
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:37:19.7
  STEP: Creating configMap with name projected-configmap-test-volume-0175ea74-975c-4fec-8ced-42c72f456214 @ 04/28/23 17:37:19.702
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:37:19.713
  E0428 17:37:20.365382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:21.365603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:22.365748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:23.365876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:37:23.77
  Apr 28 17:37:23.791: INFO: Trying to get logs from node ip-172-31-10-53 pod pod-projected-configmaps-db8ca6a3-78a9-45d2-a0dd-931ed3ebda99 container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 17:37:23.827
  Apr 28 17:37:23.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2710" for this suite. @ 04/28/23 17:37:23.88
• [4.223 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 04/28/23 17:37:23.892
  Apr 28 17:37:23.892: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 17:37:23.893
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:37:23.975
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:37:23.98
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 17:37:23.983
  E0428 17:37:24.366832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:25.366973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:26.368084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:27.368344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:37:28.072
  Apr 28 17:37:28.075: INFO: Trying to get logs from node ip-172-31-10-53 pod downwardapi-volume-4f246f48-32c0-4b94-859b-ea4e8c0c4cd2 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 17:37:28.08
  Apr 28 17:37:28.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6417" for this suite. @ 04/28/23 17:37:28.102
• [4.217 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 04/28/23 17:37:28.109
  Apr 28 17:37:28.110: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename daemonsets @ 04/28/23 17:37:28.11
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:37:28.126
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:37:28.129
  STEP: Creating a simple DaemonSet "daemon-set" @ 04/28/23 17:37:28.173
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/28/23 17:37:28.207
  Apr 28 17:37:28.335: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 17:37:28.335: INFO: Node ip-172-31-1-213 is running 0 daemon pod, expected 1
  E0428 17:37:28.369067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:37:29.342: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 17:37:29.342: INFO: Node ip-172-31-1-213 is running 0 daemon pod, expected 1
  E0428 17:37:29.369496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:37:30.342: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
  Apr 28 17:37:30.342: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 04/28/23 17:37:30.345
  Apr 28 17:37:30.367: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 28 17:37:30.367: INFO: Node ip-172-31-10-53 is running 0 daemon pod, expected 1
  E0428 17:37:30.370509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:31.370509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:37:31.807: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
  Apr 28 17:37:31.807: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 04/28/23 17:37:31.807
  STEP: Deleting DaemonSet "daemon-set" @ 04/28/23 17:37:31.811
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6049, will wait for the garbage collector to delete the pods @ 04/28/23 17:37:31.811
  Apr 28 17:37:31.882: INFO: Deleting DaemonSet.extensions daemon-set took: 17.717462ms
  Apr 28 17:37:31.983: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.08627ms
  E0428 17:37:32.370924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:37:33.286: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 17:37:33.286: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 28 17:37:33.290: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"268269"},"items":null}

  Apr 28 17:37:33.292: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"268269"},"items":null}

  Apr 28 17:37:33.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6049" for this suite. @ 04/28/23 17:37:33.31
• [5.208 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 04/28/23 17:37:33.318
  Apr 28 17:37:33.318: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:37:33.319
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:37:33.339
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:37:33.342
  STEP: Creating configMap with name projected-configmap-test-volume-map-acc80d67-8156-481a-a44e-b46d9cafeee3 @ 04/28/23 17:37:33.345
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:37:33.351
  E0428 17:37:33.371974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:34.372107      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:35.372328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:36.372436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:37.372570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:37:37.387
  Apr 28 17:37:37.391: INFO: Trying to get logs from node ip-172-31-10-53 pod pod-projected-configmaps-b6276876-d6e0-456a-8a4a-e9d3137a744d container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 17:37:37.401
  Apr 28 17:37:37.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1358" for this suite. @ 04/28/23 17:37:37.442
• [4.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 04/28/23 17:37:37.458
  Apr 28 17:37:37.458: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/28/23 17:37:37.459
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:37:37.505
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:37:37.508
  STEP: fetching the /apis discovery document @ 04/28/23 17:37:37.51
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 04/28/23 17:37:37.512
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 04/28/23 17:37:37.512
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 04/28/23 17:37:37.512
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 04/28/23 17:37:37.513
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 04/28/23 17:37:37.513
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 04/28/23 17:37:37.515
  Apr 28 17:37:37.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-3516" for this suite. @ 04/28/23 17:37:37.52
• [0.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 04/28/23 17:37:37.536
  Apr 28 17:37:37.536: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename disruption @ 04/28/23 17:37:37.537
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:37:37.581
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:37:37.584
  STEP: Waiting for the pdb to be processed @ 04/28/23 17:37:37.607
  E0428 17:37:38.372745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:39.373184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 04/28/23 17:37:39.668
  Apr 28 17:37:39.689: INFO: running pods: 0 < 3
  E0428 17:37:40.373297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:41.373425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:37:41.763: INFO: running pods: 2 < 3
  E0428 17:37:42.374431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:43.374883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:37:43.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-7327" for this suite. @ 04/28/23 17:37:43.702
• [6.173 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 04/28/23 17:37:43.709
  Apr 28 17:37:43.709: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename svcaccounts @ 04/28/23 17:37:43.71
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:37:43.726
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:37:43.729
  STEP: creating a ServiceAccount @ 04/28/23 17:37:43.731
  STEP: watching for the ServiceAccount to be added @ 04/28/23 17:37:43.739
  STEP: patching the ServiceAccount @ 04/28/23 17:37:43.742
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 04/28/23 17:37:43.749
  STEP: deleting the ServiceAccount @ 04/28/23 17:37:43.754
  Apr 28 17:37:43.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4260" for this suite. @ 04/28/23 17:37:43.778
• [0.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 04/28/23 17:37:43.799
  Apr 28 17:37:43.799: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename configmap @ 04/28/23 17:37:43.799
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:37:43.813
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:37:43.822
  STEP: Creating configMap with name configmap-test-volume-0f69bd80-7be3-414b-902e-9cc3c39b848c @ 04/28/23 17:37:43.826
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:37:43.832
  E0428 17:37:44.375608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:45.375801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:46.376390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:47.376518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:37:47.87
  Apr 28 17:37:47.875: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-configmaps-2fcb792f-55cf-41fd-b6c1-a51030a79632 container configmap-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 17:37:47.888
  Apr 28 17:37:47.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9834" for this suite. @ 04/28/23 17:37:47.915
• [4.129 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 04/28/23 17:37:47.928
  Apr 28 17:37:47.928: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename secrets @ 04/28/23 17:37:47.929
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:37:47.945
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:37:47.949
  STEP: Creating secret with name secret-test-25e06fbe-4865-4da2-b540-6f6cf1e80a41 @ 04/28/23 17:37:47.977
  STEP: Creating a pod to test consume secrets @ 04/28/23 17:37:47.983
  E0428 17:37:48.377041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:49.377324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:50.378232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:51.378472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:37:52.013
  Apr 28 17:37:52.020: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-secrets-b680e077-aa11-408f-97b3-b8110c37bb8d container secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 17:37:52.028
  Apr 28 17:37:52.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4778" for this suite. @ 04/28/23 17:37:52.062
  STEP: Destroying namespace "secret-namespace-6598" for this suite. @ 04/28/23 17:37:52.072
• [4.151 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 04/28/23 17:37:52.08
  Apr 28 17:37:52.080: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:37:52.081
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:37:52.102
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:37:52.107
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 17:37:52.11
  E0428 17:37:52.379060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:53.379306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:54.380039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:55.380186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:37:56.143
  Apr 28 17:37:56.147: INFO: Trying to get logs from node ip-172-31-10-53 pod downwardapi-volume-a35ccaa9-5543-4885-8cc8-5b9bd22413c2 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 17:37:56.158
  Apr 28 17:37:56.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7806" for this suite. @ 04/28/23 17:37:56.2
• [4.133 seconds]
------------------------------
S
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 04/28/23 17:37:56.213
  Apr 28 17:37:56.213: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename services @ 04/28/23 17:37:56.214
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:37:56.278
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:37:56.281
  STEP: creating service in namespace services-8700 @ 04/28/23 17:37:56.285
  STEP: creating service affinity-clusterip in namespace services-8700 @ 04/28/23 17:37:56.285
  STEP: creating replication controller affinity-clusterip in namespace services-8700 @ 04/28/23 17:37:56.305
  I0428 17:37:56.326838      19 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-8700, replica count: 3
  E0428 17:37:56.381184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:57.381593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:58.382073      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 17:37:59.379415      19 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0428 17:37:59.382665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:37:59.385: INFO: Creating new exec pod
  E0428 17:38:00.382762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:01.382900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:02.383301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:38:02.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-8700 exec execpod-affinitykt22k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Apr 28 17:38:02.582: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Apr 28 17:38:02.582: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:38:02.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-8700 exec execpod-affinitykt22k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.11.49 80'
  Apr 28 17:38:02.767: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.11.49 80\nConnection to 10.43.11.49 80 port [tcp/http] succeeded!\n"
  Apr 28 17:38:02.767: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:38:02.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-8700 exec execpod-affinitykt22k -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.11.49:80/ ; done'
  Apr 28 17:38:03.333: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.11.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.11.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.11.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.11.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.11.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.11.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.11.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.11.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.11.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.11.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.11.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.11.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.11.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.11.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.11.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.11.49:80/\n"
  Apr 28 17:38:03.334: INFO: stdout: "\naffinity-clusterip-swrl9\naffinity-clusterip-swrl9\naffinity-clusterip-swrl9\naffinity-clusterip-swrl9\naffinity-clusterip-swrl9\naffinity-clusterip-swrl9\naffinity-clusterip-swrl9\naffinity-clusterip-swrl9\naffinity-clusterip-swrl9\naffinity-clusterip-swrl9\naffinity-clusterip-swrl9\naffinity-clusterip-swrl9\naffinity-clusterip-swrl9\naffinity-clusterip-swrl9\naffinity-clusterip-swrl9\naffinity-clusterip-swrl9"
  Apr 28 17:38:03.334: INFO: Received response from host: affinity-clusterip-swrl9
  Apr 28 17:38:03.334: INFO: Received response from host: affinity-clusterip-swrl9
  Apr 28 17:38:03.334: INFO: Received response from host: affinity-clusterip-swrl9
  Apr 28 17:38:03.334: INFO: Received response from host: affinity-clusterip-swrl9
  Apr 28 17:38:03.334: INFO: Received response from host: affinity-clusterip-swrl9
  Apr 28 17:38:03.334: INFO: Received response from host: affinity-clusterip-swrl9
  Apr 28 17:38:03.334: INFO: Received response from host: affinity-clusterip-swrl9
  Apr 28 17:38:03.334: INFO: Received response from host: affinity-clusterip-swrl9
  Apr 28 17:38:03.334: INFO: Received response from host: affinity-clusterip-swrl9
  Apr 28 17:38:03.334: INFO: Received response from host: affinity-clusterip-swrl9
  Apr 28 17:38:03.334: INFO: Received response from host: affinity-clusterip-swrl9
  Apr 28 17:38:03.334: INFO: Received response from host: affinity-clusterip-swrl9
  Apr 28 17:38:03.334: INFO: Received response from host: affinity-clusterip-swrl9
  Apr 28 17:38:03.334: INFO: Received response from host: affinity-clusterip-swrl9
  Apr 28 17:38:03.334: INFO: Received response from host: affinity-clusterip-swrl9
  Apr 28 17:38:03.334: INFO: Received response from host: affinity-clusterip-swrl9
  Apr 28 17:38:03.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 17:38:03.338: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-8700, will wait for the garbage collector to delete the pods @ 04/28/23 17:38:03.369
  E0428 17:38:03.383998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:38:03.441: INFO: Deleting ReplicationController affinity-clusterip took: 15.617329ms
  Apr 28 17:38:03.542: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.762374ms
  E0428 17:38:04.384629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:05.385263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-8700" for this suite. @ 04/28/23 17:38:05.667
• [9.459 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 04/28/23 17:38:05.674
  Apr 28 17:38:05.674: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename deployment @ 04/28/23 17:38:05.674
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:38:05.69
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:38:05.694
  STEP: creating a Deployment @ 04/28/23 17:38:05.7
  Apr 28 17:38:05.700: INFO: Creating simple deployment test-deployment-vkz6b
  Apr 28 17:38:05.725: INFO: deployment "test-deployment-vkz6b" doesn't have the required revision set
  E0428 17:38:06.385765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:07.388525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Getting /status @ 04/28/23 17:38:07.757
  Apr 28 17:38:07.761: INFO: Deployment test-deployment-vkz6b has Conditions: [{Available True 2023-04-28 17:38:07 +0000 UTC 2023-04-28 17:38:07 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-04-28 17:38:07 +0000 UTC 2023-04-28 17:38:05 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vkz6b-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 04/28/23 17:38:07.761
  Apr 28 17:38:07.775: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 38, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 38, 7, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 38, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 38, 5, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-vkz6b-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 04/28/23 17:38:07.775
  Apr 28 17:38:07.777: INFO: Observed &Deployment event: ADDED
  Apr 28 17:38:07.777: INFO: Observed Deployment test-deployment-vkz6b in namespace deployment-7820 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-28 17:38:05 +0000 UTC 2023-04-28 17:38:05 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vkz6b-5994cf9475"}
  Apr 28 17:38:07.777: INFO: Observed &Deployment event: MODIFIED
  Apr 28 17:38:07.777: INFO: Observed Deployment test-deployment-vkz6b in namespace deployment-7820 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-28 17:38:05 +0000 UTC 2023-04-28 17:38:05 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vkz6b-5994cf9475"}
  Apr 28 17:38:07.777: INFO: Observed Deployment test-deployment-vkz6b in namespace deployment-7820 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-28 17:38:05 +0000 UTC 2023-04-28 17:38:05 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 28 17:38:07.777: INFO: Observed &Deployment event: MODIFIED
  Apr 28 17:38:07.777: INFO: Observed Deployment test-deployment-vkz6b in namespace deployment-7820 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-28 17:38:05 +0000 UTC 2023-04-28 17:38:05 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 28 17:38:07.777: INFO: Observed Deployment test-deployment-vkz6b in namespace deployment-7820 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-28 17:38:05 +0000 UTC 2023-04-28 17:38:05 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-vkz6b-5994cf9475" is progressing.}
  Apr 28 17:38:07.778: INFO: Observed &Deployment event: MODIFIED
  Apr 28 17:38:07.778: INFO: Observed Deployment test-deployment-vkz6b in namespace deployment-7820 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-28 17:38:07 +0000 UTC 2023-04-28 17:38:07 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 28 17:38:07.778: INFO: Observed Deployment test-deployment-vkz6b in namespace deployment-7820 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-28 17:38:07 +0000 UTC 2023-04-28 17:38:05 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vkz6b-5994cf9475" has successfully progressed.}
  Apr 28 17:38:07.778: INFO: Observed &Deployment event: MODIFIED
  Apr 28 17:38:07.778: INFO: Observed Deployment test-deployment-vkz6b in namespace deployment-7820 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-28 17:38:07 +0000 UTC 2023-04-28 17:38:07 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 28 17:38:07.778: INFO: Observed Deployment test-deployment-vkz6b in namespace deployment-7820 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-28 17:38:07 +0000 UTC 2023-04-28 17:38:05 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vkz6b-5994cf9475" has successfully progressed.}
  Apr 28 17:38:07.778: INFO: Found Deployment test-deployment-vkz6b in namespace deployment-7820 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 28 17:38:07.778: INFO: Deployment test-deployment-vkz6b has an updated status
  STEP: patching the Statefulset Status @ 04/28/23 17:38:07.778
  Apr 28 17:38:07.778: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 28 17:38:07.788: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 04/28/23 17:38:07.788
  Apr 28 17:38:07.790: INFO: Observed &Deployment event: ADDED
  Apr 28 17:38:07.790: INFO: Observed deployment test-deployment-vkz6b in namespace deployment-7820 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-28 17:38:05 +0000 UTC 2023-04-28 17:38:05 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vkz6b-5994cf9475"}
  Apr 28 17:38:07.791: INFO: Observed &Deployment event: MODIFIED
  Apr 28 17:38:07.791: INFO: Observed deployment test-deployment-vkz6b in namespace deployment-7820 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-28 17:38:05 +0000 UTC 2023-04-28 17:38:05 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vkz6b-5994cf9475"}
  Apr 28 17:38:07.791: INFO: Observed deployment test-deployment-vkz6b in namespace deployment-7820 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-28 17:38:05 +0000 UTC 2023-04-28 17:38:05 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 28 17:38:07.791: INFO: Observed &Deployment event: MODIFIED
  Apr 28 17:38:07.791: INFO: Observed deployment test-deployment-vkz6b in namespace deployment-7820 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-28 17:38:05 +0000 UTC 2023-04-28 17:38:05 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 28 17:38:07.791: INFO: Observed deployment test-deployment-vkz6b in namespace deployment-7820 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-28 17:38:05 +0000 UTC 2023-04-28 17:38:05 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-vkz6b-5994cf9475" is progressing.}
  Apr 28 17:38:07.791: INFO: Observed &Deployment event: MODIFIED
  Apr 28 17:38:07.791: INFO: Observed deployment test-deployment-vkz6b in namespace deployment-7820 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-28 17:38:07 +0000 UTC 2023-04-28 17:38:07 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 28 17:38:07.791: INFO: Observed deployment test-deployment-vkz6b in namespace deployment-7820 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-28 17:38:07 +0000 UTC 2023-04-28 17:38:05 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vkz6b-5994cf9475" has successfully progressed.}
  Apr 28 17:38:07.792: INFO: Observed &Deployment event: MODIFIED
  Apr 28 17:38:07.792: INFO: Observed deployment test-deployment-vkz6b in namespace deployment-7820 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-28 17:38:07 +0000 UTC 2023-04-28 17:38:07 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 28 17:38:07.792: INFO: Observed deployment test-deployment-vkz6b in namespace deployment-7820 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-28 17:38:07 +0000 UTC 2023-04-28 17:38:05 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vkz6b-5994cf9475" has successfully progressed.}
  Apr 28 17:38:07.792: INFO: Observed deployment test-deployment-vkz6b in namespace deployment-7820 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 28 17:38:07.792: INFO: Observed &Deployment event: MODIFIED
  Apr 28 17:38:07.792: INFO: Found deployment test-deployment-vkz6b in namespace deployment-7820 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Apr 28 17:38:07.792: INFO: Deployment test-deployment-vkz6b has a patched status
  Apr 28 17:38:07.795: INFO: Deployment "test-deployment-vkz6b":
  &Deployment{ObjectMeta:{test-deployment-vkz6b  deployment-7820  76233d6c-15f6-4314-b925-a6f867690c14 268785 1 2023-04-28 17:38:05 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-04-28 17:38:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-04-28 17:38:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {k3s Update apps/v1 2023-04-28 17:38:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005cf8da8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-vkz6b-5994cf9475",LastUpdateTime:2023-04-28 17:38:07 +0000 UTC,LastTransitionTime:2023-04-28 17:38:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 28 17:38:07.800: INFO: New ReplicaSet "test-deployment-vkz6b-5994cf9475" of Deployment "test-deployment-vkz6b":
  &ReplicaSet{ObjectMeta:{test-deployment-vkz6b-5994cf9475  deployment-7820  2dbeddb6-8203-46f5-8556-d31f4c17bf50 268781 1 2023-04-28 17:38:05 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-vkz6b 76233d6c-15f6-4314-b925-a6f867690c14 0xc005d0aca0 0xc005d0aca1}] [] [{k3s Update apps/v1 2023-04-28 17:38:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76233d6c-15f6-4314-b925-a6f867690c14\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2023-04-28 17:38:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d0ad58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 17:38:07.806: INFO: Pod "test-deployment-vkz6b-5994cf9475-sbgck" is available:
  &Pod{ObjectMeta:{test-deployment-vkz6b-5994cf9475-sbgck test-deployment-vkz6b-5994cf9475- deployment-7820  4f9b04b3-6ea7-432c-a30c-fab1cdfe1a31 268780 0 2023-04-28 17:38:05 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-vkz6b-5994cf9475 2dbeddb6-8203-46f5-8556-d31f4c17bf50 0xc005d0b110 0xc005d0b111}] [] [{k3s Update v1 2023-04-28 17:38:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2dbeddb6-8203-46f5-8556-d31f4c17bf50\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:38:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jkvch,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jkvch,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-10-53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:38:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:38:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:38:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:38:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.10.53,PodIP:10.42.1.80,StartTime:2023-04-28 17:38:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:38:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ade723f458f92b8d4adfefcf4ef6694f31c50349f704c07474f0d3253ed177cd,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.80,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:38:07.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7820" for this suite. @ 04/28/23 17:38:07.81
• [2.145 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 04/28/23 17:38:07.82
  Apr 28 17:38:07.820: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename gc @ 04/28/23 17:38:07.821
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:38:07.834
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:38:07.837
  STEP: create the deployment @ 04/28/23 17:38:07.841
  W0428 17:38:07.848535      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 04/28/23 17:38:07.848
  STEP: delete the deployment @ 04/28/23 17:38:07.853
  STEP: wait for all rs to be garbage collected @ 04/28/23 17:38:07.865
  STEP: expected 0 rs, got 1 rs @ 04/28/23 17:38:07.92
  STEP: expected 0 pods, got 2 pods @ 04/28/23 17:38:07.946
  E0428 17:38:08.389874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 04/28/23 17:38:08.454
  W0428 17:38:08.458854      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Apr 28 17:38:08.458: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 28 17:38:08.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8965" for this suite. @ 04/28/23 17:38:08.462
• [0.649 seconds]
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 04/28/23 17:38:08.469
  Apr 28 17:38:08.469: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename security-context @ 04/28/23 17:38:08.47
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:38:08.485
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:38:08.488
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 04/28/23 17:38:08.496
  E0428 17:38:09.389660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:10.389745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:11.390749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:12.390910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:38:12.528
  Apr 28 17:38:12.531: INFO: Trying to get logs from node ip-172-31-1-213 pod security-context-70e3e883-4034-431d-b850-27df3fe35b78 container test-container: <nil>
  STEP: delete the pod @ 04/28/23 17:38:12.537
  Apr 28 17:38:12.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-6011" for this suite. @ 04/28/23 17:38:12.556
• [4.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 04/28/23 17:38:12.566
  Apr 28 17:38:12.566: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:38:12.567
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:38:12.586
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:38:12.589
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 17:38:12.593
  E0428 17:38:13.391867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:14.391972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:15.392119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:16.392220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:38:16.619
  Apr 28 17:38:16.623: INFO: Trying to get logs from node ip-172-31-5-174 pod downwardapi-volume-e3f24a02-d40a-4818-b46e-76e8e274b94b container client-container: <nil>
  STEP: delete the pod @ 04/28/23 17:38:16.634
  Apr 28 17:38:16.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-483" for this suite. @ 04/28/23 17:38:16.658
• [4.101 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 04/28/23 17:38:16.668
  Apr 28 17:38:16.668: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename webhook @ 04/28/23 17:38:16.669
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:38:16.688
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:38:16.691
  STEP: Setting up server cert @ 04/28/23 17:38:16.724
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 17:38:17.371
  STEP: Deploying the webhook pod @ 04/28/23 17:38:17.379
  E0428 17:38:17.392713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Wait for the deployment to be ready @ 04/28/23 17:38:17.393
  Apr 28 17:38:17.398: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0428 17:38:18.392920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:19.393095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 17:38:19.407
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 17:38:19.419
  E0428 17:38:20.393273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:38:20.419: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 04/28/23 17:38:20.422
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/28/23 17:38:20.422
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 04/28/23 17:38:20.439
  E0428 17:38:21.393964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 04/28/23 17:38:21.449
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/28/23 17:38:21.449
  E0428 17:38:22.394213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is longer than webhook latency @ 04/28/23 17:38:22.487
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/28/23 17:38:22.487
  E0428 17:38:23.394551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:24.394931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:25.395072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:26.395275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:27.395296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 04/28/23 17:38:27.524
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/28/23 17:38:27.524
  E0428 17:38:28.395502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:29.395531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:30.395738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:31.395976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:32.396050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:38:33.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7699" for this suite. @ 04/28/23 17:38:33.315
  STEP: Destroying namespace "webhook-markers-1269" for this suite. @ 04/28/23 17:38:33.341
• [16.693 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 04/28/23 17:38:33.363
  Apr 28 17:38:33.363: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename var-expansion @ 04/28/23 17:38:33.363
  E0428 17:38:33.397176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:38:33.451
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:38:33.471
  E0428 17:38:34.397322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:35.397509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:38:35.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 17:38:35.520: INFO: Deleting pod "var-expansion-28068181-57bd-4490-b7d4-cf6248e3d352" in namespace "var-expansion-7019"
  Apr 28 17:38:35.525: INFO: Wait up to 5m0s for pod "var-expansion-28068181-57bd-4490-b7d4-cf6248e3d352" to be fully deleted
  E0428 17:38:36.397569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:37.397802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-7019" for this suite. @ 04/28/23 17:38:37.534
• [4.180 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 04/28/23 17:38:37.544
  Apr 28 17:38:37.544: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename configmap @ 04/28/23 17:38:37.544
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:38:37.57
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:38:37.573
  STEP: Creating configMap configmap-970/configmap-test-963d20f2-08c9-4820-a274-01558c1c894d @ 04/28/23 17:38:37.579
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:38:37.59
  E0428 17:38:38.398459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:39.398840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:40.399038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:41.399351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:38:41.614
  Apr 28 17:38:41.616: INFO: Trying to get logs from node ip-172-31-10-53 pod pod-configmaps-8720fa69-53ec-437d-92a2-90d05ac4243d container env-test: <nil>
  STEP: delete the pod @ 04/28/23 17:38:41.621
  Apr 28 17:38:41.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-970" for this suite. @ 04/28/23 17:38:41.638
• [4.101 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 04/28/23 17:38:41.645
  Apr 28 17:38:41.645: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename configmap @ 04/28/23 17:38:41.648
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:38:41.667
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:38:41.671
  STEP: Creating configMap with name configmap-test-volume-16a5aff7-177c-4cae-abd0-c03bef9b06b3 @ 04/28/23 17:38:41.674
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:38:41.681
  E0428 17:38:42.400564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:43.400579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:44.401619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:45.401820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:38:45.708
  Apr 28 17:38:45.712: INFO: Trying to get logs from node ip-172-31-10-53 pod pod-configmaps-ee2544c7-ae2a-4dc5-8706-ab1e3dc543b4 container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 17:38:45.719
  Apr 28 17:38:45.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8957" for this suite. @ 04/28/23 17:38:45.75
• [4.117 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 04/28/23 17:38:45.765
  Apr 28 17:38:45.765: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/28/23 17:38:45.766
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:38:45.796
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:38:45.803
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 04/28/23 17:38:45.808
  Apr 28 17:38:45.808: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 17:38:46.402894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:47.403394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:38:47.485: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 17:38:48.404103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:49.405083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:50.405739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:51.406438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:52.406678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:53.407701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:38:53.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8526" for this suite. @ 04/28/23 17:38:53.72
• [7.960 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 04/28/23 17:38:53.726
  Apr 28 17:38:53.726: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename gc @ 04/28/23 17:38:53.726
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:38:53.745
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:38:53.75
  Apr 28 17:38:53.816: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"061ad1e5-4830-4817-83b9-eda43fbec0ca", Controller:(*bool)(0xc003464dd2), BlockOwnerDeletion:(*bool)(0xc003464dd3)}}
  Apr 28 17:38:53.823: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"2df2f130-9f07-4ae7-a093-df9bdf01b182", Controller:(*bool)(0xc0041240da), BlockOwnerDeletion:(*bool)(0xc0041240db)}}
  Apr 28 17:38:53.845: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"bd2d98ee-599b-4052-b33c-940bfbdb0bff", Controller:(*bool)(0xc004124322), BlockOwnerDeletion:(*bool)(0xc004124323)}}
  E0428 17:38:54.408169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:55.408281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:56.408738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:57.409071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:58.409462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:38:58.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6362" for this suite. @ 04/28/23 17:38:58.861
• [5.141 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 04/28/23 17:38:58.867
  Apr 28 17:38:58.867: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename var-expansion @ 04/28/23 17:38:58.868
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:38:58.89
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:38:58.896
  STEP: creating the pod with failed condition @ 04/28/23 17:38:58.903
  E0428 17:38:59.410120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:00.410310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:01.411255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:02.411394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:03.412359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:04.412532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:05.413546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:06.413749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:07.413969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:08.414513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:09.414671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:10.414822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:11.415044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:12.415297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:13.416014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:14.416147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:15.416269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:16.416414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:17.417406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:18.417678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:19.417805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:20.418006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:21.418727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:22.418860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:23.419922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:24.420072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:25.421038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:26.421171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:27.421938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:28.422469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:29.423454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:30.423592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:31.424595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:32.424710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:33.425523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:34.425632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:35.426653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:36.426875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:37.427651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:38.428225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:39.428969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:40.429130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:41.430046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:42.430162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:43.430470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:44.430662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:45.431598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:46.431708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:47.431816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:48.432351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:49.433244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:50.433460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:51.434208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:52.434364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:53.435182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:54.435585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:55.435726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:56.435824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:57.436741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:58.437288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:59.437926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:00.438049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:01.438609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:02.438767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:03.439312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:04.439557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:05.440591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:06.440728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:07.440833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:08.441284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:09.441375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:10.441593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:11.442175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:12.442461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:13.443191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:14.443392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:15.444289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:16.444413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:17.445278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:18.446245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:19.446807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:20.446938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:21.447059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:22.447734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:23.447893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:24.448206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:25.448331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:26.448542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:27.448784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:28.449566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:29.450255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:30.450471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:31.451218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:32.451434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:33.451734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:34.452025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:35.453057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:36.453190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:37.453272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:38.453828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:39.453962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:40.454161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:41.455112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:42.455258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:43.455333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:44.455550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:45.456267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:46.456579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:47.457323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:48.457990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:49.458569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:50.458673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:51.458841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:52.458906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:53.462184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:54.462336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:55.463384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:56.463859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:57.464964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:58.465435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pod @ 04/28/23 17:40:58.932
  Apr 28 17:40:59.443: INFO: Successfully updated pod "var-expansion-3d1d9abb-8a8d-4c9f-b05e-8ae17e17ac99"
  STEP: waiting for pod running @ 04/28/23 17:40:59.443
  E0428 17:40:59.465857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:00.465985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 04/28/23 17:41:01.449
  Apr 28 17:41:01.449: INFO: Deleting pod "var-expansion-3d1d9abb-8a8d-4c9f-b05e-8ae17e17ac99" in namespace "var-expansion-3929"
  Apr 28 17:41:01.455: INFO: Wait up to 5m0s for pod "var-expansion-3d1d9abb-8a8d-4c9f-b05e-8ae17e17ac99" to be fully deleted
  E0428 17:41:01.466066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:02.466213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:03.466993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:04.467127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:05.467441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:06.467693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:07.467803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:08.468357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:09.468736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:10.469123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:11.469404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:12.469581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:13.470146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:14.470360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:15.470423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:16.470560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:17.470896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:18.471207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:19.471318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:20.471575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:21.471718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:22.471853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:23.472202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:24.472388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:25.472504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:26.472691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:27.472841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:28.473272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:29.473393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:30.473543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:31.473806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:32.473910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:33.474390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:41:33.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3929" for this suite. @ 04/28/23 17:41:33.52
• [154.658 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 04/28/23 17:41:33.526
  Apr 28 17:41:33.526: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename hostport @ 04/28/23 17:41:33.527
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:41:33.544
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:41:33.547
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 04/28/23 17:41:33.555
  E0428 17:41:34.474502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:35.474714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:36.474846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:37.475576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:38.476212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:39.477045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:40.477159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:41.477252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:42.477578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:43.478380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:44.478486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:45.478620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.10.53 on the node which pod1 resides and expect scheduled @ 04/28/23 17:41:45.593
  E0428 17:41:46.479017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:47.479184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:48.479367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:49.480459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:50.480625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:51.480744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:52.481766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:53.482452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:54.482574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:55.482812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:56.483521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:57.483910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.10.53 but use UDP protocol on the node which pod2 resides @ 04/28/23 17:41:57.633
  E0428 17:41:58.484384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:59.484516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:00.484651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:01.484853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:02.484999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:03.485504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:04.485650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:05.485865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:06.486110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:07.486826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:08.487244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:09.487293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:10.488294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:11.488401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 04/28/23 17:42:11.677
  Apr 28 17:42:11.677: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.10.53 http://127.0.0.1:54323/hostname] Namespace:hostport-2222 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:42:11.677: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 17:42:11.677: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:42:11.677: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/hostport-2222/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.10.53+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.10.53, port: 54323 @ 04/28/23 17:42:11.752
  Apr 28 17:42:11.752: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.10.53:54323/hostname] Namespace:hostport-2222 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:42:11.752: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 17:42:11.752: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:42:11.753: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/hostport-2222/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.10.53%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.10.53, port: 54323 UDP @ 04/28/23 17:42:11.808
  Apr 28 17:42:11.808: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.10.53 54323] Namespace:hostport-2222 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:42:11.808: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 17:42:11.809: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:42:11.809: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/hostport-2222/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.10.53+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0428 17:42:12.488497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:13.488863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:14.489006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:15.489171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:16.489305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:42:16.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-2222" for this suite. @ 04/28/23 17:42:16.873
• [43.353 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 04/28/23 17:42:16.88
  Apr 28 17:42:16.880: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename controllerrevisions @ 04/28/23 17:42:16.88
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:42:16.895
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:42:16.9
  STEP: Creating DaemonSet "e2e-h548r-daemon-set" @ 04/28/23 17:42:16.931
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/28/23 17:42:16.942
  Apr 28 17:42:16.957: INFO: Number of nodes with available pods controlled by daemonset e2e-h548r-daemon-set: 0
  Apr 28 17:42:16.957: INFO: Node ip-172-31-1-213 is running 0 daemon pod, expected 1
  E0428 17:42:17.489871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:42:17.962: INFO: Number of nodes with available pods controlled by daemonset e2e-h548r-daemon-set: 0
  Apr 28 17:42:17.962: INFO: Node ip-172-31-1-213 is running 0 daemon pod, expected 1
  E0428 17:42:18.491201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:42:18.967: INFO: Number of nodes with available pods controlled by daemonset e2e-h548r-daemon-set: 3
  Apr 28 17:42:18.967: INFO: Node ip-172-31-5-174 is running 0 daemon pod, expected 1
  E0428 17:42:19.491291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:42:19.965: INFO: Number of nodes with available pods controlled by daemonset e2e-h548r-daemon-set: 4
  Apr 28 17:42:19.965: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset e2e-h548r-daemon-set
  STEP: Confirm DaemonSet "e2e-h548r-daemon-set" successfully created with "daemonset-name=e2e-h548r-daemon-set" label @ 04/28/23 17:42:19.967
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-h548r-daemon-set" @ 04/28/23 17:42:19.972
  Apr 28 17:42:19.974: INFO: Located ControllerRevision: "e2e-h548r-daemon-set-85dbc87c94"
  STEP: Patching ControllerRevision "e2e-h548r-daemon-set-85dbc87c94" @ 04/28/23 17:42:19.976
  Apr 28 17:42:19.981: INFO: e2e-h548r-daemon-set-85dbc87c94 has been patched
  STEP: Create a new ControllerRevision @ 04/28/23 17:42:19.981
  Apr 28 17:42:19.986: INFO: Created ControllerRevision: e2e-h548r-daemon-set-6697b68b96
  STEP: Confirm that there are two ControllerRevisions @ 04/28/23 17:42:19.986
  Apr 28 17:42:19.986: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 28 17:42:19.988: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-h548r-daemon-set-85dbc87c94" @ 04/28/23 17:42:19.988
  STEP: Confirm that there is only one ControllerRevision @ 04/28/23 17:42:19.993
  Apr 28 17:42:19.993: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 28 17:42:19.996: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-h548r-daemon-set-6697b68b96" @ 04/28/23 17:42:19.997
  Apr 28 17:42:20.017: INFO: e2e-h548r-daemon-set-6697b68b96 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 04/28/23 17:42:20.017
  W0428 17:42:20.029771      19 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 04/28/23 17:42:20.03
  Apr 28 17:42:20.030: INFO: Requesting list of ControllerRevisions to confirm quantity
  E0428 17:42:20.492008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:42:21.035: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 28 17:42:21.038: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-h548r-daemon-set-6697b68b96=updated" @ 04/28/23 17:42:21.038
  STEP: Confirm that there is only one ControllerRevision @ 04/28/23 17:42:21.05
  Apr 28 17:42:21.050: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 28 17:42:21.052: INFO: Found 1 ControllerRevisions
  Apr 28 17:42:21.056: INFO: ControllerRevision "e2e-h548r-daemon-set-7cb848d7b9" has revision 3
  STEP: Deleting DaemonSet "e2e-h548r-daemon-set" @ 04/28/23 17:42:21.059
  STEP: deleting DaemonSet.extensions e2e-h548r-daemon-set in namespace controllerrevisions-2960, will wait for the garbage collector to delete the pods @ 04/28/23 17:42:21.059
  Apr 28 17:42:21.118: INFO: Deleting DaemonSet.extensions e2e-h548r-daemon-set took: 5.87226ms
  Apr 28 17:42:21.219: INFO: Terminating DaemonSet.extensions e2e-h548r-daemon-set pods took: 100.65793ms
  E0428 17:42:21.492571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:22.493450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:42:22.622: INFO: Number of nodes with available pods controlled by daemonset e2e-h548r-daemon-set: 0
  Apr 28 17:42:22.622: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-h548r-daemon-set
  Apr 28 17:42:22.625: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"270520"},"items":null}

  Apr 28 17:42:22.627: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"270520"},"items":null}

  Apr 28 17:42:22.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-2960" for this suite. @ 04/28/23 17:42:22.648
• [5.778 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 04/28/23 17:42:22.658
  Apr 28 17:42:22.658: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename pods @ 04/28/23 17:42:22.659
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:42:22.674
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:42:22.678
  Apr 28 17:42:22.681: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: creating the pod @ 04/28/23 17:42:22.682
  STEP: submitting the pod to kubernetes @ 04/28/23 17:42:22.682
  E0428 17:42:23.494187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:24.494549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:42:24.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6448" for this suite. @ 04/28/23 17:42:24.801
• [2.149 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 04/28/23 17:42:24.807
  Apr 28 17:42:24.807: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename aggregator @ 04/28/23 17:42:24.808
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:42:24.822
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:42:24.826
  Apr 28 17:42:24.828: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Registering the sample API server. @ 04/28/23 17:42:24.829
  Apr 28 17:42:25.305: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Apr 28 17:42:25.326: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
  E0428 17:42:25.495289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:26.495494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:42:27.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:42:27.495965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:28.496406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:42:29.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:42:29.496768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:30.496884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:42:31.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:42:31.497894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:32.498063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:42:33.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:42:33.498811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:34.499052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:42:35.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:42:35.499118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:36.499314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:42:37.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:42:37.500078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:38.500640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:42:39.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:42:39.501681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:40.501957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:42:41.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:42:41.501992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:42.502094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:42:43.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:42:43.502330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:44.502596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:42:45.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:42:45.503526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:46.503777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:42:47.406: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 42, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:42:47.504445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:48.505010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:49.505578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:42:49.571: INFO: Waited 171.185075ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 04/28/23 17:42:49.695
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 04/28/23 17:42:49.698
  STEP: List APIServices @ 04/28/23 17:42:49.705
  Apr 28 17:42:49.722: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 04/28/23 17:42:49.722
  Apr 28 17:42:49.750: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 04/28/23 17:42:49.75
  Apr 28 17:42:49.784: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.April, 28, 17, 42, 49, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 04/28/23 17:42:49.784
  Apr 28 17:42:49.794: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-04-28 17:42:49 +0000 UTC Passed all checks passed}
  Apr 28 17:42:49.794: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 28 17:42:49.794: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 04/28/23 17:42:49.794
  Apr 28 17:42:49.809: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-1858287996" @ 04/28/23 17:42:49.81
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 04/28/23 17:42:49.911
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 04/28/23 17:42:49.934
  STEP: Patch APIService Status @ 04/28/23 17:42:49.937
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 04/28/23 17:42:49.95
  Apr 28 17:42:49.965: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-04-28 17:42:49 +0000 UTC Passed all checks passed}
  Apr 28 17:42:49.965: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 28 17:42:49.965: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Apr 28 17:42:49.965: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 04/28/23 17:42:49.965
  STEP: Confirm that the generated APIService has been deleted @ 04/28/23 17:42:49.97
  Apr 28 17:42:49.970: INFO: Requesting list of APIServices to confirm quantity
  Apr 28 17:42:49.982: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Apr 28 17:42:49.982: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Apr 28 17:42:50.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-150" for this suite. @ 04/28/23 17:42:50.153
• [25.352 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 04/28/23 17:42:50.161
  Apr 28 17:42:50.161: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename configmap @ 04/28/23 17:42:50.162
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:42:50.176
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:42:50.179
  STEP: creating a ConfigMap @ 04/28/23 17:42:50.182
  STEP: fetching the ConfigMap @ 04/28/23 17:42:50.19
  STEP: patching the ConfigMap @ 04/28/23 17:42:50.193
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 04/28/23 17:42:50.201
  STEP: deleting the ConfigMap by collection with a label selector @ 04/28/23 17:42:50.204
  STEP: listing all ConfigMaps in test namespace @ 04/28/23 17:42:50.214
  Apr 28 17:42:50.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8452" for this suite. @ 04/28/23 17:42:50.22
• [0.069 seconds]
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 04/28/23 17:42:50.23
  Apr 28 17:42:50.230: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 17:42:50.231
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:42:50.249
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:42:50.252
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 04/28/23 17:42:50.256
  E0428 17:42:50.505935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:51.506088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:52.506790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:53.507052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:42:54.287
  Apr 28 17:42:54.290: INFO: Trying to get logs from node ip-172-31-10-53 pod pod-13285938-cad1-4d8f-b804-462e6d8fa526 container test-container: <nil>
  STEP: delete the pod @ 04/28/23 17:42:54.3
  Apr 28 17:42:54.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9049" for this suite. @ 04/28/23 17:42:54.316
• [4.091 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 04/28/23 17:42:54.323
  Apr 28 17:42:54.323: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename watch @ 04/28/23 17:42:54.324
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:42:54.337
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:42:54.341
  STEP: creating a watch on configmaps @ 04/28/23 17:42:54.348
  STEP: creating a new configmap @ 04/28/23 17:42:54.351
  STEP: modifying the configmap once @ 04/28/23 17:42:54.356
  STEP: closing the watch once it receives two notifications @ 04/28/23 17:42:54.365
  Apr 28 17:42:54.365: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9177  7d313654-aaa4-42eb-8c4b-15d40c75013f 270829 0 2023-04-28 17:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-28 17:42:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 17:42:54.366: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9177  7d313654-aaa4-42eb-8c4b-15d40c75013f 270830 0 2023-04-28 17:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-28 17:42:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 04/28/23 17:42:54.366
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 04/28/23 17:42:54.373
  STEP: deleting the configmap @ 04/28/23 17:42:54.374
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 04/28/23 17:42:54.381
  Apr 28 17:42:54.381: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9177  7d313654-aaa4-42eb-8c4b-15d40c75013f 270831 0 2023-04-28 17:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-28 17:42:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 17:42:54.381: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9177  7d313654-aaa4-42eb-8c4b-15d40c75013f 270832 0 2023-04-28 17:42:54 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-28 17:42:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 17:42:54.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9177" for this suite. @ 04/28/23 17:42:54.386
• [0.070 seconds]
------------------------------
SS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 04/28/23 17:42:54.392
  Apr 28 17:42:54.392: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename dns @ 04/28/23 17:42:54.393
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:42:54.41
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:42:54.413
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 04/28/23 17:42:54.418
  Apr 28 17:42:54.427: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1895  70abcde9-5ce2-418a-a9e6-7e7ef0f76ca9 270837 0 2023-04-28 17:42:54 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-04-28 17:42:54 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f48mb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f48mb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0428 17:42:54.507531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:55.507608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS suffix list is configured on pod... @ 04/28/23 17:42:56.441
  Apr 28 17:42:56.441: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1895 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:42:56.441: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 17:42:56.441: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:42:56.441: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/dns-1895/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0428 17:42:56.508171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS server is configured on pod... @ 04/28/23 17:42:56.518
  Apr 28 17:42:56.518: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1895 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:42:56.519: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 17:42:56.519: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:42:56.519: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/dns-1895/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 28 17:42:56.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 17:42:56.592: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-1895" for this suite. @ 04/28/23 17:42:56.605
• [2.220 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 04/28/23 17:42:56.613
  Apr 28 17:42:56.613: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 17:42:56.615
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:42:56.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:42:56.634
  STEP: Counting existing ResourceQuota @ 04/28/23 17:42:56.639
  E0428 17:42:57.508482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:58.508880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:59.509746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:00.510713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:01.510847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/28/23 17:43:01.658
  STEP: Ensuring resource quota status is calculated @ 04/28/23 17:43:01.682
  E0428 17:43:02.510964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:03.511481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod that fits quota @ 04/28/23 17:43:03.685
  STEP: Ensuring ResourceQuota status captures the pod usage @ 04/28/23 17:43:03.703
  E0428 17:43:04.511599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:05.511804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 04/28/23 17:43:05.707
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 04/28/23 17:43:05.709
  STEP: Ensuring a pod cannot update its resource requirements @ 04/28/23 17:43:05.711
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 04/28/23 17:43:05.715
  E0428 17:43:06.512100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:07.512302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 04/28/23 17:43:07.72
  STEP: Ensuring resource quota status released the pod usage @ 04/28/23 17:43:07.739
  E0428 17:43:08.513265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:09.513469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:43:09.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4834" for this suite. @ 04/28/23 17:43:09.746
• [13.139 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 04/28/23 17:43:09.755
  Apr 28 17:43:09.755: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename events @ 04/28/23 17:43:09.756
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:43:09.768
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:43:09.771
  STEP: Create set of events @ 04/28/23 17:43:09.774
  STEP: get a list of Events with a label in the current namespace @ 04/28/23 17:43:09.788
  STEP: delete a list of events @ 04/28/23 17:43:09.791
  Apr 28 17:43:09.791: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 04/28/23 17:43:09.808
  Apr 28 17:43:09.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-1662" for this suite. @ 04/28/23 17:43:09.815
• [0.066 seconds]
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 04/28/23 17:43:09.821
  Apr 28 17:43:09.821: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename gc @ 04/28/23 17:43:09.822
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:43:09.841
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:43:09.844
  STEP: create the rc @ 04/28/23 17:43:09.854
  W0428 17:43:09.859502      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0428 17:43:10.522443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:11.522899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:12.522994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:13.523529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:14.523625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:15.523739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 04/28/23 17:43:15.891
  STEP: wait for the rc to be deleted @ 04/28/23 17:43:15.926
  E0428 17:43:16.523834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:43:17.013: INFO: 80 pods remaining
  Apr 28 17:43:17.013: INFO: 80 pods has nil DeletionTimestamp
  Apr 28 17:43:17.013: INFO: 
  E0428 17:43:17.524249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:43:17.971: INFO: 72 pods remaining
  Apr 28 17:43:17.971: INFO: 71 pods has nil DeletionTimestamp
  Apr 28 17:43:17.971: INFO: 
  E0428 17:43:18.524616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:43:19.006: INFO: 60 pods remaining
  Apr 28 17:43:19.006: INFO: 60 pods has nil DeletionTimestamp
  Apr 28 17:43:19.006: INFO: 
  E0428 17:43:19.525286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:43:19.946: INFO: 40 pods remaining
  Apr 28 17:43:19.947: INFO: 40 pods has nil DeletionTimestamp
  Apr 28 17:43:19.947: INFO: 
  E0428 17:43:20.525423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:43:20.943: INFO: 31 pods remaining
  Apr 28 17:43:20.943: INFO: 31 pods has nil DeletionTimestamp
  Apr 28 17:43:20.943: INFO: 
  E0428 17:43:21.525550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:43:21.932: INFO: 20 pods remaining
  Apr 28 17:43:21.932: INFO: 20 pods has nil DeletionTimestamp
  Apr 28 17:43:21.932: INFO: 
  E0428 17:43:22.525696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 04/28/23 17:43:22.936
  W0428 17:43:22.944252      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Apr 28 17:43:22.944: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 28 17:43:22.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7889" for this suite. @ 04/28/23 17:43:22.953
• [13.139 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 04/28/23 17:43:22.961
  Apr 28 17:43:22.961: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:43:22.962
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:43:22.989
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:43:22.993
  STEP: Creating projection with secret that has name projected-secret-test-1e5328a5-4635-48cc-8b21-e61ba509e308 @ 04/28/23 17:43:22.999
  STEP: Creating a pod to test consume secrets @ 04/28/23 17:43:23.008
  E0428 17:43:23.526410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:24.526526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:25.527110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:26.527805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:43:27.045
  Apr 28 17:43:27.048: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-projected-secrets-8c6a0807-1891-4d90-8bce-a3d7cadf1a87 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 17:43:27.065
  Apr 28 17:43:27.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-452" for this suite. @ 04/28/23 17:43:27.091
• [4.149 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 04/28/23 17:43:27.114
  Apr 28 17:43:27.114: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename emptydir-wrapper @ 04/28/23 17:43:27.116
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:43:27.183
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:43:27.189
  STEP: Creating 50 configmaps @ 04/28/23 17:43:27.197
  STEP: Creating RC which spawns configmap-volume pods @ 04/28/23 17:43:27.411
  Apr 28 17:43:27.489: INFO: Pod name wrapped-volume-race-9d933fcd-33eb-4845-8799-f389e05c0f05: Found 3 pods out of 5
  E0428 17:43:27.528078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:28.529164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:29.529292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:30.529430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:31.529700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:43:32.500: INFO: Pod name wrapped-volume-race-9d933fcd-33eb-4845-8799-f389e05c0f05: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/28/23 17:43:32.5
  STEP: Creating RC which spawns configmap-volume pods @ 04/28/23 17:43:32.523
  E0428 17:43:32.530515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:43:32.539: INFO: Pod name wrapped-volume-race-1a15f514-9351-4b66-bcb7-b37c793597e7: Found 0 pods out of 5
  E0428 17:43:33.531551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:34.537800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:35.538199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:36.538368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:37.538579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:43:37.547: INFO: Pod name wrapped-volume-race-1a15f514-9351-4b66-bcb7-b37c793597e7: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/28/23 17:43:37.547
  STEP: Creating RC which spawns configmap-volume pods @ 04/28/23 17:43:37.57
  Apr 28 17:43:37.606: INFO: Pod name wrapped-volume-race-cbae2c56-db41-4254-8c2a-aea707b6be4f: Found 0 pods out of 5
  E0428 17:43:38.538648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:39.538894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:40.538978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:41.539192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:42.539559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:43:42.613: INFO: Pod name wrapped-volume-race-cbae2c56-db41-4254-8c2a-aea707b6be4f: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/28/23 17:43:42.613
  Apr 28 17:43:42.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-cbae2c56-db41-4254-8c2a-aea707b6be4f in namespace emptydir-wrapper-470, will wait for the garbage collector to delete the pods @ 04/28/23 17:43:42.629
  Apr 28 17:43:42.689: INFO: Deleting ReplicationController wrapped-volume-race-cbae2c56-db41-4254-8c2a-aea707b6be4f took: 7.389986ms
  Apr 28 17:43:42.790: INFO: Terminating ReplicationController wrapped-volume-race-cbae2c56-db41-4254-8c2a-aea707b6be4f pods took: 100.297369ms
  E0428 17:43:43.540267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:44.540679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-1a15f514-9351-4b66-bcb7-b37c793597e7 in namespace emptydir-wrapper-470, will wait for the garbage collector to delete the pods @ 04/28/23 17:43:44.79
  Apr 28 17:43:44.853: INFO: Deleting ReplicationController wrapped-volume-race-1a15f514-9351-4b66-bcb7-b37c793597e7 took: 5.661819ms
  Apr 28 17:43:44.953: INFO: Terminating ReplicationController wrapped-volume-race-1a15f514-9351-4b66-bcb7-b37c793597e7 pods took: 100.791145ms
  E0428 17:43:45.541555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:46.541680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-9d933fcd-33eb-4845-8799-f389e05c0f05 in namespace emptydir-wrapper-470, will wait for the garbage collector to delete the pods @ 04/28/23 17:43:46.654
  Apr 28 17:43:46.720: INFO: Deleting ReplicationController wrapped-volume-race-9d933fcd-33eb-4845-8799-f389e05c0f05 took: 9.503092ms
  Apr 28 17:43:46.821: INFO: Terminating ReplicationController wrapped-volume-race-9d933fcd-33eb-4845-8799-f389e05c0f05 pods took: 100.836267ms
  E0428 17:43:47.541890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 04/28/23 17:43:48.422
  E0428 17:43:48.542044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "emptydir-wrapper-470" for this suite. @ 04/28/23 17:43:48.635
• [21.525 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 04/28/23 17:43:48.642
  Apr 28 17:43:48.642: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename pods @ 04/28/23 17:43:48.643
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:43:48.661
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:43:48.664
  STEP: creating the pod @ 04/28/23 17:43:48.667
  STEP: submitting the pod to kubernetes @ 04/28/23 17:43:48.667
  E0428 17:43:49.542390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:50.542535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 04/28/23 17:43:50.69
  STEP: updating the pod @ 04/28/23 17:43:50.701
  Apr 28 17:43:51.210: INFO: Successfully updated pod "pod-update-5b67b540-a432-4a31-886e-aecb799371c0"
  STEP: verifying the updated pod is in kubernetes @ 04/28/23 17:43:51.215
  Apr 28 17:43:51.218: INFO: Pod update OK
  Apr 28 17:43:51.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8052" for this suite. @ 04/28/23 17:43:51.225
• [2.592 seconds]
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 04/28/23 17:43:51.235
  Apr 28 17:43:51.235: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/28/23 17:43:51.236
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:43:51.25
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:43:51.254
  STEP: create the container to handle the HTTPGet hook request. @ 04/28/23 17:43:51.261
  E0428 17:43:51.543286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:52.543416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 04/28/23 17:43:53.298
  E0428 17:43:53.544391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:54.544730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 04/28/23 17:43:55.311
  E0428 17:43:55.545544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:56.545873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 04/28/23 17:43:57.323
  Apr 28 17:43:57.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-3206" for this suite. @ 04/28/23 17:43:57.331
• [6.104 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 04/28/23 17:43:57.339
  Apr 28 17:43:57.339: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename services @ 04/28/23 17:43:57.34
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:43:57.365
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:43:57.37
  STEP: fetching services @ 04/28/23 17:43:57.373
  Apr 28 17:43:57.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8007" for this suite. @ 04/28/23 17:43:57.381
• [0.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 04/28/23 17:43:57.392
  Apr 28 17:43:57.392: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename deployment @ 04/28/23 17:43:57.393
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:43:57.424
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:43:57.427
  Apr 28 17:43:57.429: INFO: Creating simple deployment test-new-deployment
  Apr 28 17:43:57.443: INFO: deployment "test-new-deployment" doesn't have the required revision set
  E0428 17:43:57.546125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:58.546215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 04/28/23 17:43:59.462
  STEP: updating a scale subresource @ 04/28/23 17:43:59.466
  STEP: verifying the deployment Spec.Replicas was modified @ 04/28/23 17:43:59.473
  STEP: Patch a scale subresource @ 04/28/23 17:43:59.48
  Apr 28 17:43:59.521: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-7682  51583c5b-d07a-4e28-aa79-758c31e61f32 273651 3 2023-04-28 17:43:57 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-04-28 17:43:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2023-04-28 17:43:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004982ce8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-04-28 17:43:59 +0000 UTC,LastTransitionTime:2023-04-28 17:43:57 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-28 17:43:59 +0000 UTC,LastTransitionTime:2023-04-28 17:43:59 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 28 17:43:59.524: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-7682  d51e0e33-1f1c-401a-88fd-1045e0d15886 273654 3 2023-04-28 17:43:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 51583c5b-d07a-4e28-aa79-758c31e61f32 0xc00505c797 0xc00505c798}] [] [{k3s Update apps/v1 2023-04-28 17:43:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"51583c5b-d07a-4e28-aa79-758c31e61f32\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2023-04-28 17:43:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00505c838 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 17:43:59.532: INFO: Pod "test-new-deployment-67bd4bf6dc-tgn6r" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-tgn6r test-new-deployment-67bd4bf6dc- deployment-7682  743a02f3-d337-4ad6-bde9-5b935d663b74 273638 0 2023-04-28 17:43:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc d51e0e33-1f1c-401a-88fd-1045e0d15886 0xc00505cc07 0xc00505cc08}] [] [{k3s Update v1 2023-04-28 17:43:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d51e0e33-1f1c-401a-88fd-1045e0d15886\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 17:43:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4rpzq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4rpzq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-10-53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:43:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:43:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:43:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:43:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.10.53,PodIP:10.42.1.117,StartTime:2023-04-28 17:43:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:43:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c3ab9773adcdbf0e2d33f3cd0aa7ca783e6d29562cc3a6cdd3a3664434f63f38,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.117,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:43:59.532: INFO: Pod "test-new-deployment-67bd4bf6dc-xftmd" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-xftmd test-new-deployment-67bd4bf6dc- deployment-7682  324ccb08-9e89-476a-95cb-2958d14394f3 273650 0 2023-04-28 17:43:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc d51e0e33-1f1c-401a-88fd-1045e0d15886 0xc00505ce77 0xc00505ce78}] [] [{k3s Update v1 2023-04-28 17:43:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d51e0e33-1f1c-401a-88fd-1045e0d15886\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8mvmk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8mvmk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-5-174,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:43:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:43:59.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7682" for this suite. @ 04/28/23 17:43:59.544
  E0428 17:43:59.550437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
• [2.175 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 04/28/23 17:43:59.568
  Apr 28 17:43:59.568: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 17:43:59.569
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:43:59.602
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:43:59.605
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 17:43:59.609
  E0428 17:44:00.550576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:01.550828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:02.551306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:03.551624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:44:03.631
  Apr 28 17:44:03.636: INFO: Trying to get logs from node ip-172-31-10-53 pod downwardapi-volume-822e7a41-5fd9-433a-ad58-cf1a47d7f2a7 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 17:44:03.646
  Apr 28 17:44:03.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5331" for this suite. @ 04/28/23 17:44:03.671
• [4.134 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 04/28/23 17:44:03.706
  Apr 28 17:44:03.707: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 17:44:03.707
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:44:03.744
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:44:03.75
  STEP: Creating a ResourceQuota @ 04/28/23 17:44:03.755
  STEP: Getting a ResourceQuota @ 04/28/23 17:44:03.764
  STEP: Updating a ResourceQuota @ 04/28/23 17:44:03.788
  STEP: Verifying a ResourceQuota was modified @ 04/28/23 17:44:03.793
  STEP: Deleting a ResourceQuota @ 04/28/23 17:44:03.799
  STEP: Verifying the deleted ResourceQuota @ 04/28/23 17:44:03.805
  Apr 28 17:44:03.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6887" for this suite. @ 04/28/23 17:44:03.81
• [0.110 seconds]
------------------------------
SSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 04/28/23 17:44:03.817
  Apr 28 17:44:03.817: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename dns @ 04/28/23 17:44:03.818
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:44:03.835
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:44:03.838
  STEP: Creating a test headless service @ 04/28/23 17:44:03.841
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6593.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6593.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 04/28/23 17:44:03.848
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6593.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6593.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 04/28/23 17:44:03.848
  STEP: creating a pod to probe DNS @ 04/28/23 17:44:03.848
  STEP: submitting the pod to kubernetes @ 04/28/23 17:44:03.848
  E0428 17:44:04.551506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:05.551759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/28/23 17:44:05.89
  STEP: looking for the results for each expected name from probers @ 04/28/23 17:44:05.893
  Apr 28 17:44:05.905: INFO: DNS probes using dns-6593/dns-test-7b94e715-1b89-44c9-9ed3-80428994707e succeeded

  Apr 28 17:44:05.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 17:44:05.907
  STEP: deleting the test headless service @ 04/28/23 17:44:05.927
  STEP: Destroying namespace "dns-6593" for this suite. @ 04/28/23 17:44:05.955
• [2.153 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 04/28/23 17:44:05.971
  Apr 28 17:44:05.971: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename crd-watch @ 04/28/23 17:44:05.972
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:44:05.991
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:44:05.995
  Apr 28 17:44:05.998: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 17:44:06.552628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:07.552826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 04/28/23 17:44:08.533
  Apr 28 17:44:08.539: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-28T17:44:08Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-28T17:44:08Z]] name:name1 resourceVersion:273823 uid:dcfc8004-1577-46b1-9750-b1343f5992d6] num:map[num1:9223372036854775807 num2:1000000]]}
  E0428 17:44:08.553235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:09.553438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:10.553700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:11.553932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:12.554064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:13.554864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:14.554995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:15.555122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:16.555373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:17.555609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 04/28/23 17:44:18.54
  Apr 28 17:44:18.546: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-28T17:44:18Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-28T17:44:18Z]] name:name2 resourceVersion:273895 uid:aa7cf0b2-5114-431c-b7d1-a21102d71409] num:map[num1:9223372036854775807 num2:1000000]]}
  E0428 17:44:18.555691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:19.555843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:20.556382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:21.556625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:22.556853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:23.557602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:24.557835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:25.558072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:26.558332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:27.558692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 04/28/23 17:44:28.547
  Apr 28 17:44:28.553: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-28T17:44:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-28T17:44:28Z]] name:name1 resourceVersion:273941 uid:dcfc8004-1577-46b1-9750-b1343f5992d6] num:map[num1:9223372036854775807 num2:1000000]]}
  E0428 17:44:28.559590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:29.559758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:30.560392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:31.560523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:32.560741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:33.561610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:34.561758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:35.564493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:36.564652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:37.564860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 04/28/23 17:44:38.554
  Apr 28 17:44:38.561: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-28T17:44:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-28T17:44:38Z]] name:name2 resourceVersion:273985 uid:aa7cf0b2-5114-431c-b7d1-a21102d71409] num:map[num1:9223372036854775807 num2:1000000]]}
  E0428 17:44:38.565709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:39.565926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:40.566073      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:41.566297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:42.566441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:43.566537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:44.566684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:45.566908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:46.567055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:47.567275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 04/28/23 17:44:48.561
  E0428 17:44:48.567908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:44:48.574: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-28T17:44:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-28T17:44:28Z]] name:name1 resourceVersion:274031 uid:dcfc8004-1577-46b1-9750-b1343f5992d6] num:map[num1:9223372036854775807 num2:1000000]]}
  E0428 17:44:49.568097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:50.568332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:51.568475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:52.568680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:53.569582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:54.569792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:55.569951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:56.570158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:57.570353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:58.571328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 04/28/23 17:44:58.575
  Apr 28 17:44:58.581: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-28T17:44:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-28T17:44:38Z]] name:name2 resourceVersion:274077 uid:aa7cf0b2-5114-431c-b7d1-a21102d71409] num:map[num1:9223372036854775807 num2:1000000]]}
  E0428 17:44:59.572357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:00.572579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:01.572743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:02.572944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:03.573381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:04.573997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:05.573807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:06.574022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:07.574147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:08.574242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:45:09.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-2152" for this suite. @ 04/28/23 17:45:09.104
• [63.140 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 04/28/23 17:45:09.113
  Apr 28 17:45:09.113: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename endpointslice @ 04/28/23 17:45:09.114
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:45:09.15
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:45:09.158
  E0428 17:45:09.574945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:10.575078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:45:11.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-6587" for this suite. @ 04/28/23 17:45:11.261
• [2.153 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 04/28/23 17:45:11.267
  Apr 28 17:45:11.267: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename field-validation @ 04/28/23 17:45:11.268
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:45:11.283
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:45:11.286
  Apr 28 17:45:11.289: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  W0428 17:45:11.290281      19 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc00137a580 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E0428 17:45:11.576159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:12.577047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:13.577461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0428 17:45:14.048333      19 warnings.go:70] unknown field "alpha"
  W0428 17:45:14.048357      19 warnings.go:70] unknown field "beta"
  W0428 17:45:14.048363      19 warnings.go:70] unknown field "delta"
  W0428 17:45:14.048369      19 warnings.go:70] unknown field "epsilon"
  W0428 17:45:14.048375      19 warnings.go:70] unknown field "gamma"
  Apr 28 17:45:14.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-3943" for this suite. @ 04/28/23 17:45:14.084
• [2.826 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 04/28/23 17:45:14.095
  Apr 28 17:45:14.095: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename secrets @ 04/28/23 17:45:14.095
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:45:14.116
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:45:14.128
  STEP: Creating secret with name secret-test-map-8bc63137-c139-47bf-909e-ddad1fd1acc1 @ 04/28/23 17:45:14.144
  STEP: Creating a pod to test consume secrets @ 04/28/23 17:45:14.158
  E0428 17:45:14.578117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:15.578223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:16.578349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:17.578442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:45:18.209
  Apr 28 17:45:18.211: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-secrets-3cb4feed-f5b7-4175-87e0-d8d417d5a60c container secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 17:45:18.219
  Apr 28 17:45:18.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7911" for this suite. @ 04/28/23 17:45:18.253
• [4.168 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 04/28/23 17:45:18.265
  Apr 28 17:45:18.265: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 17:45:18.266
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:45:18.303
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:45:18.312
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 04/28/23 17:45:18.318
  E0428 17:45:18.579037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:19.579386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:20.580090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:21.580178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:45:22.359
  Apr 28 17:45:22.361: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-8c2e2a52-0903-4362-8abf-92ddf0791b5b container test-container: <nil>
  STEP: delete the pod @ 04/28/23 17:45:22.366
  Apr 28 17:45:22.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-777" for this suite. @ 04/28/23 17:45:22.383
• [4.126 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 04/28/23 17:45:22.391
  Apr 28 17:45:22.392: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:45:22.393
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:45:22.457
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:45:22.46
  STEP: Creating configMap with name projected-configmap-test-volume-9c1ca518-6f44-4e48-a3ed-aa0aa36a145c @ 04/28/23 17:45:22.464
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:45:22.471
  E0428 17:45:22.580760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:23.581370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:24.582178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:25.582306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:45:26.492
  Apr 28 17:45:26.495: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-projected-configmaps-85c4b2cf-8ac6-4ba0-beec-f9ae2fc46443 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 17:45:26.5
  Apr 28 17:45:26.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1371" for this suite. @ 04/28/23 17:45:26.517
• [4.132 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 04/28/23 17:45:26.525
  Apr 28 17:45:26.525: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 17:45:26.532
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:45:26.546
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:45:26.555
  STEP: Creating a ResourceQuota with best effort scope @ 04/28/23 17:45:26.56
  STEP: Ensuring ResourceQuota status is calculated @ 04/28/23 17:45:26.565
  E0428 17:45:26.583228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:27.583512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not best effort scope @ 04/28/23 17:45:28.569
  STEP: Ensuring ResourceQuota status is calculated @ 04/28/23 17:45:28.573
  E0428 17:45:28.584029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:29.584142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a best-effort pod @ 04/28/23 17:45:30.577
  E0428 17:45:30.584695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 04/28/23 17:45:30.588
  E0428 17:45:31.584947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:32.585000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 04/28/23 17:45:32.592
  E0428 17:45:33.585541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:34.585770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 04/28/23 17:45:34.595
  STEP: Ensuring resource quota status released the pod usage @ 04/28/23 17:45:34.608
  E0428 17:45:35.585828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:36.586048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a not best-effort pod @ 04/28/23 17:45:36.611
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 04/28/23 17:45:36.62
  E0428 17:45:37.586222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:38.586328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 04/28/23 17:45:38.624
  E0428 17:45:39.587086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:40.587416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 04/28/23 17:45:40.627
  STEP: Ensuring resource quota status released the pod usage @ 04/28/23 17:45:40.638
  E0428 17:45:41.587546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:42.587758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:45:42.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5437" for this suite. @ 04/28/23 17:45:42.644
• [16.130 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 04/28/23 17:45:42.656
  Apr 28 17:45:42.656: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename configmap @ 04/28/23 17:45:42.657
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:45:42.674
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:45:42.677
  STEP: Creating configMap with name configmap-test-upd-8eaa9e4f-44d1-43bd-b943-14ef325db97e @ 04/28/23 17:45:42.684
  STEP: Creating the pod @ 04/28/23 17:45:42.69
  E0428 17:45:43.588561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:44.588799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod with text data @ 04/28/23 17:45:44.718
  STEP: Waiting for pod with binary data @ 04/28/23 17:45:44.722
  Apr 28 17:45:44.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9796" for this suite. @ 04/28/23 17:45:44.73
• [2.079 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 04/28/23 17:45:44.735
  Apr 28 17:45:44.735: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename webhook @ 04/28/23 17:45:44.736
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:45:44.751
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:45:44.759
  STEP: Setting up server cert @ 04/28/23 17:45:44.789
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 17:45:45.443
  STEP: Deploying the webhook pod @ 04/28/23 17:45:45.454
  STEP: Wait for the deployment to be ready @ 04/28/23 17:45:45.465
  Apr 28 17:45:45.483: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0428 17:45:45.589472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:46.589626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 17:45:47.506
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 17:45:47.536
  E0428 17:45:47.589752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:45:48.537: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 04/28/23 17:45:48.586
  E0428 17:45:48.589845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a configMap that should be mutated @ 04/28/23 17:45:48.6
  STEP: Deleting the collection of validation webhooks @ 04/28/23 17:45:48.624
  STEP: Creating a configMap that should not be mutated @ 04/28/23 17:45:48.655
  Apr 28 17:45:48.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8770" for this suite. @ 04/28/23 17:45:48.705
  STEP: Destroying namespace "webhook-markers-835" for this suite. @ 04/28/23 17:45:48.71
• [3.982 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 04/28/23 17:45:48.717
  Apr 28 17:45:48.717: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename sched-pred @ 04/28/23 17:45:48.718
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:45:48.735
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:45:48.74
  Apr 28 17:45:48.742: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 28 17:45:48.750: INFO: Waiting for terminating namespaces to be deleted...
  Apr 28 17:45:48.752: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-1-213 before test
  Apr 28 17:45:48.757: INFO: pod-configmaps-e405e5cd-87ac-469b-ae6a-c62be63308fd from configmap-9796 started at 2023-04-28 17:45:42 +0000 UTC (2 container statuses recorded)
  Apr 28 17:45:48.757: INFO: 	Container agnhost-container ready: true, restart count 0
  Apr 28 17:45:48.757: INFO: 	Container configmap-volume-binary-test ready: false, restart count 0
  Apr 28 17:45:48.757: INFO: svclb-traefik-0a77677b-4z972 from kube-system started at 2023-04-28 17:22:42 +0000 UTC (2 container statuses recorded)
  Apr 28 17:45:48.757: INFO: 	Container lb-tcp-443 ready: true, restart count 0
  Apr 28 17:45:48.757: INFO: 	Container lb-tcp-80 ready: true, restart count 0
  Apr 28 17:45:48.757: INFO: sonobuoy-systemd-logs-daemon-set-d17888d62c134fe7-v6wcc from sonobuoy started at 2023-04-28 16:35:07 +0000 UTC (2 container statuses recorded)
  Apr 28 17:45:48.757: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 17:45:48.757: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 17:45:48.757: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-10-53 before test
  Apr 28 17:45:48.771: INFO: svclb-traefik-0a77677b-d6qnt from kube-system started at 2023-04-28 03:31:43 +0000 UTC (2 container statuses recorded)
  Apr 28 17:45:48.771: INFO: 	Container lb-tcp-443 ready: true, restart count 0
  Apr 28 17:45:48.771: INFO: 	Container lb-tcp-80 ready: true, restart count 0
  Apr 28 17:45:48.771: INFO: sonobuoy from sonobuoy started at 2023-04-28 16:35:06 +0000 UTC (1 container statuses recorded)
  Apr 28 17:45:48.771: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 28 17:45:48.771: INFO: sonobuoy-systemd-logs-daemon-set-d17888d62c134fe7-clf8z from sonobuoy started at 2023-04-28 16:35:07 +0000 UTC (2 container statuses recorded)
  Apr 28 17:45:48.771: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 17:45:48.771: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 17:45:48.771: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-12-59 before test
  Apr 28 17:45:48.777: INFO: coredns-77ccd57875-lg57c from kube-system started at 2023-04-28 03:29:34 +0000 UTC (1 container statuses recorded)
  Apr 28 17:45:48.777: INFO: 	Container coredns ready: true, restart count 0
  Apr 28 17:45:48.778: INFO: helm-install-traefik-crd-rxpjj from kube-system started at 2023-04-28 03:29:34 +0000 UTC (1 container statuses recorded)
  Apr 28 17:45:48.778: INFO: 	Container helm ready: false, restart count 0
  Apr 28 17:45:48.778: INFO: helm-install-traefik-q2g86 from kube-system started at 2023-04-28 03:29:34 +0000 UTC (1 container statuses recorded)
  Apr 28 17:45:48.778: INFO: 	Container helm ready: false, restart count 1
  Apr 28 17:45:48.778: INFO: local-path-provisioner-957fdf8bc-jc5qz from kube-system started at 2023-04-28 03:29:34 +0000 UTC (1 container statuses recorded)
  Apr 28 17:45:48.778: INFO: 	Container local-path-provisioner ready: true, restart count 0
  Apr 28 17:45:48.778: INFO: metrics-server-54dc485875-gbbbl from kube-system started at 2023-04-28 03:29:34 +0000 UTC (1 container statuses recorded)
  Apr 28 17:45:48.779: INFO: 	Container metrics-server ready: true, restart count 0
  Apr 28 17:45:48.779: INFO: svclb-traefik-0a77677b-jtrmg from kube-system started at 2023-04-28 03:29:54 +0000 UTC (2 container statuses recorded)
  Apr 28 17:45:48.779: INFO: 	Container lb-tcp-443 ready: true, restart count 0
  Apr 28 17:45:48.779: INFO: 	Container lb-tcp-80 ready: true, restart count 0
  Apr 28 17:45:48.779: INFO: traefik-84745cf649-mwthv from kube-system started at 2023-04-28 03:29:54 +0000 UTC (1 container statuses recorded)
  Apr 28 17:45:48.779: INFO: 	Container traefik ready: true, restart count 0
  Apr 28 17:45:48.779: INFO: sonobuoy-systemd-logs-daemon-set-d17888d62c134fe7-4jl87 from sonobuoy started at 2023-04-28 16:35:07 +0000 UTC (2 container statuses recorded)
  Apr 28 17:45:48.779: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 17:45:48.779: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 17:45:48.779: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-5-174 before test
  Apr 28 17:45:48.785: INFO: svclb-traefik-0a77677b-x527f from kube-system started at 2023-04-28 03:33:13 +0000 UTC (2 container statuses recorded)
  Apr 28 17:45:48.785: INFO: 	Container lb-tcp-443 ready: true, restart count 0
  Apr 28 17:45:48.785: INFO: 	Container lb-tcp-80 ready: true, restart count 0
  Apr 28 17:45:48.785: INFO: sonobuoy-e2e-job-4c96b6c95eb94b68 from sonobuoy started at 2023-04-28 16:35:07 +0000 UTC (2 container statuses recorded)
  Apr 28 17:45:48.785: INFO: 	Container e2e ready: true, restart count 0
  Apr 28 17:45:48.785: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 17:45:48.785: INFO: sonobuoy-systemd-logs-daemon-set-d17888d62c134fe7-wkkfr from sonobuoy started at 2023-04-28 16:35:07 +0000 UTC (2 container statuses recorded)
  Apr 28 17:45:48.785: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 17:45:48.785: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/28/23 17:45:48.785
  E0428 17:45:49.589913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:50.590143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/28/23 17:45:50.808
  STEP: Trying to apply a random label on the found node. @ 04/28/23 17:45:50.822
  STEP: verifying the node has the label kubernetes.io/e2e-6f96cd83-041f-4243-aba3-64d7b261d9b2 95 @ 04/28/23 17:45:50.836
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 04/28/23 17:45:50.843
  E0428 17:45:51.590217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:52.590343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.10.53 on the node which pod4 resides and expect not scheduled @ 04/28/23 17:45:52.863
  E0428 17:45:53.590434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:54.590891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:55.591029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:56.591286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:57.591328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:58.591408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:59.592358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:00.592568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:01.593375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:02.593566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:03.594521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:04.594783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:05.594893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:06.595116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:07.595253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:08.595814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:09.595924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:10.596083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:11.596211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:12.596356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:13.596486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:14.596721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:15.596771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:16.596895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:17.597020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:18.597446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:19.597523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:20.597623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:21.597729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:22.597830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:23.598261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:24.598350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:25.599367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:26.599470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:27.600427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:28.600922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:29.601126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:30.601253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:31.601350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:32.601459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:33.601549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:34.601997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:35.602285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:36.602532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:37.602601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:38.603044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:39.604105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:40.604289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:41.604345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:42.604453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:43.604563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:44.604639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:45.605485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:46.606001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:47.606823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:48.607244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:49.607394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:50.607484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:51.608348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:52.609381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:53.610079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:54.610362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:55.610418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:56.610597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:57.610694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:58.611476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:59.612398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:00.612519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:01.613607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:02.613729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:03.614153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:04.615272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:05.616364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:06.616612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:07.617729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:08.618349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:09.618473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:10.619104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:11.620165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:12.620317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:13.620436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:14.620679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:15.620795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:16.621005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:17.622060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:18.622271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:19.622418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:20.622542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:21.622669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:22.622905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:23.623368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:24.623485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:25.624180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:26.624349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:27.624457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:28.624832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:29.625625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:30.625865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:31.626483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:32.626623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:33.627310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:34.627444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:35.627956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:36.628093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:37.628213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:38.628689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:39.629701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:40.629828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:41.629904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:42.630041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:43.630986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:44.631098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:45.632247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:46.632403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:47.632835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:48.632997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:49.634001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:50.634261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:51.634371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:52.634579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:53.634644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:54.635075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:55.635871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:56.636357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:57.637381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:58.637927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:59.638207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:00.638393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:01.638499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:02.638700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:03.639608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:04.639818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:05.640206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:06.640370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:07.641142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:08.641766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:09.642803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:10.643014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:11.644033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:12.644247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:13.644927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:14.645149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:15.645898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:16.646031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:17.646125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:18.646871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:19.647347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:20.647589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:21.648360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:22.648800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:23.649474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:24.649676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:25.650772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:26.650881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:27.651911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:28.652887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:29.653623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:30.653856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:31.654454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:32.655278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:33.656338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:34.656453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:35.656608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:36.656825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:37.657577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:38.658232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:39.659256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:40.659354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:41.660331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:42.660496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:43.660548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:44.661297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:45.662117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:46.662160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:47.663115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:48.663339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:49.663969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:50.664383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:51.664912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:52.665101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:53.665568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:54.665948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:55.666752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:56.666857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:57.667063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:58.667459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:59.667878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:00.668338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:01.669019      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:02.669222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:03.669362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:04.669974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:05.669879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:06.670096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:07.671168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:08.671323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:09.672280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:10.672322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:11.672469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:12.672688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:13.673200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:14.673345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:15.673726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:16.673919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:17.674626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:18.674770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:19.674873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:20.675055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:21.675214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:22.675653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:23.676075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:24.676269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:25.676971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:26.677058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:27.678040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:28.678508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:29.679362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:30.679971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:31.680931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:32.681126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:33.681960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:34.682096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:35.682737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:36.682925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:37.683789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:38.684347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:39.684489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:40.684678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:41.685451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:42.685583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:43.686611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:44.686835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:45.687428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:46.688006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:47.688094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:48.688602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:49.689570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:50.689703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:51.690193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:52.690325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:53.691339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:54.691689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:55.692813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:56.692853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:57.693915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:58.694540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:59.695448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:00.695592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:01.696085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:02.696202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:03.697169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:04.697336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:05.697485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:06.697610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:07.698276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:08.699288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:09.700134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:10.700249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:11.700360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:12.700621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:13.700742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:14.701084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:15.701127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:16.701331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:17.702071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:18.702343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:19.703265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:20.703384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:21.703930      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:22.704210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:23.704828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:24.705029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:25.705623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:26.705758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:27.706027      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:28.706470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:29.707028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:30.707168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:31.707794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:32.708330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:33.708439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:34.708567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:35.709407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:36.709662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:37.710478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:38.710982      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:39.711568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:40.711919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:41.712043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:42.712760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:43.713845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:44.713950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:45.714947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:46.715757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:47.716742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:48.717508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:49.717957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:50.718181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:51.718316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:52.718589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-6f96cd83-041f-4243-aba3-64d7b261d9b2 off the node ip-172-31-10-53 @ 04/28/23 17:50:52.869
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-6f96cd83-041f-4243-aba3-64d7b261d9b2 @ 04/28/23 17:50:52.886
  Apr 28 17:50:52.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-5257" for this suite. @ 04/28/23 17:50:52.901
• [304.199 seconds]
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 04/28/23 17:50:52.917
  Apr 28 17:50:52.917: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename statefulset @ 04/28/23 17:50:52.918
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:50:52.943
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:50:52.947
  STEP: Creating service test in namespace statefulset-3932 @ 04/28/23 17:50:52.95
  Apr 28 17:50:52.982: INFO: Found 0 stateful pods, waiting for 1
  E0428 17:50:53.718749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:54.718948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:55.719271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:56.719466      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:57.719646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:58.720373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:59.720648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:00.720886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:01.721096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:02.721322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:51:02.986: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 04/28/23 17:51:02.992
  W0428 17:51:03.005146      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 28 17:51:03.010: INFO: Found 1 stateful pods, waiting for 2
  E0428 17:51:03.721712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:04.721960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:05.722049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:06.722190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:07.722305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:08.722700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:09.722930      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:10.723042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:11.723281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:12.724340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:51:13.016: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 17:51:13.016: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 04/28/23 17:51:13.02
  STEP: Delete all of the StatefulSets @ 04/28/23 17:51:13.023
  STEP: Verify that StatefulSets have been deleted @ 04/28/23 17:51:13.029
  Apr 28 17:51:13.032: INFO: Deleting all statefulset in ns statefulset-3932
  Apr 28 17:51:13.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3932" for this suite. @ 04/28/23 17:51:13.046
• [20.144 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 04/28/23 17:51:13.061
  Apr 28 17:51:13.061: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename daemonsets @ 04/28/23 17:51:13.062
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:51:13.123
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:51:13.132
  Apr 28 17:51:13.174: INFO: Create a RollingUpdate DaemonSet
  Apr 28 17:51:13.180: INFO: Check that daemon pods launch on every node of the cluster
  Apr 28 17:51:13.191: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 17:51:13.191: INFO: Node ip-172-31-1-213 is running 0 daemon pod, expected 1
  E0428 17:51:13.725243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:51:14.197: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 28 17:51:14.197: INFO: Node ip-172-31-1-213 is running 0 daemon pod, expected 1
  E0428 17:51:14.725643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:51:15.197: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
  Apr 28 17:51:15.197: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  Apr 28 17:51:15.197: INFO: Update the DaemonSet to trigger a rollout
  Apr 28 17:51:15.203: INFO: Updating DaemonSet daemon-set
  E0428 17:51:15.726124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:51:16.222: INFO: Roll back the DaemonSet before rollout is complete
  Apr 28 17:51:16.230: INFO: Updating DaemonSet daemon-set
  Apr 28 17:51:16.230: INFO: Make sure DaemonSet rollback is complete
  Apr 28 17:51:16.247: INFO: Wrong image for pod: daemon-set-4ghjj. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Apr 28 17:51:16.247: INFO: Pod daemon-set-4ghjj is not available
  E0428 17:51:16.726219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:17.726361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:18.726892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:51:19.257: INFO: Pod daemon-set-8wpss is not available
  STEP: Deleting DaemonSet "daemon-set" @ 04/28/23 17:51:19.266
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7291, will wait for the garbage collector to delete the pods @ 04/28/23 17:51:19.266
  Apr 28 17:51:19.325: INFO: Deleting DaemonSet.extensions daemon-set took: 4.799734ms
  Apr 28 17:51:19.426: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.865094ms
  E0428 17:51:19.727587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:20.727869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:21.728743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:51:21.929: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 17:51:21.929: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 28 17:51:21.932: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"276326"},"items":null}

  Apr 28 17:51:21.934: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"276326"},"items":null}

  Apr 28 17:51:21.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7291" for this suite. @ 04/28/23 17:51:21.949
• [8.897 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 04/28/23 17:51:21.979
  Apr 28 17:51:21.979: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename var-expansion @ 04/28/23 17:51:21.98
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:51:21.996
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:51:22.005
  STEP: Creating a pod to test substitution in volume subpath @ 04/28/23 17:51:22.013
  E0428 17:51:22.728898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:23.729243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:24.729384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:25.729513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:51:26.038
  Apr 28 17:51:26.040: INFO: Trying to get logs from node ip-172-31-1-213 pod var-expansion-06992162-cd63-4fe2-879b-f351dd51e989 container dapi-container: <nil>
  STEP: delete the pod @ 04/28/23 17:51:26.057
  Apr 28 17:51:26.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3571" for this suite. @ 04/28/23 17:51:26.086
• [4.117 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 04/28/23 17:51:26.096
  Apr 28 17:51:26.096: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/28/23 17:51:26.097
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:51:26.129
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:51:26.137
  Apr 28 17:51:26.141: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 17:51:26.730377      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:27.730491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:28.731336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:51:29.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-7043" for this suite. @ 04/28/23 17:51:29.218
• [3.127 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 04/28/23 17:51:29.225
  Apr 28 17:51:29.225: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/28/23 17:51:29.226
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:51:29.249
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:51:29.253
  Apr 28 17:51:29.258: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 17:51:29.732420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:30.733201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/28/23 17:51:31.072
  Apr 28 17:51:31.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-1570 --namespace=crd-publish-openapi-1570 create -f -'
  E0428 17:51:31.733577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:51:31.985: INFO: stderr: ""
  Apr 28 17:51:31.985: INFO: stdout: "e2e-test-crd-publish-openapi-5858-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Apr 28 17:51:31.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-1570 --namespace=crd-publish-openapi-1570 delete e2e-test-crd-publish-openapi-5858-crds test-cr'
  Apr 28 17:51:32.064: INFO: stderr: ""
  Apr 28 17:51:32.064: INFO: stdout: "e2e-test-crd-publish-openapi-5858-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Apr 28 17:51:32.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-1570 --namespace=crd-publish-openapi-1570 apply -f -'
  Apr 28 17:51:32.377: INFO: stderr: ""
  Apr 28 17:51:32.377: INFO: stdout: "e2e-test-crd-publish-openapi-5858-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Apr 28 17:51:32.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-1570 --namespace=crd-publish-openapi-1570 delete e2e-test-crd-publish-openapi-5858-crds test-cr'
  Apr 28 17:51:32.454: INFO: stderr: ""
  Apr 28 17:51:32.454: INFO: stdout: "e2e-test-crd-publish-openapi-5858-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 04/28/23 17:51:32.454
  Apr 28 17:51:32.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-1570 explain e2e-test-crd-publish-openapi-5858-crds'
  Apr 28 17:51:32.690: INFO: stderr: ""
  Apr 28 17:51:32.690: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-5858-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0428 17:51:32.733956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:33.734911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:51:34.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1570" for this suite. @ 04/28/23 17:51:34.369
• [5.150 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 04/28/23 17:51:34.379
  Apr 28 17:51:34.379: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename webhook @ 04/28/23 17:51:34.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:51:34.401
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:51:34.405
  STEP: Setting up server cert @ 04/28/23 17:51:34.443
  E0428 17:51:34.735962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 17:51:35.018
  STEP: Deploying the webhook pod @ 04/28/23 17:51:35.025
  STEP: Wait for the deployment to be ready @ 04/28/23 17:51:35.036
  Apr 28 17:51:35.041: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0428 17:51:35.736351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:36.736453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 17:51:37.049
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 17:51:37.066
  E0428 17:51:37.736585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:51:38.067: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 04/28/23 17:51:38.07
  STEP: create a configmap that should be updated by the webhook @ 04/28/23 17:51:38.087
  Apr 28 17:51:38.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5531" for this suite. @ 04/28/23 17:51:38.168
  STEP: Destroying namespace "webhook-markers-5647" for this suite. @ 04/28/23 17:51:38.174
• [3.803 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 04/28/23 17:51:38.186
  Apr 28 17:51:38.186: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 17:51:38.188
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:51:38.207
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:51:38.211
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 04/28/23 17:51:38.222
  E0428 17:51:38.737629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:39.737787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:40.738625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:41.738757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:51:42.252
  Apr 28 17:51:42.255: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-0b0c0545-f133-4793-af82-2236a4324af0 container test-container: <nil>
  STEP: delete the pod @ 04/28/23 17:51:42.259
  Apr 28 17:51:42.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8535" for this suite. @ 04/28/23 17:51:42.275
• [4.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 04/28/23 17:51:42.283
  Apr 28 17:51:42.283: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename field-validation @ 04/28/23 17:51:42.283
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:51:42.299
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:51:42.302
  Apr 28 17:51:42.305: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 17:51:42.739817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:43.740620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:44.740733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0428 17:51:44.836939      19 warnings.go:70] unknown field "alpha"
  W0428 17:51:44.836963      19 warnings.go:70] unknown field "beta"
  W0428 17:51:44.836970      19 warnings.go:70] unknown field "delta"
  W0428 17:51:44.836976      19 warnings.go:70] unknown field "epsilon"
  W0428 17:51:44.837008      19 warnings.go:70] unknown field "gamma"
  Apr 28 17:51:44.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1781" for this suite. @ 04/28/23 17:51:44.871
• [2.594 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 04/28/23 17:51:44.878
  Apr 28 17:51:44.878: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 17:51:44.879
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:51:44.914
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:51:44.933
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 04/28/23 17:51:44.941
  E0428 17:51:45.740839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:46.741058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:47.741169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:48.741470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:51:48.981
  Apr 28 17:51:48.983: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-3fff21fb-d028-41bb-901d-5e1284ce1102 container test-container: <nil>
  STEP: delete the pod @ 04/28/23 17:51:48.99
  Apr 28 17:51:49.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-125" for this suite. @ 04/28/23 17:51:49.023
• [4.155 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 04/28/23 17:51:49.035
  Apr 28 17:51:49.035: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename dns @ 04/28/23 17:51:49.035
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:51:49.092
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:51:49.112
  STEP: Creating a test headless service @ 04/28/23 17:51:49.122
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2613.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2613.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2613.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2613.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2613.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2613.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2613.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2613.svc.cluster.local;sleep 1; done
   @ 04/28/23 17:51:49.143
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2613.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2613.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2613.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2613.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2613.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2613.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2613.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2613.svc.cluster.local;sleep 1; done
   @ 04/28/23 17:51:49.143
  STEP: creating a pod to probe DNS @ 04/28/23 17:51:49.143
  STEP: submitting the pod to kubernetes @ 04/28/23 17:51:49.143
  E0428 17:51:49.741545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:50.741933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/28/23 17:51:51.177
  STEP: looking for the results for each expected name from probers @ 04/28/23 17:51:51.179
  Apr 28 17:51:51.183: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2613.svc.cluster.local from pod dns-2613/dns-test-6ba4c9d9-5d1e-49ad-9079-157bcccb940d: the server could not find the requested resource (get pods dns-test-6ba4c9d9-5d1e-49ad-9079-157bcccb940d)
  Apr 28 17:51:51.186: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2613.svc.cluster.local from pod dns-2613/dns-test-6ba4c9d9-5d1e-49ad-9079-157bcccb940d: the server could not find the requested resource (get pods dns-test-6ba4c9d9-5d1e-49ad-9079-157bcccb940d)
  Apr 28 17:51:51.189: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2613.svc.cluster.local from pod dns-2613/dns-test-6ba4c9d9-5d1e-49ad-9079-157bcccb940d: the server could not find the requested resource (get pods dns-test-6ba4c9d9-5d1e-49ad-9079-157bcccb940d)
  Apr 28 17:51:51.192: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2613.svc.cluster.local from pod dns-2613/dns-test-6ba4c9d9-5d1e-49ad-9079-157bcccb940d: the server could not find the requested resource (get pods dns-test-6ba4c9d9-5d1e-49ad-9079-157bcccb940d)
  Apr 28 17:51:51.194: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2613.svc.cluster.local from pod dns-2613/dns-test-6ba4c9d9-5d1e-49ad-9079-157bcccb940d: the server could not find the requested resource (get pods dns-test-6ba4c9d9-5d1e-49ad-9079-157bcccb940d)
  Apr 28 17:51:51.196: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2613.svc.cluster.local from pod dns-2613/dns-test-6ba4c9d9-5d1e-49ad-9079-157bcccb940d: the server could not find the requested resource (get pods dns-test-6ba4c9d9-5d1e-49ad-9079-157bcccb940d)
  Apr 28 17:51:51.198: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2613.svc.cluster.local from pod dns-2613/dns-test-6ba4c9d9-5d1e-49ad-9079-157bcccb940d: the server could not find the requested resource (get pods dns-test-6ba4c9d9-5d1e-49ad-9079-157bcccb940d)
  Apr 28 17:51:51.201: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2613.svc.cluster.local from pod dns-2613/dns-test-6ba4c9d9-5d1e-49ad-9079-157bcccb940d: the server could not find the requested resource (get pods dns-test-6ba4c9d9-5d1e-49ad-9079-157bcccb940d)
  Apr 28 17:51:51.201: INFO: Lookups using dns-2613/dns-test-6ba4c9d9-5d1e-49ad-9079-157bcccb940d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2613.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2613.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2613.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2613.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2613.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2613.svc.cluster.local jessie_udp@dns-test-service-2.dns-2613.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2613.svc.cluster.local]

  E0428 17:51:51.742855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:52.742983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:53.743656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:54.743669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:55.743896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:51:56.227: INFO: DNS probes using dns-2613/dns-test-6ba4c9d9-5d1e-49ad-9079-157bcccb940d succeeded

  Apr 28 17:51:56.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 17:51:56.232
  STEP: deleting the test headless service @ 04/28/23 17:51:56.25
  STEP: Destroying namespace "dns-2613" for this suite. @ 04/28/23 17:51:56.289
• [7.266 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 04/28/23 17:51:56.302
  Apr 28 17:51:56.302: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename container-probe @ 04/28/23 17:51:56.303
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:51:56.319
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:51:56.322
  STEP: Creating pod test-grpc-f9c793f9-49c5-4351-a84a-51711f43eb49 in namespace container-probe-1166 @ 04/28/23 17:51:56.325
  E0428 17:51:56.744905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:57.745146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:51:58.345: INFO: Started pod test-grpc-f9c793f9-49c5-4351-a84a-51711f43eb49 in namespace container-probe-1166
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/28/23 17:51:58.345
  Apr 28 17:51:58.347: INFO: Initial restart count of pod test-grpc-f9c793f9-49c5-4351-a84a-51711f43eb49 is 0
  E0428 17:51:58.745400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:59.745550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:00.746550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:01.747783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:02.747910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:03.748199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:04.749241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:05.749484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:06.750106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:07.750262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:08.750410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:09.750637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:10.751225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:11.751310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:12.752300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:13.752595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:14.752727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:15.752865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:16.753414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:17.753567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:18.753684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:19.753820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:20.754441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:21.754613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:22.755190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:23.755684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:24.756448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:25.756686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:26.756700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:27.756883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:28.758195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:29.758332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:30.759024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:31.759181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:32.759274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:33.759620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:34.760363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:35.760630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:36.761238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:37.761392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:38.761522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:39.761645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:40.762461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:41.762673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:42.763420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:43.763704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:44.763742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:45.764387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:46.764837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:47.765026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:48.765374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:49.765594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:50.766354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:51.766520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:52.767388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:53.767693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:54.767832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:55.767949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:56.768078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:57.768237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:58.769262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:59.769603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:00.770561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:01.771318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:02.772098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:03.772488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:04.772601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:05.774192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:06.775280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:07.775464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:08.776627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:09.777515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:10.778173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:11.778276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:12.779174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:13.779674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:14.780589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:15.780695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:16.780822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:17.781065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:18.781527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:19.781734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:20.782745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:21.782936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:22.783618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:23.784366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:24.784514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:25.784649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:26.784778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:27.784904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:28.785838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:29.786131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:30.786970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:31.787192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:32.787319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:33.787615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:34.787876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:35.787887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:36.788934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:37.789282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:38.789835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:39.790021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:40.791015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:41.791253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:42.791352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:43.791554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:44.792332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:45.792608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:46.792729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:47.792933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:48.793062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:49.793253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:50.794061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:51.794264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:52.795035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:53.795274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:54.795313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:55.796357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:56.797335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:57.797472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:58.797534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:59.797640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:00.798554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:01.798822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:02.798909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:03.799301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:04.799451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:05.799670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:06.800143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:07.800526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:08.801200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:09.801470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:10.801681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:11.801922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:12.802077      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:13.802555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:14.802951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:15.803185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:16.804168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:17.804275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:18.804405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:19.804603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:20.805447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:21.805748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:22.806584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:23.806877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:24.807838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:25.808040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:26.808880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:27.809484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:28.810388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:29.811220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:30.812213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:31.812429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:32.812490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:33.812751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:34.812967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:35.813157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:36.813870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:37.814102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:38.814169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:39.814491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:40.815265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:41.815402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:42.816356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:43.816738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:44.817703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:45.817912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:46.818588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:47.818821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:48.818994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:49.819275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:50.819748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:51.820045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:52.820754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:53.820819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:54.821848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:55.822051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:56.822600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:57.822808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:58.822940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:59.823184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:00.824089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:01.824319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:02.824618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:03.824774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:04.825533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:05.825717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:06.826491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:07.826680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:08.827824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:09.828030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:10.829020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:11.829274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:12.829373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:13.829691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:14.830607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:15.830743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:16.831588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:17.831796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:18.832294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:19.832520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:20.833249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:21.833438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:22.834376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:23.834673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:24.835587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:25.835804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:26.836370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:27.836599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:28.838240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:29.838471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:30.839336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:31.839461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:32.840389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:33.840527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:34.840806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:35.841437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:36.841862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:37.841985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:38.842062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:39.842276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:40.843025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:41.843231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:42.844160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:43.844519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:44.845066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:45.845286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:46.845757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:47.845980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:48.846294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:49.846526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:50.846869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:51.847108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:52.847956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:53.848070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:54.848955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:55.849150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:56.849538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:57.849675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:55:58.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 17:55:58.797
  STEP: Destroying namespace "container-probe-1166" for this suite. @ 04/28/23 17:55:58.807
• [242.512 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 04/28/23 17:55:58.816
  Apr 28 17:55:58.816: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 17:55:58.816
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:55:58.845
  E0428 17:55:58.849665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:55:58.851
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 17:55:58.857
  E0428 17:55:59.850471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:00.850857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:01.850999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:02.851255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:56:02.903
  Apr 28 17:56:02.906: INFO: Trying to get logs from node ip-172-31-1-213 pod downwardapi-volume-3a0381bc-e207-4e77-8e4a-04d7268826d8 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 17:56:02.915
  Apr 28 17:56:02.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1841" for this suite. @ 04/28/23 17:56:02.936
• [4.130 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 04/28/23 17:56:02.95
  Apr 28 17:56:02.950: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename replicaset @ 04/28/23 17:56:02.951
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:56:02.977
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:56:02.981
  Apr 28 17:56:03.005: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0428 17:56:03.851759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:04.851887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:05.852051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:06.852279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:07.852596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:08.018: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/28/23 17:56:08.018
  STEP: Scaling up "test-rs" replicaset  @ 04/28/23 17:56:08.018
  Apr 28 17:56:08.058: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 04/28/23 17:56:08.058
  W0428 17:56:08.102308      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 28 17:56:08.124: INFO: observed ReplicaSet test-rs in namespace replicaset-6264 with ReadyReplicas 1, AvailableReplicas 1
  Apr 28 17:56:08.226: INFO: observed ReplicaSet test-rs in namespace replicaset-6264 with ReadyReplicas 1, AvailableReplicas 1
  Apr 28 17:56:08.252: INFO: observed ReplicaSet test-rs in namespace replicaset-6264 with ReadyReplicas 1, AvailableReplicas 1
  Apr 28 17:56:08.260: INFO: observed ReplicaSet test-rs in namespace replicaset-6264 with ReadyReplicas 1, AvailableReplicas 1
  E0428 17:56:08.853489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:09.433: INFO: observed ReplicaSet test-rs in namespace replicaset-6264 with ReadyReplicas 2, AvailableReplicas 2
  Apr 28 17:56:09.702: INFO: observed Replicaset test-rs in namespace replicaset-6264 with ReadyReplicas 3 found true
  Apr 28 17:56:09.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-6264" for this suite. @ 04/28/23 17:56:09.707
• [6.764 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 04/28/23 17:56:09.715
  Apr 28 17:56:09.715: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 17:56:09.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:56:09.74
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:56:09.742
  STEP: creating all guestbook components @ 04/28/23 17:56:09.745
  Apr 28 17:56:09.746: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Apr 28 17:56:09.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-9197 create -f -'
  E0428 17:56:09.853754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:10.814: INFO: stderr: ""
  Apr 28 17:56:10.814: INFO: stdout: "service/agnhost-replica created\n"
  Apr 28 17:56:10.814: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Apr 28 17:56:10.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-9197 create -f -'
  E0428 17:56:10.854350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:11.854457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:12.246: INFO: stderr: ""
  Apr 28 17:56:12.246: INFO: stdout: "service/agnhost-primary created\n"
  Apr 28 17:56:12.246: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Apr 28 17:56:12.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-9197 create -f -'
  Apr 28 17:56:12.669: INFO: stderr: ""
  Apr 28 17:56:12.670: INFO: stdout: "service/frontend created\n"
  Apr 28 17:56:12.670: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Apr 28 17:56:12.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-9197 create -f -'
  E0428 17:56:12.854637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:12.911: INFO: stderr: ""
  Apr 28 17:56:12.911: INFO: stdout: "deployment.apps/frontend created\n"
  Apr 28 17:56:12.911: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Apr 28 17:56:12.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-9197 create -f -'
  Apr 28 17:56:13.192: INFO: stderr: ""
  Apr 28 17:56:13.192: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Apr 28 17:56:13.192: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Apr 28 17:56:13.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-9197 create -f -'
  Apr 28 17:56:13.501: INFO: stderr: ""
  Apr 28 17:56:13.502: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 04/28/23 17:56:13.502
  Apr 28 17:56:13.502: INFO: Waiting for all frontend pods to be Running.
  E0428 17:56:13.854847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:14.854971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:15.855212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:16.855347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:17.855463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:18.554: INFO: Waiting for frontend to serve content.
  Apr 28 17:56:18.564: INFO: Trying to add a new entry to the guestbook.
  Apr 28 17:56:18.575: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 04/28/23 17:56:18.581
  Apr 28 17:56:18.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-9197 delete --grace-period=0 --force -f -'
  Apr 28 17:56:18.700: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 28 17:56:18.700: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 04/28/23 17:56:18.7
  Apr 28 17:56:18.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-9197 delete --grace-period=0 --force -f -'
  Apr 28 17:56:18.814: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 28 17:56:18.814: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 04/28/23 17:56:18.815
  Apr 28 17:56:18.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-9197 delete --grace-period=0 --force -f -'
  E0428 17:56:18.855519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:18.951: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 28 17:56:18.951: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 04/28/23 17:56:18.951
  Apr 28 17:56:18.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-9197 delete --grace-period=0 --force -f -'
  Apr 28 17:56:19.054: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 28 17:56:19.054: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 04/28/23 17:56:19.054
  Apr 28 17:56:19.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-9197 delete --grace-period=0 --force -f -'
  Apr 28 17:56:19.146: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 28 17:56:19.146: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 04/28/23 17:56:19.146
  Apr 28 17:56:19.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-9197 delete --grace-period=0 --force -f -'
  Apr 28 17:56:19.250: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 28 17:56:19.250: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Apr 28 17:56:19.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9197" for this suite. @ 04/28/23 17:56:19.263
• [9.567 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 04/28/23 17:56:19.282
  Apr 28 17:56:19.282: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/28/23 17:56:19.284
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:56:19.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:56:19.372
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 04/28/23 17:56:19.394
  Apr 28 17:56:19.394: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 17:56:19.856334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:20.856741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:21.079: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 17:56:21.857374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:22.857614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:23.858214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:24.858412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:25.859132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:26.859780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:27.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3707" for this suite. @ 04/28/23 17:56:27.615
• [8.341 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 04/28/23 17:56:27.624
  Apr 28 17:56:27.624: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename services @ 04/28/23 17:56:27.625
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:56:27.642
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:56:27.647
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4156 @ 04/28/23 17:56:27.653
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 04/28/23 17:56:27.665
  STEP: creating service externalsvc in namespace services-4156 @ 04/28/23 17:56:27.667
  STEP: creating replication controller externalsvc in namespace services-4156 @ 04/28/23 17:56:27.711
  I0428 17:56:27.725100      19 runners.go:194] Created replication controller with name: externalsvc, namespace: services-4156, replica count: 2
  E0428 17:56:27.860035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:28.860431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:29.860550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 17:56:30.776609      19 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 04/28/23 17:56:30.78
  Apr 28 17:56:30.794: INFO: Creating new exec pod
  E0428 17:56:30.861313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:31.861388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:32.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-4156 exec execpodf8rkr -- /bin/sh -x -c nslookup clusterip-service.services-4156.svc.cluster.local'
  E0428 17:56:32.861880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:33.016: INFO: stderr: "+ nslookup clusterip-service.services-4156.svc.cluster.local\n"
  Apr 28 17:56:33.016: INFO: stdout: "Server:\t\t10.43.0.10\nAddress:\t10.43.0.10#53\n\nclusterip-service.services-4156.svc.cluster.local\tcanonical name = externalsvc.services-4156.svc.cluster.local.\nName:\texternalsvc.services-4156.svc.cluster.local\nAddress: 10.43.29.54\n\n"
  Apr 28 17:56:33.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-4156, will wait for the garbage collector to delete the pods @ 04/28/23 17:56:33.02
  Apr 28 17:56:33.079: INFO: Deleting ReplicationController externalsvc took: 6.331432ms
  Apr 28 17:56:33.179: INFO: Terminating ReplicationController externalsvc pods took: 100.350317ms
  E0428 17:56:33.862591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:34.863526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:35.323: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-4156" for this suite. @ 04/28/23 17:56:35.337
• [7.727 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 04/28/23 17:56:35.351
  Apr 28 17:56:35.351: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename kubelet-test @ 04/28/23 17:56:35.352
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:56:35.366
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:56:35.37
  E0428 17:56:35.863666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:36.863917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:37.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-840" for this suite. @ 04/28/23 17:56:37.404
• [2.058 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 04/28/23 17:56:37.411
  Apr 28 17:56:37.411: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename security-context-test @ 04/28/23 17:56:37.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:56:37.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:56:37.439
  E0428 17:56:37.863962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:38.864420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:39.864490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:40.864824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:41.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-3499" for this suite. @ 04/28/23 17:56:41.5
• [4.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 04/28/23 17:56:41.507
  Apr 28 17:56:41.507: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename services @ 04/28/23 17:56:41.508
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:56:41.535
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:56:41.54
  Apr 28 17:56:41.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3992" for this suite. @ 04/28/23 17:56:41.55
• [0.050 seconds]
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 04/28/23 17:56:41.558
  Apr 28 17:56:41.558: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:56:41.559
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:56:41.572
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:56:41.575
  STEP: Creating configMap with name projected-configmap-test-volume-map-90284f7c-f635-46a3-8e07-9637ccafe5e1 @ 04/28/23 17:56:41.577
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:56:41.581
  E0428 17:56:41.865346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:42.865500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:43.866710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:44.866928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:56:45.603
  Apr 28 17:56:45.606: INFO: Trying to get logs from node ip-172-31-10-53 pod pod-projected-configmaps-362a00e6-acd7-490c-8a3b-5959e607ac3f container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 17:56:45.61
  Apr 28 17:56:45.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3251" for this suite. @ 04/28/23 17:56:45.634
• [4.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 04/28/23 17:56:45.643
  Apr 28 17:56:45.643: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename pods @ 04/28/23 17:56:45.644
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:56:45.657
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:56:45.661
  E0428 17:56:45.867056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:46.868278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:47.869179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:48.869573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:49.869619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:50.869768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:56:51.726
  Apr 28 17:56:51.728: INFO: Trying to get logs from node ip-172-31-10-53 pod client-envvars-f2aa4aed-fcf9-4c34-b1b4-a86cf1b798e6 container env3cont: <nil>
  STEP: delete the pod @ 04/28/23 17:56:51.734
  Apr 28 17:56:51.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3457" for this suite. @ 04/28/23 17:56:51.751
• [6.114 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 04/28/23 17:56:51.758
  Apr 28 17:56:51.758: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename endpointslice @ 04/28/23 17:56:51.759
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:56:51.774
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:56:51.776
  E0428 17:56:51.870411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:52.870406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:53.870735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:54.870937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:55.871188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:56.871723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 04/28/23 17:56:56.88
  E0428 17:56:57.872570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:58.872990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:59.873096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:00.873313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:01.873828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 04/28/23 17:57:01.89
  E0428 17:57:02.874029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:03.874412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:04.874778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:05.874805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:06.875084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 04/28/23 17:57:06.899
  E0428 17:57:07.875280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:08.875554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:09.875990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:10.876108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:11.876190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 04/28/23 17:57:11.905
  Apr 28 17:57:11.922: INFO: EndpointSlice for Service endpointslice-2489/example-named-port not found
  E0428 17:57:12.877110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:13.877216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:14.877312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:15.877422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:16.877696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:17.877791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:18.877942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:19.878041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:20.878371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:21.878580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:57:21.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-2489" for this suite. @ 04/28/23 17:57:21.934
• [30.180 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 04/28/23 17:57:21.94
  Apr 28 17:57:21.940: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename namespaces @ 04/28/23 17:57:21.94
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:57:21.954
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:57:21.957
  STEP: Read namespace status @ 04/28/23 17:57:21.959
  Apr 28 17:57:21.962: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 04/28/23 17:57:21.963
  Apr 28 17:57:21.968: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 04/28/23 17:57:21.968
  Apr 28 17:57:21.980: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Apr 28 17:57:21.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-412" for this suite. @ 04/28/23 17:57:21.985
• [0.055 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 04/28/23 17:57:21.996
  Apr 28 17:57:21.996: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename services @ 04/28/23 17:57:21.997
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:57:22.012
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:57:22.015
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-8182 @ 04/28/23 17:57:22.017
  STEP: changing the ExternalName service to type=ClusterIP @ 04/28/23 17:57:22.024
  STEP: creating replication controller externalname-service in namespace services-8182 @ 04/28/23 17:57:22.038
  I0428 17:57:22.048114      19 runners.go:194] Created replication controller with name: externalname-service, namespace: services-8182, replica count: 2
  E0428 17:57:22.878898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:23.879004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:24.879047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 17:57:25.099544      19 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 28 17:57:25.099: INFO: Creating new exec pod
  E0428 17:57:25.879293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:26.879397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:27.879494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:57:28.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-8182 exec execpodqxgmn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 28 17:57:28.277: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 28 17:57:28.277: INFO: stdout: ""
  E0428 17:57:28.880208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:57:29.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-8182 exec execpodqxgmn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 28 17:57:29.500: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 28 17:57:29.500: INFO: stdout: "externalname-service-7qfvn"
  Apr 28 17:57:29.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=services-8182 exec execpodqxgmn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.30.221 80'
  Apr 28 17:57:29.803: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.30.221 80\nConnection to 10.43.30.221 80 port [tcp/http] succeeded!\n"
  Apr 28 17:57:29.803: INFO: stdout: "externalname-service-lwzhj"
  Apr 28 17:57:29.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 17:57:29.809: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-8182" for this suite. @ 04/28/23 17:57:29.837
• [7.846 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 04/28/23 17:57:29.843
  Apr 28 17:57:29.843: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename dns @ 04/28/23 17:57:29.844
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:57:29.854
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:57:29.857
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8115.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8115.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 04/28/23 17:57:29.86
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8115.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8115.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 04/28/23 17:57:29.86
  STEP: creating a pod to probe /etc/hosts @ 04/28/23 17:57:29.86
  STEP: submitting the pod to kubernetes @ 04/28/23 17:57:29.86
  E0428 17:57:29.880365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:30.880958      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:31.881061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:32.881157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:33.881536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/28/23 17:57:33.892
  STEP: looking for the results for each expected name from probers @ 04/28/23 17:57:33.894
  Apr 28 17:57:33.907: INFO: DNS probes using dns-8115/dns-test-a143849f-0814-47f7-ae3f-6fb689ca77bd succeeded

  Apr 28 17:57:33.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 17:57:33.91
  STEP: Destroying namespace "dns-8115" for this suite. @ 04/28/23 17:57:33.921
• [4.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 04/28/23 17:57:33.933
  Apr 28 17:57:33.933: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename discovery @ 04/28/23 17:57:33.935
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:57:33.949
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:57:33.952
  STEP: Setting up server cert @ 04/28/23 17:57:33.957
  E0428 17:57:34.882414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:57:35.131: INFO: Checking APIGroup: apiregistration.k8s.io
  Apr 28 17:57:35.132: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Apr 28 17:57:35.132: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Apr 28 17:57:35.132: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Apr 28 17:57:35.132: INFO: Checking APIGroup: apps
  Apr 28 17:57:35.133: INFO: PreferredVersion.GroupVersion: apps/v1
  Apr 28 17:57:35.133: INFO: Versions found [{apps/v1 v1}]
  Apr 28 17:57:35.133: INFO: apps/v1 matches apps/v1
  Apr 28 17:57:35.133: INFO: Checking APIGroup: events.k8s.io
  Apr 28 17:57:35.134: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Apr 28 17:57:35.134: INFO: Versions found [{events.k8s.io/v1 v1}]
  Apr 28 17:57:35.134: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Apr 28 17:57:35.134: INFO: Checking APIGroup: authentication.k8s.io
  Apr 28 17:57:35.134: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Apr 28 17:57:35.134: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Apr 28 17:57:35.134: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Apr 28 17:57:35.134: INFO: Checking APIGroup: authorization.k8s.io
  Apr 28 17:57:35.135: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Apr 28 17:57:35.135: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Apr 28 17:57:35.135: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Apr 28 17:57:35.135: INFO: Checking APIGroup: autoscaling
  Apr 28 17:57:35.136: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Apr 28 17:57:35.136: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Apr 28 17:57:35.136: INFO: autoscaling/v2 matches autoscaling/v2
  Apr 28 17:57:35.136: INFO: Checking APIGroup: batch
  Apr 28 17:57:35.137: INFO: PreferredVersion.GroupVersion: batch/v1
  Apr 28 17:57:35.137: INFO: Versions found [{batch/v1 v1}]
  Apr 28 17:57:35.137: INFO: batch/v1 matches batch/v1
  Apr 28 17:57:35.137: INFO: Checking APIGroup: certificates.k8s.io
  Apr 28 17:57:35.138: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Apr 28 17:57:35.138: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Apr 28 17:57:35.138: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Apr 28 17:57:35.138: INFO: Checking APIGroup: networking.k8s.io
  Apr 28 17:57:35.138: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Apr 28 17:57:35.138: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Apr 28 17:57:35.138: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Apr 28 17:57:35.138: INFO: Checking APIGroup: policy
  Apr 28 17:57:35.139: INFO: PreferredVersion.GroupVersion: policy/v1
  Apr 28 17:57:35.139: INFO: Versions found [{policy/v1 v1}]
  Apr 28 17:57:35.139: INFO: policy/v1 matches policy/v1
  Apr 28 17:57:35.139: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Apr 28 17:57:35.140: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Apr 28 17:57:35.140: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Apr 28 17:57:35.140: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Apr 28 17:57:35.140: INFO: Checking APIGroup: storage.k8s.io
  Apr 28 17:57:35.141: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Apr 28 17:57:35.141: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Apr 28 17:57:35.141: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Apr 28 17:57:35.141: INFO: Checking APIGroup: admissionregistration.k8s.io
  Apr 28 17:57:35.142: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Apr 28 17:57:35.142: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Apr 28 17:57:35.142: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Apr 28 17:57:35.142: INFO: Checking APIGroup: apiextensions.k8s.io
  Apr 28 17:57:35.142: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Apr 28 17:57:35.142: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Apr 28 17:57:35.142: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Apr 28 17:57:35.142: INFO: Checking APIGroup: scheduling.k8s.io
  Apr 28 17:57:35.143: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Apr 28 17:57:35.143: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Apr 28 17:57:35.143: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Apr 28 17:57:35.143: INFO: Checking APIGroup: coordination.k8s.io
  Apr 28 17:57:35.144: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Apr 28 17:57:35.144: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Apr 28 17:57:35.144: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Apr 28 17:57:35.144: INFO: Checking APIGroup: node.k8s.io
  Apr 28 17:57:35.145: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Apr 28 17:57:35.145: INFO: Versions found [{node.k8s.io/v1 v1}]
  Apr 28 17:57:35.145: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Apr 28 17:57:35.145: INFO: Checking APIGroup: discovery.k8s.io
  Apr 28 17:57:35.146: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Apr 28 17:57:35.146: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Apr 28 17:57:35.146: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Apr 28 17:57:35.146: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Apr 28 17:57:35.147: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Apr 28 17:57:35.147: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Apr 28 17:57:35.147: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Apr 28 17:57:35.147: INFO: Checking APIGroup: helm.cattle.io
  Apr 28 17:57:35.147: INFO: PreferredVersion.GroupVersion: helm.cattle.io/v1
  Apr 28 17:57:35.147: INFO: Versions found [{helm.cattle.io/v1 v1}]
  Apr 28 17:57:35.148: INFO: helm.cattle.io/v1 matches helm.cattle.io/v1
  Apr 28 17:57:35.148: INFO: Checking APIGroup: k3s.cattle.io
  Apr 28 17:57:35.148: INFO: PreferredVersion.GroupVersion: k3s.cattle.io/v1
  Apr 28 17:57:35.148: INFO: Versions found [{k3s.cattle.io/v1 v1}]
  Apr 28 17:57:35.148: INFO: k3s.cattle.io/v1 matches k3s.cattle.io/v1
  Apr 28 17:57:35.149: INFO: Checking APIGroup: traefik.containo.us
  Apr 28 17:57:35.150: INFO: PreferredVersion.GroupVersion: traefik.containo.us/v1alpha1
  Apr 28 17:57:35.150: INFO: Versions found [{traefik.containo.us/v1alpha1 v1alpha1}]
  Apr 28 17:57:35.150: INFO: traefik.containo.us/v1alpha1 matches traefik.containo.us/v1alpha1
  Apr 28 17:57:35.150: INFO: Checking APIGroup: metrics.k8s.io
  Apr 28 17:57:35.151: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
  Apr 28 17:57:35.151: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
  Apr 28 17:57:35.151: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
  Apr 28 17:57:35.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-7005" for this suite. @ 04/28/23 17:57:35.156
• [1.230 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 04/28/23 17:57:35.165
  Apr 28 17:57:35.165: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename replication-controller @ 04/28/23 17:57:35.165
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:57:35.181
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:57:35.184
  STEP: Creating replication controller my-hostname-basic-516cfd0c-a4db-406f-b121-85ec5f9cf167 @ 04/28/23 17:57:35.188
  Apr 28 17:57:35.208: INFO: Pod name my-hostname-basic-516cfd0c-a4db-406f-b121-85ec5f9cf167: Found 0 pods out of 1
  E0428 17:57:35.882496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:36.885033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:37.885313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:38.885653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:39.886302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:57:40.212: INFO: Pod name my-hostname-basic-516cfd0c-a4db-406f-b121-85ec5f9cf167: Found 1 pods out of 1
  Apr 28 17:57:40.212: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-516cfd0c-a4db-406f-b121-85ec5f9cf167" are running
  Apr 28 17:57:40.214: INFO: Pod "my-hostname-basic-516cfd0c-a4db-406f-b121-85ec5f9cf167-4fnqf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-28 17:57:35 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-28 17:57:36 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-28 17:57:36 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-28 17:57:35 +0000 UTC Reason: Message:}])
  Apr 28 17:57:40.214: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 04/28/23 17:57:40.214
  Apr 28 17:57:40.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9889" for this suite. @ 04/28/23 17:57:40.229
• [5.070 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 04/28/23 17:57:40.235
  Apr 28 17:57:40.235: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename cronjob @ 04/28/23 17:57:40.236
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:57:40.254
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:57:40.259
  STEP: Creating a ReplaceConcurrent cronjob @ 04/28/23 17:57:40.264
  STEP: Ensuring a job is scheduled @ 04/28/23 17:57:40.275
  E0428 17:57:40.887350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:41.887582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:42.888908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:43.889323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:44.890093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:45.890358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:46.890440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:47.890630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:48.890701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:49.890916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:50.891343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:51.891496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:52.892371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:53.892585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:54.892617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:55.892762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:56.892820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:57.893093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:58.893761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:59.893871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 04/28/23 17:58:00.29
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 04/28/23 17:58:00.295
  STEP: Ensuring the job is replaced with a new one @ 04/28/23 17:58:00.3
  E0428 17:58:00.893978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:01.894198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:02.899307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:03.899683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:04.899807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:05.899914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:06.900024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:07.900261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:08.900565      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:09.900766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:10.901766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:11.901987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:12.903044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:13.903395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:14.903536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:15.903694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:16.903905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:17.904092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:18.905084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:19.905310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:20.905522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:21.905769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:22.905893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:23.909219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:24.907359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:25.907678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:26.907747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:27.907965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:28.908122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:29.908332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:30.908466      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:31.908610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:32.909521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:33.909742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:34.910742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:35.910876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:36.911729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:37.911876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:38.912666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:39.912867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:40.912995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:41.913293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:42.914012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:43.914143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:44.914315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:45.914537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:46.914762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:47.914990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:48.915625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:49.915846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:50.916036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:51.916383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:52.916515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:53.916657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:54.916742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:55.916966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:56.917095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:57.917318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:58.917430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:59.917661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:59:00.316: INFO: Warning: Found 0 jobs in namespace cronjob-6006
  E0428 17:59:00.917908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:01.918063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 04/28/23 17:59:02.305
  Apr 28 17:59:02.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-6006" for this suite. @ 04/28/23 17:59:02.318
• [82.090 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 04/28/23 17:59:02.326
  Apr 28 17:59:02.326: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 17:59:02.327
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:59:02.371
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:59:02.388
  STEP: validating cluster-info @ 04/28/23 17:59:02.398
  Apr 28 17:59:02.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-9122 cluster-info'
  Apr 28 17:59:02.508: INFO: stderr: ""
  Apr 28 17:59:02.508: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.43.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Apr 28 17:59:02.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9122" for this suite. @ 04/28/23 17:59:02.514
• [0.200 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 04/28/23 17:59:02.526
  Apr 28 17:59:02.526: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename secrets @ 04/28/23 17:59:02.527
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:59:02.543
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:59:02.549
  STEP: Creating secret with name secret-test-7f9f8d5c-2fc5-4107-af6b-e1071034a2f8 @ 04/28/23 17:59:02.554
  STEP: Creating a pod to test consume secrets @ 04/28/23 17:59:02.562
  E0428 17:59:02.918184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:03.918943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:04.919954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:05.920185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:59:06.588
  Apr 28 17:59:06.591: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-secrets-55862a78-4403-4a2c-971c-3406a89c2ee5 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 17:59:06.602
  Apr 28 17:59:06.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5847" for this suite. @ 04/28/23 17:59:06.626
• [4.107 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 04/28/23 17:59:06.634
  Apr 28 17:59:06.634: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename secrets @ 04/28/23 17:59:06.636
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:59:06.655
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:59:06.659
  STEP: Creating secret with name secret-test-e392c771-068b-4b14-b417-ed67427f5f15 @ 04/28/23 17:59:06.663
  STEP: Creating a pod to test consume secrets @ 04/28/23 17:59:06.67
  E0428 17:59:06.921242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:07.921522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:08.921623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:09.921803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:59:10.703
  Apr 28 17:59:10.706: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-secrets-780d59a4-5051-4c2a-a63e-e3e5a4f7ca6a container secret-env-test: <nil>
  STEP: delete the pod @ 04/28/23 17:59:10.71
  Apr 28 17:59:10.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9080" for this suite. @ 04/28/23 17:59:10.728
• [4.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 04/28/23 17:59:10.736
  Apr 28 17:59:10.736: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename sched-preemption @ 04/28/23 17:59:10.736
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:59:10.773
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:59:10.784
  Apr 28 17:59:10.812: INFO: Waiting up to 1m0s for all nodes to be ready
  E0428 17:59:10.922365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:11.922582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:12.923359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:13.924382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:14.925492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:15.925716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:16.926434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:17.926588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:18.927980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:19.927940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:20.928347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:21.928500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:22.928797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:23.928885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:24.929476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:25.929615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:26.929685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:27.929898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:28.930509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:29.930734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:30.931686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:31.931826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:32.932810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:33.932908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:34.934717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:35.934860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:36.935598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:37.935782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:38.936866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:39.936964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:40.937029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:41.937273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:42.937851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:43.938140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:44.938309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:45.939080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:46.939650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:47.939879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:48.940017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:49.940161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:50.940885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:51.941479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:52.941840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:53.941892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:54.942658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:55.942562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:56.943488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:57.943753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:58.944421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:59.944651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:00.945532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:01.945692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:02.946404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:03.946552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:04.947403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:05.947598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:06.947965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:07.948090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:08.948785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:09.948940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:00:10.838: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 04/28/23 18:00:10.843
  Apr 28 18:00:10.843: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename sched-preemption-path @ 04/28/23 18:00:10.844
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:00:10.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:00:10.873
  STEP: Finding an available node @ 04/28/23 18:00:10.876
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/28/23 18:00:10.877
  E0428 18:00:10.949655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:11.949870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/28/23 18:00:12.91
  Apr 28 18:00:12.925: INFO: found a healthy node: ip-172-31-1-213
  E0428 18:00:12.950739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:13.950769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:14.950890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:15.951020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:16.951245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:17.952923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:18.953725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:00:19.025: INFO: pods created so far: [1 1 1]
  Apr 28 18:00:19.025: INFO: length of pods created so far: 3
  E0428 18:00:19.954819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:20.955417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:00:21.036: INFO: pods created so far: [2 2 1]
  E0428 18:00:21.955464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:22.955690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:23.956244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:24.956543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:25.956668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:26.956799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:27.956999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:00:28.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 18:00:28.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-4981" for this suite. @ 04/28/23 18:00:28.126
  STEP: Destroying namespace "sched-preemption-5059" for this suite. @ 04/28/23 18:00:28.131
• [77.401 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 04/28/23 18:00:28.137
  Apr 28 18:00:28.137: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 18:00:28.139
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:00:28.153
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:00:28.156
  STEP: creating Agnhost RC @ 04/28/23 18:00:28.16
  Apr 28 18:00:28.160: INFO: namespace kubectl-8816
  Apr 28 18:00:28.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-8816 create -f -'
  Apr 28 18:00:28.547: INFO: stderr: ""
  Apr 28 18:00:28.547: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/28/23 18:00:28.547
  E0428 18:00:28.957403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:00:29.552: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 28 18:00:29.552: INFO: Found 0 / 1
  E0428 18:00:29.957556      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:00:30.551: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 28 18:00:30.551: INFO: Found 1 / 1
  Apr 28 18:00:30.551: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Apr 28 18:00:30.553: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 28 18:00:30.553: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 28 18:00:30.553: INFO: wait on agnhost-primary startup in kubectl-8816 
  Apr 28 18:00:30.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-8816 logs agnhost-primary-tz5th agnhost-primary'
  Apr 28 18:00:30.675: INFO: stderr: ""
  Apr 28 18:00:30.675: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 04/28/23 18:00:30.675
  Apr 28 18:00:30.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-8816 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Apr 28 18:00:30.766: INFO: stderr: ""
  Apr 28 18:00:30.766: INFO: stdout: "service/rm2 exposed\n"
  Apr 28 18:00:30.774: INFO: Service rm2 in namespace kubectl-8816 found.
  E0428 18:00:30.958467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:31.958625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: exposing service @ 04/28/23 18:00:32.78
  Apr 28 18:00:32.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-8816 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Apr 28 18:00:32.867: INFO: stderr: ""
  Apr 28 18:00:32.867: INFO: stdout: "service/rm3 exposed\n"
  Apr 28 18:00:32.873: INFO: Service rm3 in namespace kubectl-8816 found.
  E0428 18:00:32.959378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:33.961726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:00:34.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8816" for this suite. @ 04/28/23 18:00:34.881
• [6.750 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 04/28/23 18:00:34.887
  Apr 28 18:00:34.887: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename init-container @ 04/28/23 18:00:34.888
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:00:34.956
  E0428 18:00:34.961895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:00:34.964
  STEP: creating the pod @ 04/28/23 18:00:34.967
  Apr 28 18:00:34.967: INFO: PodSpec: initContainers in spec.initContainers
  E0428 18:00:35.962142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:36.963064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:37.963255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:38.963470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:39.963720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:40.963820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:41.963945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:42.964352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:43.964676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:44.964871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:45.965105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:46.965285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:47.966337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:48.966767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:49.966923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:50.967044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:51.967208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:52.967330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:53.968378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:54.968595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:55.968823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:56.969052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:57.969248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:58.970001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:59.970631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:00.970820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:01.971020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:02.971194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:03.971603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:04.971760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:05.971996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:06.972103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:07.972448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:08.972422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:09.972624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:10.972828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:11.973013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:01:12.006: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-e43dabf1-b22c-4b7f-a6c1-91a340d449f8", GenerateName:"", Namespace:"init-container-6734", SelfLink:"", UID:"55d77062-4515-4055-9cfb-4bd543d3fde3", ResourceVersion:"280469", Generation:0, CreationTimestamp:time.Date(2023, time.April, 28, 18, 0, 34, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"967865422"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 28, 18, 0, 34, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000c1df50), Subresource:""}, v1.ManagedFieldsEntry{Manager:"k3s", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 28, 18, 1, 11, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000c1df80), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-k8x8x", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000d754e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-k8x8x", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-k8x8x", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-k8x8x", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00388acf0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-1-213", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00055e540), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00388ad80)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00388ada0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00388ada8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00388adac), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000e3f390), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 28, 18, 0, 34, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 28, 18, 0, 34, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 28, 18, 0, 34, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 28, 18, 0, 34, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.1.213", PodIP:"10.42.2.199", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.42.2.199"}}, StartTime:time.Date(2023, time.April, 28, 18, 0, 34, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00055e700)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00055e7e0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://3073419a81903cf30fa5aeebd646f12ea56ebc8fa0c7d6e124a4c855105762ab", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000d755a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000d75580), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00388ae2f), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Apr 28 18:01:12.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-6734" for this suite. @ 04/28/23 18:01:12.022
• [37.147 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 04/28/23 18:01:12.036
  Apr 28 18:01:12.036: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename replicaset @ 04/28/23 18:01:12.037
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:01:12.081
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:01:12.087
  STEP: Create a ReplicaSet @ 04/28/23 18:01:12.091
  STEP: Verify that the required pods have come up @ 04/28/23 18:01:12.106
  Apr 28 18:01:12.109: INFO: Pod name sample-pod: Found 0 pods out of 3
  E0428 18:01:12.973162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:13.973544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:14.973686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:15.973831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:16.974053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:01:17.116: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 04/28/23 18:01:17.116
  Apr 28 18:01:17.126: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 04/28/23 18:01:17.126
  STEP: DeleteCollection of the ReplicaSets @ 04/28/23 18:01:17.138
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 04/28/23 18:01:17.155
  Apr 28 18:01:17.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1105" for this suite. @ 04/28/23 18:01:17.207
• [5.208 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 04/28/23 18:01:17.246
  Apr 28 18:01:17.246: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename webhook @ 04/28/23 18:01:17.247
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:01:17.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:01:17.425
  STEP: Setting up server cert @ 04/28/23 18:01:17.553
  E0428 18:01:17.975001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 18:01:18.311
  STEP: Deploying the webhook pod @ 04/28/23 18:01:18.317
  STEP: Wait for the deployment to be ready @ 04/28/23 18:01:18.325
  Apr 28 18:01:18.330: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0428 18:01:18.976078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:19.976340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:01:20.339: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 18, 1, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 1, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 18, 1, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 1, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 18:01:20.977175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:21.977382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:01:22.347: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 18, 1, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 1, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 18, 1, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 1, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 18:01:22.977650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:23.977962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:01:24.343: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 18, 1, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 1, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 18, 1, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 1, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 18:01:24.979036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:25.979297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:01:26.342: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 18, 1, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 1, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 18, 1, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 1, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 18:01:26.979957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:27.980278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 18:01:28.342
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 18:01:28.35
  E0428 18:01:28.981301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:01:29.350: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 04/28/23 18:01:29.353
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 04/28/23 18:01:29.354
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 04/28/23 18:01:29.354
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 04/28/23 18:01:29.354
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 04/28/23 18:01:29.355
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 04/28/23 18:01:29.355
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 04/28/23 18:01:29.356
  Apr 28 18:01:29.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9824" for this suite. @ 04/28/23 18:01:29.406
  STEP: Destroying namespace "webhook-markers-9699" for this suite. @ 04/28/23 18:01:29.413
• [12.176 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 04/28/23 18:01:29.423
  Apr 28 18:01:29.423: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 18:01:29.424
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:01:29.441
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:01:29.444
  STEP: validating api versions @ 04/28/23 18:01:29.447
  Apr 28 18:01:29.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-5273 api-versions'
  Apr 28 18:01:29.549: INFO: stderr: ""
  Apr 28 18:01:29.549: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nhelm.cattle.io/v1\nk3s.cattle.io/v1\nmetrics.k8s.io/v1beta1\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\ntraefik.containo.us/v1alpha1\nv1\n"
  Apr 28 18:01:29.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5273" for this suite. @ 04/28/23 18:01:29.554
• [0.135 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 04/28/23 18:01:29.559
  Apr 28 18:01:29.559: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 18:01:29.56
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:01:29.587
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:01:29.599
  STEP: Creating secret with name s-test-opt-del-1015a629-fab8-4bf3-a1b5-b9ebce3d32a0 @ 04/28/23 18:01:29.608
  STEP: Creating secret with name s-test-opt-upd-b387a43e-9284-498f-9909-2a9fb71f6154 @ 04/28/23 18:01:29.616
  STEP: Creating the pod @ 04/28/23 18:01:29.628
  E0428 18:01:29.982198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:30.982667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-1015a629-fab8-4bf3-a1b5-b9ebce3d32a0 @ 04/28/23 18:01:31.689
  STEP: Updating secret s-test-opt-upd-b387a43e-9284-498f-9909-2a9fb71f6154 @ 04/28/23 18:01:31.694
  STEP: Creating secret with name s-test-opt-create-a5dcba0b-037d-4485-bcef-64b13a717a48 @ 04/28/23 18:01:31.699
  STEP: waiting to observe update in volume @ 04/28/23 18:01:31.703
  E0428 18:01:31.983697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:32.983848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:33.984784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:34.985174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:35.985251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:36.985473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:37.985581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:38.985904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:39.986726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:40.987962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:41.988054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:42.988405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:43.989507      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:44.989746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:45.990568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:46.990778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:47.991528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:48.991898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:49.992025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:50.992238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:51.993319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:52.993426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:53.994628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:54.994960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:55.995522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:56.996351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:57.997413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:58.997638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:59.998103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:00.998290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:01.998807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:02.999039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:04.001298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:05.000341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:06.000562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:07.000969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:08.001570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:09.002093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:10.002151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:11.002696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:12.003026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:13.003162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:14.004153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:15.004269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:16.005287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:17.005407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:18.005712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:19.005851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:20.006907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:21.007127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:22.007665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:23.007804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:24.008506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:25.008589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:26.008972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:27.009102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:28.010542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:29.011410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:30.012353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:31.013025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:32.013788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:33.013935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:34.014200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:35.014322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:36.014686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:37.014879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:38.017062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:39.017583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:40.018008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:41.018333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:42.019372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:43.019581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:44.020015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:45.020390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:46.021182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:47.021294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:48.021994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:49.022191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:50.023213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:51.023383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:52.024487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:53.024588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:54.024713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:55.024949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:56.025010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:57.025152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:58.025378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:59.025522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:00.025631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:01.025742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:02.026180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:03:02.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9226" for this suite. @ 04/28/23 18:03:02.178
• [92.625 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 04/28/23 18:03:02.186
  Apr 28 18:03:02.186: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename kubelet-test @ 04/28/23 18:03:02.193
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:03:02.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:03:02.217
  STEP: Waiting for pod completion @ 04/28/23 18:03:02.244
  E0428 18:03:03.026574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:04.027121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:05.027298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:06.027431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:03:06.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-5393" for this suite. @ 04/28/23 18:03:06.276
• [4.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 04/28/23 18:03:06.283
  Apr 28 18:03:06.283: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename sched-pred @ 04/28/23 18:03:06.286
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:03:06.35
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:03:06.355
  Apr 28 18:03:06.358: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 28 18:03:06.375: INFO: Waiting for terminating namespaces to be deleted...
  Apr 28 18:03:06.382: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-1-213 before test
  Apr 28 18:03:06.389: INFO: svclb-traefik-0a77677b-4z972 from kube-system started at 2023-04-28 17:22:42 +0000 UTC (2 container statuses recorded)
  Apr 28 18:03:06.390: INFO: 	Container lb-tcp-443 ready: true, restart count 0
  Apr 28 18:03:06.390: INFO: 	Container lb-tcp-80 ready: true, restart count 0
  Apr 28 18:03:06.390: INFO: pod-projected-secrets-f10f0760-140d-4b65-861b-84a72178aacc from projected-9226 started at 2023-04-28 18:01:29 +0000 UTC (3 container statuses recorded)
  Apr 28 18:03:06.390: INFO: 	Container creates-volume-test ready: true, restart count 0
  Apr 28 18:03:06.390: INFO: 	Container dels-volume-test ready: true, restart count 0
  Apr 28 18:03:06.390: INFO: 	Container upds-volume-test ready: true, restart count 0
  Apr 28 18:03:06.390: INFO: sonobuoy-systemd-logs-daemon-set-d17888d62c134fe7-v6wcc from sonobuoy started at 2023-04-28 16:35:07 +0000 UTC (2 container statuses recorded)
  Apr 28 18:03:06.390: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 18:03:06.390: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 18:03:06.390: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-10-53 before test
  Apr 28 18:03:06.395: INFO: svclb-traefik-0a77677b-d6qnt from kube-system started at 2023-04-28 03:31:43 +0000 UTC (2 container statuses recorded)
  Apr 28 18:03:06.395: INFO: 	Container lb-tcp-443 ready: true, restart count 0
  Apr 28 18:03:06.395: INFO: 	Container lb-tcp-80 ready: true, restart count 0
  Apr 28 18:03:06.395: INFO: agnhost-host-aliases0bc8233c-03f9-47ca-a3c0-1abb1f8cf683 from kubelet-test-5393 started at 2023-04-28 18:03:02 +0000 UTC (1 container statuses recorded)
  Apr 28 18:03:06.395: INFO: 	Container agnhost-container ready: false, restart count 0
  Apr 28 18:03:06.395: INFO: sonobuoy from sonobuoy started at 2023-04-28 16:35:06 +0000 UTC (1 container statuses recorded)
  Apr 28 18:03:06.395: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 28 18:03:06.395: INFO: sonobuoy-systemd-logs-daemon-set-d17888d62c134fe7-clf8z from sonobuoy started at 2023-04-28 16:35:07 +0000 UTC (2 container statuses recorded)
  Apr 28 18:03:06.395: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 18:03:06.395: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 18:03:06.395: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-12-59 before test
  Apr 28 18:03:06.401: INFO: coredns-77ccd57875-lg57c from kube-system started at 2023-04-28 03:29:34 +0000 UTC (1 container statuses recorded)
  Apr 28 18:03:06.401: INFO: 	Container coredns ready: true, restart count 0
  Apr 28 18:03:06.401: INFO: helm-install-traefik-crd-rxpjj from kube-system started at 2023-04-28 03:29:34 +0000 UTC (1 container statuses recorded)
  Apr 28 18:03:06.401: INFO: 	Container helm ready: false, restart count 0
  Apr 28 18:03:06.401: INFO: helm-install-traefik-q2g86 from kube-system started at 2023-04-28 03:29:34 +0000 UTC (1 container statuses recorded)
  Apr 28 18:03:06.401: INFO: 	Container helm ready: false, restart count 1
  Apr 28 18:03:06.401: INFO: local-path-provisioner-957fdf8bc-jc5qz from kube-system started at 2023-04-28 03:29:34 +0000 UTC (1 container statuses recorded)
  Apr 28 18:03:06.401: INFO: 	Container local-path-provisioner ready: true, restart count 0
  Apr 28 18:03:06.401: INFO: metrics-server-54dc485875-gbbbl from kube-system started at 2023-04-28 03:29:34 +0000 UTC (1 container statuses recorded)
  Apr 28 18:03:06.401: INFO: 	Container metrics-server ready: true, restart count 0
  Apr 28 18:03:06.401: INFO: svclb-traefik-0a77677b-jtrmg from kube-system started at 2023-04-28 03:29:54 +0000 UTC (2 container statuses recorded)
  Apr 28 18:03:06.401: INFO: 	Container lb-tcp-443 ready: true, restart count 0
  Apr 28 18:03:06.401: INFO: 	Container lb-tcp-80 ready: true, restart count 0
  Apr 28 18:03:06.401: INFO: traefik-84745cf649-mwthv from kube-system started at 2023-04-28 03:29:54 +0000 UTC (1 container statuses recorded)
  Apr 28 18:03:06.401: INFO: 	Container traefik ready: true, restart count 0
  Apr 28 18:03:06.401: INFO: sonobuoy-systemd-logs-daemon-set-d17888d62c134fe7-4jl87 from sonobuoy started at 2023-04-28 16:35:07 +0000 UTC (2 container statuses recorded)
  Apr 28 18:03:06.401: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 18:03:06.401: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 18:03:06.401: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-5-174 before test
  Apr 28 18:03:06.416: INFO: svclb-traefik-0a77677b-x527f from kube-system started at 2023-04-28 03:33:13 +0000 UTC (2 container statuses recorded)
  Apr 28 18:03:06.416: INFO: 	Container lb-tcp-443 ready: true, restart count 0
  Apr 28 18:03:06.416: INFO: 	Container lb-tcp-80 ready: true, restart count 0
  Apr 28 18:03:06.416: INFO: sonobuoy-e2e-job-4c96b6c95eb94b68 from sonobuoy started at 2023-04-28 16:35:07 +0000 UTC (2 container statuses recorded)
  Apr 28 18:03:06.416: INFO: 	Container e2e ready: true, restart count 0
  Apr 28 18:03:06.416: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 18:03:06.416: INFO: sonobuoy-systemd-logs-daemon-set-d17888d62c134fe7-wkkfr from sonobuoy started at 2023-04-28 16:35:07 +0000 UTC (2 container statuses recorded)
  Apr 28 18:03:06.416: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 18:03:06.416: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node ip-172-31-1-213 @ 04/28/23 18:03:06.439
  STEP: verifying the node has the label node ip-172-31-10-53 @ 04/28/23 18:03:06.45
  STEP: verifying the node has the label node ip-172-31-12-59 @ 04/28/23 18:03:06.465
  STEP: verifying the node has the label node ip-172-31-5-174 @ 04/28/23 18:03:06.496
  Apr 28 18:03:06.513: INFO: Pod coredns-77ccd57875-lg57c requesting resource cpu=100m on Node ip-172-31-12-59
  Apr 28 18:03:06.513: INFO: Pod local-path-provisioner-957fdf8bc-jc5qz requesting resource cpu=0m on Node ip-172-31-12-59
  Apr 28 18:03:06.513: INFO: Pod metrics-server-54dc485875-gbbbl requesting resource cpu=100m on Node ip-172-31-12-59
  Apr 28 18:03:06.513: INFO: Pod svclb-traefik-0a77677b-4z972 requesting resource cpu=0m on Node ip-172-31-1-213
  Apr 28 18:03:06.513: INFO: Pod svclb-traefik-0a77677b-d6qnt requesting resource cpu=0m on Node ip-172-31-10-53
  Apr 28 18:03:06.513: INFO: Pod svclb-traefik-0a77677b-jtrmg requesting resource cpu=0m on Node ip-172-31-12-59
  Apr 28 18:03:06.513: INFO: Pod svclb-traefik-0a77677b-x527f requesting resource cpu=0m on Node ip-172-31-5-174
  Apr 28 18:03:06.513: INFO: Pod traefik-84745cf649-mwthv requesting resource cpu=0m on Node ip-172-31-12-59
  Apr 28 18:03:06.513: INFO: Pod pod-projected-secrets-f10f0760-140d-4b65-861b-84a72178aacc requesting resource cpu=0m on Node ip-172-31-1-213
  Apr 28 18:03:06.513: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-10-53
  Apr 28 18:03:06.513: INFO: Pod sonobuoy-e2e-job-4c96b6c95eb94b68 requesting resource cpu=0m on Node ip-172-31-5-174
  Apr 28 18:03:06.513: INFO: Pod sonobuoy-systemd-logs-daemon-set-d17888d62c134fe7-4jl87 requesting resource cpu=0m on Node ip-172-31-12-59
  Apr 28 18:03:06.513: INFO: Pod sonobuoy-systemd-logs-daemon-set-d17888d62c134fe7-clf8z requesting resource cpu=0m on Node ip-172-31-10-53
  Apr 28 18:03:06.513: INFO: Pod sonobuoy-systemd-logs-daemon-set-d17888d62c134fe7-v6wcc requesting resource cpu=0m on Node ip-172-31-1-213
  Apr 28 18:03:06.513: INFO: Pod sonobuoy-systemd-logs-daemon-set-d17888d62c134fe7-wkkfr requesting resource cpu=0m on Node ip-172-31-5-174
  STEP: Starting Pods to consume most of the cluster CPU. @ 04/28/23 18:03:06.513
  Apr 28 18:03:06.513: INFO: Creating a pod which consumes cpu=1400m on Node ip-172-31-1-213
  Apr 28 18:03:06.540: INFO: Creating a pod which consumes cpu=1400m on Node ip-172-31-10-53
  Apr 28 18:03:06.558: INFO: Creating a pod which consumes cpu=1260m on Node ip-172-31-12-59
  Apr 28 18:03:06.573: INFO: Creating a pod which consumes cpu=1400m on Node ip-172-31-5-174
  E0428 18:03:07.030943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:08.031298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating another pod that requires unavailable amount of CPU. @ 04/28/23 18:03:08.63
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-79a778b1-dca3-49b1-ad9f-a13e2d55da40.175a2b451b3555c5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1651/filler-pod-79a778b1-dca3-49b1-ad9f-a13e2d55da40 to ip-172-31-12-59] @ 04/28/23 18:03:08.633
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-79a778b1-dca3-49b1-ad9f-a13e2d55da40.175a2b453a68c250], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/28/23 18:03:08.633
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-79a778b1-dca3-49b1-ad9f-a13e2d55da40.175a2b453b80d8cb], Reason = [Created], Message = [Created container filler-pod-79a778b1-dca3-49b1-ad9f-a13e2d55da40] @ 04/28/23 18:03:08.633
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-79a778b1-dca3-49b1-ad9f-a13e2d55da40.175a2b4542cc84c5], Reason = [Started], Message = [Started container filler-pod-79a778b1-dca3-49b1-ad9f-a13e2d55da40] @ 04/28/23 18:03:08.633
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-9fca3a92-ed4e-4ae2-a32e-8c60b45438e9.175a2b451e66055e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1651/filler-pod-9fca3a92-ed4e-4ae2-a32e-8c60b45438e9 to ip-172-31-5-174] @ 04/28/23 18:03:08.633
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-9fca3a92-ed4e-4ae2-a32e-8c60b45438e9.175a2b453e943af8], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/28/23 18:03:08.633
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-9fca3a92-ed4e-4ae2-a32e-8c60b45438e9.175a2b453fd686a2], Reason = [Created], Message = [Created container filler-pod-9fca3a92-ed4e-4ae2-a32e-8c60b45438e9] @ 04/28/23 18:03:08.633
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-9fca3a92-ed4e-4ae2-a32e-8c60b45438e9.175a2b45449c8bf6], Reason = [Started], Message = [Started container filler-pod-9fca3a92-ed4e-4ae2-a32e-8c60b45438e9] @ 04/28/23 18:03:08.633
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c48f6acc-4567-4d00-a0e1-bb970ec3fb7e.175a2b451965401d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1651/filler-pod-c48f6acc-4567-4d00-a0e1-bb970ec3fb7e to ip-172-31-1-213] @ 04/28/23 18:03:08.633
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c48f6acc-4567-4d00-a0e1-bb970ec3fb7e.175a2b453932cf62], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/28/23 18:03:08.633
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c48f6acc-4567-4d00-a0e1-bb970ec3fb7e.175a2b453a9c4917], Reason = [Created], Message = [Created container filler-pod-c48f6acc-4567-4d00-a0e1-bb970ec3fb7e] @ 04/28/23 18:03:08.633
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c48f6acc-4567-4d00-a0e1-bb970ec3fb7e.175a2b453ff0a973], Reason = [Started], Message = [Started container filler-pod-c48f6acc-4567-4d00-a0e1-bb970ec3fb7e] @ 04/28/23 18:03:08.633
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-f5a151f2-d48c-4e58-831e-a66ea2e1ff61.175a2b451a4462cd], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1651/filler-pod-f5a151f2-d48c-4e58-831e-a66ea2e1ff61 to ip-172-31-10-53] @ 04/28/23 18:03:08.633
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-f5a151f2-d48c-4e58-831e-a66ea2e1ff61.175a2b4537551cce], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/28/23 18:03:08.633
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-f5a151f2-d48c-4e58-831e-a66ea2e1ff61.175a2b453930390b], Reason = [Created], Message = [Created container filler-pod-f5a151f2-d48c-4e58-831e-a66ea2e1ff61] @ 04/28/23 18:03:08.633
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-f5a151f2-d48c-4e58-831e-a66ea2e1ff61.175a2b4541e00233], Reason = [Started], Message = [Started container filler-pod-f5a151f2-d48c-4e58-831e-a66ea2e1ff61] @ 04/28/23 18:03:08.633
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.175a2b45959f8d6c], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu. preemption: 0/4 nodes are available: 4 No preemption victims found for incoming pod..] @ 04/28/23 18:03:08.644
  E0428 18:03:09.031735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label node off the node ip-172-31-1-213 @ 04/28/23 18:03:09.644
  STEP: verifying the node doesn't have the label node @ 04/28/23 18:03:09.655
  STEP: removing the label node off the node ip-172-31-10-53 @ 04/28/23 18:03:09.66
  STEP: verifying the node doesn't have the label node @ 04/28/23 18:03:09.672
  STEP: removing the label node off the node ip-172-31-12-59 @ 04/28/23 18:03:09.677
  STEP: verifying the node doesn't have the label node @ 04/28/23 18:03:09.696
  STEP: removing the label node off the node ip-172-31-5-174 @ 04/28/23 18:03:09.704
  STEP: verifying the node doesn't have the label node @ 04/28/23 18:03:09.737
  Apr 28 18:03:09.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-1651" for this suite. @ 04/28/23 18:03:09.746
• [3.469 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 04/28/23 18:03:09.752
  Apr 28 18:03:09.753: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 18:03:09.754
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:03:09.769
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:03:09.773
  STEP: Creating configMap with name projected-configmap-test-volume-decf11eb-eae1-476b-a906-c088135c326d @ 04/28/23 18:03:09.777
  STEP: Creating a pod to test consume configMaps @ 04/28/23 18:03:09.783
  E0428 18:03:10.031821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:11.032023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:12.033002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:13.033435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 18:03:13.804
  Apr 28 18:03:13.807: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-projected-configmaps-0a1632f5-e425-448d-9ef4-72599bf4e968 container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 18:03:13.813
  Apr 28 18:03:13.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-77" for this suite. @ 04/28/23 18:03:13.835
• [4.088 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 04/28/23 18:03:13.841
  Apr 28 18:03:13.841: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename gc @ 04/28/23 18:03:13.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:03:13.855
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:03:13.858
  STEP: create the rc @ 04/28/23 18:03:13.862
  W0428 18:03:13.869126      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0428 18:03:14.033358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:15.033470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:16.033628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:17.033767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:18.033989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 04/28/23 18:03:18.872
  STEP: wait for all pods to be garbage collected @ 04/28/23 18:03:18.877
  E0428 18:03:19.037583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:20.037699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:21.037799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:22.038036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:23.038160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 04/28/23 18:03:23.883
  W0428 18:03:23.893086      19 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Apr 28 18:03:23.893: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 28 18:03:23.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-266" for this suite. @ 04/28/23 18:03:23.898
• [10.064 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 04/28/23 18:03:23.906
  Apr 28 18:03:23.906: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename webhook @ 04/28/23 18:03:23.907
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:03:23.923
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:03:23.928
  STEP: Setting up server cert @ 04/28/23 18:03:23.961
  E0428 18:03:24.038389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 18:03:24.309
  STEP: Deploying the webhook pod @ 04/28/23 18:03:24.316
  STEP: Wait for the deployment to be ready @ 04/28/23 18:03:24.331
  Apr 28 18:03:24.337: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0428 18:03:25.039264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:26.039386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 18:03:26.345
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 18:03:26.353
  E0428 18:03:27.040231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:03:27.353: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 04/28/23 18:03:27.356
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 04/28/23 18:03:27.38
  STEP: Creating a dummy validating-webhook-configuration object @ 04/28/23 18:03:27.396
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 04/28/23 18:03:27.405
  STEP: Creating a dummy mutating-webhook-configuration object @ 04/28/23 18:03:27.409
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 04/28/23 18:03:27.418
  Apr 28 18:03:27.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-203" for this suite. @ 04/28/23 18:03:27.498
  STEP: Destroying namespace "webhook-markers-7826" for this suite. @ 04/28/23 18:03:27.504
• [3.605 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 04/28/23 18:03:27.514
  Apr 28 18:03:27.514: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename cronjob @ 04/28/23 18:03:27.515
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:03:27.537
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:03:27.54
  STEP: Creating a ForbidConcurrent cronjob @ 04/28/23 18:03:27.543
  STEP: Ensuring a job is scheduled @ 04/28/23 18:03:27.549
  E0428 18:03:28.040334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:29.040584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:30.041580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:31.041705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:32.041911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:33.042152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:34.043080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:35.043312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:36.043517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:37.043526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:38.043664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:39.044196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:40.045202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:41.045314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:42.046076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:43.046220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:44.046528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:45.046779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:46.047852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:47.048014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:48.049051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:49.049538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:50.050489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:51.050622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:52.050768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:53.050967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:54.051263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:55.051411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:56.052361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:57.052568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:58.053333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:59.053820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:00.054903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:01.055092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 04/28/23 18:04:01.558
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 04/28/23 18:04:01.563
  STEP: Ensuring no more jobs are scheduled @ 04/28/23 18:04:01.571
  E0428 18:04:02.055495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:03.055906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:04.056632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:05.056846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:06.057706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:07.058033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:08.058164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:09.058279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:10.058409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:11.058510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:12.058648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:13.058778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:14.059482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:15.059629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:16.059733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:17.059860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:18.060606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:19.061105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:20.062181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:21.062393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:22.063445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:23.063670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:24.064387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:25.064591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:26.064705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:27.064916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:28.065810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:29.066270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:30.066374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:31.066593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:32.066730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:33.066865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:34.066955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:35.067215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:36.067285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:37.067380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:38.068343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:39.068474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:40.069185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:41.069319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:42.069462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:43.069621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:44.069732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:45.069839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:46.070200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:47.070122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:48.070577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:49.071508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:50.072346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:51.072567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:52.072703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:53.072828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:54.072981      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:55.073124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:56.073241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:57.073456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:58.073506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:59.073640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:00.073731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:01.074597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:02.074762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:03.074886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:04.075619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:05.075843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:06.076592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:07.076790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:08.077455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:09.077572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:10.077726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:11.077904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:12.077925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:13.078124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:14.078256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:15.078385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:16.079297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:17.079399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:18.080374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:19.080821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:20.081640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:21.081753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:22.082784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:23.083041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:24.083775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:25.083984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:26.084094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:27.084318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:28.084445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:29.084635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:30.085645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:31.085855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:32.086087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:33.086870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:34.086956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:35.087210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:36.087616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:37.087577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:38.088401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:39.088783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:40.089662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:41.089799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:42.090841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:43.090991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:44.091167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:45.091357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:46.091511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:47.091695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:48.092749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:49.093768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:50.094023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:51.094222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:52.094432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:53.094632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:54.095554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:55.095771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:56.096768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:57.096908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:58.097062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:59.097531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:00.097666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:01.097811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:02.098540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:03.098865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:04.099036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:05.099215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:06.099338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:07.099456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:08.100420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:09.100464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:10.101531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:11.101759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:12.102621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:13.102713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:14.103283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:15.104372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:16.105194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:17.105310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:18.106052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:19.106572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:20.107395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:21.108377      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:22.108514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:23.109070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:24.109204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:25.109774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:26.109882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:27.109993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:28.110021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:29.110815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:30.111914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:31.112370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:32.113439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:33.113639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:34.114284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:35.114518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:36.114637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:37.114900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:38.115818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:39.116373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:40.117261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:41.117384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:42.118204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:43.118338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:44.118688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:45.118847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:46.118978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:47.119101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:48.119785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:49.120317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:50.120452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:51.120691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:52.120807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:53.121048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:54.121101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:55.121223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:56.121295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:57.121535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:58.122334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:59.122760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:00.122930      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:01.123047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:02.123519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:03.124268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:04.125177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:05.125308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:06.125444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:07.125668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:08.125802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:09.126267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:10.126425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:11.126676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:12.127627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:13.127770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:14.127820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:15.128072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:16.128893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:17.129029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:18.129150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:19.129681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:20.130564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:21.130705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:22.131808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:23.131955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:24.132253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:25.132817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:26.132917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:27.133085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:28.133933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:29.134417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:30.135488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:31.136369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:32.136476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:33.136638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:34.136663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:35.136774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:36.137009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:37.137121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:38.137848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:39.138379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:40.138906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:41.139131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:42.139377      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:43.140329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:44.140538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:45.140638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:46.141607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:47.141810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:48.142702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:49.143259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:50.143876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:51.144148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:52.145095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:53.145232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:54.145371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:55.145577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:56.146499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:57.147654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:58.147794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:59.148317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:00.148469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:01.148677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:02.148823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:03.149037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:04.149154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:05.149275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:06.150300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:07.151293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:08.151422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:09.152401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:10.153359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:11.153470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:12.153529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:13.153759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:14.153791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:15.153898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:16.154761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:17.154866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:18.155006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:19.155166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:20.155274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:21.155489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:22.156349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:23.156468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:24.156582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:25.156723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:26.156880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:27.157013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:28.157292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:29.157752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:30.157846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:31.158376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:32.158441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:33.158633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:34.159517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:35.159621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:36.160350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:37.160617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:38.161701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:39.162518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:40.163532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:41.163680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:42.164424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:43.164607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:44.164719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:45.164854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:46.165766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:47.166022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:48.166145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:49.166732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:50.167580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:51.167777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:52.168940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:53.169050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:54.169156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:55.169284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:56.169627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:57.169784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:58.170800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:59.170938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:00.171042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:01.171185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 04/28/23 18:09:01.578
  Apr 28 18:09:01.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-830" for this suite. @ 04/28/23 18:09:01.59
• [334.082 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 04/28/23 18:09:01.597
  Apr 28 18:09:01.597: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 18:09:01.599
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:09:01.631
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:09:01.636
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 18:09:01.644
  E0428 18:09:02.171309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:03.171443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:04.171549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:05.172364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 18:09:05.679
  Apr 28 18:09:05.682: INFO: Trying to get logs from node ip-172-31-1-213 pod downwardapi-volume-cd96de5d-f1b1-4317-9850-0f59fa75e8cd container client-container: <nil>
  STEP: delete the pod @ 04/28/23 18:09:05.707
  Apr 28 18:09:05.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1911" for this suite. @ 04/28/23 18:09:05.736
• [4.153 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 04/28/23 18:09:05.751
  Apr 28 18:09:05.751: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 18:09:05.752
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:09:05.778
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:09:05.786
  STEP: Creating a ResourceQuota with terminating scope @ 04/28/23 18:09:05.791
  STEP: Ensuring ResourceQuota status is calculated @ 04/28/23 18:09:05.796
  E0428 18:09:06.173332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:07.173472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not terminating scope @ 04/28/23 18:09:07.799
  STEP: Ensuring ResourceQuota status is calculated @ 04/28/23 18:09:07.805
  E0428 18:09:08.173555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:09.174081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a long running pod @ 04/28/23 18:09:09.81
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 04/28/23 18:09:09.822
  E0428 18:09:10.175066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:11.175234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 04/28/23 18:09:11.826
  E0428 18:09:12.175848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:13.175965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 04/28/23 18:09:13.829
  STEP: Ensuring resource quota status released the pod usage @ 04/28/23 18:09:13.842
  E0428 18:09:14.176089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:15.176314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a terminating pod @ 04/28/23 18:09:15.855
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 04/28/23 18:09:15.878
  E0428 18:09:16.176459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:17.176607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 04/28/23 18:09:17.884
  E0428 18:09:18.177440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:19.178168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 04/28/23 18:09:19.889
  STEP: Ensuring resource quota status released the pod usage @ 04/28/23 18:09:19.901
  E0428 18:09:20.179007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:21.179152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:09:21.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9934" for this suite. @ 04/28/23 18:09:21.91
• [16.185 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 04/28/23 18:09:21.937
  Apr 28 18:09:21.937: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename runtimeclass @ 04/28/23 18:09:21.937
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:09:21.988
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:09:21.991
  STEP: Deleting RuntimeClass runtimeclass-8083-delete-me @ 04/28/23 18:09:22.009
  STEP: Waiting for the RuntimeClass to disappear @ 04/28/23 18:09:22.017
  Apr 28 18:09:22.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-8083" for this suite. @ 04/28/23 18:09:22.044
• [0.117 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 04/28/23 18:09:22.054
  Apr 28 18:09:22.054: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 18:09:22.054
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:09:22.067
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:09:22.072
  STEP: creating the pod @ 04/28/23 18:09:22.077
  Apr 28 18:09:22.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-7978 create -f -'
  E0428 18:09:22.180692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:09:22.685: INFO: stderr: ""
  Apr 28 18:09:22.685: INFO: stdout: "pod/pause created\n"
  E0428 18:09:23.180357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:24.180719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 04/28/23 18:09:24.694
  Apr 28 18:09:24.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-7978 label pods pause testing-label=testing-label-value'
  Apr 28 18:09:24.763: INFO: stderr: ""
  Apr 28 18:09:24.763: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 04/28/23 18:09:24.763
  Apr 28 18:09:24.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-7978 get pod pause -L testing-label'
  Apr 28 18:09:24.845: INFO: stderr: ""
  Apr 28 18:09:24.845: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 04/28/23 18:09:24.845
  Apr 28 18:09:24.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-7978 label pods pause testing-label-'
  Apr 28 18:09:24.928: INFO: stderr: ""
  Apr 28 18:09:24.928: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 04/28/23 18:09:24.928
  Apr 28 18:09:24.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-7978 get pod pause -L testing-label'
  Apr 28 18:09:25.017: INFO: stderr: ""
  Apr 28 18:09:25.017: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
  STEP: using delete to clean up resources @ 04/28/23 18:09:25.017
  Apr 28 18:09:25.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-7978 delete --grace-period=0 --force -f -'
  Apr 28 18:09:25.109: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 28 18:09:25.109: INFO: stdout: "pod \"pause\" force deleted\n"
  Apr 28 18:09:25.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-7978 get rc,svc -l name=pause --no-headers'
  E0428 18:09:25.181567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:09:25.192: INFO: stderr: "No resources found in kubectl-7978 namespace.\n"
  Apr 28 18:09:25.192: INFO: stdout: ""
  Apr 28 18:09:25.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=kubectl-7978 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 28 18:09:25.264: INFO: stderr: ""
  Apr 28 18:09:25.264: INFO: stdout: ""
  Apr 28 18:09:25.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7978" for this suite. @ 04/28/23 18:09:25.267
• [3.218 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 04/28/23 18:09:25.273
  Apr 28 18:09:25.273: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename container-probe @ 04/28/23 18:09:25.273
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:09:25.286
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:09:25.288
  E0428 18:09:26.181611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:27.181745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:28.181903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:29.182560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:30.182760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:31.183010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:32.183091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:33.183256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:34.183951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:35.184188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:36.184457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:37.184707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:38.185662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:39.185773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:40.185886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:41.186015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:42.187070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:43.187309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:44.188385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:45.189387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:46.189513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:47.189737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:09:47.353: INFO: Container started at 2023-04-28 18:09:25 +0000 UTC, pod became ready at 2023-04-28 18:09:45 +0000 UTC
  Apr 28 18:09:47.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-1109" for this suite. @ 04/28/23 18:09:47.356
• [22.088 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 04/28/23 18:09:47.361
  Apr 28 18:09:47.361: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename init-container @ 04/28/23 18:09:47.362
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:09:47.382
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:09:47.389
  STEP: creating the pod @ 04/28/23 18:09:47.392
  Apr 28 18:09:47.392: INFO: PodSpec: initContainers in spec.initContainers
  E0428 18:09:48.190699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:49.191350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:50.192265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:09:51.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-3080" for this suite. @ 04/28/23 18:09:51.17
• [3.815 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 04/28/23 18:09:51.179
  Apr 28 18:09:51.179: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 18:09:51.18
  E0428 18:09:51.192396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:09:51.198
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:09:51.203
  STEP: Creating resourceQuota "e2e-rq-status-fg2m7" @ 04/28/23 18:09:51.209
  Apr 28 18:09:51.225: INFO: Resource quota "e2e-rq-status-fg2m7" reports spec: hard cpu limit of 500m
  Apr 28 18:09:51.226: INFO: Resource quota "e2e-rq-status-fg2m7" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-fg2m7" /status @ 04/28/23 18:09:51.226
  STEP: Confirm /status for "e2e-rq-status-fg2m7" resourceQuota via watch @ 04/28/23 18:09:51.235
  Apr 28 18:09:51.237: INFO: observed resourceQuota "e2e-rq-status-fg2m7" in namespace "resourcequota-5174" with hard status: v1.ResourceList(nil)
  Apr 28 18:09:51.237: INFO: Found resourceQuota "e2e-rq-status-fg2m7" in namespace "resourcequota-5174" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Apr 28 18:09:51.237: INFO: ResourceQuota "e2e-rq-status-fg2m7" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 04/28/23 18:09:51.239
  Apr 28 18:09:51.251: INFO: Resource quota "e2e-rq-status-fg2m7" reports spec: hard cpu limit of 1
  Apr 28 18:09:51.251: INFO: Resource quota "e2e-rq-status-fg2m7" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-fg2m7" /status @ 04/28/23 18:09:51.251
  STEP: Confirm /status for "e2e-rq-status-fg2m7" resourceQuota via watch @ 04/28/23 18:09:51.261
  Apr 28 18:09:51.263: INFO: observed resourceQuota "e2e-rq-status-fg2m7" in namespace "resourcequota-5174" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Apr 28 18:09:51.263: INFO: Found resourceQuota "e2e-rq-status-fg2m7" in namespace "resourcequota-5174" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Apr 28 18:09:51.263: INFO: ResourceQuota "e2e-rq-status-fg2m7" /status was patched
  STEP: Get "e2e-rq-status-fg2m7" /status @ 04/28/23 18:09:51.263
  Apr 28 18:09:51.270: INFO: Resourcequota "e2e-rq-status-fg2m7" reports status: hard cpu of 1
  Apr 28 18:09:51.270: INFO: Resourcequota "e2e-rq-status-fg2m7" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-fg2m7" /status before checking Spec is unchanged @ 04/28/23 18:09:51.273
  Apr 28 18:09:51.285: INFO: Resourcequota "e2e-rq-status-fg2m7" reports status: hard cpu of 2
  Apr 28 18:09:51.285: INFO: Resourcequota "e2e-rq-status-fg2m7" reports status: hard memory of 2Gi
  Apr 28 18:09:51.286: INFO: Found resourceQuota "e2e-rq-status-fg2m7" in namespace "resourcequota-5174" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  E0428 18:09:52.192582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:53.192928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:54.193029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:55.193159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:56.193406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:57.193530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:58.193744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:59.194304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:00.194527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:01.194707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:02.194794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:03.195011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:04.195372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:05.195606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:06.195755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:07.195949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:08.196015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:09.196145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:10.196261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:11.196451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:12.196614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:13.197267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:14.197612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:15.197776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:16.197882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:17.197997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:18.198757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:19.199279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:20.199501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:21.199593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:22.199747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:23.199843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:24.200073      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:25.200285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:26.200451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:27.200505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:28.200651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:29.201182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:30.201380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:31.201589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:32.201691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:33.201889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:34.202135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:35.202260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:36.202458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:37.202598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:38.202757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:39.203248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:40.203291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:41.204376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:42.204528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:43.204761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:44.204875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:45.205081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:46.205401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:47.205452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:48.205574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:49.206023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:50.206160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:51.206267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:52.206396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:53.206581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:54.206959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:55.207065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:56.207248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:57.207406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:58.207510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:59.208377      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:00.208511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:01.208646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:02.208784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:03.208913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:04.209047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:05.209278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:06.209494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:07.209632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:08.209748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:09.210230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:10.210378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:11.210514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:12.210629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:13.210736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:14.211160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:15.211364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:16.211375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:17.211474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:18.211595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:19.212234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:20.212368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:21.212468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:22.212628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:23.212792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:24.212888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:25.213013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:26.213144      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:27.213361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:28.213647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:29.214212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:30.214399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:31.214598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:32.214731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:33.214982      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:34.215282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:35.215407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:36.215617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:37.215814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:38.216680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:39.216809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:40.217012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:41.217214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:42.217347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:43.217549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:44.217925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:45.218160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:46.218356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:47.218482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:48.218677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:49.218786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:50.218984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:51.219438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:52.219612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:53.219864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:54.220132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:55.220376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:56.220575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:57.220699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:58.220895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:59.221030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:00.221321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:01.221545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:02.221689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:03.222727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:04.223116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:05.223267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:06.223600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:07.223773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:08.223956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:09.224429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:10.224641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:11.224886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:12.225304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:13.225602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:14.226004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:15.226209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:16.226407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:17.226549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:18.226748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:19.227257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:20.227470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:21.228401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:22.228538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:23.228770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:24.229042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:25.229362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:26.229563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:27.229699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:28.231117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:29.231334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:30.231521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:31.231626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:32.231821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:33.232182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:34.232187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:35.232330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:36.232553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:37.232808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:38.232997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:39.233136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:40.233356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:41.233550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:42.233687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:43.233866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:44.234158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:45.234394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:46.234631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:47.234742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:48.234930      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:49.235402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:50.235600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:51.236348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:52.236496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:53.236684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:54.236817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:55.237026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:56.237220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:57.237432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:58.237626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:59.238199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:00.238401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:01.238597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:02.238673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:03.238880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:04.239008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:05.239228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:06.239418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:07.239617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:08.239815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:09.240316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:10.240517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:11.240723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:12.240838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:13.241038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:14.241411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:15.241602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:16.241828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:17.241950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:18.242148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:19.242778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:20.242972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:21.243187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:22.244072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:23.244337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:24.244714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:25.244913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:26.245488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:27.245686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:28.245895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:29.246022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:30.246217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:31.246424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:32.246631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:33.246832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:34.247187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:35.247406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:36.248387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:37.249064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:38.248886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:39.249733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:40.249956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:41.250163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:42.250380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:43.250532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:44.250903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:45.251029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:46.251258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:47.251392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:48.251612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:49.252074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:50.252272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:51.252461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:52.252986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:53.253121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:54.253622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:55.253827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:56.254011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:57.254155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:58.254335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:59.254462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:00.254607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:01.254797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:02.255594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:03.255786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:04.256137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:05.256361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:06.256547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:07.257063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:08.257256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:09.257747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:10.257971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:11.258163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:12.258295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:13.258528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:14.258867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:15.259084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:16.259292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:17.260362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:18.260508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:19.260954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:20.261098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:21.261288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:22.261423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:23.262028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:24.262400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:25.262598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:26.262934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:27.263131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:28.263281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:29.263897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:30.264089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:31.264281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:32.265003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:33.265261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:34.265617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:35.265819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:36.266000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:14:36.292: INFO: ResourceQuota "e2e-rq-status-fg2m7" Spec was unchanged and /status reset
  Apr 28 18:14:36.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5174" for this suite. @ 04/28/23 18:14:36.296
• [285.122 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 04/28/23 18:14:36.302
  Apr 28 18:14:36.302: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename namespaces @ 04/28/23 18:14:36.305
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:14:36.345
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:14:36.35
  STEP: Creating a test namespace @ 04/28/23 18:14:36.354
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:14:36.374
  STEP: Creating a service in the namespace @ 04/28/23 18:14:36.385
  STEP: Deleting the namespace @ 04/28/23 18:14:36.403
  STEP: Waiting for the namespace to be removed. @ 04/28/23 18:14:36.43
  E0428 18:14:37.266277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:38.266397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:39.266537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:40.266681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:41.266805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:42.267560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 04/28/23 18:14:42.433
  STEP: Verifying there is no service in the namespace @ 04/28/23 18:14:42.506
  Apr 28 18:14:42.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4058" for this suite. @ 04/28/23 18:14:42.522
  STEP: Destroying namespace "nsdeletetest-9040" for this suite. @ 04/28/23 18:14:42.526
  Apr 28 18:14:42.528: INFO: Namespace nsdeletetest-9040 was already deleted
  STEP: Destroying namespace "nsdeletetest-6377" for this suite. @ 04/28/23 18:14:42.528
• [6.230 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 04/28/23 18:14:42.532
  Apr 28 18:14:42.532: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename sysctl @ 04/28/23 18:14:42.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:14:42.544
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:14:42.546
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 04/28/23 18:14:42.55
  STEP: Watching for error events or started pod @ 04/28/23 18:14:42.561
  E0428 18:14:43.268587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:44.268729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 04/28/23 18:14:44.565
  E0428 18:14:45.270121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:46.270584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 04/28/23 18:14:46.573
  STEP: Getting logs from the pod @ 04/28/23 18:14:46.573
  STEP: Checking that the sysctl is actually updated @ 04/28/23 18:14:46.583
  Apr 28 18:14:46.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-1157" for this suite. @ 04/28/23 18:14:46.587
• [4.059 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 04/28/23 18:14:46.602
  Apr 28 18:14:46.602: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename security-context-test @ 04/28/23 18:14:46.602
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:14:46.621
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:14:46.625
  E0428 18:14:47.271042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:48.271116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:49.271268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:50.271407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:14:50.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-9116" for this suite. @ 04/28/23 18:14:50.653
• [4.060 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 04/28/23 18:14:50.66
  Apr 28 18:14:50.660: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename sched-preemption @ 04/28/23 18:14:50.663
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:14:50.675
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:14:50.679
  Apr 28 18:14:50.705: INFO: Waiting up to 1m0s for all nodes to be ready
  E0428 18:14:51.272300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:52.272508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:53.272637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:54.273415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:55.273458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:56.273741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:57.273895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:58.274108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:59.274914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:00.275214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:01.280717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:02.280840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:03.281882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:04.282000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:05.282151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:06.282285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:07.282748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:08.282955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:09.283106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:10.283259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:11.284366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:12.284565      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:13.284658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:14.284756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:15.285370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:16.285707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:17.286123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:18.286583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:19.286639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:20.286912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:21.287050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:22.287178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:23.287321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:24.287449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:25.287514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:26.287790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:27.287946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:28.288146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:29.289003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:30.289296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:31.289385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:32.289499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:33.289769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:34.289910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:35.290013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:36.290151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:37.290718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:38.291014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:39.291368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:40.291577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:41.292390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:42.292629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:43.293463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:44.293603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:45.294459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:46.294600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:47.295463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:48.295700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:49.296473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:50.296636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:15:50.731: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 04/28/23 18:15:50.744
  Apr 28 18:15:50.762: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Apr 28 18:15:50.770: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Apr 28 18:15:50.786: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Apr 28 18:15:50.792: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Apr 28 18:15:50.817: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Apr 28 18:15:50.885: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  Apr 28 18:15:50.960: INFO: Created pod: pod3-0-sched-preemption-medium-priority
  Apr 28 18:15:50.979: INFO: Created pod: pod3-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 04/28/23 18:15:50.979
  E0428 18:15:51.297252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:52.297361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 04/28/23 18:15:53.028
  E0428 18:15:53.298277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:54.298615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:55.299549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:56.299645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:15:57.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-242" for this suite. @ 04/28/23 18:15:57.135
• [66.481 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 04/28/23 18:15:57.143
  Apr 28 18:15:57.143: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename watch @ 04/28/23 18:15:57.144
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:15:57.161
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:15:57.168
  STEP: getting a starting resourceVersion @ 04/28/23 18:15:57.175
  STEP: starting a background goroutine to produce watch events @ 04/28/23 18:15:57.178
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 04/28/23 18:15:57.178
  E0428 18:15:57.299827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:58.300217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:59.300216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:15:59.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7508" for this suite. @ 04/28/23 18:16:00.009
• [2.909 seconds]
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 04/28/23 18:16:00.052
  Apr 28 18:16:00.052: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename pod-network-test @ 04/28/23 18:16:00.053
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:16:00.087
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:16:00.095
  STEP: Performing setup for networking test in namespace pod-network-test-8891 @ 04/28/23 18:16:00.109
  STEP: creating a selector @ 04/28/23 18:16:00.109
  STEP: Creating the service pods in kubernetes @ 04/28/23 18:16:00.109
  Apr 28 18:16:00.109: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0428 18:16:00.300691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:01.301760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:02.302637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:03.303570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:04.303837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:05.304353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:06.305222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:07.306288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:08.308058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:09.307651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:10.308069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:11.308278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:12.309049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:13.309214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:14.309465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:15.309711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:16.310840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:17.311084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:18.311741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:19.312398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:20.312702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:21.312904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:22.313393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 04/28/23 18:16:22.316
  E0428 18:16:23.313538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:24.313720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:16:24.328: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
  Apr 28 18:16:24.328: INFO: Breadth first check of 10.42.2.215 on host 172.31.1.213...
  Apr 28 18:16:24.330: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.2.216:9080/dial?request=hostname&protocol=http&host=10.42.2.215&port=8083&tries=1'] Namespace:pod-network-test-8891 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 18:16:24.330: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 18:16:24.331: INFO: ExecWithOptions: Clientset creation
  Apr 28 18:16:24.331: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-8891/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.2.216%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.42.2.215%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 28 18:16:24.487: INFO: Waiting for responses: map[]
  Apr 28 18:16:24.487: INFO: reached 10.42.2.215 after 0/1 tries
  Apr 28 18:16:24.487: INFO: Breadth first check of 10.42.1.143 on host 172.31.10.53...
  Apr 28 18:16:24.490: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.2.216:9080/dial?request=hostname&protocol=http&host=10.42.1.143&port=8083&tries=1'] Namespace:pod-network-test-8891 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 18:16:24.490: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 18:16:24.491: INFO: ExecWithOptions: Clientset creation
  Apr 28 18:16:24.491: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-8891/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.2.216%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.42.1.143%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 28 18:16:24.585: INFO: Waiting for responses: map[]
  Apr 28 18:16:24.585: INFO: reached 10.42.1.143 after 0/1 tries
  Apr 28 18:16:24.585: INFO: Breadth first check of 10.42.0.212 on host 172.31.12.59...
  Apr 28 18:16:24.588: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.2.216:9080/dial?request=hostname&protocol=http&host=10.42.0.212&port=8083&tries=1'] Namespace:pod-network-test-8891 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 18:16:24.588: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 18:16:24.588: INFO: ExecWithOptions: Clientset creation
  Apr 28 18:16:24.588: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-8891/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.2.216%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.42.0.212%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 28 18:16:24.659: INFO: Waiting for responses: map[]
  Apr 28 18:16:24.659: INFO: reached 10.42.0.212 after 0/1 tries
  Apr 28 18:16:24.659: INFO: Breadth first check of 10.42.3.34 on host 172.31.5.174...
  Apr 28 18:16:24.662: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.2.216:9080/dial?request=hostname&protocol=http&host=10.42.3.34&port=8083&tries=1'] Namespace:pod-network-test-8891 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 18:16:24.662: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  Apr 28 18:16:24.663: INFO: ExecWithOptions: Clientset creation
  Apr 28 18:16:24.663: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-8891/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.2.216%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.42.3.34%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 28 18:16:24.722: INFO: Waiting for responses: map[]
  Apr 28 18:16:24.722: INFO: reached 10.42.3.34 after 0/1 tries
  Apr 28 18:16:24.722: INFO: Going to retry 0 out of 4 pods....
  Apr 28 18:16:24.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-8891" for this suite. @ 04/28/23 18:16:24.732
• [24.694 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 04/28/23 18:16:24.752
  Apr 28 18:16:24.752: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename pods @ 04/28/23 18:16:24.753
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:16:24.777
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:16:24.782
  Apr 28 18:16:24.786: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: creating the pod @ 04/28/23 18:16:24.787
  STEP: submitting the pod to kubernetes @ 04/28/23 18:16:24.787
  E0428 18:16:25.313803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:26.314032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:16:26.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8418" for this suite. @ 04/28/23 18:16:26.849
• [2.103 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 04/28/23 18:16:26.856
  Apr 28 18:16:26.856: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename crd-webhook @ 04/28/23 18:16:26.857
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:16:26.878
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:16:26.888
  STEP: Setting up server cert @ 04/28/23 18:16:26.893
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 04/28/23 18:16:27.287
  STEP: Deploying the custom resource conversion webhook pod @ 04/28/23 18:16:27.293
  STEP: Wait for the deployment to be ready @ 04/28/23 18:16:27.304
  Apr 28 18:16:27.311: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  E0428 18:16:27.315443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:28.315298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:29.315688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 18:16:29.32
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 18:16:29.331
  E0428 18:16:30.316385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:16:30.333: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Apr 28 18:16:30.337: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 18:16:31.316535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:32.316895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 04/28/23 18:16:32.905
  STEP: v2 custom resource should be converted @ 04/28/23 18:16:32.91
  Apr 28 18:16:32.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0428 18:16:33.317029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-1158" for this suite. @ 04/28/23 18:16:33.529
• [6.697 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 04/28/23 18:16:33.554
  Apr 28 18:16:33.554: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename namespaces @ 04/28/23 18:16:33.556
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:16:33.593
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:16:33.597
  STEP: Updating Namespace "namespaces-7086" @ 04/28/23 18:16:33.601
  Apr 28 18:16:33.616: INFO: Namespace "namespaces-7086" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"e19fda74-19df-4e20-970a-6122324dd6fc", "kubernetes.io/metadata.name":"namespaces-7086", "namespaces-7086":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Apr 28 18:16:33.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7086" for this suite. @ 04/28/23 18:16:33.626
• [0.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 04/28/23 18:16:33.64
  Apr 28 18:16:33.640: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 18:16:33.641
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:16:33.659
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:16:33.662
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 04/28/23 18:16:33.666
  E0428 18:16:34.317113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:35.317352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 18:16:35.708
  Apr 28 18:16:35.711: INFO: Trying to get logs from node ip-172-31-1-213 pod pod-01f5a963-2bbd-43b8-99ad-6e49a7aa384b container test-container: <nil>
  STEP: delete the pod @ 04/28/23 18:16:35.724
  Apr 28 18:16:35.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7409" for this suite. @ 04/28/23 18:16:35.748
• [2.113 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 04/28/23 18:16:35.756
  Apr 28 18:16:35.756: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename container-runtime @ 04/28/23 18:16:35.758
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:16:35.783
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:16:35.786
  STEP: create the container @ 04/28/23 18:16:35.788
  W0428 18:16:35.797220      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/28/23 18:16:35.797
  E0428 18:16:36.317516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:37.318606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 04/28/23 18:16:37.814
  STEP: the container should be terminated @ 04/28/23 18:16:37.816
  STEP: the termination message should be set @ 04/28/23 18:16:37.816
  Apr 28 18:16:37.816: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 04/28/23 18:16:37.816
  Apr 28 18:16:37.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-4142" for this suite. @ 04/28/23 18:16:37.834
• [2.085 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 04/28/23 18:16:37.842
  Apr 28 18:16:37.842: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename ingress @ 04/28/23 18:16:37.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:16:37.858
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:16:37.862
  STEP: getting /apis @ 04/28/23 18:16:37.865
  STEP: getting /apis/networking.k8s.io @ 04/28/23 18:16:37.869
  STEP: getting /apis/networking.k8s.iov1 @ 04/28/23 18:16:37.87
  STEP: creating @ 04/28/23 18:16:37.871
  STEP: getting @ 04/28/23 18:16:37.897
  STEP: listing @ 04/28/23 18:16:37.903
  STEP: watching @ 04/28/23 18:16:37.906
  Apr 28 18:16:37.906: INFO: starting watch
  STEP: cluster-wide listing @ 04/28/23 18:16:37.907
  STEP: cluster-wide watching @ 04/28/23 18:16:37.91
  Apr 28 18:16:37.910: INFO: starting watch
  STEP: patching @ 04/28/23 18:16:37.911
  STEP: updating @ 04/28/23 18:16:37.917
  Apr 28 18:16:37.926: INFO: waiting for watch events with expected annotations
  Apr 28 18:16:37.926: INFO: saw patched and updated annotations
  STEP: patching /status @ 04/28/23 18:16:37.926
  STEP: updating /status @ 04/28/23 18:16:37.933
  STEP: get /status @ 04/28/23 18:16:37.967
  STEP: deleting @ 04/28/23 18:16:37.979
  STEP: deleting a collection @ 04/28/23 18:16:37.994
  Apr 28 18:16:38.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-8692" for this suite. @ 04/28/23 18:16:38.011
• [0.175 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 04/28/23 18:16:38.017
  Apr 28 18:16:38.017: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename deployment @ 04/28/23 18:16:38.018
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:16:38.031
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:16:38.035
  Apr 28 18:16:38.049: INFO: Pod name rollover-pod: Found 0 pods out of 1
  E0428 18:16:38.319111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:39.319612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:40.319762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:41.319980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:42.320384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:16:43.060: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/28/23 18:16:43.06
  Apr 28 18:16:43.060: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  E0428 18:16:43.321053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:44.321149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:16:45.067: INFO: Creating deployment "test-rollover-deployment"
  Apr 28 18:16:45.075: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  E0428 18:16:45.321574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:46.321700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:16:47.081: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Apr 28 18:16:47.085: INFO: Ensure that both replica sets have 1 created replica
  Apr 28 18:16:47.090: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Apr 28 18:16:47.097: INFO: Updating deployment test-rollover-deployment
  Apr 28 18:16:47.097: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  E0428 18:16:47.322509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:48.322844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:16:49.104: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Apr 28 18:16:49.109: INFO: Make sure deployment "test-rollover-deployment" is complete
  Apr 28 18:16:49.113: INFO: all replica sets need to contain the pod-template-hash label
  Apr 28 18:16:49.113: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 18, 16, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 16, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 18, 16, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 16, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 18:16:49.322848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:50.323038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:16:51.126: INFO: all replica sets need to contain the pod-template-hash label
  Apr 28 18:16:51.126: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 18, 16, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 16, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 18, 16, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 16, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 18:16:51.323456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:52.323615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:16:53.119: INFO: all replica sets need to contain the pod-template-hash label
  Apr 28 18:16:53.119: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 18, 16, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 16, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 18, 16, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 16, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 18:16:53.324451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:54.324820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:16:55.122: INFO: all replica sets need to contain the pod-template-hash label
  Apr 28 18:16:55.122: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 18, 16, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 16, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 18, 16, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 16, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 18:16:55.325412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:56.325637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:16:57.119: INFO: all replica sets need to contain the pod-template-hash label
  Apr 28 18:16:57.119: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 18, 16, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 16, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 18, 16, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 16, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 18:16:57.326091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:58.326301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:16:59.122: INFO: 
  Apr 28 18:16:59.122: INFO: Ensure that both old replica sets have no replicas
  Apr 28 18:16:59.130: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-8668  a947135b-d24f-4d07-a128-308977ea00ba 286097 2 2023-04-28 18:16:45 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-28 18:16:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2023-04-28 18:16:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00626ae78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-28 18:16:45 +0000 UTC,LastTransitionTime:2023-04-28 18:16:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-04-28 18:16:58 +0000 UTC,LastTransitionTime:2023-04-28 18:16:45 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 28 18:16:59.133: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-8668  cf60006b-68b0-490b-83cd-dd33d8fd6a57 286087 2 2023-04-28 18:16:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment a947135b-d24f-4d07-a128-308977ea00ba 0xc00626b357 0xc00626b358}] [] [{k3s Update apps/v1 2023-04-28 18:16:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a947135b-d24f-4d07-a128-308977ea00ba\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2023-04-28 18:16:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00626b418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 18:16:59.133: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Apr 28 18:16:59.133: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8668  4a78a2bd-0384-4f2d-8248-351e5af89cbd 286096 2 2023-04-28 18:16:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment a947135b-d24f-4d07-a128-308977ea00ba 0xc00626b217 0xc00626b218}] [] [{e2e.test Update apps/v1 2023-04-28 18:16:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2023-04-28 18:16:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a947135b-d24f-4d07-a128-308977ea00ba\"}":{}}},"f:spec":{"f:replicas":{}}} } {k3s Update apps/v1 2023-04-28 18:16:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00626b2e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 18:16:59.133: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-8668  e145efef-6d19-4598-84b8-b9923b439128 286022 2 2023-04-28 18:16:45 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment a947135b-d24f-4d07-a128-308977ea00ba 0xc00626b487 0xc00626b488}] [] [{k3s Update apps/v1 2023-04-28 18:16:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a947135b-d24f-4d07-a128-308977ea00ba\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {k3s Update apps/v1 2023-04-28 18:16:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00626b548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 18:16:59.135: INFO: Pod "test-rollover-deployment-57777854c9-d87dl" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-d87dl test-rollover-deployment-57777854c9- deployment-8668  2c9c07af-db8e-498c-957a-ae3fd8d69e8d 286039 0 2023-04-28 18:16:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 cf60006b-68b0-490b-83cd-dd33d8fd6a57 0xc00626baa7 0xc00626baa8}] [] [{k3s Update v1 2023-04-28 18:16:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf60006b-68b0-490b-83cd-dd33d8fd6a57\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {k3s Update v1 2023-04-28 18:16:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.222\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dzt9w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dzt9w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-1-213,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 18:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 18:16:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 18:16:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 18:16:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.1.213,PodIP:10.42.2.222,StartTime:2023-04-28 18:16:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 18:16:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://be5926a4edb00e17de47043f61550bd8fdb3a54be21b37cca928b9e562728f67,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.222,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 18:16:59.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8668" for this suite. @ 04/28/23 18:16:59.143
• [21.130 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 04/28/23 18:16:59.149
  Apr 28 18:16:59.149: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename statefulset @ 04/28/23 18:16:59.149
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:16:59.172
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:16:59.176
  STEP: Creating service test in namespace statefulset-4639 @ 04/28/23 18:16:59.18
  STEP: Creating statefulset ss in namespace statefulset-4639 @ 04/28/23 18:16:59.193
  Apr 28 18:16:59.231: INFO: Found 0 stateful pods, waiting for 1
  E0428 18:16:59.327266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:00.327622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:01.327850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:02.328097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:03.328333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:04.328780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:05.329000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:06.329177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:07.329270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:08.329573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:17:09.235: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 04/28/23 18:17:09.241
  STEP: updating a scale subresource @ 04/28/23 18:17:09.247
  STEP: verifying the statefulset Spec.Replicas was modified @ 04/28/23 18:17:09.252
  STEP: Patch a scale subresource @ 04/28/23 18:17:09.268
  STEP: verifying the statefulset Spec.Replicas was modified @ 04/28/23 18:17:09.281
  Apr 28 18:17:09.285: INFO: Deleting all statefulset in ns statefulset-4639
  Apr 28 18:17:09.289: INFO: Scaling statefulset ss to 0
  E0428 18:17:09.329761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:10.331343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:11.331460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:12.331564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:13.331692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:14.331891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:15.332033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:16.332387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:17.332539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:18.332735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:17:19.305: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 18:17:19.307: INFO: Deleting statefulset ss
  Apr 28 18:17:19.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4639" for this suite. @ 04/28/23 18:17:19.327
• [20.183 seconds]
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 04/28/23 18:17:19.332
  Apr 28 18:17:19.332: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 18:17:19.333225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Building a namespace api object, basename emptydir-wrapper @ 04/28/23 18:17:19.333
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:17:19.347
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:17:19.353
  E0428 18:17:20.334021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:21.334216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:17:21.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 04/28/23 18:17:21.406
  STEP: Cleaning up the configmap @ 04/28/23 18:17:21.414
  STEP: Cleaning up the pod @ 04/28/23 18:17:21.423
  STEP: Destroying namespace "emptydir-wrapper-9363" for this suite. @ 04/28/23 18:17:21.437
• [2.139 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 04/28/23 18:17:21.472
  Apr 28 18:17:21.472: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename daemonsets @ 04/28/23 18:17:21.473
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:17:21.533
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:17:21.549
  STEP: Creating simple DaemonSet "daemon-set" @ 04/28/23 18:17:21.637
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/28/23 18:17:21.66
  Apr 28 18:17:21.683: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 18:17:21.683: INFO: Node ip-172-31-1-213 is running 0 daemon pod, expected 1
  E0428 18:17:22.342648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:17:22.702: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 28 18:17:22.702: INFO: Node ip-172-31-1-213 is running 0 daemon pod, expected 1
  E0428 18:17:23.343688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:17:23.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
  Apr 28 18:17:23.690: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 04/28/23 18:17:23.692
  Apr 28 18:17:23.708: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 28 18:17:23.708: INFO: Node ip-172-31-12-59 is running 0 daemon pod, expected 1
  E0428 18:17:24.343973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:17:24.715: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 28 18:17:24.715: INFO: Node ip-172-31-12-59 is running 0 daemon pod, expected 1
  E0428 18:17:25.344742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:17:25.715: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
  Apr 28 18:17:25.715: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/28/23 18:17:25.719
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6389, will wait for the garbage collector to delete the pods @ 04/28/23 18:17:25.719
  Apr 28 18:17:25.778: INFO: Deleting DaemonSet.extensions daemon-set took: 6.44934ms
  Apr 28 18:17:25.881: INFO: Terminating DaemonSet.extensions daemon-set pods took: 103.411389ms
  E0428 18:17:26.345526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:27.345937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:17:27.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 18:17:27.685: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 28 18:17:27.687: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"286473"},"items":null}

  Apr 28 18:17:27.689: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"286473"},"items":null}

  Apr 28 18:17:27.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6389" for this suite. @ 04/28/23 18:17:27.702
• [6.234 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 04/28/23 18:17:27.708
  Apr 28 18:17:27.708: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename container-probe @ 04/28/23 18:17:27.709
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:17:27.72
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:17:27.723
  STEP: Creating pod busybox-abf7b033-3a32-4f27-ae3e-a1cc236bf6d1 in namespace container-probe-2616 @ 04/28/23 18:17:27.725
  E0428 18:17:28.345987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:29.346122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:17:29.743: INFO: Started pod busybox-abf7b033-3a32-4f27-ae3e-a1cc236bf6d1 in namespace container-probe-2616
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/28/23 18:17:29.743
  Apr 28 18:17:29.746: INFO: Initial restart count of pod busybox-abf7b033-3a32-4f27-ae3e-a1cc236bf6d1 is 0
  E0428 18:17:30.346293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:31.346431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:32.346566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:33.346689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:34.347612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:35.347811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:36.348389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:37.348630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:38.349087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:39.349210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:40.349308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:41.349497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:42.350217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:43.350456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:44.350669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:45.350969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:46.351111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:47.351272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:48.352364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:49.352907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:50.352997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:51.353180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:52.353283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:53.353541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:54.353741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:55.353954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:56.354046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:57.354255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:58.354392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:59.354874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:00.355262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:01.355479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:02.356171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:03.356374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:04.356487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:05.356739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:06.357455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:07.358263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:08.358664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:09.358782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:10.359202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:11.360135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:12.361160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:13.361340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:14.362286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:15.362489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:16.362616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:17.362817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:18.363850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:19.364358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:18:19.851: INFO: Restart count of pod container-probe-2616/busybox-abf7b033-3a32-4f27-ae3e-a1cc236bf6d1 is now 1 (50.10579727s elapsed)
  Apr 28 18:18:19.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 18:18:19.857
  STEP: Destroying namespace "container-probe-2616" for this suite. @ 04/28/23 18:18:19.874
• [52.185 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 04/28/23 18:18:19.893
  Apr 28 18:18:19.893: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename var-expansion @ 04/28/23 18:18:19.894
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:18:19.922
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:18:19.928
  STEP: Creating a pod to test substitution in container's command @ 04/28/23 18:18:19.933
  E0428 18:18:20.364632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:21.364847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 18:18:21.949
  Apr 28 18:18:21.952: INFO: Trying to get logs from node ip-172-31-1-213 pod var-expansion-4e99805a-a594-4a03-aab8-207a8f6ff981 container dapi-container: <nil>
  STEP: delete the pod @ 04/28/23 18:18:21.967
  Apr 28 18:18:21.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7130" for this suite. @ 04/28/23 18:18:21.993
• [2.112 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 04/28/23 18:18:22.005
  Apr 28 18:18:22.005: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename svc-latency @ 04/28/23 18:18:22.007
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:18:22.037
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:18:22.044
  Apr 28 18:18:22.048: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-5870 @ 04/28/23 18:18:22.05
  I0428 18:18:22.060895      19 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5870, replica count: 1
  E0428 18:18:22.364927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 18:18:23.111518      19 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0428 18:18:23.365912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 18:18:24.113312      19 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 28 18:18:24.226: INFO: Created: latency-svc-gjrw2
  Apr 28 18:18:24.241: INFO: Got endpoints: latency-svc-gjrw2 [27.52768ms]
  Apr 28 18:18:24.260: INFO: Created: latency-svc-mgc64
  Apr 28 18:18:24.271: INFO: Got endpoints: latency-svc-mgc64 [29.784406ms]
  Apr 28 18:18:24.274: INFO: Created: latency-svc-94p2j
  Apr 28 18:18:24.284: INFO: Got endpoints: latency-svc-94p2j [42.994042ms]
  Apr 28 18:18:24.289: INFO: Created: latency-svc-wm2t7
  Apr 28 18:18:24.296: INFO: Got endpoints: latency-svc-wm2t7 [55.045309ms]
  Apr 28 18:18:24.300: INFO: Created: latency-svc-ld58s
  Apr 28 18:18:24.309: INFO: Got endpoints: latency-svc-ld58s [66.733086ms]
  Apr 28 18:18:24.311: INFO: Created: latency-svc-rw99c
  Apr 28 18:18:24.317: INFO: Created: latency-svc-bgwx5
  Apr 28 18:18:24.320: INFO: Got endpoints: latency-svc-rw99c [78.330708ms]
  Apr 28 18:18:24.326: INFO: Got endpoints: latency-svc-bgwx5 [83.896642ms]
  Apr 28 18:18:24.334: INFO: Created: latency-svc-6xrpk
  Apr 28 18:18:24.339: INFO: Created: latency-svc-2jjx4
  Apr 28 18:18:24.342: INFO: Got endpoints: latency-svc-6xrpk [99.654445ms]
  Apr 28 18:18:24.350: INFO: Got endpoints: latency-svc-2jjx4 [103.842766ms]
  Apr 28 18:18:24.361: INFO: Created: latency-svc-4zk4z
  Apr 28 18:18:24.361: INFO: Got endpoints: latency-svc-4zk4z [114.884698ms]
  E0428 18:18:24.367006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:18:24.369: INFO: Created: latency-svc-q27kk
  Apr 28 18:18:24.377: INFO: Got endpoints: latency-svc-q27kk [130.531443ms]
  Apr 28 18:18:24.384: INFO: Created: latency-svc-9c4pp
  Apr 28 18:18:24.387: INFO: Got endpoints: latency-svc-9c4pp [140.457969ms]
  Apr 28 18:18:24.391: INFO: Created: latency-svc-95m4l
  Apr 28 18:18:24.396: INFO: Got endpoints: latency-svc-95m4l [149.64467ms]
  Apr 28 18:18:24.401: INFO: Created: latency-svc-qppzg
  Apr 28 18:18:24.408: INFO: Got endpoints: latency-svc-qppzg [161.50398ms]
  Apr 28 18:18:24.412: INFO: Created: latency-svc-v79bj
  Apr 28 18:18:24.424: INFO: Got endpoints: latency-svc-v79bj [177.008197ms]
  Apr 28 18:18:24.428: INFO: Created: latency-svc-sdlwz
  Apr 28 18:18:24.436: INFO: Got endpoints: latency-svc-sdlwz [188.791577ms]
  Apr 28 18:18:24.439: INFO: Created: latency-svc-xncc6
  Apr 28 18:18:24.444: INFO: Got endpoints: latency-svc-xncc6 [173.257364ms]
  Apr 28 18:18:24.449: INFO: Created: latency-svc-wl6pg
  Apr 28 18:18:24.456: INFO: Got endpoints: latency-svc-wl6pg [171.448493ms]
  Apr 28 18:18:24.458: INFO: Created: latency-svc-gvllq
  Apr 28 18:18:24.467: INFO: Got endpoints: latency-svc-gvllq [170.073048ms]
  Apr 28 18:18:24.469: INFO: Created: latency-svc-8x59p
  Apr 28 18:18:24.489: INFO: Got endpoints: latency-svc-8x59p [180.551078ms]
  Apr 28 18:18:24.491: INFO: Created: latency-svc-z8wmh
  Apr 28 18:18:24.498: INFO: Got endpoints: latency-svc-z8wmh [178.109002ms]
  Apr 28 18:18:24.506: INFO: Created: latency-svc-bf8bs
  Apr 28 18:18:24.516: INFO: Created: latency-svc-xrkxv
  Apr 28 18:18:24.517: INFO: Got endpoints: latency-svc-bf8bs [191.478836ms]
  Apr 28 18:18:24.524: INFO: Got endpoints: latency-svc-xrkxv [182.397344ms]
  Apr 28 18:18:24.529: INFO: Created: latency-svc-dgfkr
  Apr 28 18:18:24.537: INFO: Got endpoints: latency-svc-dgfkr [187.05593ms]
  Apr 28 18:18:24.548: INFO: Created: latency-svc-f8rbs
  Apr 28 18:18:24.561: INFO: Got endpoints: latency-svc-f8rbs [199.371772ms]
  Apr 28 18:18:24.568: INFO: Created: latency-svc-bsp7w
  Apr 28 18:18:24.580: INFO: Created: latency-svc-nfz8g
  Apr 28 18:18:24.587: INFO: Got endpoints: latency-svc-bsp7w [209.14038ms]
  Apr 28 18:18:24.591: INFO: Got endpoints: latency-svc-nfz8g [203.724877ms]
  Apr 28 18:18:24.595: INFO: Created: latency-svc-pr95w
  Apr 28 18:18:24.601: INFO: Got endpoints: latency-svc-pr95w [204.574901ms]
  Apr 28 18:18:24.606: INFO: Created: latency-svc-sdtx7
  Apr 28 18:18:24.618: INFO: Got endpoints: latency-svc-sdtx7 [210.03509ms]
  Apr 28 18:18:24.621: INFO: Created: latency-svc-drdjc
  Apr 28 18:18:24.627: INFO: Got endpoints: latency-svc-drdjc [203.907094ms]
  Apr 28 18:18:24.633: INFO: Created: latency-svc-6qrxf
  Apr 28 18:18:24.640: INFO: Got endpoints: latency-svc-6qrxf [204.561005ms]
  Apr 28 18:18:24.642: INFO: Created: latency-svc-2lk9f
  Apr 28 18:18:24.650: INFO: Got endpoints: latency-svc-2lk9f [205.540559ms]
  Apr 28 18:18:24.654: INFO: Created: latency-svc-wdxxn
  Apr 28 18:18:24.666: INFO: Got endpoints: latency-svc-wdxxn [209.670953ms]
  Apr 28 18:18:24.670: INFO: Created: latency-svc-dljpz
  Apr 28 18:18:24.676: INFO: Got endpoints: latency-svc-dljpz [209.526654ms]
  Apr 28 18:18:24.684: INFO: Created: latency-svc-gf2fn
  Apr 28 18:18:24.688: INFO: Got endpoints: latency-svc-gf2fn [199.016592ms]
  Apr 28 18:18:24.695: INFO: Created: latency-svc-w5cjk
  Apr 28 18:18:24.702: INFO: Got endpoints: latency-svc-w5cjk [204.039103ms]
  Apr 28 18:18:24.707: INFO: Created: latency-svc-jhmgd
  Apr 28 18:18:24.711: INFO: Got endpoints: latency-svc-jhmgd [194.078964ms]
  Apr 28 18:18:24.714: INFO: Created: latency-svc-n29mb
  Apr 28 18:18:24.724: INFO: Got endpoints: latency-svc-n29mb [200.361914ms]
  Apr 28 18:18:24.727: INFO: Created: latency-svc-4k8h4
  Apr 28 18:18:24.735: INFO: Got endpoints: latency-svc-4k8h4 [198.043591ms]
  Apr 28 18:18:24.741: INFO: Created: latency-svc-4plqv
  Apr 28 18:18:24.751: INFO: Got endpoints: latency-svc-4plqv [190.15886ms]
  Apr 28 18:18:24.755: INFO: Created: latency-svc-rm5qx
  Apr 28 18:18:24.763: INFO: Created: latency-svc-q5vb5
  Apr 28 18:18:24.770: INFO: Created: latency-svc-cspgt
  Apr 28 18:18:24.780: INFO: Created: latency-svc-clr9d
  Apr 28 18:18:24.784: INFO: Created: latency-svc-w6qbz
  Apr 28 18:18:24.790: INFO: Created: latency-svc-72pwx
  Apr 28 18:18:24.792: INFO: Got endpoints: latency-svc-rm5qx [205.032987ms]
  Apr 28 18:18:24.799: INFO: Created: latency-svc-27hgt
  Apr 28 18:18:24.842: INFO: Got endpoints: latency-svc-q5vb5 [250.823398ms]
  Apr 28 18:18:24.848: INFO: Created: latency-svc-dxwwn
  Apr 28 18:18:24.848: INFO: Created: latency-svc-77c26
  Apr 28 18:18:24.850: INFO: Created: latency-svc-sfkzq
  Apr 28 18:18:24.856: INFO: Created: latency-svc-xpvqr
  Apr 28 18:18:24.856: INFO: Created: latency-svc-g4lbc
  Apr 28 18:18:24.856: INFO: Created: latency-svc-mzbrt
  Apr 28 18:18:24.856: INFO: Created: latency-svc-xcpcq
  Apr 28 18:18:24.856: INFO: Created: latency-svc-b5kjd
  Apr 28 18:18:24.856: INFO: Created: latency-svc-zddmf
  Apr 28 18:18:24.867: INFO: Created: latency-svc-hkj7t
  Apr 28 18:18:24.891: INFO: Got endpoints: latency-svc-cspgt [290.032244ms]
  Apr 28 18:18:24.901: INFO: Created: latency-svc-6fcnw
  Apr 28 18:18:24.940: INFO: Got endpoints: latency-svc-clr9d [322.048536ms]
  Apr 28 18:18:24.951: INFO: Created: latency-svc-6cjfj
  Apr 28 18:18:24.991: INFO: Got endpoints: latency-svc-w6qbz [363.322571ms]
  Apr 28 18:18:25.004: INFO: Created: latency-svc-47s5z
  Apr 28 18:18:25.039: INFO: Got endpoints: latency-svc-72pwx [398.864863ms]
  Apr 28 18:18:25.051: INFO: Created: latency-svc-hkcrp
  Apr 28 18:18:25.089: INFO: Got endpoints: latency-svc-27hgt [439.608344ms]
  Apr 28 18:18:25.099: INFO: Created: latency-svc-nrxdl
  Apr 28 18:18:25.144: INFO: Got endpoints: latency-svc-dxwwn [408.541349ms]
  Apr 28 18:18:25.160: INFO: Created: latency-svc-fkg6k
  Apr 28 18:18:25.192: INFO: Got endpoints: latency-svc-77c26 [467.155243ms]
  Apr 28 18:18:25.201: INFO: Created: latency-svc-8tc28
  Apr 28 18:18:25.241: INFO: Got endpoints: latency-svc-sfkzq [489.808052ms]
  Apr 28 18:18:25.263: INFO: Created: latency-svc-6pmc4
  Apr 28 18:18:25.288: INFO: Got endpoints: latency-svc-zddmf [496.657417ms]
  Apr 28 18:18:25.298: INFO: Created: latency-svc-7mwkg
  Apr 28 18:18:25.340: INFO: Got endpoints: latency-svc-b5kjd [674.253402ms]
  Apr 28 18:18:25.349: INFO: Created: latency-svc-6r9zm
  E0428 18:18:25.367416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:18:25.391: INFO: Got endpoints: latency-svc-g4lbc [714.502458ms]
  Apr 28 18:18:25.402: INFO: Created: latency-svc-k7qsv
  Apr 28 18:18:25.439: INFO: Got endpoints: latency-svc-xcpcq [750.468721ms]
  Apr 28 18:18:25.447: INFO: Created: latency-svc-dntrq
  Apr 28 18:18:25.490: INFO: Got endpoints: latency-svc-xpvqr [787.909521ms]
  Apr 28 18:18:25.499: INFO: Created: latency-svc-grkqj
  Apr 28 18:18:25.540: INFO: Got endpoints: latency-svc-mzbrt [828.841625ms]
  Apr 28 18:18:25.548: INFO: Created: latency-svc-xl9hv
  Apr 28 18:18:25.590: INFO: Got endpoints: latency-svc-hkj7t [748.236113ms]
  Apr 28 18:18:25.598: INFO: Created: latency-svc-4cxbg
  Apr 28 18:18:25.639: INFO: Got endpoints: latency-svc-6fcnw [747.471526ms]
  Apr 28 18:18:25.647: INFO: Created: latency-svc-ql9mc
  Apr 28 18:18:25.690: INFO: Got endpoints: latency-svc-6cjfj [749.906656ms]
  Apr 28 18:18:25.698: INFO: Created: latency-svc-dtrmd
  Apr 28 18:18:25.739: INFO: Got endpoints: latency-svc-47s5z [748.348165ms]
  Apr 28 18:18:25.747: INFO: Created: latency-svc-6bchx
  Apr 28 18:18:25.790: INFO: Got endpoints: latency-svc-hkcrp [751.160713ms]
  Apr 28 18:18:25.799: INFO: Created: latency-svc-6g2kr
  Apr 28 18:18:25.840: INFO: Got endpoints: latency-svc-nrxdl [750.871507ms]
  Apr 28 18:18:25.851: INFO: Created: latency-svc-w4c6r
  Apr 28 18:18:25.890: INFO: Got endpoints: latency-svc-fkg6k [745.810382ms]
  Apr 28 18:18:25.900: INFO: Created: latency-svc-8p6x2
  Apr 28 18:18:25.939: INFO: Got endpoints: latency-svc-8tc28 [747.767636ms]
  Apr 28 18:18:25.951: INFO: Created: latency-svc-x7tkt
  Apr 28 18:18:25.990: INFO: Got endpoints: latency-svc-6pmc4 [749.360066ms]
  Apr 28 18:18:26.000: INFO: Created: latency-svc-c4kzc
  Apr 28 18:18:26.040: INFO: Got endpoints: latency-svc-7mwkg [751.797739ms]
  Apr 28 18:18:26.051: INFO: Created: latency-svc-645v5
  Apr 28 18:18:26.091: INFO: Got endpoints: latency-svc-6r9zm [750.91908ms]
  Apr 28 18:18:26.099: INFO: Created: latency-svc-kxfxb
  Apr 28 18:18:26.140: INFO: Got endpoints: latency-svc-k7qsv [749.203049ms]
  Apr 28 18:18:26.149: INFO: Created: latency-svc-zrwvc
  Apr 28 18:18:26.189: INFO: Got endpoints: latency-svc-dntrq [750.452562ms]
  Apr 28 18:18:26.197: INFO: Created: latency-svc-2jbhw
  Apr 28 18:18:26.240: INFO: Got endpoints: latency-svc-grkqj [749.361525ms]
  Apr 28 18:18:26.254: INFO: Created: latency-svc-fwl58
  Apr 28 18:18:26.289: INFO: Got endpoints: latency-svc-xl9hv [748.318113ms]
  Apr 28 18:18:26.297: INFO: Created: latency-svc-sbzgh
  Apr 28 18:18:26.340: INFO: Got endpoints: latency-svc-4cxbg [749.821756ms]
  Apr 28 18:18:26.348: INFO: Created: latency-svc-n727c
  E0428 18:18:26.368004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:18:26.389: INFO: Got endpoints: latency-svc-ql9mc [750.590702ms]
  Apr 28 18:18:26.398: INFO: Created: latency-svc-kg8m8
  Apr 28 18:18:26.440: INFO: Got endpoints: latency-svc-dtrmd [750.122241ms]
  Apr 28 18:18:26.450: INFO: Created: latency-svc-rhtrg
  Apr 28 18:18:26.496: INFO: Got endpoints: latency-svc-6bchx [756.287683ms]
  Apr 28 18:18:26.505: INFO: Created: latency-svc-7rskv
  Apr 28 18:18:26.546: INFO: Got endpoints: latency-svc-6g2kr [755.746414ms]
  Apr 28 18:18:26.556: INFO: Created: latency-svc-5mczf
  Apr 28 18:18:26.590: INFO: Got endpoints: latency-svc-w4c6r [749.298074ms]
  Apr 28 18:18:26.598: INFO: Created: latency-svc-f22c8
  Apr 28 18:18:26.641: INFO: Got endpoints: latency-svc-8p6x2 [750.996661ms]
  Apr 28 18:18:26.650: INFO: Created: latency-svc-pkd67
  Apr 28 18:18:26.692: INFO: Got endpoints: latency-svc-x7tkt [752.113923ms]
  Apr 28 18:18:26.702: INFO: Created: latency-svc-fd9c6
  Apr 28 18:18:26.741: INFO: Got endpoints: latency-svc-c4kzc [750.202109ms]
  Apr 28 18:18:26.751: INFO: Created: latency-svc-c449g
  Apr 28 18:18:26.788: INFO: Got endpoints: latency-svc-645v5 [747.593642ms]
  Apr 28 18:18:26.796: INFO: Created: latency-svc-7zb2m
  Apr 28 18:18:26.841: INFO: Got endpoints: latency-svc-kxfxb [749.853765ms]
  Apr 28 18:18:26.850: INFO: Created: latency-svc-z6v8j
  Apr 28 18:18:26.891: INFO: Got endpoints: latency-svc-zrwvc [751.350395ms]
  Apr 28 18:18:26.904: INFO: Created: latency-svc-pq9j7
  Apr 28 18:18:26.940: INFO: Got endpoints: latency-svc-2jbhw [750.837406ms]
  Apr 28 18:18:26.957: INFO: Created: latency-svc-lbd8h
  Apr 28 18:18:26.989: INFO: Got endpoints: latency-svc-fwl58 [749.310876ms]
  Apr 28 18:18:26.999: INFO: Created: latency-svc-l47mp
  Apr 28 18:18:27.041: INFO: Got endpoints: latency-svc-sbzgh [752.703338ms]
  Apr 28 18:18:27.053: INFO: Created: latency-svc-8gbwd
  Apr 28 18:18:27.089: INFO: Got endpoints: latency-svc-n727c [748.787086ms]
  Apr 28 18:18:27.099: INFO: Created: latency-svc-nplbp
  Apr 28 18:18:27.140: INFO: Got endpoints: latency-svc-kg8m8 [750.524539ms]
  Apr 28 18:18:27.150: INFO: Created: latency-svc-h468h
  Apr 28 18:18:27.192: INFO: Got endpoints: latency-svc-rhtrg [751.089582ms]
  Apr 28 18:18:27.214: INFO: Created: latency-svc-zrb4b
  Apr 28 18:18:27.239: INFO: Got endpoints: latency-svc-7rskv [742.912517ms]
  Apr 28 18:18:27.256: INFO: Created: latency-svc-98lqj
  Apr 28 18:18:27.289: INFO: Got endpoints: latency-svc-5mczf [742.608039ms]
  Apr 28 18:18:27.302: INFO: Created: latency-svc-dczj8
  Apr 28 18:18:27.340: INFO: Got endpoints: latency-svc-f22c8 [750.32594ms]
  Apr 28 18:18:27.349: INFO: Created: latency-svc-6lq9q
  E0428 18:18:27.368493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:18:27.391: INFO: Got endpoints: latency-svc-pkd67 [749.702372ms]
  Apr 28 18:18:27.399: INFO: Created: latency-svc-mtwbb
  Apr 28 18:18:27.439: INFO: Got endpoints: latency-svc-fd9c6 [747.687068ms]
  Apr 28 18:18:27.450: INFO: Created: latency-svc-wmjsl
  Apr 28 18:18:27.489: INFO: Got endpoints: latency-svc-c449g [747.751143ms]
  Apr 28 18:18:27.498: INFO: Created: latency-svc-hffvt
  Apr 28 18:18:27.539: INFO: Got endpoints: latency-svc-7zb2m [751.492682ms]
  Apr 28 18:18:27.549: INFO: Created: latency-svc-26p96
  Apr 28 18:18:27.591: INFO: Got endpoints: latency-svc-z6v8j [749.980221ms]
  Apr 28 18:18:27.599: INFO: Created: latency-svc-hcsbj
  Apr 28 18:18:27.639: INFO: Got endpoints: latency-svc-pq9j7 [747.649008ms]
  Apr 28 18:18:27.649: INFO: Created: latency-svc-gfbz5
  Apr 28 18:18:27.716: INFO: Got endpoints: latency-svc-lbd8h [775.434546ms]
  Apr 28 18:18:27.773: INFO: Got endpoints: latency-svc-l47mp [783.932249ms]
  Apr 28 18:18:27.812: INFO: Got endpoints: latency-svc-8gbwd [770.342848ms]
  Apr 28 18:18:27.834: INFO: Created: latency-svc-2wh88
  Apr 28 18:18:27.863: INFO: Created: latency-svc-mvm8r
  Apr 28 18:18:27.878: INFO: Got endpoints: latency-svc-nplbp [789.473989ms]
  Apr 28 18:18:27.909: INFO: Created: latency-svc-ldt6x
  Apr 28 18:18:27.914: INFO: Got endpoints: latency-svc-h468h [773.83076ms]
  Apr 28 18:18:27.927: INFO: Created: latency-svc-fjqpd
  Apr 28 18:18:27.935: INFO: Created: latency-svc-ztgvc
  Apr 28 18:18:27.941: INFO: Got endpoints: latency-svc-zrb4b [748.722395ms]
  Apr 28 18:18:27.949: INFO: Created: latency-svc-m7h5b
  Apr 28 18:18:27.990: INFO: Got endpoints: latency-svc-98lqj [751.44719ms]
  Apr 28 18:18:27.999: INFO: Created: latency-svc-jsm5w
  Apr 28 18:18:28.041: INFO: Got endpoints: latency-svc-dczj8 [751.864715ms]
  Apr 28 18:18:28.050: INFO: Created: latency-svc-849hg
  Apr 28 18:18:28.090: INFO: Got endpoints: latency-svc-6lq9q [749.411082ms]
  Apr 28 18:18:28.099: INFO: Created: latency-svc-7mbdk
  Apr 28 18:18:28.140: INFO: Got endpoints: latency-svc-mtwbb [748.933156ms]
  Apr 28 18:18:28.147: INFO: Created: latency-svc-b4pc6
  Apr 28 18:18:28.191: INFO: Got endpoints: latency-svc-wmjsl [751.631984ms]
  Apr 28 18:18:28.200: INFO: Created: latency-svc-t2bsk
  Apr 28 18:18:28.240: INFO: Got endpoints: latency-svc-hffvt [750.988773ms]
  Apr 28 18:18:28.253: INFO: Created: latency-svc-srrv9
  Apr 28 18:18:28.291: INFO: Got endpoints: latency-svc-26p96 [751.382026ms]
  Apr 28 18:18:28.300: INFO: Created: latency-svc-rhx7j
  Apr 28 18:18:28.340: INFO: Got endpoints: latency-svc-hcsbj [749.166806ms]
  Apr 28 18:18:28.348: INFO: Created: latency-svc-ppl6l
  E0428 18:18:28.369155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:18:28.390: INFO: Got endpoints: latency-svc-gfbz5 [750.553886ms]
  Apr 28 18:18:28.399: INFO: Created: latency-svc-dm8lr
  Apr 28 18:18:28.440: INFO: Got endpoints: latency-svc-2wh88 [723.904194ms]
  Apr 28 18:18:28.451: INFO: Created: latency-svc-tljdq
  Apr 28 18:18:28.490: INFO: Got endpoints: latency-svc-mvm8r [717.267449ms]
  Apr 28 18:18:28.498: INFO: Created: latency-svc-6g9tv
  Apr 28 18:18:28.543: INFO: Got endpoints: latency-svc-ldt6x [730.799395ms]
  Apr 28 18:18:28.551: INFO: Created: latency-svc-tv8x6
  Apr 28 18:18:28.589: INFO: Got endpoints: latency-svc-fjqpd [710.586093ms]
  Apr 28 18:18:28.598: INFO: Created: latency-svc-tfgtr
  Apr 28 18:18:28.640: INFO: Got endpoints: latency-svc-ztgvc [725.971621ms]
  Apr 28 18:18:28.648: INFO: Created: latency-svc-nrg5b
  Apr 28 18:18:28.689: INFO: Got endpoints: latency-svc-m7h5b [748.392075ms]
  Apr 28 18:18:28.702: INFO: Created: latency-svc-4pbvw
  Apr 28 18:18:28.740: INFO: Got endpoints: latency-svc-jsm5w [749.484736ms]
  Apr 28 18:18:28.749: INFO: Created: latency-svc-m5hwq
  Apr 28 18:18:28.794: INFO: Got endpoints: latency-svc-849hg [752.539527ms]
  Apr 28 18:18:28.806: INFO: Created: latency-svc-7567w
  Apr 28 18:18:28.840: INFO: Got endpoints: latency-svc-7mbdk [750.415879ms]
  Apr 28 18:18:28.849: INFO: Created: latency-svc-4bl6n
  Apr 28 18:18:28.890: INFO: Got endpoints: latency-svc-b4pc6 [750.104533ms]
  Apr 28 18:18:28.898: INFO: Created: latency-svc-6lvqr
  Apr 28 18:18:28.941: INFO: Got endpoints: latency-svc-t2bsk [750.464188ms]
  Apr 28 18:18:28.950: INFO: Created: latency-svc-65kx2
  Apr 28 18:18:28.990: INFO: Got endpoints: latency-svc-srrv9 [749.876558ms]
  Apr 28 18:18:28.999: INFO: Created: latency-svc-4rkdd
  Apr 28 18:18:29.040: INFO: Got endpoints: latency-svc-rhx7j [748.609522ms]
  Apr 28 18:18:29.050: INFO: Created: latency-svc-75wdl
  Apr 28 18:18:29.089: INFO: Got endpoints: latency-svc-ppl6l [748.613155ms]
  Apr 28 18:18:29.099: INFO: Created: latency-svc-ms688
  Apr 28 18:18:29.138: INFO: Got endpoints: latency-svc-dm8lr [748.757133ms]
  Apr 28 18:18:29.146: INFO: Created: latency-svc-fmklx
  Apr 28 18:18:29.190: INFO: Got endpoints: latency-svc-tljdq [750.169295ms]
  Apr 28 18:18:29.199: INFO: Created: latency-svc-jfj4z
  Apr 28 18:18:29.240: INFO: Got endpoints: latency-svc-6g9tv [749.313201ms]
  Apr 28 18:18:29.255: INFO: Created: latency-svc-jbd9w
  Apr 28 18:18:29.290: INFO: Got endpoints: latency-svc-tv8x6 [746.973547ms]
  Apr 28 18:18:29.299: INFO: Created: latency-svc-h6hpn
  Apr 28 18:18:29.340: INFO: Got endpoints: latency-svc-tfgtr [750.540927ms]
  Apr 28 18:18:29.350: INFO: Created: latency-svc-s2nf9
  E0428 18:18:29.369655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:18:29.390: INFO: Got endpoints: latency-svc-nrg5b [749.645242ms]
  Apr 28 18:18:29.397: INFO: Created: latency-svc-cmwgq
  Apr 28 18:18:29.440: INFO: Got endpoints: latency-svc-4pbvw [750.420065ms]
  Apr 28 18:18:29.450: INFO: Created: latency-svc-pvnb2
  Apr 28 18:18:29.490: INFO: Got endpoints: latency-svc-m5hwq [749.873344ms]
  Apr 28 18:18:29.499: INFO: Created: latency-svc-k88nz
  Apr 28 18:18:29.539: INFO: Got endpoints: latency-svc-7567w [745.562814ms]
  Apr 28 18:18:29.549: INFO: Created: latency-svc-w5hjj
  Apr 28 18:18:29.591: INFO: Got endpoints: latency-svc-4bl6n [751.012635ms]
  Apr 28 18:18:29.598: INFO: Created: latency-svc-f4drh
  Apr 28 18:18:29.641: INFO: Got endpoints: latency-svc-6lvqr [750.745566ms]
  Apr 28 18:18:29.648: INFO: Created: latency-svc-2lkmk
  Apr 28 18:18:29.690: INFO: Got endpoints: latency-svc-65kx2 [748.164277ms]
  Apr 28 18:18:29.699: INFO: Created: latency-svc-twbjl
  Apr 28 18:18:29.743: INFO: Got endpoints: latency-svc-4rkdd [752.877436ms]
  Apr 28 18:18:29.754: INFO: Created: latency-svc-w5d5q
  Apr 28 18:18:29.791: INFO: Got endpoints: latency-svc-75wdl [750.978971ms]
  Apr 28 18:18:29.800: INFO: Created: latency-svc-lvd4z
  Apr 28 18:18:29.844: INFO: Got endpoints: latency-svc-ms688 [755.703011ms]
  Apr 28 18:18:29.855: INFO: Created: latency-svc-7p8r9
  Apr 28 18:18:29.890: INFO: Got endpoints: latency-svc-fmklx [751.489131ms]
  Apr 28 18:18:29.899: INFO: Created: latency-svc-mh4mz
  Apr 28 18:18:29.942: INFO: Got endpoints: latency-svc-jfj4z [752.215433ms]
  Apr 28 18:18:29.962: INFO: Created: latency-svc-jdfkl
  Apr 28 18:18:29.990: INFO: Got endpoints: latency-svc-jbd9w [750.527882ms]
  Apr 28 18:18:30.000: INFO: Created: latency-svc-8bq8d
  Apr 28 18:18:30.042: INFO: Got endpoints: latency-svc-h6hpn [751.982546ms]
  Apr 28 18:18:30.051: INFO: Created: latency-svc-kq28w
  Apr 28 18:18:30.089: INFO: Got endpoints: latency-svc-s2nf9 [749.235015ms]
  Apr 28 18:18:30.098: INFO: Created: latency-svc-7mnhf
  Apr 28 18:18:30.140: INFO: Got endpoints: latency-svc-cmwgq [750.084791ms]
  Apr 28 18:18:30.149: INFO: Created: latency-svc-pm8hp
  Apr 28 18:18:30.191: INFO: Got endpoints: latency-svc-pvnb2 [750.333124ms]
  Apr 28 18:18:30.200: INFO: Created: latency-svc-5qfz5
  Apr 28 18:18:30.244: INFO: Got endpoints: latency-svc-k88nz [753.801441ms]
  Apr 28 18:18:30.274: INFO: Created: latency-svc-54sdd
  Apr 28 18:18:30.290: INFO: Got endpoints: latency-svc-w5hjj [751.224626ms]
  Apr 28 18:18:30.300: INFO: Created: latency-svc-gljfs
  Apr 28 18:18:30.340: INFO: Got endpoints: latency-svc-f4drh [748.807012ms]
  Apr 28 18:18:30.348: INFO: Created: latency-svc-8sbv5
  E0428 18:18:30.370521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:18:30.390: INFO: Got endpoints: latency-svc-2lkmk [748.464532ms]
  Apr 28 18:18:30.409: INFO: Created: latency-svc-ftlbg
  Apr 28 18:18:30.439: INFO: Got endpoints: latency-svc-twbjl [748.98815ms]
  Apr 28 18:18:30.447: INFO: Created: latency-svc-vx6mx
  Apr 28 18:18:30.489: INFO: Got endpoints: latency-svc-w5d5q [745.839628ms]
  Apr 28 18:18:30.498: INFO: Created: latency-svc-69x4k
  Apr 28 18:18:30.540: INFO: Got endpoints: latency-svc-lvd4z [749.872072ms]
  Apr 28 18:18:30.548: INFO: Created: latency-svc-mv82b
  Apr 28 18:18:30.590: INFO: Got endpoints: latency-svc-7p8r9 [744.841277ms]
  Apr 28 18:18:30.599: INFO: Created: latency-svc-p4wqn
  Apr 28 18:18:30.639: INFO: Got endpoints: latency-svc-mh4mz [749.302472ms]
  Apr 28 18:18:30.648: INFO: Created: latency-svc-tzp7g
  Apr 28 18:18:30.695: INFO: Got endpoints: latency-svc-jdfkl [752.372992ms]
  Apr 28 18:18:30.704: INFO: Created: latency-svc-hk6dv
  Apr 28 18:18:30.741: INFO: Got endpoints: latency-svc-8bq8d [750.402338ms]
  Apr 28 18:18:30.749: INFO: Created: latency-svc-2c8xt
  Apr 28 18:18:30.791: INFO: Got endpoints: latency-svc-kq28w [749.059366ms]
  Apr 28 18:18:30.800: INFO: Created: latency-svc-8m8lb
  Apr 28 18:18:30.841: INFO: Got endpoints: latency-svc-7mnhf [751.594543ms]
  Apr 28 18:18:30.849: INFO: Created: latency-svc-7fnj4
  Apr 28 18:18:30.896: INFO: Got endpoints: latency-svc-pm8hp [756.131707ms]
  Apr 28 18:18:30.912: INFO: Created: latency-svc-4bpfb
  Apr 28 18:18:30.941: INFO: Got endpoints: latency-svc-5qfz5 [749.865183ms]
  Apr 28 18:18:30.949: INFO: Created: latency-svc-494wd
  Apr 28 18:18:30.992: INFO: Got endpoints: latency-svc-54sdd [748.045596ms]
  Apr 28 18:18:31.001: INFO: Created: latency-svc-ltklv
  Apr 28 18:18:31.041: INFO: Got endpoints: latency-svc-gljfs [750.394601ms]
  Apr 28 18:18:31.052: INFO: Created: latency-svc-vw294
  Apr 28 18:18:31.090: INFO: Got endpoints: latency-svc-8sbv5 [749.608235ms]
  Apr 28 18:18:31.099: INFO: Created: latency-svc-l7qlv
  Apr 28 18:18:31.142: INFO: Got endpoints: latency-svc-ftlbg [751.842155ms]
  Apr 28 18:18:31.149: INFO: Created: latency-svc-gmxrb
  Apr 28 18:18:31.189: INFO: Got endpoints: latency-svc-vx6mx [750.150604ms]
  Apr 28 18:18:31.197: INFO: Created: latency-svc-4gmt8
  Apr 28 18:18:31.239: INFO: Got endpoints: latency-svc-69x4k [750.304157ms]
  Apr 28 18:18:31.251: INFO: Created: latency-svc-bztlq
  Apr 28 18:18:31.294: INFO: Got endpoints: latency-svc-mv82b [753.561159ms]
  Apr 28 18:18:31.312: INFO: Created: latency-svc-hfhcl
  Apr 28 18:18:31.342: INFO: Got endpoints: latency-svc-p4wqn [752.817033ms]
  Apr 28 18:18:31.352: INFO: Created: latency-svc-z89q8
  E0428 18:18:31.371288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:18:31.392: INFO: Got endpoints: latency-svc-tzp7g [752.090041ms]
  Apr 28 18:18:31.400: INFO: Created: latency-svc-f8fkq
  Apr 28 18:18:31.438: INFO: Got endpoints: latency-svc-hk6dv [743.660695ms]
  Apr 28 18:18:31.448: INFO: Created: latency-svc-w8brs
  Apr 28 18:18:31.489: INFO: Got endpoints: latency-svc-2c8xt [748.749284ms]
  Apr 28 18:18:31.499: INFO: Created: latency-svc-5lsp8
  Apr 28 18:18:31.540: INFO: Got endpoints: latency-svc-8m8lb [749.055417ms]
  Apr 28 18:18:31.548: INFO: Created: latency-svc-m7sjn
  Apr 28 18:18:31.590: INFO: Got endpoints: latency-svc-7fnj4 [748.920575ms]
  Apr 28 18:18:31.601: INFO: Created: latency-svc-p9c2v
  Apr 28 18:18:31.639: INFO: Got endpoints: latency-svc-4bpfb [742.759365ms]
  Apr 28 18:18:31.647: INFO: Created: latency-svc-hn9mj
  Apr 28 18:18:31.691: INFO: Got endpoints: latency-svc-494wd [749.973932ms]
  Apr 28 18:18:31.699: INFO: Created: latency-svc-n4ngl
  Apr 28 18:18:31.740: INFO: Got endpoints: latency-svc-ltklv [748.185193ms]
  Apr 28 18:18:31.749: INFO: Created: latency-svc-rc76p
  Apr 28 18:18:31.790: INFO: Got endpoints: latency-svc-vw294 [749.029514ms]
  Apr 28 18:18:31.797: INFO: Created: latency-svc-bnqsz
  Apr 28 18:18:31.840: INFO: Got endpoints: latency-svc-l7qlv [750.099362ms]
  Apr 28 18:18:31.850: INFO: Created: latency-svc-s2xqg
  Apr 28 18:18:31.890: INFO: Got endpoints: latency-svc-gmxrb [748.578546ms]
  Apr 28 18:18:31.903: INFO: Created: latency-svc-w449p
  Apr 28 18:18:31.941: INFO: Got endpoints: latency-svc-4gmt8 [751.966662ms]
  Apr 28 18:18:31.950: INFO: Created: latency-svc-s7hp4
  Apr 28 18:18:31.992: INFO: Got endpoints: latency-svc-bztlq [752.505353ms]
  Apr 28 18:18:32.004: INFO: Created: latency-svc-bftdz
  Apr 28 18:18:32.041: INFO: Got endpoints: latency-svc-hfhcl [746.59286ms]
  Apr 28 18:18:32.049: INFO: Created: latency-svc-6nfcs
  Apr 28 18:18:32.090: INFO: Got endpoints: latency-svc-z89q8 [747.028864ms]
  Apr 28 18:18:32.142: INFO: Got endpoints: latency-svc-f8fkq [750.367313ms]
  Apr 28 18:18:32.192: INFO: Got endpoints: latency-svc-w8brs [753.362819ms]
  Apr 28 18:18:32.240: INFO: Got endpoints: latency-svc-5lsp8 [750.575493ms]
  Apr 28 18:18:32.290: INFO: Got endpoints: latency-svc-m7sjn [749.831065ms]
  Apr 28 18:18:32.341: INFO: Got endpoints: latency-svc-p9c2v [751.149241ms]
  E0428 18:18:32.371960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:18:32.391: INFO: Got endpoints: latency-svc-hn9mj [751.333144ms]
  Apr 28 18:18:32.440: INFO: Got endpoints: latency-svc-n4ngl [749.55563ms]
  Apr 28 18:18:32.488: INFO: Got endpoints: latency-svc-rc76p [747.750331ms]
  Apr 28 18:18:32.539: INFO: Got endpoints: latency-svc-bnqsz [748.897158ms]
  Apr 28 18:18:32.590: INFO: Got endpoints: latency-svc-s2xqg [750.561581ms]
  Apr 28 18:18:32.641: INFO: Got endpoints: latency-svc-w449p [750.440233ms]
  Apr 28 18:18:32.689: INFO: Got endpoints: latency-svc-s7hp4 [747.398352ms]
  Apr 28 18:18:32.740: INFO: Got endpoints: latency-svc-bftdz [748.198383ms]
  Apr 28 18:18:32.798: INFO: Got endpoints: latency-svc-6nfcs [757.347885ms]
  Apr 28 18:18:32.798: INFO: Latencies: [29.784406ms 42.994042ms 55.045309ms 66.733086ms 78.330708ms 83.896642ms 99.654445ms 103.842766ms 114.884698ms 130.531443ms 140.457969ms 149.64467ms 161.50398ms 170.073048ms 171.448493ms 173.257364ms 177.008197ms 178.109002ms 180.551078ms 182.397344ms 187.05593ms 188.791577ms 190.15886ms 191.478836ms 194.078964ms 198.043591ms 199.016592ms 199.371772ms 200.361914ms 203.724877ms 203.907094ms 204.039103ms 204.561005ms 204.574901ms 205.032987ms 205.540559ms 209.14038ms 209.526654ms 209.670953ms 210.03509ms 250.823398ms 290.032244ms 322.048536ms 363.322571ms 398.864863ms 408.541349ms 439.608344ms 467.155243ms 489.808052ms 496.657417ms 674.253402ms 710.586093ms 714.502458ms 717.267449ms 723.904194ms 725.971621ms 730.799395ms 742.608039ms 742.759365ms 742.912517ms 743.660695ms 744.841277ms 745.562814ms 745.810382ms 745.839628ms 746.59286ms 746.973547ms 747.028864ms 747.398352ms 747.471526ms 747.593642ms 747.649008ms 747.687068ms 747.750331ms 747.751143ms 747.767636ms 748.045596ms 748.164277ms 748.185193ms 748.198383ms 748.236113ms 748.318113ms 748.348165ms 748.392075ms 748.464532ms 748.578546ms 748.609522ms 748.613155ms 748.722395ms 748.749284ms 748.757133ms 748.787086ms 748.807012ms 748.897158ms 748.920575ms 748.933156ms 748.98815ms 749.029514ms 749.055417ms 749.059366ms 749.166806ms 749.203049ms 749.235015ms 749.298074ms 749.302472ms 749.310876ms 749.313201ms 749.360066ms 749.361525ms 749.411082ms 749.484736ms 749.55563ms 749.608235ms 749.645242ms 749.702372ms 749.821756ms 749.831065ms 749.853765ms 749.865183ms 749.872072ms 749.873344ms 749.876558ms 749.906656ms 749.973932ms 749.980221ms 750.084791ms 750.099362ms 750.104533ms 750.122241ms 750.150604ms 750.169295ms 750.202109ms 750.304157ms 750.32594ms 750.333124ms 750.367313ms 750.394601ms 750.402338ms 750.415879ms 750.420065ms 750.440233ms 750.452562ms 750.464188ms 750.468721ms 750.524539ms 750.527882ms 750.540927ms 750.553886ms 750.561581ms 750.575493ms 750.590702ms 750.745566ms 750.837406ms 750.871507ms 750.91908ms 750.978971ms 750.988773ms 750.996661ms 751.012635ms 751.089582ms 751.149241ms 751.160713ms 751.224626ms 751.333144ms 751.350395ms 751.382026ms 751.44719ms 751.489131ms 751.492682ms 751.594543ms 751.631984ms 751.797739ms 751.842155ms 751.864715ms 751.966662ms 751.982546ms 752.090041ms 752.113923ms 752.215433ms 752.372992ms 752.505353ms 752.539527ms 752.703338ms 752.817033ms 752.877436ms 753.362819ms 753.561159ms 753.801441ms 755.703011ms 755.746414ms 756.131707ms 756.287683ms 757.347885ms 770.342848ms 773.83076ms 775.434546ms 783.932249ms 787.909521ms 789.473989ms 828.841625ms]
  Apr 28 18:18:32.798: INFO: 50 %ile: 749.166806ms
  Apr 28 18:18:32.798: INFO: 90 %ile: 752.505353ms
  Apr 28 18:18:32.798: INFO: 99 %ile: 789.473989ms
  Apr 28 18:18:32.798: INFO: Total sample count: 200
  Apr 28 18:18:32.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-5870" for this suite. @ 04/28/23 18:18:32.803
• [10.805 seconds]
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 04/28/23 18:18:32.81
  Apr 28 18:18:32.810: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/28/23 18:18:32.811
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:18:32.828
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:18:32.837
  STEP: create the container to handle the HTTPGet hook request. @ 04/28/23 18:18:32.851
  E0428 18:18:33.372047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:34.372447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 04/28/23 18:18:34.874
  E0428 18:18:35.373065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:36.373173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 04/28/23 18:18:36.891
  STEP: delete the pod with lifecycle hook @ 04/28/23 18:18:36.914
  E0428 18:18:37.373897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:38.373909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:18:38.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-8008" for this suite. @ 04/28/23 18:18:38.956
• [6.155 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 04/28/23 18:18:38.966
  Apr 28 18:18:38.966: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename webhook @ 04/28/23 18:18:38.967
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:18:39.018
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:18:39.028
  STEP: Setting up server cert @ 04/28/23 18:18:39.085
  E0428 18:18:39.377711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 18:18:40.006
  STEP: Deploying the webhook pod @ 04/28/23 18:18:40.014
  STEP: Wait for the deployment to be ready @ 04/28/23 18:18:40.025
  Apr 28 18:18:40.042: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0428 18:18:40.378568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:41.378756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:18:42.053: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 18, 18, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 18, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 18, 18, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 18, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 18:18:42.378891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:43.379025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:18:44.058: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 18, 18, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 18, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 18, 18, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 18, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 18:18:44.380063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:45.380267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:18:46.056: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 18, 18, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 18, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 18, 18, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 18, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 18:18:46.381269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:47.381402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:18:48.056: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 18, 18, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 18, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 18, 18, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 18, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 18:18:48.381986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:49.382641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:18:50.057: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 18, 18, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 18, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 18, 18, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 18, 18, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 18:18:50.383434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:51.383506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 18:18:52.057
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 18:18:52.067
  E0428 18:18:52.384224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:18:53.068: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 04/28/23 18:18:53.073
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 04/28/23 18:18:53.101
  STEP: Creating a configMap that should not be mutated @ 04/28/23 18:18:53.106
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 04/28/23 18:18:53.114
  STEP: Creating a configMap that should be mutated @ 04/28/23 18:18:53.121
  Apr 28 18:18:53.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7844" for this suite. @ 04/28/23 18:18:53.217
  STEP: Destroying namespace "webhook-markers-213" for this suite. @ 04/28/23 18:18:53.228
• [14.271 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 04/28/23 18:18:53.24
  Apr 28 18:18:53.240: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename crd-webhook @ 04/28/23 18:18:53.241
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:18:53.255
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:18:53.258
  STEP: Setting up server cert @ 04/28/23 18:18:53.261
  E0428 18:18:53.384635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:54.384906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 04/28/23 18:18:54.406
  STEP: Deploying the custom resource conversion webhook pod @ 04/28/23 18:18:54.41
  STEP: Wait for the deployment to be ready @ 04/28/23 18:18:54.421
  Apr 28 18:18:54.429: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0428 18:18:55.385168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:56.385304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 18:18:56.452
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 18:18:56.475
  E0428 18:18:57.385478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:18:57.475: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Apr 28 18:18:57.478: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 18:18:58.386480      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:18:59.387063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 04/28/23 18:19:00.105
  STEP: Create a v2 custom resource @ 04/28/23 18:19:00.135
  STEP: List CRs in v1 @ 04/28/23 18:19:00.236
  STEP: List CRs in v2 @ 04/28/23 18:19:00.243
  Apr 28 18:19:00.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0428 18:19:00.387857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-3187" for this suite. @ 04/28/23 18:19:00.882
• [7.662 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 04/28/23 18:19:00.904
  Apr 28 18:19:00.904: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/28/23 18:19:00.904
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:19:00.989
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:19:00.996
  Apr 28 18:19:01.009: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  E0428 18:19:01.388645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:19:02.389138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 04/28/23 18:19:03.149
  Apr 28 18:19:03.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-6211 --namespace=crd-publish-openapi-6211 create -f -'
  E0428 18:19:03.389718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:19:04.218: INFO: stderr: ""
  Apr 28 18:19:04.218: INFO: stdout: "e2e-test-crd-publish-openapi-4527-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Apr 28 18:19:04.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-6211 --namespace=crd-publish-openapi-6211 delete e2e-test-crd-publish-openapi-4527-crds test-foo'
  Apr 28 18:19:04.330: INFO: stderr: ""
  Apr 28 18:19:04.330: INFO: stdout: "e2e-test-crd-publish-openapi-4527-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Apr 28 18:19:04.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-6211 --namespace=crd-publish-openapi-6211 apply -f -'
  E0428 18:19:04.390232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:19:05.192: INFO: stderr: ""
  Apr 28 18:19:05.192: INFO: stdout: "e2e-test-crd-publish-openapi-4527-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Apr 28 18:19:05.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-6211 --namespace=crd-publish-openapi-6211 delete e2e-test-crd-publish-openapi-4527-crds test-foo'
  Apr 28 18:19:05.264: INFO: stderr: ""
  Apr 28 18:19:05.264: INFO: stdout: "e2e-test-crd-publish-openapi-4527-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 04/28/23 18:19:05.264
  Apr 28 18:19:05.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-6211 --namespace=crd-publish-openapi-6211 create -f -'
  E0428 18:19:05.390215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:19:05.462: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 04/28/23 18:19:05.462
  Apr 28 18:19:05.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-6211 --namespace=crd-publish-openapi-6211 create -f -'
  Apr 28 18:19:06.320: INFO: rc: 1
  Apr 28 18:19:06.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-6211 --namespace=crd-publish-openapi-6211 apply -f -'
  E0428 18:19:06.391265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:19:06.535: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 04/28/23 18:19:06.535
  Apr 28 18:19:06.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-6211 --namespace=crd-publish-openapi-6211 create -f -'
  Apr 28 18:19:06.761: INFO: rc: 1
  Apr 28 18:19:06.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-6211 --namespace=crd-publish-openapi-6211 apply -f -'
  Apr 28 18:19:06.994: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 04/28/23 18:19:06.994
  Apr 28 18:19:06.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-6211 explain e2e-test-crd-publish-openapi-4527-crds'
  Apr 28 18:19:07.204: INFO: stderr: ""
  Apr 28 18:19:07.204: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4527-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 04/28/23 18:19:07.204
  Apr 28 18:19:07.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-6211 explain e2e-test-crd-publish-openapi-4527-crds.metadata'
  E0428 18:19:07.392259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:19:07.404: INFO: stderr: ""
  Apr 28 18:19:07.404: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4527-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Apr 28 18:19:07.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-6211 explain e2e-test-crd-publish-openapi-4527-crds.spec'
  Apr 28 18:19:07.608: INFO: stderr: ""
  Apr 28 18:19:07.608: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4527-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Apr 28 18:19:07.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-6211 explain e2e-test-crd-publish-openapi-4527-crds.spec.bars'
  Apr 28 18:19:07.796: INFO: stderr: ""
  Apr 28 18:19:07.796: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4527-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 04/28/23 18:19:07.796
  Apr 28 18:19:07.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-75056194 --namespace=crd-publish-openapi-6211 explain e2e-test-crd-publish-openapi-4527-crds.spec.bars2'
  Apr 28 18:19:07.999: INFO: rc: 1
  E0428 18:19:08.392693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:19:09.396160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:19:09.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6211" for this suite. @ 04/28/23 18:19:09.694
• [8.796 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 04/28/23 18:19:09.701
  Apr 28 18:19:09.701: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename disruption @ 04/28/23 18:19:09.702
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:19:09.72
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:19:09.723
  STEP: Waiting for the pdb to be processed @ 04/28/23 18:19:09.74
  STEP: Updating PodDisruptionBudget status @ 04/28/23 18:19:09.757
  STEP: Waiting for all pods to be running @ 04/28/23 18:19:09.785
  Apr 28 18:19:09.796: INFO: running pods: 0 < 1
  E0428 18:19:10.397010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:19:11.397138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 04/28/23 18:19:11.886
  STEP: Waiting for the pdb to be processed @ 04/28/23 18:19:11.914
  STEP: Patching PodDisruptionBudget status @ 04/28/23 18:19:11.933
  STEP: Waiting for the pdb to be processed @ 04/28/23 18:19:11.954
  Apr 28 18:19:11.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2785" for this suite. @ 04/28/23 18:19:11.967
• [2.278 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 04/28/23 18:19:11.981
  Apr 28 18:19:11.981: INFO: >>> kubeConfig: /tmp/kubeconfig-75056194
  STEP: Building a namespace api object, basename projected @ 04/28/23 18:19:11.982
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:19:12.009
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:19:12.017
  STEP: Creating the pod @ 04/28/23 18:19:12.022
  E0428 18:19:12.397826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:19:13.398039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:19:14.398101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:19:14.599: INFO: Successfully updated pod "annotationupdate6588230d-389f-48ac-a9c0-001f6410fe66"
  E0428 18:19:15.399303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:19:16.399290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:19:17.399388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:19:18.399516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:19:18.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4173" for this suite. @ 04/28/23 18:19:18.646
• [6.675 seconds]
------------------------------
SSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Apr 28 18:19:18.661: INFO: Running AfterSuite actions on node 1
  Apr 28 18:19:18.661: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.081 seconds]
------------------------------

Ran 378 of 7207 Specs in 6249.901 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h44m10.510600731s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

