  I0504 06:11:48.018812      20 e2e.go:117] Starting e2e run "03fb56be-d3a0-4cb6-b1ca-4a4094abfd3d" on Ginkgo node 1
  May  4 06:11:48.114: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1683180707 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  May  4 06:11:48.499: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 06:11:48.501: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  May  4 06:11:48.552: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  May  4 06:11:48.558: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
  May  4 06:11:48.558: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
  May  4 06:11:48.558: INFO: e2e test version: v1.27.1
  May  4 06:11:48.559: INFO: kube-apiserver version: v1.27.1
  May  4 06:11:48.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 06:11:48.563: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.065 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 05/04/23 06:11:49.019
  May  4 06:11:49.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename kubectl @ 05/04/23 06:11:49.02
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:11:49.087
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:11:49.091
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/04/23 06:11:49.095
  May  4 06:11:49.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-1457 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  May  4 06:11:49.270: INFO: stderr: ""
  May  4 06:11:49.270: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 05/04/23 06:11:49.27
  May  4 06:11:49.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-1457 delete pods e2e-test-httpd-pod'
  May  4 06:11:52.369: INFO: stderr: ""
  May  4 06:11:52.369: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May  4 06:11:52.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1457" for this suite. @ 05/04/23 06:11:52.375
• [3.372 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 05/04/23 06:11:52.392
  May  4 06:11:52.392: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename endpointslicemirroring @ 05/04/23 06:11:52.393
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:11:52.434
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:11:52.438
  STEP: mirroring a new custom Endpoint @ 05/04/23 06:11:52.5
  May  4 06:11:52.522: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  STEP: mirroring an update to a custom Endpoint @ 05/04/23 06:11:54.527
  May  4 06:11:54.548: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  STEP: mirroring deletion of a custom Endpoint @ 05/04/23 06:11:56.556
  May  4 06:11:56.575: INFO: Waiting for 0 EndpointSlices to exist, got 1
  May  4 06:11:58.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-8589" for this suite. @ 05/04/23 06:11:58.584
• [6.208 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 05/04/23 06:11:58.603
  May  4 06:11:58.603: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename gc @ 05/04/23 06:11:58.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:11:58.673
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:11:58.676
  STEP: create the rc @ 05/04/23 06:11:58.681
  W0504 06:11:58.696227      20 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 05/04/23 06:12:03.702
  STEP: wait for all pods to be garbage collected @ 05/04/23 06:12:03.745
  STEP: Gathering metrics @ 05/04/23 06:12:08.752
  May  4 06:12:08.891: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May  4 06:12:08.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6439" for this suite. @ 05/04/23 06:12:08.898
• [10.330 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 05/04/23 06:12:08.934
  May  4 06:12:08.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 06:12:08.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:12:09.024
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:12:09.029
  STEP: Creating a pod to test downward API volume plugin @ 05/04/23 06:12:09.033
  STEP: Saw pod success @ 05/04/23 06:12:13.061
  May  4 06:12:13.065: INFO: Trying to get logs from node k8s-node2 pod downwardapi-volume-067a0120-dd42-4cad-b40d-3ab25e58b363 container client-container: <nil>
  STEP: delete the pod @ 05/04/23 06:12:13.072
  May  4 06:12:13.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7842" for this suite. @ 05/04/23 06:12:13.137
• [4.242 seconds]
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 05/04/23 06:12:13.177
  May  4 06:12:13.177: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename secrets @ 05/04/23 06:12:13.179
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:12:13.216
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:12:13.223
  STEP: Creating secret with name secret-test-0edf8469-8df5-4c06-8f56-5cf4ab341b3b @ 05/04/23 06:12:13.234
  STEP: Creating a pod to test consume secrets @ 05/04/23 06:12:13.266
  STEP: Saw pod success @ 05/04/23 06:12:19.329
  May  4 06:12:19.332: INFO: Trying to get logs from node k8s-node1 pod pod-secrets-6cb92aa3-a73b-400b-b235-73fd62b628a5 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/04/23 06:12:19.339
  May  4 06:12:19.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9476" for this suite. @ 05/04/23 06:12:19.398
• [6.237 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 05/04/23 06:12:19.414
  May  4 06:12:19.414: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename watch @ 05/04/23 06:12:19.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:12:19.482
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:12:19.49
  STEP: creating a watch on configmaps with a certain label @ 05/04/23 06:12:19.495
  STEP: creating a new configmap @ 05/04/23 06:12:19.498
  STEP: modifying the configmap once @ 05/04/23 06:12:19.519
  STEP: changing the label value of the configmap @ 05/04/23 06:12:19.542
  STEP: Expecting to observe a delete notification for the watched object @ 05/04/23 06:12:19.565
  May  4 06:12:19.565: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4251  31731f8f-aee8-4303-a7cb-b4330316e76d 52416 0 2023-05-04 06:12:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-04 06:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May  4 06:12:19.565: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4251  31731f8f-aee8-4303-a7cb-b4330316e76d 52417 0 2023-05-04 06:12:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-04 06:12:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  4 06:12:19.565: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4251  31731f8f-aee8-4303-a7cb-b4330316e76d 52418 0 2023-05-04 06:12:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-04 06:12:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 05/04/23 06:12:19.565
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 05/04/23 06:12:19.616
  STEP: changing the label value of the configmap back @ 05/04/23 06:12:29.617
  STEP: modifying the configmap a third time @ 05/04/23 06:12:29.645
  STEP: deleting the configmap @ 05/04/23 06:12:29.661
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 05/04/23 06:12:29.682
  May  4 06:12:29.682: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4251  31731f8f-aee8-4303-a7cb-b4330316e76d 52444 0 2023-05-04 06:12:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-04 06:12:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  4 06:12:29.682: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4251  31731f8f-aee8-4303-a7cb-b4330316e76d 52445 0 2023-05-04 06:12:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-04 06:12:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  4 06:12:29.682: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4251  31731f8f-aee8-4303-a7cb-b4330316e76d 52446 0 2023-05-04 06:12:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-04 06:12:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  4 06:12:29.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-4251" for this suite. @ 05/04/23 06:12:29.689
• [10.290 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 05/04/23 06:12:29.706
  May  4 06:12:29.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename dns @ 05/04/23 06:12:29.708
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:12:29.777
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:12:29.782
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 05/04/23 06:12:29.787
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 05/04/23 06:12:29.787
  STEP: creating a pod to probe DNS @ 05/04/23 06:12:29.787
  STEP: submitting the pod to kubernetes @ 05/04/23 06:12:29.787
  STEP: retrieving the pod @ 05/04/23 06:12:33.83
  STEP: looking for the results for each expected name from probers @ 05/04/23 06:12:33.833
  May  4 06:12:33.856: INFO: DNS probes using dns-6123/dns-test-3b77fd29-e992-450e-ba19-165d3d790658 succeeded

  May  4 06:12:33.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/04/23 06:12:33.861
  STEP: Destroying namespace "dns-6123" for this suite. @ 05/04/23 06:12:33.902
• [4.223 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 05/04/23 06:12:33.936
  May  4 06:12:33.936: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename security-context-test @ 05/04/23 06:12:33.938
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:12:34
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:12:34.005
  May  4 06:12:38.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-3978" for this suite. @ 05/04/23 06:12:38.058
• [4.181 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 05/04/23 06:12:38.119
  May  4 06:12:38.119: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/04/23 06:12:38.12
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:12:38.175
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:12:38.18
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 05/04/23 06:12:38.185
  May  4 06:12:38.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 06:12:39.830: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 06:12:46.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8503" for this suite. @ 05/04/23 06:12:46.813
• [8.717 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 05/04/23 06:12:46.836
  May  4 06:12:46.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename security-context-test @ 05/04/23 06:12:46.838
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:12:46.922
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:12:46.927
  May  4 06:12:51.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-1295" for this suite. @ 05/04/23 06:12:51.038
• [4.217 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 05/04/23 06:12:51.055
  May  4 06:12:51.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename replicaset @ 05/04/23 06:12:51.057
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:12:51.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:12:51.139
  STEP: Create a ReplicaSet @ 05/04/23 06:12:51.144
  STEP: Verify that the required pods have come up @ 05/04/23 06:12:51.169
  May  4 06:12:51.174: INFO: Pod name sample-pod: Found 0 pods out of 3
  May  4 06:12:56.182: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 05/04/23 06:12:56.182
  May  4 06:12:56.197: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 05/04/23 06:12:56.197
  STEP: DeleteCollection of the ReplicaSets @ 05/04/23 06:12:56.199
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 05/04/23 06:12:56.227
  May  4 06:12:56.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-346" for this suite. @ 05/04/23 06:12:56.236
• [5.203 seconds]
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 05/04/23 06:12:56.258
  May  4 06:12:56.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename pod-network-test @ 05/04/23 06:12:56.26
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:12:56.393
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:12:56.403
  STEP: Performing setup for networking test in namespace pod-network-test-3312 @ 05/04/23 06:12:56.438
  STEP: creating a selector @ 05/04/23 06:12:56.438
  STEP: Creating the service pods in kubernetes @ 05/04/23 06:12:56.438
  May  4 06:12:56.438: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 05/04/23 06:13:18.756
  May  4 06:13:22.897: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  May  4 06:13:22.897: INFO: Going to poll 172.16.36.110 on port 8083 at least 0 times, with a maximum of 34 tries before failing
  May  4 06:13:22.900: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.36.110:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3312 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 06:13:22.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 06:13:22.901: INFO: ExecWithOptions: Clientset creation
  May  4 06:13:22.901: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3312/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.16.36.110%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May  4 06:13:23.017: INFO: Found all 1 expected endpoints: [netserver-0]
  May  4 06:13:23.017: INFO: Going to poll 172.16.169.184 on port 8083 at least 0 times, with a maximum of 34 tries before failing
  May  4 06:13:23.021: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.169.184:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3312 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 06:13:23.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 06:13:23.022: INFO: ExecWithOptions: Clientset creation
  May  4 06:13:23.022: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3312/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.16.169.184%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May  4 06:13:23.133: INFO: Found all 1 expected endpoints: [netserver-1]
  May  4 06:13:23.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-3312" for this suite. @ 05/04/23 06:13:23.139
• [26.899 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 05/04/23 06:13:23.158
  May  4 06:13:23.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename namespaces @ 05/04/23 06:13:23.16
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:13:23.219
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:13:23.225
  STEP: creating a Namespace @ 05/04/23 06:13:23.233
  STEP: patching the Namespace @ 05/04/23 06:13:23.296
  STEP: get the Namespace and ensuring it has the label @ 05/04/23 06:13:23.311
  May  4 06:13:23.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9895" for this suite. @ 05/04/23 06:13:23.343
  STEP: Destroying namespace "nspatchtest-a0b8ae50-58da-477a-9cd8-1c860bd0c534-5286" for this suite. @ 05/04/23 06:13:23.362
• [0.220 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 05/04/23 06:13:23.381
  May  4 06:13:23.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 06:13:23.384
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:13:23.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:13:23.441
  STEP: Creating configMap with name configmap-projected-all-test-volume-3b8685d8-8599-46e5-86e9-7c9f7118ca2d @ 05/04/23 06:13:23.466
  STEP: Creating secret with name secret-projected-all-test-volume-bee64be5-6fe6-4078-8edc-1203ccd5cc01 @ 05/04/23 06:13:23.481
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 05/04/23 06:13:23.495
  STEP: Saw pod success @ 05/04/23 06:13:27.538
  May  4 06:13:27.541: INFO: Trying to get logs from node k8s-node1 pod projected-volume-d09b710e-b692-4f80-bf52-c9965d2cb900 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 05/04/23 06:13:27.55
  May  4 06:13:27.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-381" for this suite. @ 05/04/23 06:13:27.63
• [4.264 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 05/04/23 06:13:27.646
  May  4 06:13:27.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename pods @ 05/04/23 06:13:27.648
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:13:27.703
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:13:27.707
  STEP: creating the pod @ 05/04/23 06:13:27.71
  STEP: setting up watch @ 05/04/23 06:13:27.71
  STEP: submitting the pod to kubernetes @ 05/04/23 06:13:27.828
  STEP: verifying the pod is in kubernetes @ 05/04/23 06:13:27.846
  STEP: verifying pod creation was observed @ 05/04/23 06:13:27.851
  STEP: deleting the pod gracefully @ 05/04/23 06:13:31.903
  STEP: verifying pod deletion was observed @ 05/04/23 06:13:31.928
  May  4 06:13:33.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-653" for this suite. @ 05/04/23 06:13:33.664
• [6.034 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 05/04/23 06:13:33.682
  May  4 06:13:33.682: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename emptydir @ 05/04/23 06:13:33.683
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:13:33.733
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:13:33.737
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 05/04/23 06:13:33.742
  STEP: Saw pod success @ 05/04/23 06:13:37.794
  May  4 06:13:37.798: INFO: Trying to get logs from node k8s-node2 pod pod-19b4e0d3-3304-4a6f-aa8a-b7411946ee6c container test-container: <nil>
  STEP: delete the pod @ 05/04/23 06:13:37.805
  May  4 06:13:37.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3388" for this suite. @ 05/04/23 06:13:37.871
• [4.228 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 05/04/23 06:13:37.91
  May  4 06:13:37.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename subpath @ 05/04/23 06:13:37.911
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:13:37.967
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:13:37.972
  STEP: Setting up data @ 05/04/23 06:13:37.976
  STEP: Creating pod pod-subpath-test-projected-8jcv @ 05/04/23 06:13:38.009
  STEP: Creating a pod to test atomic-volume-subpath @ 05/04/23 06:13:38.009
  STEP: Saw pod success @ 05/04/23 06:14:02.131
  May  4 06:14:02.135: INFO: Trying to get logs from node k8s-node2 pod pod-subpath-test-projected-8jcv container test-container-subpath-projected-8jcv: <nil>
  STEP: delete the pod @ 05/04/23 06:14:02.144
  STEP: Deleting pod pod-subpath-test-projected-8jcv @ 05/04/23 06:14:02.197
  May  4 06:14:02.197: INFO: Deleting pod "pod-subpath-test-projected-8jcv" in namespace "subpath-1465"
  May  4 06:14:02.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-1465" for this suite. @ 05/04/23 06:14:02.206
• [24.332 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 05/04/23 06:14:02.244
  May  4 06:14:02.244: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename svcaccounts @ 05/04/23 06:14:02.246
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:14:02.28
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:14:02.284
  STEP: reading a file in the container @ 05/04/23 06:14:04.359
  May  4 06:14:04.359: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8593 pod-service-account-20358c24-815d-4914-93cc-415c1d8fc057 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 05/04/23 06:14:04.575
  May  4 06:14:04.576: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8593 pod-service-account-20358c24-815d-4914-93cc-415c1d8fc057 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 05/04/23 06:14:04.781
  May  4 06:14:04.781: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8593 pod-service-account-20358c24-815d-4914-93cc-415c1d8fc057 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  May  4 06:14:04.991: INFO: Got root ca configmap in namespace "svcaccounts-8593"
  May  4 06:14:04.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8593" for this suite. @ 05/04/23 06:14:04.999
• [2.769 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 05/04/23 06:14:05.014
  May  4 06:14:05.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename resourcequota @ 05/04/23 06:14:05.016
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:14:05.071
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:14:05.076
  STEP: Counting existing ResourceQuota @ 05/04/23 06:14:05.079
  STEP: Creating a ResourceQuota @ 05/04/23 06:14:10.099
  STEP: Ensuring resource quota status is calculated @ 05/04/23 06:14:10.122
  STEP: Creating a ReplicaSet @ 05/04/23 06:14:12.127
  STEP: Ensuring resource quota status captures replicaset creation @ 05/04/23 06:14:12.177
  STEP: Deleting a ReplicaSet @ 05/04/23 06:14:14.182
  STEP: Ensuring resource quota status released usage @ 05/04/23 06:14:14.197
  May  4 06:14:16.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1315" for this suite. @ 05/04/23 06:14:16.207
• [11.208 seconds]
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 05/04/23 06:14:16.223
  May  4 06:14:16.223: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename subjectreview @ 05/04/23 06:14:16.224
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:14:16.298
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:14:16.302
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-5371" @ 05/04/23 06:14:16.306
  May  4 06:14:16.320: INFO: saUsername: "system:serviceaccount:subjectreview-5371:e2e"
  May  4 06:14:16.320: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-5371"}
  May  4 06:14:16.320: INFO: saUID: "3167cc2a-172f-4ba8-8463-d31f0a805595"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-5371:e2e" @ 05/04/23 06:14:16.32
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-5371:e2e" @ 05/04/23 06:14:16.32
  May  4 06:14:16.323: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-5371:e2e" api 'list' configmaps in "subjectreview-5371" namespace @ 05/04/23 06:14:16.323
  May  4 06:14:16.325: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-5371:e2e" @ 05/04/23 06:14:16.325
  May  4 06:14:16.328: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  May  4 06:14:16.328: INFO: LocalSubjectAccessReview has been verified
  May  4 06:14:16.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-5371" for this suite. @ 05/04/23 06:14:16.332
• [0.135 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 05/04/23 06:14:16.359
  May  4 06:14:16.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/04/23 06:14:16.361
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:14:16.448
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:14:16.451
  STEP: set up a multi version CRD @ 05/04/23 06:14:16.455
  May  4 06:14:16.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: rename a version @ 05/04/23 06:14:21.437
  STEP: check the new version name is served @ 05/04/23 06:14:21.481
  STEP: check the old version name is removed @ 05/04/23 06:14:22.482
  STEP: check the other version is not changed @ 05/04/23 06:14:23.452
  May  4 06:14:26.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7595" for this suite. @ 05/04/23 06:14:26.734
• [10.394 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 05/04/23 06:14:26.754
  May  4 06:14:26.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename field-validation @ 05/04/23 06:14:26.755
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:14:26.82
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:14:26.824
  STEP: apply creating a deployment @ 05/04/23 06:14:26.827
  May  4 06:14:26.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7889" for this suite. @ 05/04/23 06:14:26.844
• [0.110 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 05/04/23 06:14:26.864
  May  4 06:14:26.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename downward-api @ 05/04/23 06:14:26.866
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:14:26.946
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:14:26.95
  STEP: Creating a pod to test downward API volume plugin @ 05/04/23 06:14:26.954
  STEP: Saw pod success @ 05/04/23 06:14:30.988
  May  4 06:14:30.991: INFO: Trying to get logs from node k8s-node2 pod downwardapi-volume-3f4fef7a-da5f-4476-a110-93ee38ba6cb4 container client-container: <nil>
  STEP: delete the pod @ 05/04/23 06:14:30.997
  May  4 06:14:31.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-830" for this suite. @ 05/04/23 06:14:31.056
• [4.212 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 05/04/23 06:14:31.077
  May  4 06:14:31.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename deployment @ 05/04/23 06:14:31.079
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:14:31.194
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:14:31.198
  May  4 06:14:31.202: INFO: Creating simple deployment test-new-deployment
  May  4 06:14:31.264: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
  May  4 06:14:33.285: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 6, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 14, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 6, 14, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 14, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-67bd4bf6dc\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: getting scale subresource @ 05/04/23 06:14:35.291
  STEP: updating a scale subresource @ 05/04/23 06:14:35.294
  STEP: verifying the deployment Spec.Replicas was modified @ 05/04/23 06:14:35.319
  STEP: Patch a scale subresource @ 05/04/23 06:14:35.322
  May  4 06:14:35.510: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-8542  52eecb37-9912-44c4-99fc-d7ca0be2fc83 53226 3 2023-05-04 06:14:31 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-04 06:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 06:14:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0054dbe18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-04 06:14:34 +0000 UTC,LastTransitionTime:2023-05-04 06:14:34 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-05-04 06:14:34 +0000 UTC,LastTransitionTime:2023-05-04 06:14:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May  4 06:14:35.559: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-8542  a6d45e55-2750-4210-9a18-2df8eacf6b89 53233 2 2023-05-04 06:14:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 52eecb37-9912-44c4-99fc-d7ca0be2fc83 0xc002353397 0xc002353398}] [] [{kube-controller-manager Update apps/v1 2023-05-04 06:14:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52eecb37-9912-44c4-99fc-d7ca0be2fc83\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 06:14:35 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002353428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May  4 06:14:35.570: INFO: Pod "test-new-deployment-67bd4bf6dc-55frg" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-55frg test-new-deployment-67bd4bf6dc- deployment-8542  98cf41ca-7614-4b07-bc82-05a7e55b20cd 53234 0 2023-05-04 06:14:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc a6d45e55-2750-4210-9a18-2df8eacf6b89 0xc002353817 0xc002353818}] [] [{kube-controller-manager Update v1 2023-05-04 06:14:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6d45e55-2750-4210-9a18-2df8eacf6b89\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 06:14:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nmgvx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nmgvx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 06:14:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 06:14:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 06:14:35 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 06:14:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.157,PodIP:,StartTime:2023-05-04 06:14:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 06:14:35.571: INFO: Pod "test-new-deployment-67bd4bf6dc-8lf76" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-8lf76 test-new-deployment-67bd4bf6dc- deployment-8542  1f84a1b4-4001-4d7c-ad1c-fe7d1cfef77a 53221 0 2023-05-04 06:14:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:024ae88dc9283eab29cfc925a8f6c0039f8c1cc30ebc0d112a9ed815913c2c2c cni.projectcalico.org/podIP:172.16.36.102/32 cni.projectcalico.org/podIPs:172.16.36.102/32] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc a6d45e55-2750-4210-9a18-2df8eacf6b89 0xc002353a07 0xc002353a08}] [] [{kube-controller-manager Update v1 2023-05-04 06:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6d45e55-2750-4210-9a18-2df8eacf6b89\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 06:14:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 06:14:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.36.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-974c5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-974c5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 06:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 06:14:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 06:14:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 06:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.156,PodIP:172.16.36.102,StartTime:2023-05-04 06:14:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 06:14:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://6440152d35fc489a5e653f750a589d7941506aae93b2b47edb3a8145717798ea,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.36.102,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 06:14:35.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8542" for this suite. @ 05/04/23 06:14:35.576
• [4.568 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 05/04/23 06:14:35.646
  May  4 06:14:35.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename gc @ 05/04/23 06:14:35.648
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:14:35.783
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:14:35.787
  STEP: create the deployment @ 05/04/23 06:14:35.791
  W0504 06:14:35.822351      20 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 05/04/23 06:14:35.822
  STEP: delete the deployment @ 05/04/23 06:14:36.347
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 05/04/23 06:14:36.4
  STEP: Gathering metrics @ 05/04/23 06:14:37.053
  May  4 06:14:37.244: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May  4 06:14:37.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3804" for this suite. @ 05/04/23 06:14:37.249
• [1.617 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 05/04/23 06:14:37.265
  May  4 06:14:37.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename discovery @ 05/04/23 06:14:37.266
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:14:37.322
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:14:37.326
  STEP: Setting up server cert @ 05/04/23 06:14:37.35
  May  4 06:14:37.862: INFO: Checking APIGroup: apiregistration.k8s.io
  May  4 06:14:37.863: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  May  4 06:14:37.863: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  May  4 06:14:37.863: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  May  4 06:14:37.863: INFO: Checking APIGroup: apps
  May  4 06:14:37.865: INFO: PreferredVersion.GroupVersion: apps/v1
  May  4 06:14:37.865: INFO: Versions found [{apps/v1 v1}]
  May  4 06:14:37.865: INFO: apps/v1 matches apps/v1
  May  4 06:14:37.865: INFO: Checking APIGroup: events.k8s.io
  May  4 06:14:37.866: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  May  4 06:14:37.866: INFO: Versions found [{events.k8s.io/v1 v1}]
  May  4 06:14:37.866: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  May  4 06:14:37.866: INFO: Checking APIGroup: authentication.k8s.io
  May  4 06:14:37.868: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  May  4 06:14:37.868: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  May  4 06:14:37.868: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  May  4 06:14:37.868: INFO: Checking APIGroup: authorization.k8s.io
  May  4 06:14:37.869: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  May  4 06:14:37.869: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  May  4 06:14:37.869: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  May  4 06:14:37.869: INFO: Checking APIGroup: autoscaling
  May  4 06:14:37.870: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  May  4 06:14:37.870: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  May  4 06:14:37.870: INFO: autoscaling/v2 matches autoscaling/v2
  May  4 06:14:37.871: INFO: Checking APIGroup: batch
  May  4 06:14:37.872: INFO: PreferredVersion.GroupVersion: batch/v1
  May  4 06:14:37.872: INFO: Versions found [{batch/v1 v1}]
  May  4 06:14:37.872: INFO: batch/v1 matches batch/v1
  May  4 06:14:37.872: INFO: Checking APIGroup: certificates.k8s.io
  May  4 06:14:37.873: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  May  4 06:14:37.873: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  May  4 06:14:37.873: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  May  4 06:14:37.873: INFO: Checking APIGroup: networking.k8s.io
  May  4 06:14:37.874: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  May  4 06:14:37.874: INFO: Versions found [{networking.k8s.io/v1 v1}]
  May  4 06:14:37.874: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  May  4 06:14:37.874: INFO: Checking APIGroup: policy
  May  4 06:14:37.876: INFO: PreferredVersion.GroupVersion: policy/v1
  May  4 06:14:37.876: INFO: Versions found [{policy/v1 v1}]
  May  4 06:14:37.876: INFO: policy/v1 matches policy/v1
  May  4 06:14:37.876: INFO: Checking APIGroup: rbac.authorization.k8s.io
  May  4 06:14:37.877: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  May  4 06:14:37.877: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  May  4 06:14:37.877: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  May  4 06:14:37.877: INFO: Checking APIGroup: storage.k8s.io
  May  4 06:14:37.878: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  May  4 06:14:37.879: INFO: Versions found [{storage.k8s.io/v1 v1}]
  May  4 06:14:37.879: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  May  4 06:14:37.879: INFO: Checking APIGroup: admissionregistration.k8s.io
  May  4 06:14:37.880: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  May  4 06:14:37.880: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  May  4 06:14:37.880: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  May  4 06:14:37.880: INFO: Checking APIGroup: apiextensions.k8s.io
  May  4 06:14:37.881: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  May  4 06:14:37.881: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  May  4 06:14:37.881: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  May  4 06:14:37.881: INFO: Checking APIGroup: scheduling.k8s.io
  May  4 06:14:37.882: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  May  4 06:14:37.882: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  May  4 06:14:37.882: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  May  4 06:14:37.882: INFO: Checking APIGroup: coordination.k8s.io
  May  4 06:14:37.883: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  May  4 06:14:37.884: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  May  4 06:14:37.884: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  May  4 06:14:37.884: INFO: Checking APIGroup: node.k8s.io
  May  4 06:14:37.885: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  May  4 06:14:37.885: INFO: Versions found [{node.k8s.io/v1 v1}]
  May  4 06:14:37.885: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  May  4 06:14:37.885: INFO: Checking APIGroup: discovery.k8s.io
  May  4 06:14:37.886: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  May  4 06:14:37.886: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  May  4 06:14:37.886: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  May  4 06:14:37.886: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  May  4 06:14:37.887: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  May  4 06:14:37.887: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  May  4 06:14:37.887: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  May  4 06:14:37.887: INFO: Checking APIGroup: crd.projectcalico.org
  May  4 06:14:37.889: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
  May  4 06:14:37.889: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
  May  4 06:14:37.889: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
  May  4 06:14:37.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-1277" for this suite. @ 05/04/23 06:14:37.893
• [0.643 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 05/04/23 06:14:37.909
  May  4 06:14:37.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename sysctl @ 05/04/23 06:14:37.91
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:14:37.953
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:14:37.957
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 05/04/23 06:14:37.96
  STEP: Watching for error events or started pod @ 05/04/23 06:14:38
  STEP: Waiting for pod completion @ 05/04/23 06:14:40.005
  STEP: Checking that the pod succeeded @ 05/04/23 06:14:42.014
  STEP: Getting logs from the pod @ 05/04/23 06:14:42.014
  STEP: Checking that the sysctl is actually updated @ 05/04/23 06:14:42.021
  May  4 06:14:42.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-691" for this suite. @ 05/04/23 06:14:42.026
• [4.132 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 05/04/23 06:14:42.042
  May  4 06:14:42.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename job @ 05/04/23 06:14:42.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:14:42.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:14:42.115
  STEP: Creating a job @ 05/04/23 06:14:42.119
  STEP: Ensuring active pods == parallelism @ 05/04/23 06:14:42.143
  STEP: Orphaning one of the Job's Pods @ 05/04/23 06:14:46.149
  May  4 06:14:46.686: INFO: Successfully updated pod "adopt-release-knt79"
  STEP: Checking that the Job readopts the Pod @ 05/04/23 06:14:46.686
  STEP: Removing the labels from the Job's Pod @ 05/04/23 06:14:48.694
  May  4 06:14:49.229: INFO: Successfully updated pod "adopt-release-knt79"
  STEP: Checking that the Job releases the Pod @ 05/04/23 06:14:49.229
  May  4 06:14:51.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-4754" for this suite. @ 05/04/23 06:14:51.244
• [9.230 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 05/04/23 06:14:51.274
  May  4 06:14:51.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename pods @ 05/04/23 06:14:51.275
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:14:51.34
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:14:51.346
  May  4 06:14:51.351: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: creating the pod @ 05/04/23 06:14:51.352
  STEP: submitting the pod to kubernetes @ 05/04/23 06:14:51.352
  May  4 06:14:55.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7215" for this suite. @ 05/04/23 06:14:55.438
• [4.187 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 05/04/23 06:14:55.462
  May  4 06:14:55.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename pods @ 05/04/23 06:14:55.464
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:14:55.595
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:14:55.6
  STEP: creating the pod @ 05/04/23 06:14:55.603
  STEP: submitting the pod to kubernetes @ 05/04/23 06:14:55.603
  STEP: verifying the pod is in kubernetes @ 05/04/23 06:14:59.658
  STEP: updating the pod @ 05/04/23 06:14:59.661
  May  4 06:15:00.198: INFO: Successfully updated pod "pod-update-87672d33-acb6-45e9-9fd4-f9d2e0c86c1b"
  STEP: verifying the updated pod is in kubernetes @ 05/04/23 06:15:00.202
  May  4 06:15:00.206: INFO: Pod update OK
  May  4 06:15:00.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1519" for this suite. @ 05/04/23 06:15:00.211
• [4.798 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 05/04/23 06:15:00.263
  May  4 06:15:00.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename services @ 05/04/23 06:15:00.264
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:15:00.305
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:15:00.309
  STEP: creating an Endpoint @ 05/04/23 06:15:00.338
  STEP: waiting for available Endpoint @ 05/04/23 06:15:00.355
  STEP: listing all Endpoints @ 05/04/23 06:15:00.357
  STEP: updating the Endpoint @ 05/04/23 06:15:00.387
  STEP: fetching the Endpoint @ 05/04/23 06:15:00.417
  STEP: patching the Endpoint @ 05/04/23 06:15:00.419
  STEP: fetching the Endpoint @ 05/04/23 06:15:00.438
  STEP: deleting the Endpoint by Collection @ 05/04/23 06:15:00.441
  STEP: waiting for Endpoint deletion @ 05/04/23 06:15:00.46
  STEP: fetching the Endpoint @ 05/04/23 06:15:00.462
  May  4 06:15:00.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6981" for this suite. @ 05/04/23 06:15:00.469
• [0.249 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 05/04/23 06:15:00.514
  May  4 06:15:00.514: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename var-expansion @ 05/04/23 06:15:00.515
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:15:00.586
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:15:00.59
  STEP: Creating a pod to test substitution in container's args @ 05/04/23 06:15:00.593
  STEP: Saw pod success @ 05/04/23 06:15:04.676
  May  4 06:15:04.678: INFO: Trying to get logs from node k8s-node2 pod var-expansion-a33cfc6d-c474-41d8-b478-d89621b37ba5 container dapi-container: <nil>
  STEP: delete the pod @ 05/04/23 06:15:04.685
  May  4 06:15:04.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-9594" for this suite. @ 05/04/23 06:15:04.739
• [4.248 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 05/04/23 06:15:04.763
  May  4 06:15:04.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename configmap @ 05/04/23 06:15:04.764
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:15:04.815
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:15:04.82
  STEP: Creating configMap with name configmap-test-volume-8efd101b-1688-423e-81fd-b3d5877445e1 @ 05/04/23 06:15:04.826
  STEP: Creating a pod to test consume configMaps @ 05/04/23 06:15:04.846
  STEP: Saw pod success @ 05/04/23 06:15:08.892
  May  4 06:15:08.896: INFO: Trying to get logs from node k8s-node2 pod pod-configmaps-6629f828-96c2-4d5d-9a18-515021cf043c container agnhost-container: <nil>
  STEP: delete the pod @ 05/04/23 06:15:08.903
  May  4 06:15:08.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1904" for this suite. @ 05/04/23 06:15:08.959
• [4.235 seconds]
------------------------------
SSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 05/04/23 06:15:08.998
  May  4 06:15:08.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename runtimeclass @ 05/04/23 06:15:08.999
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:15:09.048
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:15:09.052
  May  4 06:15:09.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-2126" for this suite. @ 05/04/23 06:15:09.08
• [0.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 05/04/23 06:15:09.097
  May  4 06:15:09.097: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename configmap @ 05/04/23 06:15:09.099
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:15:09.143
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:15:09.147
  STEP: Creating configMap with name configmap-test-volume-map-47bbbff1-5cc9-452d-a6b1-7c14839dbbda @ 05/04/23 06:15:09.163
  STEP: Creating a pod to test consume configMaps @ 05/04/23 06:15:09.182
  STEP: Saw pod success @ 05/04/23 06:15:13.212
  May  4 06:15:13.215: INFO: Trying to get logs from node k8s-node2 pod pod-configmaps-1d158272-cee2-40a0-9127-e4b71006999f container agnhost-container: <nil>
  STEP: delete the pod @ 05/04/23 06:15:13.222
  May  4 06:15:13.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2799" for this suite. @ 05/04/23 06:15:13.286
• [4.207 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 05/04/23 06:15:13.305
  May  4 06:15:13.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename var-expansion @ 05/04/23 06:15:13.307
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:15:13.373
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:15:13.378
  STEP: Creating a pod to test substitution in volume subpath @ 05/04/23 06:15:13.383
  STEP: Saw pod success @ 05/04/23 06:15:17.445
  May  4 06:15:17.472: INFO: Trying to get logs from node k8s-node2 pod var-expansion-ba6dbc3e-dcb6-4666-929f-b477c53a9e40 container dapi-container: <nil>
  STEP: delete the pod @ 05/04/23 06:15:17.481
  May  4 06:15:17.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-8432" for this suite. @ 05/04/23 06:15:17.533
• [4.267 seconds]
------------------------------
SSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 05/04/23 06:15:17.573
  May  4 06:15:17.573: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename cronjob @ 05/04/23 06:15:17.575
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:15:17.638
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:15:17.643
  STEP: Creating a suspended cronjob @ 05/04/23 06:15:17.648
  STEP: Ensuring no jobs are scheduled @ 05/04/23 06:15:17.676
  STEP: Ensuring no job exists by listing jobs explicitly @ 05/04/23 06:20:17.684
  STEP: Removing cronjob @ 05/04/23 06:20:17.687
  May  4 06:20:17.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-4192" for this suite. @ 05/04/23 06:20:17.708
• [300.166 seconds]
------------------------------
SS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 05/04/23 06:20:17.74
  May  4 06:20:17.740: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename events @ 05/04/23 06:20:17.741
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:20:17.819
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:20:17.823
  STEP: Create set of events @ 05/04/23 06:20:17.826
  STEP: get a list of Events with a label in the current namespace @ 05/04/23 06:20:17.909
  STEP: delete a list of events @ 05/04/23 06:20:17.912
  May  4 06:20:17.912: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 05/04/23 06:20:17.986
  May  4 06:20:17.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-6418" for this suite. @ 05/04/23 06:20:17.995
• [0.274 seconds]
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 05/04/23 06:20:18.014
  May  4 06:20:18.014: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename configmap @ 05/04/23 06:20:18.015
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:20:18.091
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:20:18.095
  STEP: Creating configMap with name configmap-test-volume-d401e51b-6aa1-405c-af71-bcaffcab9ee5 @ 05/04/23 06:20:18.098
  STEP: Creating a pod to test consume configMaps @ 05/04/23 06:20:18.115
  STEP: Saw pod success @ 05/04/23 06:20:22.184
  May  4 06:20:22.188: INFO: Trying to get logs from node k8s-node2 pod pod-configmaps-b398aeab-e0f6-4748-be97-fa2faa9ac9a8 container agnhost-container: <nil>
  STEP: delete the pod @ 05/04/23 06:20:22.212
  May  4 06:20:22.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5623" for this suite. @ 05/04/23 06:20:22.26
• [4.261 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 05/04/23 06:20:22.279
  May  4 06:20:22.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename secrets @ 05/04/23 06:20:22.281
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:20:22.329
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:20:22.333
  STEP: creating secret secrets-9329/secret-test-241a3a62-ba6a-4d91-a69b-5d0279fa8bb1 @ 05/04/23 06:20:22.353
  STEP: Creating a pod to test consume secrets @ 05/04/23 06:20:22.366
  STEP: Saw pod success @ 05/04/23 06:20:26.395
  May  4 06:20:26.398: INFO: Trying to get logs from node k8s-node1 pod pod-configmaps-eeb997f9-c0cf-45c1-a4f5-c60e1de3890e container env-test: <nil>
  STEP: delete the pod @ 05/04/23 06:20:26.42
  May  4 06:20:26.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9329" for this suite. @ 05/04/23 06:20:26.487
• [4.223 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 05/04/23 06:20:26.503
  May  4 06:20:26.503: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename kubectl @ 05/04/23 06:20:26.504
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:20:26.552
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:20:26.556
  STEP: creating Agnhost RC @ 05/04/23 06:20:26.573
  May  4 06:20:26.573: INFO: namespace kubectl-6367
  May  4 06:20:26.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-6367 create -f -'
  May  4 06:20:27.784: INFO: stderr: ""
  May  4 06:20:27.784: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/04/23 06:20:27.784
  May  4 06:20:28.789: INFO: Selector matched 1 pods for map[app:agnhost]
  May  4 06:20:28.789: INFO: Found 0 / 1
  May  4 06:20:29.789: INFO: Selector matched 1 pods for map[app:agnhost]
  May  4 06:20:29.789: INFO: Found 1 / 1
  May  4 06:20:29.789: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  May  4 06:20:29.792: INFO: Selector matched 1 pods for map[app:agnhost]
  May  4 06:20:29.792: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May  4 06:20:29.792: INFO: wait on agnhost-primary startup in kubectl-6367 
  May  4 06:20:29.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-6367 logs agnhost-primary-rbklw agnhost-primary'
  May  4 06:20:29.911: INFO: stderr: ""
  May  4 06:20:29.911: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 05/04/23 06:20:29.911
  May  4 06:20:29.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-6367 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  May  4 06:20:30.070: INFO: stderr: ""
  May  4 06:20:30.070: INFO: stdout: "service/rm2 exposed\n"
  May  4 06:20:30.100: INFO: Service rm2 in namespace kubectl-6367 found.
  STEP: exposing service @ 05/04/23 06:20:32.108
  May  4 06:20:32.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-6367 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  May  4 06:20:32.265: INFO: stderr: ""
  May  4 06:20:32.265: INFO: stdout: "service/rm3 exposed\n"
  May  4 06:20:32.283: INFO: Service rm3 in namespace kubectl-6367 found.
  May  4 06:20:34.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6367" for this suite. @ 05/04/23 06:20:34.3
• [7.812 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 05/04/23 06:20:34.316
  May  4 06:20:34.316: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename emptydir @ 05/04/23 06:20:34.317
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:20:34.383
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:20:34.387
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 05/04/23 06:20:34.39
  STEP: Saw pod success @ 05/04/23 06:20:38.418
  May  4 06:20:38.421: INFO: Trying to get logs from node k8s-node1 pod pod-bb62c4ab-517a-4d6c-a888-2cdbba830f99 container test-container: <nil>
  STEP: delete the pod @ 05/04/23 06:20:38.427
  May  4 06:20:38.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4480" for this suite. @ 05/04/23 06:20:38.48
• [4.202 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 05/04/23 06:20:38.519
  May  4 06:20:38.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 06:20:38.52
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:20:38.561
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:20:38.565
  STEP: Creating projection with secret that has name projected-secret-test-map-ab054b8f-e1d9-4345-a417-980445f3a5dd @ 05/04/23 06:20:38.569
  STEP: Creating a pod to test consume secrets @ 05/04/23 06:20:38.585
  STEP: Saw pod success @ 05/04/23 06:20:44.679
  May  4 06:20:44.682: INFO: Trying to get logs from node k8s-node1 pod pod-projected-secrets-68321619-3a72-44d8-b9d7-f8369eb11f80 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/04/23 06:20:44.709
  May  4 06:20:44.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8272" for this suite. @ 05/04/23 06:20:44.78
• [6.276 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 05/04/23 06:20:44.797
  May  4 06:20:44.797: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename sched-pred @ 05/04/23 06:20:44.798
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:20:44.869
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:20:44.873
  May  4 06:20:44.876: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May  4 06:20:44.884: INFO: Waiting for terminating namespaces to be deleted...
  May  4 06:20:44.887: INFO: 
  Logging pods the apiserver thinks is on node k8s-node1 before test
  May  4 06:20:44.893: INFO: calico-node-5st2f from kube-system started at 2023-05-04 01:43:32 +0000 UTC (1 container statuses recorded)
  May  4 06:20:44.894: INFO: 	Container calico-node ready: true, restart count 0
  May  4 06:20:44.894: INFO: kube-proxy-b6n46 from kube-system started at 2023-05-04 01:43:02 +0000 UTC (1 container statuses recorded)
  May  4 06:20:44.894: INFO: 	Container kube-proxy ready: true, restart count 0
  May  4 06:20:44.894: INFO: sonobuoy from sonobuoy started at 2023-05-04 06:11:43 +0000 UTC (1 container statuses recorded)
  May  4 06:20:44.894: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May  4 06:20:44.894: INFO: sonobuoy-e2e-job-5e11a169cb044f18 from sonobuoy started at 2023-05-04 06:11:45 +0000 UTC (2 container statuses recorded)
  May  4 06:20:44.894: INFO: 	Container e2e ready: true, restart count 0
  May  4 06:20:44.894: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  4 06:20:44.894: INFO: sonobuoy-systemd-logs-daemon-set-1f5c24134e834838-h8cct from sonobuoy started at 2023-05-04 06:11:45 +0000 UTC (2 container statuses recorded)
  May  4 06:20:44.894: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  4 06:20:44.894: INFO: 	Container systemd-logs ready: true, restart count 0
  May  4 06:20:44.894: INFO: 
  Logging pods the apiserver thinks is on node k8s-node2 before test
  May  4 06:20:44.901: INFO: calico-kube-controllers-6849cf9bcf-9j56g from kube-system started at 2023-05-04 01:44:12 +0000 UTC (1 container statuses recorded)
  May  4 06:20:44.901: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May  4 06:20:44.901: INFO: calico-node-ssrv4 from kube-system started at 2023-05-04 01:43:32 +0000 UTC (1 container statuses recorded)
  May  4 06:20:44.901: INFO: 	Container calico-node ready: true, restart count 0
  May  4 06:20:44.901: INFO: coredns-7bdc4cb885-7knh9 from kube-system started at 2023-05-04 01:44:12 +0000 UTC (1 container statuses recorded)
  May  4 06:20:44.901: INFO: 	Container coredns ready: true, restart count 0
  May  4 06:20:44.901: INFO: coredns-7bdc4cb885-pdxsn from kube-system started at 2023-05-04 01:44:12 +0000 UTC (1 container statuses recorded)
  May  4 06:20:44.901: INFO: 	Container coredns ready: true, restart count 0
  May  4 06:20:44.901: INFO: kube-proxy-zxvtj from kube-system started at 2023-05-04 01:43:07 +0000 UTC (1 container statuses recorded)
  May  4 06:20:44.901: INFO: 	Container kube-proxy ready: true, restart count 0
  May  4 06:20:44.901: INFO: sonobuoy-systemd-logs-daemon-set-1f5c24134e834838-dqrng from sonobuoy started at 2023-05-04 06:11:45 +0000 UTC (2 container statuses recorded)
  May  4 06:20:44.901: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  4 06:20:44.901: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/04/23 06:20:44.901
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/04/23 06:20:48.933
  STEP: Trying to apply a random label on the found node. @ 05/04/23 06:20:48.977
  STEP: verifying the node has the label kubernetes.io/e2e-43d9ea4f-3c39-499a-9c2b-b52073778f9e 95 @ 05/04/23 06:20:49.019
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 05/04/23 06:20:49.054
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.0.156 on the node which pod4 resides and expect not scheduled @ 05/04/23 06:20:53.1
  STEP: removing the label kubernetes.io/e2e-43d9ea4f-3c39-499a-9c2b-b52073778f9e off the node k8s-node1 @ 05/04/23 06:25:53.138
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-43d9ea4f-3c39-499a-9c2b-b52073778f9e @ 05/04/23 06:25:53.18
  May  4 06:25:53.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-5859" for this suite. @ 05/04/23 06:25:53.192
• [308.411 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 05/04/23 06:25:53.212
  May  4 06:25:53.212: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename deployment @ 05/04/23 06:25:53.213
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:25:53.271
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:25:53.276
  STEP: creating a Deployment @ 05/04/23 06:25:53.284
  May  4 06:25:53.284: INFO: Creating simple deployment test-deployment-8bzvs
  May  4 06:25:53.404: INFO: deployment "test-deployment-8bzvs" doesn't have the required revision set
  May  4 06:25:55.414: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 6, 25, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 25, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 6, 25, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 25, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-8bzvs-5994cf9475\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Getting /status @ 05/04/23 06:25:57.421
  May  4 06:25:57.425: INFO: Deployment test-deployment-8bzvs has Conditions: [{Available True 2023-05-04 06:25:55 +0000 UTC 2023-05-04 06:25:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-04 06:25:55 +0000 UTC 2023-05-04 06:25:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8bzvs-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 05/04/23 06:25:57.425
  May  4 06:25:57.445: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 6, 25, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 25, 55, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 6, 25, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 25, 53, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-8bzvs-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 05/04/23 06:25:57.445
  May  4 06:25:57.448: INFO: Observed &Deployment event: ADDED
  May  4 06:25:57.448: INFO: Observed Deployment test-deployment-8bzvs in namespace deployment-4422 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 06:25:53 +0000 UTC 2023-05-04 06:25:53 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8bzvs-5994cf9475"}
  May  4 06:25:57.448: INFO: Observed &Deployment event: MODIFIED
  May  4 06:25:57.448: INFO: Observed Deployment test-deployment-8bzvs in namespace deployment-4422 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 06:25:53 +0000 UTC 2023-05-04 06:25:53 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8bzvs-5994cf9475"}
  May  4 06:25:57.448: INFO: Observed Deployment test-deployment-8bzvs in namespace deployment-4422 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-04 06:25:53 +0000 UTC 2023-05-04 06:25:53 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May  4 06:25:57.448: INFO: Observed &Deployment event: MODIFIED
  May  4 06:25:57.448: INFO: Observed Deployment test-deployment-8bzvs in namespace deployment-4422 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-04 06:25:53 +0000 UTC 2023-05-04 06:25:53 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May  4 06:25:57.448: INFO: Observed Deployment test-deployment-8bzvs in namespace deployment-4422 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 06:25:53 +0000 UTC 2023-05-04 06:25:53 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-8bzvs-5994cf9475" is progressing.}
  May  4 06:25:57.448: INFO: Observed &Deployment event: MODIFIED
  May  4 06:25:57.448: INFO: Observed Deployment test-deployment-8bzvs in namespace deployment-4422 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-04 06:25:55 +0000 UTC 2023-05-04 06:25:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May  4 06:25:57.448: INFO: Observed Deployment test-deployment-8bzvs in namespace deployment-4422 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 06:25:55 +0000 UTC 2023-05-04 06:25:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8bzvs-5994cf9475" has successfully progressed.}
  May  4 06:25:57.449: INFO: Observed &Deployment event: MODIFIED
  May  4 06:25:57.449: INFO: Observed Deployment test-deployment-8bzvs in namespace deployment-4422 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-04 06:25:55 +0000 UTC 2023-05-04 06:25:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May  4 06:25:57.449: INFO: Observed Deployment test-deployment-8bzvs in namespace deployment-4422 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 06:25:55 +0000 UTC 2023-05-04 06:25:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8bzvs-5994cf9475" has successfully progressed.}
  May  4 06:25:57.449: INFO: Found Deployment test-deployment-8bzvs in namespace deployment-4422 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May  4 06:25:57.449: INFO: Deployment test-deployment-8bzvs has an updated status
  STEP: patching the Statefulset Status @ 05/04/23 06:25:57.449
  May  4 06:25:57.449: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May  4 06:25:57.500: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 05/04/23 06:25:57.5
  May  4 06:25:57.502: INFO: Observed &Deployment event: ADDED
  May  4 06:25:57.502: INFO: Observed deployment test-deployment-8bzvs in namespace deployment-4422 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 06:25:53 +0000 UTC 2023-05-04 06:25:53 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8bzvs-5994cf9475"}
  May  4 06:25:57.503: INFO: Observed &Deployment event: MODIFIED
  May  4 06:25:57.503: INFO: Observed deployment test-deployment-8bzvs in namespace deployment-4422 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 06:25:53 +0000 UTC 2023-05-04 06:25:53 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8bzvs-5994cf9475"}
  May  4 06:25:57.503: INFO: Observed deployment test-deployment-8bzvs in namespace deployment-4422 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-04 06:25:53 +0000 UTC 2023-05-04 06:25:53 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May  4 06:25:57.503: INFO: Observed &Deployment event: MODIFIED
  May  4 06:25:57.503: INFO: Observed deployment test-deployment-8bzvs in namespace deployment-4422 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-04 06:25:53 +0000 UTC 2023-05-04 06:25:53 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May  4 06:25:57.503: INFO: Observed deployment test-deployment-8bzvs in namespace deployment-4422 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 06:25:53 +0000 UTC 2023-05-04 06:25:53 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-8bzvs-5994cf9475" is progressing.}
  May  4 06:25:57.503: INFO: Observed &Deployment event: MODIFIED
  May  4 06:25:57.503: INFO: Observed deployment test-deployment-8bzvs in namespace deployment-4422 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-04 06:25:55 +0000 UTC 2023-05-04 06:25:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May  4 06:25:57.503: INFO: Observed deployment test-deployment-8bzvs in namespace deployment-4422 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 06:25:55 +0000 UTC 2023-05-04 06:25:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8bzvs-5994cf9475" has successfully progressed.}
  May  4 06:25:57.504: INFO: Observed &Deployment event: MODIFIED
  May  4 06:25:57.504: INFO: Observed deployment test-deployment-8bzvs in namespace deployment-4422 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-04 06:25:55 +0000 UTC 2023-05-04 06:25:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May  4 06:25:57.504: INFO: Observed deployment test-deployment-8bzvs in namespace deployment-4422 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-04 06:25:55 +0000 UTC 2023-05-04 06:25:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8bzvs-5994cf9475" has successfully progressed.}
  May  4 06:25:57.504: INFO: Observed deployment test-deployment-8bzvs in namespace deployment-4422 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May  4 06:25:57.504: INFO: Observed &Deployment event: MODIFIED
  May  4 06:25:57.504: INFO: Found deployment test-deployment-8bzvs in namespace deployment-4422 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  May  4 06:25:57.504: INFO: Deployment test-deployment-8bzvs has a patched status
  May  4 06:25:57.516: INFO: Deployment "test-deployment-8bzvs":
  &Deployment{ObjectMeta:{test-deployment-8bzvs  deployment-4422  bf33845b-0c67-4a82-a3a0-6852d8c3f571 54977 1 2023-05-04 06:25:53 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-04 06:25:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-05-04 06:25:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-05-04 06:25:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d16428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-8bzvs-5994cf9475",LastUpdateTime:2023-05-04 06:25:57 +0000 UTC,LastTransitionTime:2023-05-04 06:25:57 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May  4 06:25:57.520: INFO: New ReplicaSet "test-deployment-8bzvs-5994cf9475" of Deployment "test-deployment-8bzvs":
  &ReplicaSet{ObjectMeta:{test-deployment-8bzvs-5994cf9475  deployment-4422  3d19ed9b-5291-4062-9506-782087e39738 54971 1 2023-05-04 06:25:53 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-8bzvs bf33845b-0c67-4a82-a3a0-6852d8c3f571 0xc00510e9d0 0xc00510e9d1}] [] [{kube-controller-manager Update apps/v1 2023-05-04 06:25:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf33845b-0c67-4a82-a3a0-6852d8c3f571\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 06:25:55 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00510ea78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May  4 06:25:57.524: INFO: Pod "test-deployment-8bzvs-5994cf9475-vdd9x" is available:
  &Pod{ObjectMeta:{test-deployment-8bzvs-5994cf9475-vdd9x test-deployment-8bzvs-5994cf9475- deployment-4422  0cb505ea-1b14-4a8f-a6a8-ca9167457401 54970 0 2023-05-04 06:25:53 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[cni.projectcalico.org/containerID:e709fb929bae376d1ef25563f86725a9dbec5dc1e3dca871c0e32890ce877820 cni.projectcalico.org/podIP:172.16.169.145/32 cni.projectcalico.org/podIPs:172.16.169.145/32] [{apps/v1 ReplicaSet test-deployment-8bzvs-5994cf9475 3d19ed9b-5291-4062-9506-782087e39738 0xc00510ee40 0xc00510ee41}] [] [{kube-controller-manager Update v1 2023-05-04 06:25:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d19ed9b-5291-4062-9506-782087e39738\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 06:25:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 06:25:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.169.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ptk54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ptk54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 06:25:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 06:25:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 06:25:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 06:25:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.157,PodIP:172.16.169.145,StartTime:2023-05-04 06:25:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 06:25:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://7a0c0751595da0559175feb0546b7b3fb6ad5174580f01dfbc634c1378f2ac72,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.169.145,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 06:25:57.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4422" for this suite. @ 05/04/23 06:25:57.55
• [4.353 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 05/04/23 06:25:57.566
  May  4 06:25:57.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename services @ 05/04/23 06:25:57.567
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:25:57.628
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:25:57.633
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-4399 @ 05/04/23 06:25:57.638
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 05/04/23 06:25:57.705
  STEP: creating service externalsvc in namespace services-4399 @ 05/04/23 06:25:57.705
  STEP: creating replication controller externalsvc in namespace services-4399 @ 05/04/23 06:25:57.772
  I0504 06:25:57.805802      20 runners.go:194] Created replication controller with name: externalsvc, namespace: services-4399, replica count: 2
  I0504 06:26:00.856917      20 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 05/04/23 06:26:00.86
  May  4 06:26:00.927: INFO: Creating new exec pod
  May  4 06:26:04.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-4399 exec execpodl9nh4 -- /bin/sh -x -c nslookup nodeport-service.services-4399.svc.cluster.local'
  May  4 06:26:05.231: INFO: stderr: "+ nslookup nodeport-service.services-4399.svc.cluster.local\n"
  May  4 06:26:05.232: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-4399.svc.cluster.local\tcanonical name = externalsvc.services-4399.svc.cluster.local.\nName:\texternalsvc.services-4399.svc.cluster.local\nAddress: 10.108.41.135\n\n"
  May  4 06:26:05.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-4399, will wait for the garbage collector to delete the pods @ 05/04/23 06:26:05.237
  May  4 06:26:05.306: INFO: Deleting ReplicationController externalsvc took: 15.579426ms
  May  4 06:26:05.407: INFO: Terminating ReplicationController externalsvc pods took: 100.875826ms
  May  4 06:26:38.789: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-4399" for this suite. @ 05/04/23 06:26:38.868
• [41.358 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 05/04/23 06:26:38.926
  May  4 06:26:38.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename subpath @ 05/04/23 06:26:38.927
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:26:39.019
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:26:39.023
  STEP: Setting up data @ 05/04/23 06:26:39.026
  STEP: Creating pod pod-subpath-test-secret-tzcq @ 05/04/23 06:26:39.083
  STEP: Creating a pod to test atomic-volume-subpath @ 05/04/23 06:26:39.083
  STEP: Saw pod success @ 05/04/23 06:27:03.187
  May  4 06:27:03.191: INFO: Trying to get logs from node k8s-node2 pod pod-subpath-test-secret-tzcq container test-container-subpath-secret-tzcq: <nil>
  STEP: delete the pod @ 05/04/23 06:27:03.2
  STEP: Deleting pod pod-subpath-test-secret-tzcq @ 05/04/23 06:27:03.259
  May  4 06:27:03.259: INFO: Deleting pod "pod-subpath-test-secret-tzcq" in namespace "subpath-3642"
  May  4 06:27:03.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-3642" for this suite. @ 05/04/23 06:27:03.271
• [24.363 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 05/04/23 06:27:03.293
  May  4 06:27:03.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/04/23 06:27:03.294
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:27:03.358
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:27:03.364
  May  4 06:27:03.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/04/23 06:27:05.426
  May  4 06:27:05.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-792 --namespace=crd-publish-openapi-792 create -f -'
  May  4 06:27:06.508: INFO: stderr: ""
  May  4 06:27:06.508: INFO: stdout: "e2e-test-crd-publish-openapi-140-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  May  4 06:27:06.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-792 --namespace=crd-publish-openapi-792 delete e2e-test-crd-publish-openapi-140-crds test-cr'
  May  4 06:27:06.636: INFO: stderr: ""
  May  4 06:27:06.637: INFO: stdout: "e2e-test-crd-publish-openapi-140-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  May  4 06:27:06.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-792 --namespace=crd-publish-openapi-792 apply -f -'
  May  4 06:27:07.019: INFO: stderr: ""
  May  4 06:27:07.019: INFO: stdout: "e2e-test-crd-publish-openapi-140-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  May  4 06:27:07.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-792 --namespace=crd-publish-openapi-792 delete e2e-test-crd-publish-openapi-140-crds test-cr'
  May  4 06:27:07.147: INFO: stderr: ""
  May  4 06:27:07.147: INFO: stdout: "e2e-test-crd-publish-openapi-140-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 05/04/23 06:27:07.147
  May  4 06:27:07.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-792 explain e2e-test-crd-publish-openapi-140-crds'
  May  4 06:27:07.512: INFO: stderr: ""
  May  4 06:27:07.512: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-140-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  May  4 06:27:09.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-792" for this suite. @ 05/04/23 06:27:09.071
• [5.791 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 05/04/23 06:27:09.086
  May  4 06:27:09.086: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename deployment @ 05/04/23 06:27:09.087
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:27:09.163
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:27:09.168
  May  4 06:27:09.171: INFO: Creating deployment "test-recreate-deployment"
  May  4 06:27:09.187: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  May  4 06:27:09.229: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  May  4 06:27:11.237: INFO: Waiting deployment "test-recreate-deployment" to complete
  May  4 06:27:11.240: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 6, 27, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 27, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 6, 27, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 27, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6c99bf8bf6\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  4 06:27:13.244: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  May  4 06:27:13.267: INFO: Updating deployment test-recreate-deployment
  May  4 06:27:13.267: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  May  4 06:27:13.623: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-8972  4cc1b240-71c6-4979-b816-a420faf54aad 55359 2 2023-05-04 06:27:09 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-04 06:27:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 06:27:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0036a7cc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-04 06:27:13 +0000 UTC,LastTransitionTime:2023-05-04 06:27:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-05-04 06:27:13 +0000 UTC,LastTransitionTime:2023-05-04 06:27:09 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  May  4 06:27:13.627: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-8972  e120517e-d553-48a5-adef-11947fbdd772 55358 1 2023-05-04 06:27:13 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 4cc1b240-71c6-4979-b816-a420faf54aad 0xc0007177f7 0xc0007177f8}] [] [{kube-controller-manager Update apps/v1 2023-05-04 06:27:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4cc1b240-71c6-4979-b816-a420faf54aad\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 06:27:13 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000717898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May  4 06:27:13.627: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  May  4 06:27:13.628: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-8972  c269b88a-f14c-426f-b4d9-3d9f1295671c 55348 2 2023-05-04 06:27:09 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 4cc1b240-71c6-4979-b816-a420faf54aad 0xc000717907 0xc000717908}] [] [{kube-controller-manager Update apps/v1 2023-05-04 06:27:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4cc1b240-71c6-4979-b816-a420faf54aad\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 06:27:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007179b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May  4 06:27:13.631: INFO: Pod "test-recreate-deployment-54757ffd6c-kprgh" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-kprgh test-recreate-deployment-54757ffd6c- deployment-8972  b96f9a8f-dce0-477f-a618-699d196d878a 55360 0 2023-05-04 06:27:13 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c e120517e-d553-48a5-adef-11947fbdd772 0xc004d58057 0xc004d58058}] [] [{kube-controller-manager Update v1 2023-05-04 06:27:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e120517e-d553-48a5-adef-11947fbdd772\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 06:27:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tx8gp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tx8gp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 06:27:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 06:27:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 06:27:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 06:27:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.156,PodIP:,StartTime:2023-05-04 06:27:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 06:27:13.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8972" for this suite. @ 05/04/23 06:27:13.636
• [4.567 seconds]
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 05/04/23 06:27:13.654
  May  4 06:27:13.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename kubelet-test @ 05/04/23 06:27:13.655
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:27:13.715
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:27:13.724
  STEP: Waiting for pod completion @ 05/04/23 06:27:13.762
  May  4 06:27:17.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-731" for this suite. @ 05/04/23 06:27:17.789
• [4.162 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 05/04/23 06:27:17.817
  May  4 06:27:17.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename csistoragecapacity @ 05/04/23 06:27:17.818
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:27:17.877
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:27:17.882
  STEP: getting /apis @ 05/04/23 06:27:17.885
  STEP: getting /apis/storage.k8s.io @ 05/04/23 06:27:17.892
  STEP: getting /apis/storage.k8s.io/v1 @ 05/04/23 06:27:17.893
  STEP: creating @ 05/04/23 06:27:17.895
  STEP: watching @ 05/04/23 06:27:17.944
  May  4 06:27:17.944: INFO: starting watch
  STEP: getting @ 05/04/23 06:27:17.977
  STEP: listing in namespace @ 05/04/23 06:27:17.98
  STEP: listing across namespaces @ 05/04/23 06:27:17.984
  STEP: patching @ 05/04/23 06:27:17.988
  STEP: updating @ 05/04/23 06:27:18.003
  May  4 06:27:18.019: INFO: waiting for watch events with expected annotations in namespace
  May  4 06:27:18.019: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 05/04/23 06:27:18.02
  STEP: deleting a collection @ 05/04/23 06:27:18.04
  May  4 06:27:18.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-6294" for this suite. @ 05/04/23 06:27:18.113
• [0.316 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 05/04/23 06:27:18.138
  May  4 06:27:18.138: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename init-container @ 05/04/23 06:27:18.139
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:27:18.17
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:27:18.175
  STEP: creating the pod @ 05/04/23 06:27:18.192
  May  4 06:27:18.192: INFO: PodSpec: initContainers in spec.initContainers
  May  4 06:27:23.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-3136" for this suite. @ 05/04/23 06:27:23.37
• [5.268 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 05/04/23 06:27:23.407
  May  4 06:27:23.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 06:27:23.41
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:27:23.515
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:27:23.522
  STEP: Creating a pod to test downward API volume plugin @ 05/04/23 06:27:23.53
  STEP: Saw pod success @ 05/04/23 06:27:29.667
  May  4 06:27:29.670: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-83076f5f-3b32-4b67-bf96-2fca557b64b6 container client-container: <nil>
  STEP: delete the pod @ 05/04/23 06:27:29.676
  May  4 06:27:29.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3428" for this suite. @ 05/04/23 06:27:29.725
• [6.352 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 05/04/23 06:27:29.761
  May  4 06:27:29.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename webhook @ 05/04/23 06:27:29.762
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:27:29.817
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:27:29.821
  STEP: Setting up server cert @ 05/04/23 06:27:29.917
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/04/23 06:27:30.498
  STEP: Deploying the webhook pod @ 05/04/23 06:27:30.548
  STEP: Wait for the deployment to be ready @ 05/04/23 06:27:30.598
  May  4 06:27:30.625: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/04/23 06:27:32.637
  STEP: Verifying the service has paired with the endpoint @ 05/04/23 06:27:32.682
  May  4 06:27:33.683: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May  4 06:27:33.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 05/04/23 06:27:34.223
  STEP: Creating a custom resource that should be denied by the webhook @ 05/04/23 06:27:34.266
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 05/04/23 06:27:36.296
  STEP: Updating the custom resource with disallowed data should be denied @ 05/04/23 06:27:36.313
  STEP: Deleting the custom resource should be denied @ 05/04/23 06:27:36.323
  STEP: Remove the offending key and value from the custom resource data @ 05/04/23 06:27:36.329
  STEP: Deleting the updated custom resource should be successful @ 05/04/23 06:27:36.385
  May  4 06:27:36.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8968" for this suite. @ 05/04/23 06:27:37.215
  STEP: Destroying namespace "webhook-markers-1962" for this suite. @ 05/04/23 06:27:37.236
• [7.512 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 05/04/23 06:27:37.275
  May  4 06:27:37.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename services @ 05/04/23 06:27:37.276
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:27:37.309
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:27:37.314
  STEP: creating service in namespace services-7143 @ 05/04/23 06:27:37.318
  STEP: creating service affinity-nodeport-transition in namespace services-7143 @ 05/04/23 06:27:37.318
  STEP: creating replication controller affinity-nodeport-transition in namespace services-7143 @ 05/04/23 06:27:37.404
  I0504 06:27:37.441416      20 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-7143, replica count: 3
  I0504 06:27:40.492840      20 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  4 06:27:40.505: INFO: Creating new exec pod
  May  4 06:27:45.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-7143 exec execpod-affinitys47mt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  May  4 06:27:45.773: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  May  4 06:27:45.773: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  4 06:27:45.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-7143 exec execpod-affinitys47mt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.149.9 80'
  May  4 06:27:45.972: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.149.9 80\nConnection to 10.99.149.9 80 port [tcp/http] succeeded!\n"
  May  4 06:27:45.972: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  4 06:27:45.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-7143 exec execpod-affinitys47mt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.156 30540'
  May  4 06:27:46.180: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.156 30540\nConnection to 192.168.0.156 30540 port [tcp/*] succeeded!\n"
  May  4 06:27:46.180: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  4 06:27:46.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-7143 exec execpod-affinitys47mt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.157 30540'
  May  4 06:27:46.382: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.157 30540\nConnection to 192.168.0.157 30540 port [tcp/*] succeeded!\n"
  May  4 06:27:46.383: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  4 06:27:46.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-7143 exec execpod-affinitys47mt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.156:30540/ ; done'
  May  4 06:27:46.733: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n"
  May  4 06:27:46.733: INFO: stdout: "\naffinity-nodeport-transition-h5hlm\naffinity-nodeport-transition-t267p\naffinity-nodeport-transition-h5hlm\naffinity-nodeport-transition-h5hlm\naffinity-nodeport-transition-h5hlm\naffinity-nodeport-transition-rbg2r\naffinity-nodeport-transition-h5hlm\naffinity-nodeport-transition-h5hlm\naffinity-nodeport-transition-rbg2r\naffinity-nodeport-transition-rbg2r\naffinity-nodeport-transition-t267p\naffinity-nodeport-transition-h5hlm\naffinity-nodeport-transition-t267p\naffinity-nodeport-transition-rbg2r\naffinity-nodeport-transition-h5hlm\naffinity-nodeport-transition-rbg2r"
  May  4 06:27:46.733: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:46.733: INFO: Received response from host: affinity-nodeport-transition-t267p
  May  4 06:27:46.733: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:46.733: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:46.733: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:46.733: INFO: Received response from host: affinity-nodeport-transition-rbg2r
  May  4 06:27:46.733: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:46.733: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:46.733: INFO: Received response from host: affinity-nodeport-transition-rbg2r
  May  4 06:27:46.733: INFO: Received response from host: affinity-nodeport-transition-rbg2r
  May  4 06:27:46.733: INFO: Received response from host: affinity-nodeport-transition-t267p
  May  4 06:27:46.733: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:46.733: INFO: Received response from host: affinity-nodeport-transition-t267p
  May  4 06:27:46.733: INFO: Received response from host: affinity-nodeport-transition-rbg2r
  May  4 06:27:46.733: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:46.733: INFO: Received response from host: affinity-nodeport-transition-rbg2r
  May  4 06:27:46.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-7143 exec execpod-affinitys47mt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.156:30540/ ; done'
  May  4 06:27:47.079: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30540/\n"
  May  4 06:27:47.079: INFO: stdout: "\naffinity-nodeport-transition-h5hlm\naffinity-nodeport-transition-h5hlm\naffinity-nodeport-transition-h5hlm\naffinity-nodeport-transition-h5hlm\naffinity-nodeport-transition-h5hlm\naffinity-nodeport-transition-h5hlm\naffinity-nodeport-transition-h5hlm\naffinity-nodeport-transition-h5hlm\naffinity-nodeport-transition-h5hlm\naffinity-nodeport-transition-h5hlm\naffinity-nodeport-transition-h5hlm\naffinity-nodeport-transition-h5hlm\naffinity-nodeport-transition-h5hlm\naffinity-nodeport-transition-h5hlm\naffinity-nodeport-transition-h5hlm\naffinity-nodeport-transition-h5hlm"
  May  4 06:27:47.079: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:47.079: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:47.079: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:47.079: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:47.079: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:47.079: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:47.079: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:47.079: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:47.079: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:47.079: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:47.079: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:47.079: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:47.079: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:47.079: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:47.079: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:47.079: INFO: Received response from host: affinity-nodeport-transition-h5hlm
  May  4 06:27:47.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  4 06:27:47.084: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7143, will wait for the garbage collector to delete the pods @ 05/04/23 06:27:47.158
  May  4 06:27:47.235: INFO: Deleting ReplicationController affinity-nodeport-transition took: 21.585228ms
  May  4 06:27:47.336: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.729535ms
  STEP: Destroying namespace "services-7143" for this suite. @ 05/04/23 06:27:50.743
• [13.507 seconds]
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 05/04/23 06:27:50.782
  May  4 06:27:50.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename svc-latency @ 05/04/23 06:27:50.784
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:27:50.885
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:27:50.889
  May  4 06:27:50.892: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-9801 @ 05/04/23 06:27:50.893
  I0504 06:27:50.925958      20 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9801, replica count: 1
  I0504 06:27:51.977509      20 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0504 06:27:52.978674      20 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  4 06:27:53.152: INFO: Created: latency-svc-shxm6
  May  4 06:27:53.194: INFO: Got endpoints: latency-svc-shxm6 [115.158413ms]
  May  4 06:27:53.280: INFO: Created: latency-svc-78g2k
  May  4 06:27:53.341: INFO: Got endpoints: latency-svc-78g2k [145.727174ms]
  May  4 06:27:53.350: INFO: Created: latency-svc-f4v99
  May  4 06:27:53.408: INFO: Got endpoints: latency-svc-f4v99 [212.676005ms]
  May  4 06:27:53.421: INFO: Created: latency-svc-ctnhd
  May  4 06:27:53.481: INFO: Got endpoints: latency-svc-ctnhd [284.904999ms]
  May  4 06:27:53.492: INFO: Created: latency-svc-2j69z
  May  4 06:27:53.544: INFO: Got endpoints: latency-svc-2j69z [349.062372ms]
  May  4 06:27:53.557: INFO: Created: latency-svc-4bcv5
  May  4 06:27:53.587: INFO: Got endpoints: latency-svc-4bcv5 [391.008448ms]
  May  4 06:27:53.597: INFO: Created: latency-svc-8vvns
  May  4 06:27:53.663: INFO: Created: latency-svc-krcnv
  May  4 06:27:53.670: INFO: Got endpoints: latency-svc-8vvns [473.363246ms]
  May  4 06:27:53.706: INFO: Got endpoints: latency-svc-krcnv [509.396534ms]
  May  4 06:27:53.713: INFO: Created: latency-svc-dn6wh
  May  4 06:27:53.748: INFO: Got endpoints: latency-svc-dn6wh [550.998984ms]
  May  4 06:27:53.760: INFO: Created: latency-svc-9n6w9
  May  4 06:27:53.801: INFO: Got endpoints: latency-svc-9n6w9 [603.858842ms]
  May  4 06:27:53.821: INFO: Created: latency-svc-jcsqd
  May  4 06:27:53.852: INFO: Got endpoints: latency-svc-jcsqd [654.647354ms]
  May  4 06:27:53.895: INFO: Created: latency-svc-scvnq
  May  4 06:27:53.943: INFO: Got endpoints: latency-svc-scvnq [745.904743ms]
  May  4 06:27:53.960: INFO: Created: latency-svc-vpp9t
  May  4 06:27:53.976: INFO: Got endpoints: latency-svc-vpp9t [778.596101ms]
  May  4 06:27:54.029: INFO: Created: latency-svc-l5z4p
  May  4 06:27:54.081: INFO: Got endpoints: latency-svc-l5z4p [883.546508ms]
  May  4 06:27:54.094: INFO: Created: latency-svc-jfrsf
  May  4 06:27:54.119: INFO: Got endpoints: latency-svc-jfrsf [920.69192ms]
  May  4 06:27:54.168: INFO: Created: latency-svc-lmwdb
  May  4 06:27:54.229: INFO: Got endpoints: latency-svc-lmwdb [1.032228032s]
  May  4 06:27:54.245: INFO: Created: latency-svc-nr8fr
  May  4 06:27:54.276: INFO: Got endpoints: latency-svc-nr8fr [934.503161ms]
  May  4 06:27:54.298: INFO: Created: latency-svc-fd8wj
  May  4 06:27:54.365: INFO: Got endpoints: latency-svc-fd8wj [957.240419ms]
  May  4 06:27:54.368: INFO: Created: latency-svc-txjvr
  May  4 06:27:54.395: INFO: Got endpoints: latency-svc-txjvr [913.873677ms]
  May  4 06:27:54.438: INFO: Created: latency-svc-d4zrg
  May  4 06:27:54.451: INFO: Got endpoints: latency-svc-d4zrg [906.844181ms]
  May  4 06:27:54.511: INFO: Created: latency-svc-c8mfr
  May  4 06:27:54.553: INFO: Got endpoints: latency-svc-c8mfr [966.151401ms]
  May  4 06:27:54.591: INFO: Created: latency-svc-44h2p
  May  4 06:27:54.628: INFO: Got endpoints: latency-svc-44h2p [957.15731ms]
  May  4 06:27:54.644: INFO: Created: latency-svc-qbw5t
  May  4 06:27:54.691: INFO: Got endpoints: latency-svc-qbw5t [985.308251ms]
  May  4 06:27:54.699: INFO: Created: latency-svc-gmmk9
  May  4 06:27:54.787: INFO: Got endpoints: latency-svc-gmmk9 [1.038646901s]
  May  4 06:27:54.801: INFO: Created: latency-svc-5wr9x
  May  4 06:27:54.832: INFO: Got endpoints: latency-svc-5wr9x [1.031068269s]
  May  4 06:27:54.859: INFO: Created: latency-svc-fqjbx
  May  4 06:27:54.913: INFO: Got endpoints: latency-svc-fqjbx [1.060696388s]
  May  4 06:27:54.929: INFO: Created: latency-svc-m6fjt
  May  4 06:27:54.994: INFO: Got endpoints: latency-svc-m6fjt [1.050647043s]
  May  4 06:27:55.006: INFO: Created: latency-svc-7qt89
  May  4 06:27:55.054: INFO: Got endpoints: latency-svc-7qt89 [1.077755638s]
  May  4 06:27:55.067: INFO: Created: latency-svc-kw9sf
  May  4 06:27:55.080: INFO: Got endpoints: latency-svc-kw9sf [998.826683ms]
  May  4 06:27:55.128: INFO: Created: latency-svc-c7qz7
  May  4 06:27:55.195: INFO: Got endpoints: latency-svc-c7qz7 [1.075746183s]
  May  4 06:27:55.196: INFO: Created: latency-svc-cshk6
  May  4 06:27:55.244: INFO: Got endpoints: latency-svc-cshk6 [1.014607182s]
  May  4 06:27:55.258: INFO: Created: latency-svc-dt9fs
  May  4 06:27:55.336: INFO: Got endpoints: latency-svc-dt9fs [1.060645361s]
  May  4 06:27:55.339: INFO: Created: latency-svc-mjngq
  May  4 06:27:55.376: INFO: Got endpoints: latency-svc-mjngq [1.010488612s]
  May  4 06:27:55.390: INFO: Created: latency-svc-kqlfz
  May  4 06:27:55.404: INFO: Got endpoints: latency-svc-kqlfz [1.008984994s]
  May  4 06:27:55.461: INFO: Created: latency-svc-lx4l8
  May  4 06:27:55.493: INFO: Got endpoints: latency-svc-lx4l8 [1.042620558s]
  May  4 06:27:55.506: INFO: Created: latency-svc-2xkn5
  May  4 06:27:55.599: INFO: Got endpoints: latency-svc-2xkn5 [1.045942269s]
  May  4 06:27:55.606: INFO: Created: latency-svc-tmxkt
  May  4 06:27:55.656: INFO: Got endpoints: latency-svc-tmxkt [1.028316466s]
  May  4 06:27:55.666: INFO: Created: latency-svc-p9mkj
  May  4 06:27:55.677: INFO: Got endpoints: latency-svc-p9mkj [985.327874ms]
  May  4 06:27:55.738: INFO: Created: latency-svc-pmtjl
  May  4 06:27:55.770: INFO: Got endpoints: latency-svc-pmtjl [983.372779ms]
  May  4 06:27:55.794: INFO: Created: latency-svc-fx6b8
  May  4 06:27:55.825: INFO: Got endpoints: latency-svc-fx6b8 [992.899175ms]
  May  4 06:27:55.873: INFO: Created: latency-svc-cr6dm
  May  4 06:27:55.892: INFO: Created: latency-svc-gjw99
  May  4 06:27:55.906: INFO: Got endpoints: latency-svc-cr6dm [993.164093ms]
  May  4 06:27:55.937: INFO: Got endpoints: latency-svc-gjw99 [943.467911ms]
  May  4 06:27:55.959: INFO: Created: latency-svc-cvv5l
  May  4 06:27:56.009: INFO: Got endpoints: latency-svc-cvv5l [955.023819ms]
  May  4 06:27:56.018: INFO: Created: latency-svc-5rswb
  May  4 06:27:56.057: INFO: Got endpoints: latency-svc-5rswb [976.753079ms]
  May  4 06:27:56.082: INFO: Created: latency-svc-dxsl7
  May  4 06:27:56.151: INFO: Got endpoints: latency-svc-dxsl7 [956.791769ms]
  May  4 06:27:56.193: INFO: Created: latency-svc-8ldbt
  May  4 06:27:56.237: INFO: Got endpoints: latency-svc-8ldbt [992.837216ms]
  May  4 06:27:56.249: INFO: Created: latency-svc-trqvr
  May  4 06:27:56.304: INFO: Got endpoints: latency-svc-trqvr [967.444934ms]
  May  4 06:27:56.345: INFO: Created: latency-svc-sbwvq
  May  4 06:27:56.360: INFO: Created: latency-svc-rr8md
  May  4 06:27:56.382: INFO: Got endpoints: latency-svc-sbwvq [1.006016604s]
  May  4 06:27:56.420: INFO: Got endpoints: latency-svc-rr8md [1.01637272s]
  May  4 06:27:56.452: INFO: Created: latency-svc-scvj5
  May  4 06:27:56.480: INFO: Got endpoints: latency-svc-scvj5 [986.029039ms]
  May  4 06:27:56.489: INFO: Created: latency-svc-hw8p8
  May  4 06:27:56.563: INFO: Got endpoints: latency-svc-hw8p8 [963.447896ms]
  May  4 06:27:56.563: INFO: Created: latency-svc-wbv6z
  May  4 06:27:56.600: INFO: Got endpoints: latency-svc-wbv6z [943.900652ms]
  May  4 06:27:56.616: INFO: Created: latency-svc-9fgbk
  May  4 06:27:56.651: INFO: Got endpoints: latency-svc-9fgbk [974.151024ms]
  May  4 06:27:56.709: INFO: Created: latency-svc-8kcwc
  May  4 06:27:56.726: INFO: Created: latency-svc-n8txz
  May  4 06:27:56.764: INFO: Got endpoints: latency-svc-8kcwc [993.654579ms]
  May  4 06:27:56.777: INFO: Got endpoints: latency-svc-n8txz [951.3049ms]
  May  4 06:27:56.796: INFO: Created: latency-svc-t89cr
  May  4 06:27:56.849: INFO: Got endpoints: latency-svc-t89cr [942.647408ms]
  May  4 06:27:56.877: INFO: Created: latency-svc-vb5n6
  May  4 06:27:56.928: INFO: Got endpoints: latency-svc-vb5n6 [990.786909ms]
  May  4 06:27:56.938: INFO: Created: latency-svc-jzpbr
  May  4 06:27:56.998: INFO: Got endpoints: latency-svc-jzpbr [988.701709ms]
  May  4 06:27:57.011: INFO: Created: latency-svc-sm9vd
  May  4 06:27:57.054: INFO: Got endpoints: latency-svc-sm9vd [997.148644ms]
  May  4 06:27:57.064: INFO: Created: latency-svc-rjz2v
  May  4 06:27:57.139: INFO: Got endpoints: latency-svc-rjz2v [987.369284ms]
  May  4 06:27:57.140: INFO: Created: latency-svc-g8fcq
  May  4 06:27:57.191: INFO: Got endpoints: latency-svc-g8fcq [954.542162ms]
  May  4 06:27:57.206: INFO: Created: latency-svc-mhl4b
  May  4 06:27:57.271: INFO: Got endpoints: latency-svc-mhl4b [967.320145ms]
  May  4 06:27:57.290: INFO: Created: latency-svc-7lttg
  May  4 06:27:57.333: INFO: Got endpoints: latency-svc-7lttg [951.083625ms]
  May  4 06:27:57.345: INFO: Created: latency-svc-xprrd
  May  4 06:27:57.395: INFO: Got endpoints: latency-svc-xprrd [974.993059ms]
  May  4 06:27:57.408: INFO: Created: latency-svc-bjcnc
  May  4 06:27:57.456: INFO: Got endpoints: latency-svc-bjcnc [976.75393ms]
  May  4 06:27:57.464: INFO: Created: latency-svc-zmn7f
  May  4 06:27:57.536: INFO: Got endpoints: latency-svc-zmn7f [972.991167ms]
  May  4 06:27:57.541: INFO: Created: latency-svc-rm9w8
  May  4 06:27:57.581: INFO: Got endpoints: latency-svc-rm9w8 [980.64706ms]
  May  4 06:27:57.590: INFO: Created: latency-svc-95krz
  May  4 06:27:57.629: INFO: Got endpoints: latency-svc-95krz [978.45159ms]
  May  4 06:27:57.663: INFO: Created: latency-svc-qz5fs
  May  4 06:27:57.719: INFO: Got endpoints: latency-svc-qz5fs [954.508873ms]
  May  4 06:27:57.729: INFO: Created: latency-svc-xfv9g
  May  4 06:27:57.784: INFO: Got endpoints: latency-svc-xfv9g [1.00737807s]
  May  4 06:27:57.800: INFO: Created: latency-svc-cz2kv
  May  4 06:27:57.842: INFO: Got endpoints: latency-svc-cz2kv [992.581291ms]
  May  4 06:27:57.853: INFO: Created: latency-svc-2brbj
  May  4 06:27:57.911: INFO: Got endpoints: latency-svc-2brbj [982.749206ms]
  May  4 06:27:57.935: INFO: Created: latency-svc-vjg2g
  May  4 06:27:57.966: INFO: Got endpoints: latency-svc-vjg2g [967.49018ms]
  May  4 06:27:57.986: INFO: Created: latency-svc-c66kg
  May  4 06:27:58.002: INFO: Got endpoints: latency-svc-c66kg [947.26029ms]
  May  4 06:27:58.066: INFO: Created: latency-svc-5w5fp
  May  4 06:27:58.105: INFO: Got endpoints: latency-svc-5w5fp [966.093984ms]
  May  4 06:27:58.117: INFO: Created: latency-svc-sf7bv
  May  4 06:27:58.181: INFO: Got endpoints: latency-svc-sf7bv [989.714191ms]
  May  4 06:27:58.192: INFO: Created: latency-svc-z6gkd
  May  4 06:27:58.242: INFO: Got endpoints: latency-svc-z6gkd [970.685691ms]
  May  4 06:27:58.257: INFO: Created: latency-svc-r4fxd
  May  4 06:27:58.317: INFO: Got endpoints: latency-svc-r4fxd [983.915637ms]
  May  4 06:27:58.345: INFO: Created: latency-svc-2s7pl
  May  4 06:27:58.362: INFO: Got endpoints: latency-svc-2s7pl [966.573651ms]
  May  4 06:27:58.399: INFO: Created: latency-svc-v72r9
  May  4 06:27:58.465: INFO: Got endpoints: latency-svc-v72r9 [1.008947895s]
  May  4 06:27:58.483: INFO: Created: latency-svc-kkbk4
  May  4 06:27:58.550: INFO: Got endpoints: latency-svc-kkbk4 [1.013626402s]
  May  4 06:27:58.591: INFO: Created: latency-svc-4jckb
  May  4 06:27:58.626: INFO: Got endpoints: latency-svc-4jckb [1.045589546s]
  May  4 06:27:58.646: INFO: Created: latency-svc-cdd8g
  May  4 06:27:58.663: INFO: Got endpoints: latency-svc-cdd8g [1.033694614s]
  May  4 06:27:58.723: INFO: Created: latency-svc-4fjjn
  May  4 06:27:58.774: INFO: Got endpoints: latency-svc-4fjjn [1.054758532s]
  May  4 06:27:58.785: INFO: Created: latency-svc-lmndg
  May  4 06:27:58.859: INFO: Got endpoints: latency-svc-lmndg [1.074359742s]
  May  4 06:27:58.861: INFO: Created: latency-svc-p5gbw
  May  4 06:27:58.901: INFO: Got endpoints: latency-svc-p5gbw [1.059084482s]
  May  4 06:27:58.930: INFO: Created: latency-svc-vkpmk
  May  4 06:27:59.000: INFO: Got endpoints: latency-svc-vkpmk [1.089063816s]
  May  4 06:27:59.017: INFO: Created: latency-svc-n44cp
  May  4 06:27:59.060: INFO: Got endpoints: latency-svc-n44cp [1.094433837s]
  May  4 06:27:59.078: INFO: Created: latency-svc-gk9wh
  May  4 06:27:59.137: INFO: Got endpoints: latency-svc-gk9wh [1.135308762s]
  May  4 06:27:59.154: INFO: Created: latency-svc-m98qz
  May  4 06:27:59.171: INFO: Got endpoints: latency-svc-m98qz [1.06564194s]
  May  4 06:27:59.225: INFO: Created: latency-svc-5zlcs
  May  4 06:27:59.292: INFO: Got endpoints: latency-svc-5zlcs [1.110864184s]
  May  4 06:27:59.309: INFO: Created: latency-svc-vxz7b
  May  4 06:27:59.367: INFO: Got endpoints: latency-svc-vxz7b [1.1250045s]
  May  4 06:27:59.383: INFO: Created: latency-svc-tzbw8
  May  4 06:27:59.431: INFO: Got endpoints: latency-svc-tzbw8 [1.113817969s]
  May  4 06:27:59.445: INFO: Created: latency-svc-cjwp6
  May  4 06:27:59.486: INFO: Got endpoints: latency-svc-cjwp6 [1.123582557s]
  May  4 06:27:59.512: INFO: Created: latency-svc-6f2cv
  May  4 06:27:59.561: INFO: Got endpoints: latency-svc-6f2cv [1.095256162s]
  May  4 06:27:59.604: INFO: Created: latency-svc-fqwmw
  May  4 06:27:59.634: INFO: Got endpoints: latency-svc-fqwmw [1.084536163s]
  May  4 06:27:59.659: INFO: Created: latency-svc-vzsfj
  May  4 06:27:59.743: INFO: Got endpoints: latency-svc-vzsfj [1.116792787s]
  May  4 06:27:59.786: INFO: Created: latency-svc-gh88f
  May  4 06:27:59.800: INFO: Got endpoints: latency-svc-gh88f [1.136258954s]
  May  4 06:27:59.870: INFO: Created: latency-svc-r6vt2
  May  4 06:27:59.920: INFO: Got endpoints: latency-svc-r6vt2 [1.146010145s]
  May  4 06:27:59.935: INFO: Created: latency-svc-pxglv
  May  4 06:27:59.995: INFO: Got endpoints: latency-svc-pxglv [1.136191561s]
  May  4 06:28:00.007: INFO: Created: latency-svc-nbj2j
  May  4 06:28:00.050: INFO: Got endpoints: latency-svc-nbj2j [1.149318727s]
  May  4 06:28:00.062: INFO: Created: latency-svc-sdr7c
  May  4 06:28:00.124: INFO: Got endpoints: latency-svc-sdr7c [1.12362301s]
  May  4 06:28:00.151: INFO: Created: latency-svc-kdsct
  May  4 06:28:00.187: INFO: Got endpoints: latency-svc-kdsct [1.126440818s]
  May  4 06:28:00.197: INFO: Created: latency-svc-kmxzn
  May  4 06:28:00.256: INFO: Created: latency-svc-pt2jd
  May  4 06:28:00.260: INFO: Got endpoints: latency-svc-kmxzn [1.123195128s]
  May  4 06:28:00.322: INFO: Got endpoints: latency-svc-pt2jd [1.15111783s]
  May  4 06:28:00.332: INFO: Created: latency-svc-2z8st
  May  4 06:28:00.394: INFO: Got endpoints: latency-svc-2z8st [1.101564804s]
  May  4 06:28:00.397: INFO: Created: latency-svc-b5gsz
  May  4 06:28:00.412: INFO: Got endpoints: latency-svc-b5gsz [1.044194137s]
  May  4 06:28:00.451: INFO: Created: latency-svc-w5zwr
  May  4 06:28:00.474: INFO: Got endpoints: latency-svc-w5zwr [1.042671722s]
  May  4 06:28:00.553: INFO: Created: latency-svc-vqq8j
  May  4 06:28:00.589: INFO: Got endpoints: latency-svc-vqq8j [1.10294502s]
  May  4 06:28:00.602: INFO: Created: latency-svc-67px6
  May  4 06:28:00.632: INFO: Got endpoints: latency-svc-67px6 [1.071028634s]
  May  4 06:28:00.673: INFO: Created: latency-svc-49hqj
  May  4 06:28:00.705: INFO: Got endpoints: latency-svc-49hqj [1.07024341s]
  May  4 06:28:00.719: INFO: Created: latency-svc-g92mx
  May  4 06:28:00.815: INFO: Got endpoints: latency-svc-g92mx [1.071791948s]
  May  4 06:28:00.822: INFO: Created: latency-svc-wgrsn
  May  4 06:28:00.869: INFO: Got endpoints: latency-svc-wgrsn [1.068953109s]
  May  4 06:28:00.880: INFO: Created: latency-svc-v8ksd
  May  4 06:28:00.900: INFO: Got endpoints: latency-svc-v8ksd [980.030801ms]
  May  4 06:28:01.004: INFO: Created: latency-svc-lhxpg
  May  4 06:28:01.037: INFO: Got endpoints: latency-svc-lhxpg [1.042149299s]
  May  4 06:28:01.058: INFO: Created: latency-svc-bb4hx
  May  4 06:28:01.098: INFO: Got endpoints: latency-svc-bb4hx [1.047690487s]
  May  4 06:28:01.118: INFO: Created: latency-svc-dv4mg
  May  4 06:28:01.164: INFO: Got endpoints: latency-svc-dv4mg [1.040253292s]
  May  4 06:28:01.175: INFO: Created: latency-svc-rwtd5
  May  4 06:28:01.237: INFO: Got endpoints: latency-svc-rwtd5 [1.04995161s]
  May  4 06:28:01.302: INFO: Created: latency-svc-p8nd5
  May  4 06:28:01.333: INFO: Got endpoints: latency-svc-p8nd5 [1.072733869s]
  May  4 06:28:01.365: INFO: Created: latency-svc-c6tjr
  May  4 06:28:01.398: INFO: Got endpoints: latency-svc-c6tjr [1.076039208s]
  May  4 06:28:01.410: INFO: Created: latency-svc-55gsl
  May  4 06:28:01.463: INFO: Got endpoints: latency-svc-55gsl [1.069239364s]
  May  4 06:28:01.492: INFO: Created: latency-svc-xkzt4
  May  4 06:28:01.511: INFO: Got endpoints: latency-svc-xkzt4 [1.099714811s]
  May  4 06:28:01.551: INFO: Created: latency-svc-54d2r
  May  4 06:28:01.587: INFO: Got endpoints: latency-svc-54d2r [1.112811711s]
  May  4 06:28:01.618: INFO: Created: latency-svc-c5s2b
  May  4 06:28:01.645: INFO: Got endpoints: latency-svc-c5s2b [1.0568437s]
  May  4 06:28:01.660: INFO: Created: latency-svc-qq9m8
  May  4 06:28:01.683: INFO: Got endpoints: latency-svc-qq9m8 [1.051354272s]
  May  4 06:28:01.757: INFO: Created: latency-svc-t2fmc
  May  4 06:28:01.771: INFO: Created: latency-svc-r6hsr
  May  4 06:28:01.794: INFO: Got endpoints: latency-svc-t2fmc [1.089061641s]
  May  4 06:28:01.817: INFO: Got endpoints: latency-svc-r6hsr [1.002064486s]
  May  4 06:28:01.841: INFO: Created: latency-svc-77lsp
  May  4 06:28:01.885: INFO: Got endpoints: latency-svc-77lsp [1.016297228s]
  May  4 06:28:01.902: INFO: Created: latency-svc-95pgf
  May  4 06:28:01.935: INFO: Got endpoints: latency-svc-95pgf [1.03517884s]
  May  4 06:28:01.945: INFO: Created: latency-svc-gw95n
  May  4 06:28:02.023: INFO: Got endpoints: latency-svc-gw95n [985.318804ms]
  May  4 06:28:02.024: INFO: Created: latency-svc-ng8f8
  May  4 06:28:02.075: INFO: Got endpoints: latency-svc-ng8f8 [976.880729ms]
  May  4 06:28:02.087: INFO: Created: latency-svc-vtlsl
  May  4 06:28:02.120: INFO: Got endpoints: latency-svc-vtlsl [955.525786ms]
  May  4 06:28:02.157: INFO: Created: latency-svc-7pzjz
  May  4 06:28:02.199: INFO: Got endpoints: latency-svc-7pzjz [962.352215ms]
  May  4 06:28:02.215: INFO: Created: latency-svc-4cjvk
  May  4 06:28:02.295: INFO: Got endpoints: latency-svc-4cjvk [962.082907ms]
  May  4 06:28:02.303: INFO: Created: latency-svc-xfhtd
  May  4 06:28:02.348: INFO: Got endpoints: latency-svc-xfhtd [950.170957ms]
  May  4 06:28:02.358: INFO: Created: latency-svc-hgqj2
  May  4 06:28:02.387: INFO: Got endpoints: latency-svc-hgqj2 [923.663747ms]
  May  4 06:28:02.441: INFO: Created: latency-svc-872cz
  May  4 06:28:02.465: INFO: Created: latency-svc-w77cx
  May  4 06:28:02.497: INFO: Got endpoints: latency-svc-872cz [985.745717ms]
  May  4 06:28:02.525: INFO: Got endpoints: latency-svc-w77cx [938.462852ms]
  May  4 06:28:02.536: INFO: Created: latency-svc-pcxwm
  May  4 06:28:02.581: INFO: Got endpoints: latency-svc-pcxwm [935.924635ms]
  May  4 06:28:02.591: INFO: Created: latency-svc-lm7hp
  May  4 06:28:02.628: INFO: Got endpoints: latency-svc-lm7hp [944.621625ms]
  May  4 06:28:02.649: INFO: Created: latency-svc-mhpxn
  May  4 06:28:02.710: INFO: Got endpoints: latency-svc-mhpxn [916.043032ms]
  May  4 06:28:02.726: INFO: Created: latency-svc-g4vwn
  May  4 06:28:02.776: INFO: Got endpoints: latency-svc-g4vwn [958.897072ms]
  May  4 06:28:02.791: INFO: Created: latency-svc-bpl79
  May  4 06:28:02.803: INFO: Got endpoints: latency-svc-bpl79 [918.040396ms]
  May  4 06:28:02.865: INFO: Created: latency-svc-tpl2c
  May  4 06:28:02.882: INFO: Got endpoints: latency-svc-tpl2c [947.088191ms]
  May  4 06:28:02.930: INFO: Created: latency-svc-ff6ql
  May  4 06:28:02.984: INFO: Got endpoints: latency-svc-ff6ql [961.390231ms]
  May  4 06:28:03.005: INFO: Created: latency-svc-blq75
  May  4 06:28:03.048: INFO: Got endpoints: latency-svc-blq75 [972.937915ms]
  May  4 06:28:03.060: INFO: Created: latency-svc-cvhv2
  May  4 06:28:03.132: INFO: Got endpoints: latency-svc-cvhv2 [1.01195882s]
  May  4 06:28:03.148: INFO: Created: latency-svc-d52h6
  May  4 06:28:03.185: INFO: Got endpoints: latency-svc-d52h6 [985.538764ms]
  May  4 06:28:03.210: INFO: Created: latency-svc-5j4r9
  May  4 06:28:03.223: INFO: Got endpoints: latency-svc-5j4r9 [927.792356ms]
  May  4 06:28:03.269: INFO: Created: latency-svc-kjfwp
  May  4 06:28:03.299: INFO: Got endpoints: latency-svc-kjfwp [950.279949ms]
  May  4 06:28:03.395: INFO: Created: latency-svc-s25k9
  May  4 06:28:03.427: INFO: Got endpoints: latency-svc-s25k9 [1.040638842s]
  May  4 06:28:03.487: INFO: Created: latency-svc-hc722
  May  4 06:28:03.531: INFO: Got endpoints: latency-svc-hc722 [1.033548659s]
  May  4 06:28:03.565: INFO: Created: latency-svc-8l4c2
  May  4 06:28:03.609: INFO: Got endpoints: latency-svc-8l4c2 [1.083709166s]
  May  4 06:28:03.625: INFO: Created: latency-svc-zwlp4
  May  4 06:28:03.699: INFO: Got endpoints: latency-svc-zwlp4 [1.117422257s]
  May  4 06:28:03.721: INFO: Created: latency-svc-hd2m6
  May  4 06:28:03.817: INFO: Created: latency-svc-25bb8
  May  4 06:28:03.818: INFO: Got endpoints: latency-svc-hd2m6 [1.190192143s]
  May  4 06:28:03.886: INFO: Got endpoints: latency-svc-25bb8 [1.175804983s]
  May  4 06:28:03.967: INFO: Created: latency-svc-cdjl8
  May  4 06:28:04.017: INFO: Got endpoints: latency-svc-cdjl8 [1.240633442s]
  May  4 06:28:04.034: INFO: Created: latency-svc-ljszp
  May  4 06:28:04.108: INFO: Got endpoints: latency-svc-ljszp [1.305129219s]
  May  4 06:28:04.126: INFO: Created: latency-svc-fwg6x
  May  4 06:28:04.171: INFO: Got endpoints: latency-svc-fwg6x [1.288343332s]
  May  4 06:28:04.183: INFO: Created: latency-svc-nlvv2
  May  4 06:28:04.251: INFO: Got endpoints: latency-svc-nlvv2 [1.266708383s]
  May  4 06:28:04.269: INFO: Created: latency-svc-v87cz
  May  4 06:28:04.324: INFO: Got endpoints: latency-svc-v87cz [1.275822199s]
  May  4 06:28:04.339: INFO: Created: latency-svc-d46f5
  May  4 06:28:04.397: INFO: Got endpoints: latency-svc-d46f5 [1.264495724s]
  May  4 06:28:04.413: INFO: Created: latency-svc-xmq87
  May  4 06:28:04.457: INFO: Got endpoints: latency-svc-xmq87 [1.271811711s]
  May  4 06:28:04.472: INFO: Created: latency-svc-4xfzj
  May  4 06:28:04.514: INFO: Got endpoints: latency-svc-4xfzj [1.291130874s]
  May  4 06:28:04.531: INFO: Created: latency-svc-hhgck
  May  4 06:28:04.593: INFO: Got endpoints: latency-svc-hhgck [1.293763347s]
  May  4 06:28:04.605: INFO: Created: latency-svc-246gd
  May  4 06:28:04.660: INFO: Got endpoints: latency-svc-246gd [1.232711059s]
  May  4 06:28:04.675: INFO: Created: latency-svc-6qbx6
  May  4 06:28:04.720: INFO: Got endpoints: latency-svc-6qbx6 [1.188747229s]
  May  4 06:28:04.734: INFO: Created: latency-svc-jdvv5
  May  4 06:28:04.820: INFO: Got endpoints: latency-svc-jdvv5 [1.210429202s]
  May  4 06:28:04.839: INFO: Created: latency-svc-wv282
  May  4 06:28:04.915: INFO: Got endpoints: latency-svc-wv282 [1.216402317s]
  May  4 06:28:04.957: INFO: Created: latency-svc-hqvhb
  May  4 06:28:04.974: INFO: Got endpoints: latency-svc-hqvhb [1.156056897s]
  May  4 06:28:05.029: INFO: Created: latency-svc-fxk6t
  May  4 06:28:05.045: INFO: Got endpoints: latency-svc-fxk6t [1.158623927s]
  May  4 06:28:05.113: INFO: Created: latency-svc-hw4j5
  May  4 06:28:05.147: INFO: Got endpoints: latency-svc-hw4j5 [1.129930096s]
  May  4 06:28:05.236: INFO: Created: latency-svc-8n8bx
  May  4 06:28:05.377: INFO: Created: latency-svc-nghj4
  May  4 06:28:05.381: INFO: Got endpoints: latency-svc-8n8bx [1.273005648s]
  May  4 06:28:05.394: INFO: Created: latency-svc-vgtjc
  May  4 06:28:05.423: INFO: Got endpoints: latency-svc-nghj4 [1.252441933s]
  May  4 06:28:05.435: INFO: Got endpoints: latency-svc-vgtjc [1.183618567s]
  May  4 06:28:05.466: INFO: Created: latency-svc-jv8rs
  May  4 06:28:05.527: INFO: Got endpoints: latency-svc-jv8rs [1.203003745s]
  May  4 06:28:05.543: INFO: Created: latency-svc-sl2br
  May  4 06:28:05.574: INFO: Got endpoints: latency-svc-sl2br [1.176894845s]
  May  4 06:28:05.621: INFO: Created: latency-svc-kc59c
  May  4 06:28:05.677: INFO: Got endpoints: latency-svc-kc59c [1.219875628s]
  May  4 06:28:05.696: INFO: Created: latency-svc-z2mzb
  May  4 06:28:05.765: INFO: Got endpoints: latency-svc-z2mzb [1.250007519s]
  May  4 06:28:05.780: INFO: Created: latency-svc-trts4
  May  4 06:28:05.821: INFO: Got endpoints: latency-svc-trts4 [1.22844156s]
  May  4 06:28:05.849: INFO: Created: latency-svc-6xrrr
  May  4 06:28:05.887: INFO: Got endpoints: latency-svc-6xrrr [1.226667365s]
  May  4 06:28:05.906: INFO: Created: latency-svc-v6bj6
  May  4 06:28:05.977: INFO: Got endpoints: latency-svc-v6bj6 [1.256886853s]
  May  4 06:28:06.003: INFO: Created: latency-svc-rkr9c
  May  4 06:28:06.055: INFO: Got endpoints: latency-svc-rkr9c [1.235583493s]
  May  4 06:28:06.072: INFO: Created: latency-svc-bg7t7
  May  4 06:28:06.120: INFO: Got endpoints: latency-svc-bg7t7 [1.204922922s]
  May  4 06:28:06.144: INFO: Created: latency-svc-h2zks
  May  4 06:28:06.176: INFO: Got endpoints: latency-svc-h2zks [1.201970554s]
  May  4 06:28:06.196: INFO: Created: latency-svc-gwlsj
  May  4 06:28:06.236: INFO: Got endpoints: latency-svc-gwlsj [1.190998065s]
  May  4 06:28:06.251: INFO: Created: latency-svc-qzdq2
  May  4 06:28:06.297: INFO: Got endpoints: latency-svc-qzdq2 [1.150254016s]
  May  4 06:28:06.306: INFO: Created: latency-svc-mhkdj
  May  4 06:28:06.361: INFO: Got endpoints: latency-svc-mhkdj [979.158919ms]
  May  4 06:28:06.365: INFO: Created: latency-svc-2fztm
  May  4 06:28:06.381: INFO: Got endpoints: latency-svc-2fztm [957.455878ms]
  May  4 06:28:06.434: INFO: Created: latency-svc-7vbqk
  May  4 06:28:06.496: INFO: Got endpoints: latency-svc-7vbqk [1.061152274s]
  May  4 06:28:06.511: INFO: Created: latency-svc-4whpj
  May  4 06:28:06.549: INFO: Got endpoints: latency-svc-4whpj [1.022138821s]
  May  4 06:28:06.561: INFO: Created: latency-svc-kjfpv
  May  4 06:28:06.642: INFO: Got endpoints: latency-svc-kjfpv [1.068143724s]
  May  4 06:28:06.645: INFO: Created: latency-svc-d9568
  May  4 06:28:06.705: INFO: Got endpoints: latency-svc-d9568 [1.028306716s]
  May  4 06:28:06.744: INFO: Created: latency-svc-hzhxw
  May  4 06:28:06.786: INFO: Got endpoints: latency-svc-hzhxw [1.021066342s]
  May  4 06:28:06.797: INFO: Created: latency-svc-wjg7m
  May  4 06:28:06.825: INFO: Got endpoints: latency-svc-wjg7m [1.003365461s]
  May  4 06:28:06.872: INFO: Created: latency-svc-fz7fd
  May  4 06:28:06.943: INFO: Got endpoints: latency-svc-fz7fd [1.056202774s]
  May  4 06:28:06.952: INFO: Created: latency-svc-qzz6w
  May  4 06:28:07.001: INFO: Got endpoints: latency-svc-qzz6w [1.024273719s]
  May  4 06:28:07.013: INFO: Created: latency-svc-lvjcg
  May  4 06:28:07.056: INFO: Got endpoints: latency-svc-lvjcg [1.000362972s]
  May  4 06:28:07.087: INFO: Created: latency-svc-r2h6k
  May  4 06:28:07.138: INFO: Got endpoints: latency-svc-r2h6k [1.017901915s]
  May  4 06:28:07.154: INFO: Created: latency-svc-zddgn
  May  4 06:28:07.203: INFO: Got endpoints: latency-svc-zddgn [1.026099275s]
  May  4 06:28:07.214: INFO: Created: latency-svc-ldmfb
  May  4 06:28:07.230: INFO: Got endpoints: latency-svc-ldmfb [994.412394ms]
  May  4 06:28:07.230: INFO: Latencies: [145.727174ms 212.676005ms 284.904999ms 349.062372ms 391.008448ms 473.363246ms 509.396534ms 550.998984ms 603.858842ms 654.647354ms 745.904743ms 778.596101ms 883.546508ms 906.844181ms 913.873677ms 916.043032ms 918.040396ms 920.69192ms 923.663747ms 927.792356ms 934.503161ms 935.924635ms 938.462852ms 942.647408ms 943.467911ms 943.900652ms 944.621625ms 947.088191ms 947.26029ms 950.170957ms 950.279949ms 951.083625ms 951.3049ms 954.508873ms 954.542162ms 955.023819ms 955.525786ms 956.791769ms 957.15731ms 957.240419ms 957.455878ms 958.897072ms 961.390231ms 962.082907ms 962.352215ms 963.447896ms 966.093984ms 966.151401ms 966.573651ms 967.320145ms 967.444934ms 967.49018ms 970.685691ms 972.937915ms 972.991167ms 974.151024ms 974.993059ms 976.753079ms 976.75393ms 976.880729ms 978.45159ms 979.158919ms 980.030801ms 980.64706ms 982.749206ms 983.372779ms 983.915637ms 985.308251ms 985.318804ms 985.327874ms 985.538764ms 985.745717ms 986.029039ms 987.369284ms 988.701709ms 989.714191ms 990.786909ms 992.581291ms 992.837216ms 992.899175ms 993.164093ms 993.654579ms 994.412394ms 997.148644ms 998.826683ms 1.000362972s 1.002064486s 1.003365461s 1.006016604s 1.00737807s 1.008947895s 1.008984994s 1.010488612s 1.01195882s 1.013626402s 1.014607182s 1.016297228s 1.01637272s 1.017901915s 1.021066342s 1.022138821s 1.024273719s 1.026099275s 1.028306716s 1.028316466s 1.031068269s 1.032228032s 1.033548659s 1.033694614s 1.03517884s 1.038646901s 1.040253292s 1.040638842s 1.042149299s 1.042620558s 1.042671722s 1.044194137s 1.045589546s 1.045942269s 1.047690487s 1.04995161s 1.050647043s 1.051354272s 1.054758532s 1.056202774s 1.0568437s 1.059084482s 1.060645361s 1.060696388s 1.061152274s 1.06564194s 1.068143724s 1.068953109s 1.069239364s 1.07024341s 1.071028634s 1.071791948s 1.072733869s 1.074359742s 1.075746183s 1.076039208s 1.077755638s 1.083709166s 1.084536163s 1.089061641s 1.089063816s 1.094433837s 1.095256162s 1.099714811s 1.101564804s 1.10294502s 1.110864184s 1.112811711s 1.113817969s 1.116792787s 1.117422257s 1.123195128s 1.123582557s 1.12362301s 1.1250045s 1.126440818s 1.129930096s 1.135308762s 1.136191561s 1.136258954s 1.146010145s 1.149318727s 1.150254016s 1.15111783s 1.156056897s 1.158623927s 1.175804983s 1.176894845s 1.183618567s 1.188747229s 1.190192143s 1.190998065s 1.201970554s 1.203003745s 1.204922922s 1.210429202s 1.216402317s 1.219875628s 1.226667365s 1.22844156s 1.232711059s 1.235583493s 1.240633442s 1.250007519s 1.252441933s 1.256886853s 1.264495724s 1.266708383s 1.271811711s 1.273005648s 1.275822199s 1.288343332s 1.291130874s 1.293763347s 1.305129219s]
  May  4 06:28:07.230: INFO: 50 %ile: 1.022138821s
  May  4 06:28:07.230: INFO: 90 %ile: 1.210429202s
  May  4 06:28:07.230: INFO: 99 %ile: 1.293763347s
  May  4 06:28:07.230: INFO: Total sample count: 200
  May  4 06:28:07.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-9801" for this suite. @ 05/04/23 06:28:07.264
• [16.543 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 05/04/23 06:28:07.326
  May  4 06:28:07.326: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename pods @ 05/04/23 06:28:07.327
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:28:07.366
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:28:07.37
  STEP: creating a Pod with a static label @ 05/04/23 06:28:07.38
  STEP: watching for Pod to be ready @ 05/04/23 06:28:07.405
  May  4 06:28:07.407: INFO: observed Pod pod-test in namespace pods-9833 in phase Pending with labels: map[test-pod-static:true] & conditions []
  May  4 06:28:07.421: INFO: observed Pod pod-test in namespace pods-9833 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 06:28:07 +0000 UTC  }]
  May  4 06:28:07.510: INFO: observed Pod pod-test in namespace pods-9833 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 06:28:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 06:28:07 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 06:28:07 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 06:28:07 +0000 UTC  }]
  May  4 06:28:08.237: INFO: observed Pod pod-test in namespace pods-9833 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 06:28:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 06:28:07 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 06:28:07 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 06:28:07 +0000 UTC  }]
  May  4 06:28:09.397: INFO: Found Pod pod-test in namespace pods-9833 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 06:28:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 06:28:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 06:28:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 06:28:07 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 05/04/23 06:28:09.402
  STEP: getting the Pod and ensuring that it's patched @ 05/04/23 06:28:09.435
  STEP: replacing the Pod's status Ready condition to False @ 05/04/23 06:28:09.438
  STEP: check the Pod again to ensure its Ready conditions are False @ 05/04/23 06:28:09.488
  STEP: deleting the Pod via a Collection with a LabelSelector @ 05/04/23 06:28:09.489
  STEP: watching for the Pod to be deleted @ 05/04/23 06:28:09.505
  May  4 06:28:09.511: INFO: observed event type MODIFIED
  May  4 06:28:11.127: INFO: observed event type MODIFIED
  May  4 06:28:11.736: INFO: observed event type MODIFIED
  May  4 06:28:11.984: INFO: observed event type MODIFIED
  May  4 06:28:12.419: INFO: observed event type MODIFIED
  May  4 06:28:12.454: INFO: observed event type MODIFIED
  May  4 06:28:12.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9833" for this suite. @ 05/04/23 06:28:12.498
• [5.214 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 05/04/23 06:28:12.542
  May  4 06:28:12.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename kubectl-logs @ 05/04/23 06:28:12.544
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:28:12.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:28:12.781
  STEP: creating an pod @ 05/04/23 06:28:12.79
  May  4 06:28:12.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-logs-7881 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  May  4 06:28:13.052: INFO: stderr: ""
  May  4 06:28:13.052: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 05/04/23 06:28:13.052
  May  4 06:28:13.052: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  May  4 06:28:17.153: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 05/04/23 06:28:17.153
  May  4 06:28:17.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-logs-7881 logs logs-generator logs-generator'
  May  4 06:28:17.352: INFO: stderr: ""
  May  4 06:28:17.352: INFO: stdout: "I0504 06:28:14.995030       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/bnnp 231\nI0504 06:28:15.195624       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/mz8 370\nI0504 06:28:15.396040       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/xgm 320\nI0504 06:28:15.595598       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/4qjw 368\nI0504 06:28:15.796069       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/ll8 254\nI0504 06:28:15.995514       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/c9n9 213\nI0504 06:28:16.195970       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/prrr 584\nI0504 06:28:16.395328       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/qfdc 427\nI0504 06:28:16.595831       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/pvq 393\nI0504 06:28:16.795248       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/fj4 207\nI0504 06:28:16.995723       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/w94z 371\nI0504 06:28:17.195109       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/249 530\n"
  STEP: limiting log lines @ 05/04/23 06:28:17.353
  May  4 06:28:17.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-logs-7881 logs logs-generator logs-generator --tail=1'
  May  4 06:28:17.480: INFO: stderr: ""
  May  4 06:28:17.480: INFO: stdout: "I0504 06:28:17.395543       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/xsln 569\n"
  May  4 06:28:17.480: INFO: got output "I0504 06:28:17.395543       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/xsln 569\n"
  STEP: limiting log bytes @ 05/04/23 06:28:17.48
  May  4 06:28:17.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-logs-7881 logs logs-generator logs-generator --limit-bytes=1'
  May  4 06:28:17.665: INFO: stderr: ""
  May  4 06:28:17.665: INFO: stdout: "I"
  May  4 06:28:17.665: INFO: got output "I"
  STEP: exposing timestamps @ 05/04/23 06:28:17.665
  May  4 06:28:17.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-logs-7881 logs logs-generator logs-generator --tail=1 --timestamps'
  May  4 06:28:17.809: INFO: stderr: ""
  May  4 06:28:17.809: INFO: stdout: "2023-05-04T14:28:17.795694671+08:00 I0504 06:28:17.795485       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/dq4 335\n"
  May  4 06:28:17.809: INFO: got output "2023-05-04T14:28:17.795694671+08:00 I0504 06:28:17.795485       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/dq4 335\n"
  STEP: restricting to a time range @ 05/04/23 06:28:17.809
  May  4 06:28:20.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-logs-7881 logs logs-generator logs-generator --since=1s'
  May  4 06:28:20.447: INFO: stderr: ""
  May  4 06:28:20.447: INFO: stdout: "I0504 06:28:19.595364       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/tz8w 237\nI0504 06:28:19.795863       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/default/pods/prn 397\nI0504 06:28:19.995261       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/x6w 330\nI0504 06:28:20.195798       1 logs_generator.go:76] 26 POST /api/v1/namespaces/ns/pods/66ht 312\nI0504 06:28:20.395155       1 logs_generator.go:76] 27 POST /api/v1/namespaces/kube-system/pods/n7b 252\n"
  May  4 06:28:20.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-logs-7881 logs logs-generator logs-generator --since=24h'
  May  4 06:28:20.627: INFO: stderr: ""
  May  4 06:28:20.627: INFO: stdout: "I0504 06:28:14.995030       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/bnnp 231\nI0504 06:28:15.195624       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/mz8 370\nI0504 06:28:15.396040       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/xgm 320\nI0504 06:28:15.595598       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/4qjw 368\nI0504 06:28:15.796069       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/ll8 254\nI0504 06:28:15.995514       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/c9n9 213\nI0504 06:28:16.195970       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/prrr 584\nI0504 06:28:16.395328       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/qfdc 427\nI0504 06:28:16.595831       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/pvq 393\nI0504 06:28:16.795248       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/fj4 207\nI0504 06:28:16.995723       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/w94z 371\nI0504 06:28:17.195109       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/249 530\nI0504 06:28:17.395543       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/xsln 569\nI0504 06:28:17.595928       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/ssp 434\nI0504 06:28:17.795485       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/dq4 335\nI0504 06:28:17.995947       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/8z9m 537\nI0504 06:28:18.195328       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/z7g 519\nI0504 06:28:18.395725       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/l7l 449\nI0504 06:28:18.595146       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/x4sw 515\nI0504 06:28:18.795637       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/hbg5 410\nI0504 06:28:18.996035       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/5fl 227\nI0504 06:28:19.195457       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/ks2r 439\nI0504 06:28:19.395847       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/zkv 485\nI0504 06:28:19.595364       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/tz8w 237\nI0504 06:28:19.795863       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/default/pods/prn 397\nI0504 06:28:19.995261       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/x6w 330\nI0504 06:28:20.195798       1 logs_generator.go:76] 26 POST /api/v1/namespaces/ns/pods/66ht 312\nI0504 06:28:20.395155       1 logs_generator.go:76] 27 POST /api/v1/namespaces/kube-system/pods/n7b 252\nI0504 06:28:20.595564       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/ns/pods/b8c 462\n"
  May  4 06:28:20.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-logs-7881 delete pod logs-generator'
  May  4 06:28:22.652: INFO: stderr: ""
  May  4 06:28:22.652: INFO: stdout: "pod \"logs-generator\" deleted\n"
  May  4 06:28:22.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-7881" for this suite. @ 05/04/23 06:28:22.664
• [10.160 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 05/04/23 06:28:22.704
  May  4 06:28:22.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename daemonsets @ 05/04/23 06:28:22.705
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:28:22.783
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:28:22.789
  May  4 06:28:22.863: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 05/04/23 06:28:22.916
  May  4 06:28:22.940: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 06:28:22.940: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 05/04/23 06:28:22.941
  May  4 06:28:23.105: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 06:28:23.106: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  May  4 06:28:24.120: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 06:28:24.120: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  May  4 06:28:25.111: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 06:28:25.111: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  May  4 06:28:26.125: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  4 06:28:26.125: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 05/04/23 06:28:26.136
  May  4 06:28:26.226: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  4 06:28:26.226: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  May  4 06:28:27.279: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 06:28:27.279: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 05/04/23 06:28:27.279
  May  4 06:28:27.325: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 06:28:27.325: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  May  4 06:28:28.349: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 06:28:28.349: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  May  4 06:28:29.379: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 06:28:29.379: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  May  4 06:28:30.337: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 06:28:30.337: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  May  4 06:28:31.338: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 06:28:31.338: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  May  4 06:28:32.332: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 06:28:32.332: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  May  4 06:28:33.342: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  4 06:28:33.342: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/04/23 06:28:33.387
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5867, will wait for the garbage collector to delete the pods @ 05/04/23 06:28:33.387
  May  4 06:28:33.478: INFO: Deleting DaemonSet.extensions daemon-set took: 25.195026ms
  May  4 06:28:33.579: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.514815ms
  May  4 06:28:36.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 06:28:36.688: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May  4 06:28:36.702: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"57866"},"items":null}

  May  4 06:28:36.706: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"57866"},"items":null}

  May  4 06:28:36.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5867" for this suite. @ 05/04/23 06:28:36.845
• [14.165 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 05/04/23 06:28:36.873
  May  4 06:28:36.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename proxy @ 05/04/23 06:28:36.875
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:28:37.009
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:28:37.014
  May  4 06:28:37.018: INFO: Creating pod...
  May  4 06:28:39.099: INFO: Creating service...
  May  4 06:28:39.190: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2915/pods/agnhost/proxy/some/path/with/DELETE
  May  4 06:28:39.196: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May  4 06:28:39.196: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2915/pods/agnhost/proxy/some/path/with/GET
  May  4 06:28:39.242: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  May  4 06:28:39.242: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2915/pods/agnhost/proxy/some/path/with/HEAD
  May  4 06:28:39.257: INFO: http.Client request:HEAD | StatusCode:200
  May  4 06:28:39.257: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2915/pods/agnhost/proxy/some/path/with/OPTIONS
  May  4 06:28:39.262: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May  4 06:28:39.263: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2915/pods/agnhost/proxy/some/path/with/PATCH
  May  4 06:28:39.302: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May  4 06:28:39.302: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2915/pods/agnhost/proxy/some/path/with/POST
  May  4 06:28:39.316: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May  4 06:28:39.316: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2915/pods/agnhost/proxy/some/path/with/PUT
  May  4 06:28:39.324: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May  4 06:28:39.324: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2915/services/test-service/proxy/some/path/with/DELETE
  May  4 06:28:39.346: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May  4 06:28:39.346: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2915/services/test-service/proxy/some/path/with/GET
  May  4 06:28:39.361: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  May  4 06:28:39.361: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2915/services/test-service/proxy/some/path/with/HEAD
  May  4 06:28:39.368: INFO: http.Client request:HEAD | StatusCode:200
  May  4 06:28:39.368: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2915/services/test-service/proxy/some/path/with/OPTIONS
  May  4 06:28:39.390: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May  4 06:28:39.390: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2915/services/test-service/proxy/some/path/with/PATCH
  May  4 06:28:39.439: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May  4 06:28:39.439: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2915/services/test-service/proxy/some/path/with/POST
  May  4 06:28:39.467: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May  4 06:28:39.467: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2915/services/test-service/proxy/some/path/with/PUT
  May  4 06:28:39.483: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May  4 06:28:39.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-2915" for this suite. @ 05/04/23 06:28:39.488
• [2.658 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 05/04/23 06:28:39.531
  May  4 06:28:39.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename replicaset @ 05/04/23 06:28:39.533
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:28:39.624
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:28:39.629
  May  4 06:28:39.632: INFO: Creating ReplicaSet my-hostname-basic-1a6c94d2-2435-413c-bf77-600e29600957
  May  4 06:28:39.699: INFO: Pod name my-hostname-basic-1a6c94d2-2435-413c-bf77-600e29600957: Found 0 pods out of 1
  May  4 06:28:44.704: INFO: Pod name my-hostname-basic-1a6c94d2-2435-413c-bf77-600e29600957: Found 1 pods out of 1
  May  4 06:28:44.704: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-1a6c94d2-2435-413c-bf77-600e29600957" is running
  May  4 06:28:44.708: INFO: Pod "my-hostname-basic-1a6c94d2-2435-413c-bf77-600e29600957-wk4bz" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-04 06:28:39 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-04 06:28:42 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-04 06:28:42 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-04 06:28:39 +0000 UTC Reason: Message:}])
  May  4 06:28:44.708: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 05/04/23 06:28:44.708
  May  4 06:28:44.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1639" for this suite. @ 05/04/23 06:28:44.735
• [5.229 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 05/04/23 06:28:44.762
  May  4 06:28:44.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename statefulset @ 05/04/23 06:28:44.764
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:28:44.84
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:28:44.845
  STEP: Creating service test in namespace statefulset-5924 @ 05/04/23 06:28:44.85
  STEP: Looking for a node to schedule stateful set and pod @ 05/04/23 06:28:44.865
  STEP: Creating pod with conflicting port in namespace statefulset-5924 @ 05/04/23 06:28:44.946
  STEP: Waiting until pod test-pod will start running in namespace statefulset-5924 @ 05/04/23 06:28:44.964
  STEP: Creating statefulset with conflicting port in namespace statefulset-5924 @ 05/04/23 06:28:48.991
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5924 @ 05/04/23 06:28:49.006
  May  4 06:28:49.105: INFO: Observed stateful pod in namespace: statefulset-5924, name: ss-0, uid: f72f0c2d-4e9d-49c3-8b50-7f491ef91a2a, status phase: Pending. Waiting for statefulset controller to delete.
  May  4 06:28:49.227: INFO: Observed stateful pod in namespace: statefulset-5924, name: ss-0, uid: f72f0c2d-4e9d-49c3-8b50-7f491ef91a2a, status phase: Failed. Waiting for statefulset controller to delete.
  May  4 06:28:49.263: INFO: Observed stateful pod in namespace: statefulset-5924, name: ss-0, uid: f72f0c2d-4e9d-49c3-8b50-7f491ef91a2a, status phase: Failed. Waiting for statefulset controller to delete.
  May  4 06:28:49.276: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5924
  STEP: Removing pod with conflicting port in namespace statefulset-5924 @ 05/04/23 06:28:49.276
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5924 and will be in running state @ 05/04/23 06:28:49.355
  May  4 06:28:53.385: INFO: Deleting all statefulset in ns statefulset-5924
  May  4 06:28:53.389: INFO: Scaling statefulset ss to 0
  May  4 06:29:03.422: INFO: Waiting for statefulset status.replicas updated to 0
  May  4 06:29:03.425: INFO: Deleting statefulset ss
  May  4 06:29:03.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5924" for this suite. @ 05/04/23 06:29:03.502
• [18.760 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 05/04/23 06:29:03.523
  May  4 06:29:03.523: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename resourcequota @ 05/04/23 06:29:03.525
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:29:03.569
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:29:03.574
  STEP: Counting existing ResourceQuota @ 05/04/23 06:29:03.611
  STEP: Creating a ResourceQuota @ 05/04/23 06:29:08.621
  STEP: Ensuring resource quota status is calculated @ 05/04/23 06:29:08.668
  STEP: Creating a Service @ 05/04/23 06:29:10.672
  STEP: Creating a NodePort Service @ 05/04/23 06:29:10.748
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 05/04/23 06:29:10.867
  STEP: Ensuring resource quota status captures service creation @ 05/04/23 06:29:10.97
  STEP: Deleting Services @ 05/04/23 06:29:12.974
  STEP: Ensuring resource quota status released usage @ 05/04/23 06:29:13.159
  May  4 06:29:15.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1394" for this suite. @ 05/04/23 06:29:15.169
• [11.668 seconds]
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 05/04/23 06:29:15.192
  May  4 06:29:15.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/04/23 06:29:15.194
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:29:15.275
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:29:15.279
  STEP: create the container to handle the HTTPGet hook request. @ 05/04/23 06:29:15.287
  STEP: create the pod with lifecycle hook @ 05/04/23 06:29:19.323
  STEP: delete the pod with lifecycle hook @ 05/04/23 06:29:23.364
  STEP: check prestop hook @ 05/04/23 06:29:25.4
  May  4 06:29:25.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-8681" for this suite. @ 05/04/23 06:29:25.412
• [10.236 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 05/04/23 06:29:25.431
  May  4 06:29:25.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename downward-api @ 05/04/23 06:29:25.432
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:29:25.504
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:29:25.508
  STEP: Creating the pod @ 05/04/23 06:29:25.524
  May  4 06:29:28.103: INFO: Successfully updated pod "annotationupdateaf53388e-f1f2-44e0-b5eb-ee0274feebd2"
  May  4 06:29:32.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9201" for this suite. @ 05/04/23 06:29:32.159
• [6.756 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 05/04/23 06:29:32.187
  May  4 06:29:32.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename csiinlinevolumes @ 05/04/23 06:29:32.188
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:29:32.231
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:29:32.236
  STEP: creating @ 05/04/23 06:29:32.241
  STEP: getting @ 05/04/23 06:29:32.307
  STEP: listing @ 05/04/23 06:29:32.312
  STEP: deleting @ 05/04/23 06:29:32.316
  May  4 06:29:32.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-8557" for this suite. @ 05/04/23 06:29:32.37
• [0.216 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 05/04/23 06:29:32.403
  May  4 06:29:32.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename emptydir @ 05/04/23 06:29:32.404
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:29:32.441
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:29:32.445
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 05/04/23 06:29:32.477
  STEP: Saw pod success @ 05/04/23 06:29:36.542
  May  4 06:29:36.545: INFO: Trying to get logs from node k8s-node1 pod pod-361327b1-fdac-4719-83bd-c1478b051bb3 container test-container: <nil>
  STEP: delete the pod @ 05/04/23 06:29:36.552
  May  4 06:29:36.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1252" for this suite. @ 05/04/23 06:29:36.599
• [4.209 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 05/04/23 06:29:36.614
  May  4 06:29:36.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename resourcequota @ 05/04/23 06:29:36.616
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:29:36.655
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:29:36.659
  STEP: Creating a ResourceQuota with terminating scope @ 05/04/23 06:29:36.699
  STEP: Ensuring ResourceQuota status is calculated @ 05/04/23 06:29:36.713
  STEP: Creating a ResourceQuota with not terminating scope @ 05/04/23 06:29:38.717
  STEP: Ensuring ResourceQuota status is calculated @ 05/04/23 06:29:38.734
  STEP: Creating a long running pod @ 05/04/23 06:29:40.739
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 05/04/23 06:29:40.808
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 05/04/23 06:29:42.812
  STEP: Deleting the pod @ 05/04/23 06:29:44.816
  STEP: Ensuring resource quota status released the pod usage @ 05/04/23 06:29:44.86
  STEP: Creating a terminating pod @ 05/04/23 06:29:46.864
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 05/04/23 06:29:46.897
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 05/04/23 06:29:48.901
  STEP: Deleting the pod @ 05/04/23 06:29:50.907
  STEP: Ensuring resource quota status released the pod usage @ 05/04/23 06:29:51.005
  May  4 06:29:53.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6147" for this suite. @ 05/04/23 06:29:53.015
• [16.418 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 05/04/23 06:29:53.033
  May  4 06:29:53.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename secrets @ 05/04/23 06:29:53.035
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:29:53.079
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:29:53.085
  STEP: Creating secret with name secret-test-f25059a5-62ec-4041-a4da-1cf9ebf19b88 @ 05/04/23 06:29:53.091
  STEP: Creating a pod to test consume secrets @ 05/04/23 06:29:53.111
  STEP: Saw pod success @ 05/04/23 06:29:57.161
  May  4 06:29:57.164: INFO: Trying to get logs from node k8s-node1 pod pod-secrets-431a6c9a-3a03-4aa9-bfb8-77deb19f03b4 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/04/23 06:29:57.171
  May  4 06:29:57.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3106" for this suite. @ 05/04/23 06:29:57.216
• [4.200 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 05/04/23 06:29:57.234
  May  4 06:29:57.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename tables @ 05/04/23 06:29:57.236
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:29:57.322
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:29:57.326
  May  4 06:29:57.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-849" for this suite. @ 05/04/23 06:29:57.337
• [0.128 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 05/04/23 06:29:57.363
  May  4 06:29:57.363: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename services @ 05/04/23 06:29:57.365
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:29:57.413
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:29:57.419
  STEP: creating service in namespace services-2848 @ 05/04/23 06:29:57.424
  STEP: creating service affinity-clusterip-transition in namespace services-2848 @ 05/04/23 06:29:57.424
  STEP: creating replication controller affinity-clusterip-transition in namespace services-2848 @ 05/04/23 06:29:57.479
  I0504 06:29:57.539141      20 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-2848, replica count: 3
  I0504 06:30:00.591031      20 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  4 06:30:00.596: INFO: Creating new exec pod
  May  4 06:30:03.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-2848 exec execpod-affinity2vd5l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  May  4 06:30:03.859: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  May  4 06:30:03.859: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  4 06:30:03.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-2848 exec execpod-affinity2vd5l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.3.62 80'
  May  4 06:30:04.058: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.3.62 80\nConnection to 10.101.3.62 80 port [tcp/http] succeeded!\n"
  May  4 06:30:04.058: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  4 06:30:04.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-2848 exec execpod-affinity2vd5l -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.101.3.62:80/ ; done'
  May  4 06:30:04.407: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n"
  May  4 06:30:04.407: INFO: stdout: "\naffinity-clusterip-transition-skhhd\naffinity-clusterip-transition-2pvv4\naffinity-clusterip-transition-5rgbz\naffinity-clusterip-transition-2pvv4\naffinity-clusterip-transition-5rgbz\naffinity-clusterip-transition-skhhd\naffinity-clusterip-transition-skhhd\naffinity-clusterip-transition-2pvv4\naffinity-clusterip-transition-skhhd\naffinity-clusterip-transition-2pvv4\naffinity-clusterip-transition-2pvv4\naffinity-clusterip-transition-5rgbz\naffinity-clusterip-transition-skhhd\naffinity-clusterip-transition-skhhd\naffinity-clusterip-transition-5rgbz\naffinity-clusterip-transition-2pvv4"
  May  4 06:30:04.407: INFO: Received response from host: affinity-clusterip-transition-skhhd
  May  4 06:30:04.407: INFO: Received response from host: affinity-clusterip-transition-2pvv4
  May  4 06:30:04.407: INFO: Received response from host: affinity-clusterip-transition-5rgbz
  May  4 06:30:04.407: INFO: Received response from host: affinity-clusterip-transition-2pvv4
  May  4 06:30:04.407: INFO: Received response from host: affinity-clusterip-transition-5rgbz
  May  4 06:30:04.407: INFO: Received response from host: affinity-clusterip-transition-skhhd
  May  4 06:30:04.407: INFO: Received response from host: affinity-clusterip-transition-skhhd
  May  4 06:30:04.407: INFO: Received response from host: affinity-clusterip-transition-2pvv4
  May  4 06:30:04.407: INFO: Received response from host: affinity-clusterip-transition-skhhd
  May  4 06:30:04.407: INFO: Received response from host: affinity-clusterip-transition-2pvv4
  May  4 06:30:04.407: INFO: Received response from host: affinity-clusterip-transition-2pvv4
  May  4 06:30:04.407: INFO: Received response from host: affinity-clusterip-transition-5rgbz
  May  4 06:30:04.408: INFO: Received response from host: affinity-clusterip-transition-skhhd
  May  4 06:30:04.408: INFO: Received response from host: affinity-clusterip-transition-skhhd
  May  4 06:30:04.408: INFO: Received response from host: affinity-clusterip-transition-5rgbz
  May  4 06:30:04.408: INFO: Received response from host: affinity-clusterip-transition-2pvv4
  May  4 06:30:04.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-2848 exec execpod-affinity2vd5l -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.101.3.62:80/ ; done'
  May  4 06:30:04.720: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.3.62:80/\n"
  May  4 06:30:04.720: INFO: stdout: "\naffinity-clusterip-transition-5rgbz\naffinity-clusterip-transition-5rgbz\naffinity-clusterip-transition-5rgbz\naffinity-clusterip-transition-5rgbz\naffinity-clusterip-transition-5rgbz\naffinity-clusterip-transition-5rgbz\naffinity-clusterip-transition-5rgbz\naffinity-clusterip-transition-5rgbz\naffinity-clusterip-transition-5rgbz\naffinity-clusterip-transition-5rgbz\naffinity-clusterip-transition-5rgbz\naffinity-clusterip-transition-5rgbz\naffinity-clusterip-transition-5rgbz\naffinity-clusterip-transition-5rgbz\naffinity-clusterip-transition-5rgbz\naffinity-clusterip-transition-5rgbz"
  May  4 06:30:04.720: INFO: Received response from host: affinity-clusterip-transition-5rgbz
  May  4 06:30:04.720: INFO: Received response from host: affinity-clusterip-transition-5rgbz
  May  4 06:30:04.721: INFO: Received response from host: affinity-clusterip-transition-5rgbz
  May  4 06:30:04.721: INFO: Received response from host: affinity-clusterip-transition-5rgbz
  May  4 06:30:04.721: INFO: Received response from host: affinity-clusterip-transition-5rgbz
  May  4 06:30:04.721: INFO: Received response from host: affinity-clusterip-transition-5rgbz
  May  4 06:30:04.721: INFO: Received response from host: affinity-clusterip-transition-5rgbz
  May  4 06:30:04.721: INFO: Received response from host: affinity-clusterip-transition-5rgbz
  May  4 06:30:04.721: INFO: Received response from host: affinity-clusterip-transition-5rgbz
  May  4 06:30:04.721: INFO: Received response from host: affinity-clusterip-transition-5rgbz
  May  4 06:30:04.721: INFO: Received response from host: affinity-clusterip-transition-5rgbz
  May  4 06:30:04.721: INFO: Received response from host: affinity-clusterip-transition-5rgbz
  May  4 06:30:04.721: INFO: Received response from host: affinity-clusterip-transition-5rgbz
  May  4 06:30:04.721: INFO: Received response from host: affinity-clusterip-transition-5rgbz
  May  4 06:30:04.721: INFO: Received response from host: affinity-clusterip-transition-5rgbz
  May  4 06:30:04.721: INFO: Received response from host: affinity-clusterip-transition-5rgbz
  May  4 06:30:04.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  4 06:30:04.726: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2848, will wait for the garbage collector to delete the pods @ 05/04/23 06:30:04.762
  May  4 06:30:04.885: INFO: Deleting ReplicationController affinity-clusterip-transition took: 62.55961ms
  May  4 06:30:05.086: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 200.881503ms
  STEP: Destroying namespace "services-2848" for this suite. @ 05/04/23 06:30:08.071
• [10.744 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 05/04/23 06:30:08.109
  May  4 06:30:08.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename crd-webhook @ 05/04/23 06:30:08.111
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:30:08.18
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:30:08.185
  STEP: Setting up server cert @ 05/04/23 06:30:08.188
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 05/04/23 06:30:09.337
  STEP: Deploying the custom resource conversion webhook pod @ 05/04/23 06:30:09.354
  STEP: Wait for the deployment to be ready @ 05/04/23 06:30:09.425
  May  4 06:30:09.474: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  May  4 06:30:11.485: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 6, 30, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 30, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 6, 30, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 30, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 05/04/23 06:30:13.503
  STEP: Verifying the service has paired with the endpoint @ 05/04/23 06:30:13.565
  May  4 06:30:14.567: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  May  4 06:30:14.570: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Creating a v1 custom resource @ 05/04/23 06:30:17.215
  STEP: Create a v2 custom resource @ 05/04/23 06:30:17.308
  STEP: List CRs in v1 @ 05/04/23 06:30:17.368
  STEP: List CRs in v2 @ 05/04/23 06:30:17.374
  May  4 06:30:17.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-2422" for this suite. @ 05/04/23 06:30:18.14
• [10.096 seconds]
------------------------------
S
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 05/04/23 06:30:18.205
  May  4 06:30:18.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename secrets @ 05/04/23 06:30:18.209
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:30:18.258
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:30:18.264
  STEP: Creating secret with name secret-test-7395069e-a19d-468e-9bdc-2a91659bcef6 @ 05/04/23 06:30:18.29
  STEP: Creating a pod to test consume secrets @ 05/04/23 06:30:18.349
  STEP: Saw pod success @ 05/04/23 06:30:22.405
  May  4 06:30:22.408: INFO: Trying to get logs from node k8s-node2 pod pod-secrets-2c6e6890-6b35-4a50-9ba1-2fe75c3e06c6 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/04/23 06:30:22.415
  May  4 06:30:22.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3890" for this suite. @ 05/04/23 06:30:22.472
• [4.281 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 05/04/23 06:30:22.488
  May  4 06:30:22.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename dns @ 05/04/23 06:30:22.489
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:30:22.555
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:30:22.559
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 05/04/23 06:30:22.563
  May  4 06:30:22.590: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3000  fac6f186-f108-4b1b-a1c7-cfe9ec927fbd 58814 0 2023-05-04 06:30:22 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-05-04 06:30:22 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4dbbx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4dbbx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  STEP: Verifying customized DNS suffix list is configured on pod... @ 05/04/23 06:30:24.599
  May  4 06:30:24.599: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3000 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 06:30:24.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 06:30:24.600: INFO: ExecWithOptions: Clientset creation
  May  4 06:30:24.600: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3000/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 05/04/23 06:30:24.71
  May  4 06:30:24.710: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3000 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 06:30:24.710: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 06:30:24.711: INFO: ExecWithOptions: Clientset creation
  May  4 06:30:24.711: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3000/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May  4 06:30:24.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  4 06:30:24.843: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-3000" for this suite. @ 05/04/23 06:30:24.874
• [2.403 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 05/04/23 06:30:24.893
  May  4 06:30:24.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename security-context-test @ 05/04/23 06:30:24.894
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:30:24.971
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:30:24.975
  May  4 06:30:29.056: INFO: Got logs for pod "busybox-privileged-false-858e1876-64ef-41cd-a564-2e791b550aaf": "ip: RTNETLINK answers: Operation not permitted\n"
  May  4 06:30:29.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-6647" for this suite. @ 05/04/23 06:30:29.06
• [4.196 seconds]
------------------------------
S
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 05/04/23 06:30:29.088
  May  4 06:30:29.088: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename pods @ 05/04/23 06:30:29.09
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:30:29.134
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:30:29.143
  STEP: Create set of pods @ 05/04/23 06:30:29.147
  May  4 06:30:29.197: INFO: created test-pod-1
  May  4 06:30:29.212: INFO: created test-pod-2
  May  4 06:30:29.267: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 05/04/23 06:30:29.267
  STEP: waiting for all pods to be deleted @ 05/04/23 06:30:33.628
  May  4 06:30:33.633: INFO: Pod quantity 3 is different from expected quantity 0
  May  4 06:30:34.667: INFO: Pod quantity 3 is different from expected quantity 0
  May  4 06:30:35.637: INFO: Pod quantity 1 is different from expected quantity 0
  May  4 06:30:36.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4521" for this suite. @ 05/04/23 06:30:36.642
• [7.568 seconds]
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 05/04/23 06:30:36.657
  May  4 06:30:36.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename svcaccounts @ 05/04/23 06:30:36.659
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:30:36.733
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:30:36.738
  May  4 06:30:36.745: INFO: Got root ca configmap in namespace "svcaccounts-4718"
  May  4 06:30:36.758: INFO: Deleted root ca configmap in namespace "svcaccounts-4718"
  STEP: waiting for a new root ca configmap created @ 05/04/23 06:30:37.259
  May  4 06:30:37.263: INFO: Recreated root ca configmap in namespace "svcaccounts-4718"
  May  4 06:30:37.277: INFO: Updated root ca configmap in namespace "svcaccounts-4718"
  STEP: waiting for the root ca configmap reconciled @ 05/04/23 06:30:37.778
  May  4 06:30:37.782: INFO: Reconciled root ca configmap in namespace "svcaccounts-4718"
  May  4 06:30:37.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4718" for this suite. @ 05/04/23 06:30:37.786
• [1.144 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 05/04/23 06:30:37.805
  May  4 06:30:37.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename aggregator @ 05/04/23 06:30:37.806
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:30:37.839
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:30:37.843
  May  4 06:30:37.879: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Registering the sample API server. @ 05/04/23 06:30:37.88
  May  4 06:30:38.550: INFO: Found ClusterRoles; assuming RBAC is enabled.
  May  4 06:30:38.674: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  May  4 06:30:40.864: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  4 06:30:42.878: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  4 06:30:44.868: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  4 06:30:46.868: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  4 06:30:48.868: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  4 06:30:50.869: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  4 06:30:52.868: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  4 06:30:54.868: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  4 06:30:56.869: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  4 06:30:58.868: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 30, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  4 06:31:01.029: INFO: Waited 115.905933ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 05/04/23 06:31:01.145
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 05/04/23 06:31:01.148
  STEP: List APIServices @ 05/04/23 06:31:01.173
  May  4 06:31:01.182: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 05/04/23 06:31:01.182
  May  4 06:31:01.259: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 05/04/23 06:31:01.26
  May  4 06:31:01.303: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.May, 4, 6, 31, 0, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 05/04/23 06:31:01.303
  May  4 06:31:01.308: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-05-04 06:31:00 +0000 UTC Passed all checks passed}
  May  4 06:31:01.308: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May  4 06:31:01.308: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 05/04/23 06:31:01.308
  May  4 06:31:01.330: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-720481873" @ 05/04/23 06:31:01.33
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 05/04/23 06:31:01.382
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 05/04/23 06:31:01.407
  STEP: Patch APIService Status @ 05/04/23 06:31:01.411
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 05/04/23 06:31:01.437
  May  4 06:31:01.443: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-05-04 06:31:00 +0000 UTC Passed all checks passed}
  May  4 06:31:01.443: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May  4 06:31:01.443: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  May  4 06:31:01.443: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 05/04/23 06:31:01.443
  STEP: Confirm that the generated APIService has been deleted @ 05/04/23 06:31:01.448
  May  4 06:31:01.448: INFO: Requesting list of APIServices to confirm quantity
  May  4 06:31:01.454: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  May  4 06:31:01.454: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  May  4 06:31:01.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-1528" for this suite. @ 05/04/23 06:31:01.949
• [24.163 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 05/04/23 06:31:01.969
  May  4 06:31:01.969: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename pods @ 05/04/23 06:31:01.97
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:31:02.046
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:31:02.05
  STEP: creating pod @ 05/04/23 06:31:02.054
  May  4 06:31:06.116: INFO: Pod pod-hostip-1606b689-fd29-44c0-8381-1526d774f400 has hostIP: 192.168.0.157
  May  4 06:31:06.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4793" for this suite. @ 05/04/23 06:31:06.121
• [4.178 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 05/04/23 06:31:06.147
  May  4 06:31:06.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename namespaces @ 05/04/23 06:31:06.148
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:31:06.177
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:31:06.181
  STEP: Creating a test namespace @ 05/04/23 06:31:06.188
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:31:06.253
  STEP: Creating a pod in the namespace @ 05/04/23 06:31:06.257
  STEP: Waiting for the pod to have running status @ 05/04/23 06:31:06.275
  STEP: Deleting the namespace @ 05/04/23 06:31:08.283
  STEP: Waiting for the namespace to be removed. @ 05/04/23 06:31:08.307
  STEP: Recreating the namespace @ 05/04/23 06:31:19.311
  STEP: Verifying there are no pods in the namespace @ 05/04/23 06:31:19.369
  May  4 06:31:19.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-6705" for this suite. @ 05/04/23 06:31:19.388
  STEP: Destroying namespace "nsdeletetest-5632" for this suite. @ 05/04/23 06:31:19.415
  May  4 06:31:19.419: INFO: Namespace nsdeletetest-5632 was already deleted
  STEP: Destroying namespace "nsdeletetest-4140" for this suite. @ 05/04/23 06:31:19.419
• [13.288 seconds]
------------------------------
SSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 05/04/23 06:31:19.436
  May  4 06:31:19.436: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename configmap @ 05/04/23 06:31:19.438
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:31:19.485
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:31:19.492
  STEP: Creating configMap configmap-243/configmap-test-d0abb221-06d1-410f-9086-eaab9c0e540f @ 05/04/23 06:31:19.496
  STEP: Creating a pod to test consume configMaps @ 05/04/23 06:31:19.52
  STEP: Saw pod success @ 05/04/23 06:31:23.551
  May  4 06:31:23.553: INFO: Trying to get logs from node k8s-node2 pod pod-configmaps-68d53e72-9625-4557-b809-5d479b800d6c container env-test: <nil>
  STEP: delete the pod @ 05/04/23 06:31:23.56
  May  4 06:31:23.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-243" for this suite. @ 05/04/23 06:31:23.653
• [4.242 seconds]
------------------------------
S
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 05/04/23 06:31:23.678
  May  4 06:31:23.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename taint-single-pod @ 05/04/23 06:31:23.679
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:31:23.712
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:31:23.717
  May  4 06:31:23.724: INFO: Waiting up to 1m0s for all nodes to be ready
  May  4 06:32:23.751: INFO: Waiting for terminating namespaces to be deleted...
  May  4 06:32:23.754: INFO: Starting informer...
  STEP: Starting pod... @ 05/04/23 06:32:23.754
  May  4 06:32:24.002: INFO: Pod is running on k8s-node2. Tainting Node
  STEP: Trying to apply a taint on the Node @ 05/04/23 06:32:24.002
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/04/23 06:32:24.047
  STEP: Waiting short time to make sure Pod is queued for deletion @ 05/04/23 06:32:24.054
  May  4 06:32:24.055: INFO: Pod wasn't evicted. Proceeding
  May  4 06:32:24.055: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/04/23 06:32:24.093
  STEP: Waiting some time to make sure that toleration time passed. @ 05/04/23 06:32:24.105
  May  4 06:33:39.105: INFO: Pod wasn't evicted. Test successful
  May  4 06:33:39.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-1606" for this suite. @ 05/04/23 06:33:39.111
• [135.460 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 05/04/23 06:33:39.143
  May  4 06:33:39.143: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 06:33:39.144
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:33:39.2
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:33:39.204
  STEP: Creating configMap with name projected-configmap-test-volume-bfc57e15-bdfb-4227-911b-4fd29d24018e @ 05/04/23 06:33:39.209
  STEP: Creating a pod to test consume configMaps @ 05/04/23 06:33:39.235
  STEP: Saw pod success @ 05/04/23 06:33:43.285
  May  4 06:33:43.291: INFO: Trying to get logs from node k8s-node2 pod pod-projected-configmaps-59e2503b-dc5e-44f3-bfea-c4666398c177 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 05/04/23 06:33:43.331
  May  4 06:33:43.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5196" for this suite. @ 05/04/23 06:33:43.4
• [4.277 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 05/04/23 06:33:43.424
  May  4 06:33:43.424: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename container-probe @ 05/04/23 06:33:43.428
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:33:43.494
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:33:43.498
  STEP: Creating pod liveness-372221e5-8848-43e3-b728-f15b7710b25c in namespace container-probe-1287 @ 05/04/23 06:33:43.502
  May  4 06:33:47.535: INFO: Started pod liveness-372221e5-8848-43e3-b728-f15b7710b25c in namespace container-probe-1287
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/04/23 06:33:47.535
  May  4 06:33:47.538: INFO: Initial restart count of pod liveness-372221e5-8848-43e3-b728-f15b7710b25c is 0
  May  4 06:34:05.633: INFO: Restart count of pod container-probe-1287/liveness-372221e5-8848-43e3-b728-f15b7710b25c is now 1 (18.094881018s elapsed)
  May  4 06:34:05.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/04/23 06:34:05.638
  STEP: Destroying namespace "container-probe-1287" for this suite. @ 05/04/23 06:34:05.688
• [22.284 seconds]
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 05/04/23 06:34:05.709
  May  4 06:34:05.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename pod-network-test @ 05/04/23 06:34:05.711
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:34:05.759
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:34:05.762
  STEP: Performing setup for networking test in namespace pod-network-test-2236 @ 05/04/23 06:34:05.779
  STEP: creating a selector @ 05/04/23 06:34:05.779
  STEP: Creating the service pods in kubernetes @ 05/04/23 06:34:05.779
  May  4 06:34:05.779: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 05/04/23 06:34:27.965
  May  4 06:34:30.036: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  May  4 06:34:30.036: INFO: Going to poll 172.16.36.92 on port 8081 at least 0 times, with a maximum of 34 tries before failing
  May  4 06:34:30.039: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.36.92 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2236 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 06:34:30.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 06:34:30.039: INFO: ExecWithOptions: Clientset creation
  May  4 06:34:30.039: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2236/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.16.36.92+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May  4 06:34:31.134: INFO: Found all 1 expected endpoints: [netserver-0]
  May  4 06:34:31.134: INFO: Going to poll 172.16.169.179 on port 8081 at least 0 times, with a maximum of 34 tries before failing
  May  4 06:34:31.137: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.169.179 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2236 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 06:34:31.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 06:34:31.138: INFO: ExecWithOptions: Clientset creation
  May  4 06:34:31.138: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2236/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.16.169.179+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May  4 06:34:32.223: INFO: Found all 1 expected endpoints: [netserver-1]
  May  4 06:34:32.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-2236" for this suite. @ 05/04/23 06:34:32.229
• [26.569 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 05/04/23 06:34:32.279
  May  4 06:34:32.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename job @ 05/04/23 06:34:32.281
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:34:32.331
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:34:32.335
  STEP: Creating a suspended job @ 05/04/23 06:34:32.341
  STEP: Patching the Job @ 05/04/23 06:34:32.354
  STEP: Watching for Job to be patched @ 05/04/23 06:34:32.389
  May  4 06:34:32.392: INFO: Event ADDED observed for Job e2e-k29j8 in namespace job-2649 with labels: map[e2e-job-label:e2e-k29j8] and annotations: map[batch.kubernetes.io/job-tracking:]
  May  4 06:34:32.392: INFO: Event MODIFIED found for Job e2e-k29j8 in namespace job-2649 with labels: map[e2e-job-label:e2e-k29j8 e2e-k29j8:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 05/04/23 06:34:32.392
  STEP: Watching for Job to be updated @ 05/04/23 06:34:32.466
  May  4 06:34:32.468: INFO: Event MODIFIED found for Job e2e-k29j8 in namespace job-2649 with labels: map[e2e-job-label:e2e-k29j8 e2e-k29j8:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May  4 06:34:32.468: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 05/04/23 06:34:32.468
  May  4 06:34:32.472: INFO: Job: e2e-k29j8 as labels: map[e2e-job-label:e2e-k29j8 e2e-k29j8:patched]
  STEP: Waiting for job to complete @ 05/04/23 06:34:32.472
  STEP: Delete a job collection with a labelselector @ 05/04/23 06:34:44.477
  STEP: Watching for Job to be deleted @ 05/04/23 06:34:44.493
  May  4 06:34:44.496: INFO: Event MODIFIED observed for Job e2e-k29j8 in namespace job-2649 with labels: map[e2e-job-label:e2e-k29j8 e2e-k29j8:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May  4 06:34:44.496: INFO: Event MODIFIED observed for Job e2e-k29j8 in namespace job-2649 with labels: map[e2e-job-label:e2e-k29j8 e2e-k29j8:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May  4 06:34:44.496: INFO: Event MODIFIED observed for Job e2e-k29j8 in namespace job-2649 with labels: map[e2e-job-label:e2e-k29j8 e2e-k29j8:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May  4 06:34:44.496: INFO: Event MODIFIED observed for Job e2e-k29j8 in namespace job-2649 with labels: map[e2e-job-label:e2e-k29j8 e2e-k29j8:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May  4 06:34:44.496: INFO: Event MODIFIED observed for Job e2e-k29j8 in namespace job-2649 with labels: map[e2e-job-label:e2e-k29j8 e2e-k29j8:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May  4 06:34:44.496: INFO: Event MODIFIED observed for Job e2e-k29j8 in namespace job-2649 with labels: map[e2e-job-label:e2e-k29j8 e2e-k29j8:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May  4 06:34:44.496: INFO: Event MODIFIED observed for Job e2e-k29j8 in namespace job-2649 with labels: map[e2e-job-label:e2e-k29j8 e2e-k29j8:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May  4 06:34:44.496: INFO: Event MODIFIED observed for Job e2e-k29j8 in namespace job-2649 with labels: map[e2e-job-label:e2e-k29j8 e2e-k29j8:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May  4 06:34:44.496: INFO: Event MODIFIED observed for Job e2e-k29j8 in namespace job-2649 with labels: map[e2e-job-label:e2e-k29j8 e2e-k29j8:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May  4 06:34:44.497: INFO: Event DELETED found for Job e2e-k29j8 in namespace job-2649 with labels: map[e2e-job-label:e2e-k29j8 e2e-k29j8:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 05/04/23 06:34:44.497
  May  4 06:34:44.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2649" for this suite. @ 05/04/23 06:34:44.525
• [12.296 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 05/04/23 06:34:44.585
  May  4 06:34:44.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/04/23 06:34:44.586
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:34:44.704
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:34:44.707
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 05/04/23 06:34:44.711
  May  4 06:34:44.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 05/04/23 06:34:51.354
  May  4 06:34:51.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 06:34:52.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 06:34:59.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-406" for this suite. @ 05/04/23 06:34:59.857
• [15.298 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 05/04/23 06:34:59.885
  May  4 06:34:59.885: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename statefulset @ 05/04/23 06:34:59.887
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:34:59.95
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:34:59.955
  STEP: Creating service test in namespace statefulset-253 @ 05/04/23 06:34:59.958
  May  4 06:35:00.007: INFO: Found 0 stateful pods, waiting for 1
  May  4 06:35:10.012: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 05/04/23 06:35:10.018
  W0504 06:35:10.033207      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May  4 06:35:10.061: INFO: Found 1 stateful pods, waiting for 2
  May  4 06:35:20.066: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May  4 06:35:20.066: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 05/04/23 06:35:20.072
  STEP: Delete all of the StatefulSets @ 05/04/23 06:35:20.075
  STEP: Verify that StatefulSets have been deleted @ 05/04/23 06:35:20.093
  May  4 06:35:20.096: INFO: Deleting all statefulset in ns statefulset-253
  May  4 06:35:20.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-253" for this suite. @ 05/04/23 06:35:20.164
• [20.304 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 05/04/23 06:35:20.192
  May  4 06:35:20.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename proxy @ 05/04/23 06:35:20.193
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:35:20.256
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:35:20.26
  May  4 06:35:20.320: INFO: Creating pod...
  May  4 06:35:22.390: INFO: Creating service...
  May  4 06:35:22.443: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6937/pods/agnhost/proxy?method=DELETE
  May  4 06:35:22.489: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May  4 06:35:22.489: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6937/pods/agnhost/proxy?method=OPTIONS
  May  4 06:35:22.492: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May  4 06:35:22.492: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6937/pods/agnhost/proxy?method=PATCH
  May  4 06:35:22.496: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May  4 06:35:22.496: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6937/pods/agnhost/proxy?method=POST
  May  4 06:35:22.499: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May  4 06:35:22.499: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6937/pods/agnhost/proxy?method=PUT
  May  4 06:35:22.503: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May  4 06:35:22.503: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6937/services/e2e-proxy-test-service/proxy?method=DELETE
  May  4 06:35:22.507: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May  4 06:35:22.507: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6937/services/e2e-proxy-test-service/proxy?method=OPTIONS
  May  4 06:35:22.511: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May  4 06:35:22.511: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6937/services/e2e-proxy-test-service/proxy?method=PATCH
  May  4 06:35:22.515: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May  4 06:35:22.515: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6937/services/e2e-proxy-test-service/proxy?method=POST
  May  4 06:35:22.520: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May  4 06:35:22.520: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6937/services/e2e-proxy-test-service/proxy?method=PUT
  May  4 06:35:22.524: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May  4 06:35:22.524: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6937/pods/agnhost/proxy?method=GET
  May  4 06:35:22.526: INFO: http.Client request:GET StatusCode:301
  May  4 06:35:22.526: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6937/services/e2e-proxy-test-service/proxy?method=GET
  May  4 06:35:22.529: INFO: http.Client request:GET StatusCode:301
  May  4 06:35:22.529: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6937/pods/agnhost/proxy?method=HEAD
  May  4 06:35:22.532: INFO: http.Client request:HEAD StatusCode:301
  May  4 06:35:22.532: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6937/services/e2e-proxy-test-service/proxy?method=HEAD
  May  4 06:35:22.535: INFO: http.Client request:HEAD StatusCode:301
  May  4 06:35:22.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-6937" for this suite. @ 05/04/23 06:35:22.54
• [2.365 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 05/04/23 06:35:22.559
  May  4 06:35:22.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename configmap @ 05/04/23 06:35:22.56
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:35:22.626
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:35:22.63
  STEP: Creating configMap with name configmap-test-volume-map-273af59f-c1ba-4290-8e43-fb2e500e2bed @ 05/04/23 06:35:22.636
  STEP: Creating a pod to test consume configMaps @ 05/04/23 06:35:22.653
  STEP: Saw pod success @ 05/04/23 06:35:26.745
  May  4 06:35:26.748: INFO: Trying to get logs from node k8s-node2 pod pod-configmaps-afe0f912-e1f7-463b-ab3f-e4d2cea27f03 container agnhost-container: <nil>
  STEP: delete the pod @ 05/04/23 06:35:26.77
  May  4 06:35:26.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8912" for this suite. @ 05/04/23 06:35:26.858
• [4.316 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 05/04/23 06:35:26.876
  May  4 06:35:26.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/04/23 06:35:26.877
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:35:26.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:35:26.96
  STEP: set up a multi version CRD @ 05/04/23 06:35:26.966
  May  4 06:35:26.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: mark a version not serverd @ 05/04/23 06:35:31.272
  STEP: check the unserved version gets removed @ 05/04/23 06:35:31.319
  STEP: check the other version is not changed @ 05/04/23 06:35:32.351
  May  4 06:35:35.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3196" for this suite. @ 05/04/23 06:35:35.85
• [9.000 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 05/04/23 06:35:35.876
  May  4 06:35:35.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename job @ 05/04/23 06:35:35.878
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:35:35.97
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:35:35.975
  STEP: Creating a job @ 05/04/23 06:35:35.979
  STEP: Ensuring job reaches completions @ 05/04/23 06:35:36.014
  May  4 06:35:48.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2950" for this suite. @ 05/04/23 06:35:48.023
• [12.160 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 05/04/23 06:35:48.041
  May  4 06:35:48.041: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename limitrange @ 05/04/23 06:35:48.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:35:48.095
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:35:48.099
  STEP: Creating a LimitRange @ 05/04/23 06:35:48.106
  STEP: Setting up watch @ 05/04/23 06:35:48.106
  STEP: Submitting a LimitRange @ 05/04/23 06:35:48.21
  STEP: Verifying LimitRange creation was observed @ 05/04/23 06:35:48.225
  STEP: Fetching the LimitRange to ensure it has proper values @ 05/04/23 06:35:48.225
  May  4 06:35:48.227: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  May  4 06:35:48.227: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 05/04/23 06:35:48.227
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 05/04/23 06:35:48.253
  May  4 06:35:48.257: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  May  4 06:35:48.257: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 05/04/23 06:35:48.257
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 05/04/23 06:35:48.292
  May  4 06:35:48.296: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  May  4 06:35:48.296: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 05/04/23 06:35:48.296
  STEP: Failing to create a Pod with more than max resources @ 05/04/23 06:35:48.299
  STEP: Updating a LimitRange @ 05/04/23 06:35:48.302
  STEP: Verifying LimitRange updating is effective @ 05/04/23 06:35:48.337
  STEP: Creating a Pod with less than former min resources @ 05/04/23 06:35:50.342
  STEP: Failing to create a Pod with more than max resources @ 05/04/23 06:35:50.368
  STEP: Deleting a LimitRange @ 05/04/23 06:35:50.372
  STEP: Verifying the LimitRange was deleted @ 05/04/23 06:35:50.398
  May  4 06:35:55.404: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 05/04/23 06:35:55.404
  May  4 06:35:55.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-7877" for this suite. @ 05/04/23 06:35:55.447
• [7.427 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 05/04/23 06:35:55.47
  May  4 06:35:55.470: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename dns @ 05/04/23 06:35:55.472
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:35:55.544
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:35:55.548
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9004.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9004.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 05/04/23 06:35:55.551
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9004.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9004.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 05/04/23 06:35:55.551
  STEP: creating a pod to probe /etc/hosts @ 05/04/23 06:35:55.551
  STEP: submitting the pod to kubernetes @ 05/04/23 06:35:55.551
  STEP: retrieving the pod @ 05/04/23 06:35:59.593
  STEP: looking for the results for each expected name from probers @ 05/04/23 06:35:59.596
  May  4 06:35:59.611: INFO: DNS probes using dns-9004/dns-test-ebc7a557-5210-4bce-815f-7b64017c26dc succeeded

  May  4 06:35:59.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/04/23 06:35:59.616
  STEP: Destroying namespace "dns-9004" for this suite. @ 05/04/23 06:35:59.664
• [4.235 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 05/04/23 06:35:59.705
  May  4 06:35:59.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename webhook @ 05/04/23 06:35:59.706
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:35:59.752
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:35:59.758
  STEP: Setting up server cert @ 05/04/23 06:35:59.841
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/04/23 06:36:00.656
  STEP: Deploying the webhook pod @ 05/04/23 06:36:00.699
  STEP: Wait for the deployment to be ready @ 05/04/23 06:36:00.741
  May  4 06:36:00.813: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  May  4 06:36:02.824: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 6, 36, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 36, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 6, 36, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 36, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 05/04/23 06:36:04.829
  STEP: Verifying the service has paired with the endpoint @ 05/04/23 06:36:04.945
  May  4 06:36:05.945: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May  4 06:36:05.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2389-crds.webhook.example.com via the AdmissionRegistration API @ 05/04/23 06:36:06.486
  STEP: Creating a custom resource that should be mutated by the webhook @ 05/04/23 06:36:06.534
  May  4 06:36:08.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1378" for this suite. @ 05/04/23 06:36:09.383
  STEP: Destroying namespace "webhook-markers-8729" for this suite. @ 05/04/23 06:36:09.468
• [9.795 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 05/04/23 06:36:09.504
  May  4 06:36:09.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename container-probe @ 05/04/23 06:36:09.505
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:36:09.602
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:36:09.607
  May  4 06:37:09.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-8191" for this suite. @ 05/04/23 06:37:09.676
• [60.190 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 05/04/23 06:37:09.697
  May  4 06:37:09.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename endpointslice @ 05/04/23 06:37:09.698
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:37:09.747
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:37:09.751
  May  4 06:37:09.765: INFO: Endpoints addresses: [192.168.0.155] , ports: [6443]
  May  4 06:37:09.765: INFO: EndpointSlices addresses: [192.168.0.155] , ports: [6443]
  May  4 06:37:09.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-8642" for this suite. @ 05/04/23 06:37:09.77
• [0.112 seconds]
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 05/04/23 06:37:09.81
  May  4 06:37:09.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename endpointslice @ 05/04/23 06:37:09.811
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:37:09.849
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:37:09.854
  May  4 06:37:10.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-474" for this suite. @ 05/04/23 06:37:10.047
• [0.314 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 05/04/23 06:37:10.126
  May  4 06:37:10.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename emptydir @ 05/04/23 06:37:10.127
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:37:10.219
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:37:10.224
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 05/04/23 06:37:10.228
  STEP: Saw pod success @ 05/04/23 06:37:14.32
  May  4 06:37:14.323: INFO: Trying to get logs from node k8s-node2 pod pod-dd6bb0a6-47ce-4d6c-b39a-54752b7d4a08 container test-container: <nil>
  STEP: delete the pod @ 05/04/23 06:37:14.344
  May  4 06:37:14.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-582" for this suite. @ 05/04/23 06:37:14.44
• [4.333 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 05/04/23 06:37:14.46
  May  4 06:37:14.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename runtimeclass @ 05/04/23 06:37:14.462
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:37:14.558
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:37:14.562
  STEP: Deleting RuntimeClass runtimeclass-1228-delete-me @ 05/04/23 06:37:14.591
  STEP: Waiting for the RuntimeClass to disappear @ 05/04/23 06:37:14.609
  May  4 06:37:14.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-1228" for this suite. @ 05/04/23 06:37:14.623
• [0.192 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 05/04/23 06:37:14.655
  May  4 06:37:14.655: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 06:37:14.656
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:37:14.745
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:37:14.75
  STEP: Creating configMap with name cm-test-opt-del-ed51ebc1-c7c0-489a-96ae-4ae4802ba8c5 @ 05/04/23 06:37:14.76
  STEP: Creating configMap with name cm-test-opt-upd-976e2c5b-cc20-4bf3-8caa-c6b34f8aa8c1 @ 05/04/23 06:37:14.817
  STEP: Creating the pod @ 05/04/23 06:37:14.858
  STEP: Deleting configmap cm-test-opt-del-ed51ebc1-c7c0-489a-96ae-4ae4802ba8c5 @ 05/04/23 06:37:18.986
  STEP: Updating configmap cm-test-opt-upd-976e2c5b-cc20-4bf3-8caa-c6b34f8aa8c1 @ 05/04/23 06:37:19
  STEP: Creating configMap with name cm-test-opt-create-451169da-32d2-4132-b81f-eb6bb43528c7 @ 05/04/23 06:37:19.025
  STEP: waiting to observe update in volume @ 05/04/23 06:37:19.06
  May  4 06:38:31.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1475" for this suite. @ 05/04/23 06:38:31.512
• [76.875 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 05/04/23 06:38:31.531
  May  4 06:38:31.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename sched-pred @ 05/04/23 06:38:31.532
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:38:31.626
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:38:31.63
  May  4 06:38:31.633: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May  4 06:38:31.641: INFO: Waiting for terminating namespaces to be deleted...
  May  4 06:38:31.644: INFO: 
  Logging pods the apiserver thinks is on node k8s-node1 before test
  May  4 06:38:31.652: INFO: calico-kube-controllers-6849cf9bcf-6tzqn from kube-system started at 2023-05-04 06:32:24 +0000 UTC (1 container statuses recorded)
  May  4 06:38:31.652: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May  4 06:38:31.652: INFO: calico-node-5st2f from kube-system started at 2023-05-04 01:43:32 +0000 UTC (1 container statuses recorded)
  May  4 06:38:31.652: INFO: 	Container calico-node ready: true, restart count 0
  May  4 06:38:31.652: INFO: coredns-7bdc4cb885-57mgw from kube-system started at 2023-05-04 06:32:24 +0000 UTC (1 container statuses recorded)
  May  4 06:38:31.653: INFO: 	Container coredns ready: true, restart count 0
  May  4 06:38:31.653: INFO: coredns-7bdc4cb885-7bzz7 from kube-system started at 2023-05-04 06:32:24 +0000 UTC (1 container statuses recorded)
  May  4 06:38:31.653: INFO: 	Container coredns ready: true, restart count 0
  May  4 06:38:31.653: INFO: kube-proxy-b6n46 from kube-system started at 2023-05-04 01:43:02 +0000 UTC (1 container statuses recorded)
  May  4 06:38:31.653: INFO: 	Container kube-proxy ready: true, restart count 0
  May  4 06:38:31.653: INFO: sonobuoy from sonobuoy started at 2023-05-04 06:11:43 +0000 UTC (1 container statuses recorded)
  May  4 06:38:31.653: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May  4 06:38:31.653: INFO: sonobuoy-e2e-job-5e11a169cb044f18 from sonobuoy started at 2023-05-04 06:11:45 +0000 UTC (2 container statuses recorded)
  May  4 06:38:31.653: INFO: 	Container e2e ready: true, restart count 0
  May  4 06:38:31.653: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  4 06:38:31.653: INFO: sonobuoy-systemd-logs-daemon-set-1f5c24134e834838-h8cct from sonobuoy started at 2023-05-04 06:11:45 +0000 UTC (2 container statuses recorded)
  May  4 06:38:31.653: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  4 06:38:31.653: INFO: 	Container systemd-logs ready: true, restart count 0
  May  4 06:38:31.653: INFO: 
  Logging pods the apiserver thinks is on node k8s-node2 before test
  May  4 06:38:31.662: INFO: calico-kube-controllers-6849cf9bcf-9j56g from kube-system started at 2023-05-04 01:44:12 +0000 UTC (1 container statuses recorded)
  May  4 06:38:31.662: INFO: 	Container calico-kube-controllers ready: false, restart count 0
  May  4 06:38:31.662: INFO: calico-node-ssrv4 from kube-system started at 2023-05-04 01:43:32 +0000 UTC (1 container statuses recorded)
  May  4 06:38:31.662: INFO: 	Container calico-node ready: true, restart count 0
  May  4 06:38:31.662: INFO: coredns-7bdc4cb885-7knh9 from kube-system started at 2023-05-04 01:44:12 +0000 UTC (1 container statuses recorded)
  May  4 06:38:31.662: INFO: 	Container coredns ready: false, restart count 0
  May  4 06:38:31.662: INFO: coredns-7bdc4cb885-pdxsn from kube-system started at 2023-05-04 01:44:12 +0000 UTC (1 container statuses recorded)
  May  4 06:38:31.662: INFO: 	Container coredns ready: false, restart count 0
  May  4 06:38:31.662: INFO: kube-proxy-zxvtj from kube-system started at 2023-05-04 01:43:07 +0000 UTC (1 container statuses recorded)
  May  4 06:38:31.662: INFO: 	Container kube-proxy ready: true, restart count 0
  May  4 06:38:31.662: INFO: pod-projected-configmaps-853a6293-0c40-4d62-a411-4e4da3d1f44f from projected-1475 started at 2023-05-04 06:37:14 +0000 UTC (3 container statuses recorded)
  May  4 06:38:31.662: INFO: 	Container createcm-volume-test ready: true, restart count 0
  May  4 06:38:31.662: INFO: 	Container delcm-volume-test ready: true, restart count 0
  May  4 06:38:31.662: INFO: 	Container updcm-volume-test ready: true, restart count 0
  May  4 06:38:31.662: INFO: sonobuoy-systemd-logs-daemon-set-1f5c24134e834838-dqrng from sonobuoy started at 2023-05-04 06:11:45 +0000 UTC (2 container statuses recorded)
  May  4 06:38:31.662: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  4 06:38:31.662: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 05/04/23 06:38:31.662
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.175bdd65066dfa22], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] @ 05/04/23 06:38:31.85
  May  4 06:38:32.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-7035" for this suite. @ 05/04/23 06:38:32.73
• [1.217 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 05/04/23 06:38:32.755
  May  4 06:38:32.755: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename services @ 05/04/23 06:38:32.756
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:38:32.852
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:38:32.856
  STEP: creating service multi-endpoint-test in namespace services-1993 @ 05/04/23 06:38:32.86
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1993 to expose endpoints map[] @ 05/04/23 06:38:32.911
  May  4 06:38:32.915: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  May  4 06:38:33.923: INFO: successfully validated that service multi-endpoint-test in namespace services-1993 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-1993 @ 05/04/23 06:38:33.923
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1993 to expose endpoints map[pod1:[100]] @ 05/04/23 06:38:37.977
  May  4 06:38:38.003: INFO: successfully validated that service multi-endpoint-test in namespace services-1993 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-1993 @ 05/04/23 06:38:38.004
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1993 to expose endpoints map[pod1:[100] pod2:[101]] @ 05/04/23 06:38:40.029
  May  4 06:38:40.040: INFO: successfully validated that service multi-endpoint-test in namespace services-1993 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 05/04/23 06:38:40.04
  May  4 06:38:40.040: INFO: Creating new exec pod
  May  4 06:38:43.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-1993 exec execpodtndwd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  May  4 06:38:43.307: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  May  4 06:38:43.307: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  4 06:38:43.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-1993 exec execpodtndwd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.252.238 80'
  May  4 06:38:43.589: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.252.238 80\nConnection to 10.106.252.238 80 port [tcp/http] succeeded!\n"
  May  4 06:38:43.589: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  4 06:38:43.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-1993 exec execpodtndwd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  May  4 06:38:43.779: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  May  4 06:38:43.779: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  4 06:38:43.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-1993 exec execpodtndwd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.252.238 81'
  May  4 06:38:43.977: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.252.238 81\nConnection to 10.106.252.238 81 port [tcp/*] succeeded!\n"
  May  4 06:38:43.977: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-1993 @ 05/04/23 06:38:43.977
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1993 to expose endpoints map[pod2:[101]] @ 05/04/23 06:38:44.087
  May  4 06:38:44.124: INFO: successfully validated that service multi-endpoint-test in namespace services-1993 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-1993 @ 05/04/23 06:38:44.124
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1993 to expose endpoints map[] @ 05/04/23 06:38:44.262
  May  4 06:38:44.301: INFO: successfully validated that service multi-endpoint-test in namespace services-1993 exposes endpoints map[]
  May  4 06:38:44.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1993" for this suite. @ 05/04/23 06:38:44.442
• [11.735 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 05/04/23 06:38:44.49
  May  4 06:38:44.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename replication-controller @ 05/04/23 06:38:44.492
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:38:44.545
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:38:44.549
  STEP: Creating replication controller my-hostname-basic-ba0d34e1-aeca-41e7-8624-abbcdf1fa536 @ 05/04/23 06:38:44.553
  May  4 06:38:44.648: INFO: Pod name my-hostname-basic-ba0d34e1-aeca-41e7-8624-abbcdf1fa536: Found 0 pods out of 1
  May  4 06:38:49.661: INFO: Pod name my-hostname-basic-ba0d34e1-aeca-41e7-8624-abbcdf1fa536: Found 1 pods out of 1
  May  4 06:38:49.661: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-ba0d34e1-aeca-41e7-8624-abbcdf1fa536" are running
  May  4 06:38:49.665: INFO: Pod "my-hostname-basic-ba0d34e1-aeca-41e7-8624-abbcdf1fa536-85267" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-04 06:38:44 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-04 06:38:46 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-04 06:38:46 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-04 06:38:44 +0000 UTC Reason: Message:}])
  May  4 06:38:49.665: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 05/04/23 06:38:49.665
  May  4 06:38:49.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-1326" for this suite. @ 05/04/23 06:38:49.721
• [5.285 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 05/04/23 06:38:49.776
  May  4 06:38:49.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename gc @ 05/04/23 06:38:49.778
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:38:49.833
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:38:49.837
  STEP: create the rc @ 05/04/23 06:38:49.848
  W0504 06:38:49.870458      20 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 05/04/23 06:38:55.934
  STEP: wait for the rc to be deleted @ 05/04/23 06:38:56.279
  May  4 06:38:57.487: INFO: 81 pods remaining
  May  4 06:38:57.487: INFO: 81 pods has nil DeletionTimestamp
  May  4 06:38:57.487: INFO: 
  May  4 06:38:58.581: INFO: 69 pods remaining
  May  4 06:38:58.581: INFO: 68 pods has nil DeletionTimestamp
  May  4 06:38:58.581: INFO: 
  May  4 06:38:59.513: INFO: 57 pods remaining
  May  4 06:38:59.513: INFO: 54 pods has nil DeletionTimestamp
  May  4 06:38:59.513: INFO: 
  May  4 06:39:00.440: INFO: 43 pods remaining
  May  4 06:39:00.440: INFO: 43 pods has nil DeletionTimestamp
  May  4 06:39:00.440: INFO: 
  May  4 06:39:01.452: INFO: 32 pods remaining
  May  4 06:39:01.452: INFO: 30 pods has nil DeletionTimestamp
  May  4 06:39:01.452: INFO: 
  May  4 06:39:02.456: INFO: 17 pods remaining
  May  4 06:39:02.456: INFO: 16 pods has nil DeletionTimestamp
  May  4 06:39:02.456: INFO: 
  May  4 06:39:03.425: INFO: 0 pods remaining
  May  4 06:39:03.425: INFO: 0 pods has nil DeletionTimestamp
  May  4 06:39:03.425: INFO: 
  STEP: Gathering metrics @ 05/04/23 06:39:04.31
  May  4 06:39:04.638: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May  4 06:39:04.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7748" for this suite. @ 05/04/23 06:39:04.705
• [14.990 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 05/04/23 06:39:04.766
  May  4 06:39:04.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename webhook @ 05/04/23 06:39:04.768
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:39:04.935
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:39:04.955
  STEP: Setting up server cert @ 05/04/23 06:39:05.173
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/04/23 06:39:06.975
  STEP: Deploying the webhook pod @ 05/04/23 06:39:07.04
  STEP: Wait for the deployment to be ready @ 05/04/23 06:39:07.199
  May  4 06:39:07.397: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  May  4 06:39:09.460: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 6, 39, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 39, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 6, 39, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 39, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  4 06:39:11.476: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 6, 39, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 39, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 6, 39, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 39, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  4 06:39:13.491: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 6, 39, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 39, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 6, 39, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 39, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  4 06:39:15.480: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 6, 39, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 39, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 6, 39, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 39, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 05/04/23 06:39:17.469
  STEP: Verifying the service has paired with the endpoint @ 05/04/23 06:39:17.508
  May  4 06:39:18.509: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 05/04/23 06:39:18.513
  STEP: create a pod that should be denied by the webhook @ 05/04/23 06:39:18.558
  STEP: create a pod that causes the webhook to hang @ 05/04/23 06:39:18.575
  STEP: create a configmap that should be denied by the webhook @ 05/04/23 06:39:28.582
  STEP: create a configmap that should be admitted by the webhook @ 05/04/23 06:39:28.596
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 05/04/23 06:39:28.62
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 05/04/23 06:39:28.628
  STEP: create a namespace that bypass the webhook @ 05/04/23 06:39:28.634
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 05/04/23 06:39:28.7
  May  4 06:39:28.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4351" for this suite. @ 05/04/23 06:39:28.979
  STEP: Destroying namespace "webhook-markers-6009" for this suite. @ 05/04/23 06:39:29.013
  STEP: Destroying namespace "exempted-namespace-5342" for this suite. @ 05/04/23 06:39:29.03
• [24.295 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 05/04/23 06:39:29.062
  May  4 06:39:29.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename downward-api @ 05/04/23 06:39:29.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:39:29.167
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:39:29.172
  STEP: Creating the pod @ 05/04/23 06:39:29.176
  May  4 06:39:31.770: INFO: Successfully updated pod "labelsupdatef9b97ed3-4aec-4fda-ab4c-cffb303e52bc"
  May  4 06:39:33.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2348" for this suite. @ 05/04/23 06:39:33.812
• [4.778 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 05/04/23 06:39:33.84
  May  4 06:39:33.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename emptydir @ 05/04/23 06:39:33.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:39:33.873
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:39:33.877
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 05/04/23 06:39:33.92
  STEP: Saw pod success @ 05/04/23 06:39:37.958
  May  4 06:39:37.961: INFO: Trying to get logs from node k8s-node2 pod pod-6819f713-b345-41cb-8c8c-d01dbf7e6b69 container test-container: <nil>
  STEP: delete the pod @ 05/04/23 06:39:37.968
  May  4 06:39:38.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3477" for this suite. @ 05/04/23 06:39:38.025
• [4.233 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 05/04/23 06:39:38.075
  May  4 06:39:38.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename disruption @ 05/04/23 06:39:38.076
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:39:38.107
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:39:38.11
  STEP: Creating a pdb that targets all three pods in a test replica set @ 05/04/23 06:39:38.115
  STEP: Waiting for the pdb to be processed @ 05/04/23 06:39:38.129
  STEP: First trying to evict a pod which shouldn't be evictable @ 05/04/23 06:39:40.157
  STEP: Waiting for all pods to be running @ 05/04/23 06:39:40.157
  May  4 06:39:40.160: INFO: pods: 0 < 3
  May  4 06:39:42.211: INFO: running pods: 0 < 3
  STEP: locating a running pod @ 05/04/23 06:39:44.165
  STEP: Updating the pdb to allow a pod to be evicted @ 05/04/23 06:39:44.173
  STEP: Waiting for the pdb to be processed @ 05/04/23 06:39:44.195
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 05/04/23 06:39:46.205
  STEP: Waiting for all pods to be running @ 05/04/23 06:39:46.205
  STEP: Waiting for the pdb to observed all healthy pods @ 05/04/23 06:39:46.209
  STEP: Patching the pdb to disallow a pod to be evicted @ 05/04/23 06:39:46.33
  STEP: Waiting for the pdb to be processed @ 05/04/23 06:39:46.518
  STEP: Waiting for all pods to be running @ 05/04/23 06:39:46.566
  May  4 06:39:46.652: INFO: running pods: 2 < 3
  May  4 06:39:48.658: INFO: running pods: 2 < 3
  STEP: locating a running pod @ 05/04/23 06:39:50.656
  STEP: Deleting the pdb to allow a pod to be evicted @ 05/04/23 06:39:50.664
  STEP: Waiting for the pdb to be deleted @ 05/04/23 06:39:50.681
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 05/04/23 06:39:50.684
  STEP: Waiting for all pods to be running @ 05/04/23 06:39:50.684
  May  4 06:39:50.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-5416" for this suite. @ 05/04/23 06:39:50.76
• [12.718 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 05/04/23 06:39:50.794
  May  4 06:39:50.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename emptydir @ 05/04/23 06:39:50.796
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:39:50.885
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:39:50.889
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 05/04/23 06:39:50.933
  STEP: Saw pod success @ 05/04/23 06:39:54.964
  May  4 06:39:54.967: INFO: Trying to get logs from node k8s-node2 pod pod-64328fd2-74e6-4894-b625-72130609884a container test-container: <nil>
  STEP: delete the pod @ 05/04/23 06:39:54.976
  May  4 06:39:55.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7158" for this suite. @ 05/04/23 06:39:55.057
• [4.277 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 05/04/23 06:39:55.072
  May  4 06:39:55.072: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename field-validation @ 05/04/23 06:39:55.073
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:39:55.115
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:39:55.121
  May  4 06:39:55.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  W0504 06:39:57.753753      20 warnings.go:70] unknown field "alpha"
  W0504 06:39:57.753796      20 warnings.go:70] unknown field "beta"
  W0504 06:39:57.753813      20 warnings.go:70] unknown field "delta"
  W0504 06:39:57.753832      20 warnings.go:70] unknown field "epsilon"
  W0504 06:39:57.753849      20 warnings.go:70] unknown field "gamma"
  May  4 06:39:57.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7768" for this suite. @ 05/04/23 06:39:57.9
• [2.844 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 05/04/23 06:39:57.92
  May  4 06:39:57.920: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename statefulset @ 05/04/23 06:39:57.922
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:39:57.966
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:39:57.97
  STEP: Creating service test in namespace statefulset-5969 @ 05/04/23 06:39:57.977
  STEP: Creating a new StatefulSet @ 05/04/23 06:39:57.992
  May  4 06:39:58.029: INFO: Found 0 stateful pods, waiting for 3
  May  4 06:40:08.036: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May  4 06:40:08.036: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May  4 06:40:08.036: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  May  4 06:40:08.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=statefulset-5969 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  4 06:40:08.321: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  4 06:40:08.321: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  4 06:40:08.321: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 05/04/23 06:40:18.339
  May  4 06:40:18.371: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 05/04/23 06:40:18.371
  STEP: Updating Pods in reverse ordinal order @ 05/04/23 06:40:28.398
  May  4 06:40:28.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=statefulset-5969 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  4 06:40:28.617: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May  4 06:40:28.617: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  4 06:40:28.617: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  4 06:40:38.684: INFO: Waiting for StatefulSet statefulset-5969/ss2 to complete update
  STEP: Rolling back to a previous revision @ 05/04/23 06:40:48.693
  May  4 06:40:48.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=statefulset-5969 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  4 06:40:48.936: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  4 06:40:48.936: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  4 06:40:48.936: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  4 06:40:58.993: INFO: Updating stateful set ss2
  STEP: Rolling back update in reverse ordinal order @ 05/04/23 06:41:09.024
  May  4 06:41:09.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=statefulset-5969 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  4 06:41:09.230: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May  4 06:41:09.230: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  4 06:41:09.230: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  4 06:41:19.251: INFO: Waiting for StatefulSet statefulset-5969/ss2 to complete update
  May  4 06:41:29.263: INFO: Deleting all statefulset in ns statefulset-5969
  May  4 06:41:29.267: INFO: Scaling statefulset ss2 to 0
  May  4 06:41:39.296: INFO: Waiting for statefulset status.replicas updated to 0
  May  4 06:41:39.299: INFO: Deleting statefulset ss2
  May  4 06:41:39.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5969" for this suite. @ 05/04/23 06:41:39.331
• [101.452 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 05/04/23 06:41:39.373
  May  4 06:41:39.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename gc @ 05/04/23 06:41:39.374
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:41:39.433
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:41:39.437
  May  4 06:41:39.584: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"ea46240c-f15d-4279-b06e-003ce5655727", Controller:(*bool)(0xc004aa3a2e), BlockOwnerDeletion:(*bool)(0xc004aa3a2f)}}
  May  4 06:41:39.635: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"4d15423b-7688-432b-b97a-21c607964ba2", Controller:(*bool)(0xc004aa3cb6), BlockOwnerDeletion:(*bool)(0xc004aa3cb7)}}
  May  4 06:41:39.680: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"710fcba2-b258-4036-a5c7-e05e9d994fd4", Controller:(*bool)(0xc005429c86), BlockOwnerDeletion:(*bool)(0xc005429c87)}}
  May  4 06:41:44.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4750" for this suite. @ 05/04/23 06:41:44.746
• [5.416 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 05/04/23 06:41:44.791
  May  4 06:41:44.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename sched-preemption @ 05/04/23 06:41:44.792
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:41:44.864
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:41:44.869
  May  4 06:41:44.985: INFO: Waiting up to 1m0s for all nodes to be ready
  May  4 06:42:45.019: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 05/04/23 06:42:45.022
  May  4 06:42:45.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename sched-preemption-path @ 05/04/23 06:42:45.023
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:42:45.053
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:42:45.059
  STEP: Finding an available node @ 05/04/23 06:42:45.094
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/04/23 06:42:45.094
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/04/23 06:42:47.121
  May  4 06:42:47.191: INFO: found a healthy node: k8s-node2
  May  4 06:42:57.394: INFO: pods created so far: [1 1 1]
  May  4 06:42:57.394: INFO: length of pods created so far: 3
  May  4 06:42:59.441: INFO: pods created so far: [2 2 1]
  May  4 06:43:06.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  4 06:43:06.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-7571" for this suite. @ 05/04/23 06:43:06.639
  STEP: Destroying namespace "sched-preemption-8651" for this suite. @ 05/04/23 06:43:06.653
• [81.878 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 05/04/23 06:43:06.674
  May  4 06:43:06.674: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename hostport @ 05/04/23 06:43:06.675
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:43:06.73
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:43:06.734
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 05/04/23 06:43:06.742
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.0.157 on the node which pod1 resides and expect scheduled @ 05/04/23 06:43:10.778
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.0.157 but use UDP protocol on the node which pod2 resides @ 05/04/23 06:43:14.81
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 05/04/23 06:43:20.912
  May  4 06:43:20.912: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.0.157 http://127.0.0.1:54323/hostname] Namespace:hostport-1870 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 06:43:20.912: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 06:43:20.913: INFO: ExecWithOptions: Clientset creation
  May  4 06:43:20.913: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-1870/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.0.157+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.0.157, port: 54323 @ 05/04/23 06:43:21.013
  May  4 06:43:21.013: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.0.157:54323/hostname] Namespace:hostport-1870 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 06:43:21.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 06:43:21.014: INFO: ExecWithOptions: Clientset creation
  May  4 06:43:21.015: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-1870/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.0.157%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.0.157, port: 54323 UDP @ 05/04/23 06:43:21.101
  May  4 06:43:21.101: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.0.157 54323] Namespace:hostport-1870 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 06:43:21.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 06:43:21.102: INFO: ExecWithOptions: Clientset creation
  May  4 06:43:21.102: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-1870/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.0.157+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  May  4 06:43:26.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-1870" for this suite. @ 05/04/23 06:43:26.191
• [19.534 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 05/04/23 06:43:26.209
  May  4 06:43:26.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename resourcequota @ 05/04/23 06:43:26.21
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:43:26.279
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:43:26.283
  STEP: Counting existing ResourceQuota @ 05/04/23 06:43:43.291
  STEP: Creating a ResourceQuota @ 05/04/23 06:43:48.297
  STEP: Ensuring resource quota status is calculated @ 05/04/23 06:43:48.31
  STEP: Creating a ConfigMap @ 05/04/23 06:43:50.314
  STEP: Ensuring resource quota status captures configMap creation @ 05/04/23 06:43:50.363
  STEP: Deleting a ConfigMap @ 05/04/23 06:43:52.368
  STEP: Ensuring resource quota status released usage @ 05/04/23 06:43:52.381
  May  4 06:43:54.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-935" for this suite. @ 05/04/23 06:43:54.391
• [28.201 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 05/04/23 06:43:54.412
  May  4 06:43:54.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename webhook @ 05/04/23 06:43:54.414
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:43:54.49
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:43:54.495
  STEP: Setting up server cert @ 05/04/23 06:43:54.586
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/04/23 06:43:55.887
  STEP: Deploying the webhook pod @ 05/04/23 06:43:55.913
  STEP: Wait for the deployment to be ready @ 05/04/23 06:43:55.987
  May  4 06:43:56.020: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  May  4 06:43:58.050: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 6, 43, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 43, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 6, 43, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 43, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 05/04/23 06:44:00.055
  STEP: Verifying the service has paired with the endpoint @ 05/04/23 06:44:00.107
  May  4 06:44:01.107: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 05/04/23 06:44:01.123
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 05/04/23 06:44:01.165
  STEP: Creating a configMap that should not be mutated @ 05/04/23 06:44:01.189
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 05/04/23 06:44:01.219
  STEP: Creating a configMap that should be mutated @ 05/04/23 06:44:01.258
  May  4 06:44:01.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4268" for this suite. @ 05/04/23 06:44:01.481
  STEP: Destroying namespace "webhook-markers-6716" for this suite. @ 05/04/23 06:44:01.521
• [7.148 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 05/04/23 06:44:01.566
  May  4 06:44:01.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename subpath @ 05/04/23 06:44:01.567
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:44:01.609
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:44:01.613
  STEP: Setting up data @ 05/04/23 06:44:01.617
  STEP: Creating pod pod-subpath-test-downwardapi-sdk4 @ 05/04/23 06:44:01.693
  STEP: Creating a pod to test atomic-volume-subpath @ 05/04/23 06:44:01.693
  STEP: Saw pod success @ 05/04/23 06:44:25.776
  May  4 06:44:25.779: INFO: Trying to get logs from node k8s-node2 pod pod-subpath-test-downwardapi-sdk4 container test-container-subpath-downwardapi-sdk4: <nil>
  STEP: delete the pod @ 05/04/23 06:44:25.799
  STEP: Deleting pod pod-subpath-test-downwardapi-sdk4 @ 05/04/23 06:44:25.863
  May  4 06:44:25.863: INFO: Deleting pod "pod-subpath-test-downwardapi-sdk4" in namespace "subpath-5549"
  May  4 06:44:25.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5549" for this suite. @ 05/04/23 06:44:25.875
• [24.347 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 05/04/23 06:44:25.915
  May  4 06:44:25.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename field-validation @ 05/04/23 06:44:25.916
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:44:25.981
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:44:25.985
  May  4 06:44:25.989: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  W0504 06:44:28.606996      20 warnings.go:70] unknown field "alpha"
  W0504 06:44:28.607053      20 warnings.go:70] unknown field "beta"
  W0504 06:44:28.607084      20 warnings.go:70] unknown field "delta"
  W0504 06:44:28.607114      20 warnings.go:70] unknown field "epsilon"
  W0504 06:44:28.607137      20 warnings.go:70] unknown field "gamma"
  May  4 06:44:28.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9609" for this suite. @ 05/04/23 06:44:28.648
• [2.776 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 05/04/23 06:44:28.692
  May  4 06:44:28.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename statefulset @ 05/04/23 06:44:28.695
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:44:28.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:44:28.771
  STEP: Creating service test in namespace statefulset-3732 @ 05/04/23 06:44:28.774
  STEP: Creating statefulset ss in namespace statefulset-3732 @ 05/04/23 06:44:28.819
  May  4 06:44:28.875: INFO: Found 0 stateful pods, waiting for 1
  May  4 06:44:38.879: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 05/04/23 06:44:38.885
  STEP: Getting /status @ 05/04/23 06:44:38.9
  May  4 06:44:38.904: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 05/04/23 06:44:38.904
  May  4 06:44:38.955: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 05/04/23 06:44:38.955
  May  4 06:44:38.958: INFO: Observed &StatefulSet event: ADDED
  May  4 06:44:38.958: INFO: Found Statefulset ss in namespace statefulset-3732 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May  4 06:44:38.958: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 05/04/23 06:44:38.958
  May  4 06:44:38.958: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May  4 06:44:38.974: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 05/04/23 06:44:38.975
  May  4 06:44:38.977: INFO: Observed &StatefulSet event: ADDED
  May  4 06:44:38.977: INFO: Deleting all statefulset in ns statefulset-3732
  May  4 06:44:38.980: INFO: Scaling statefulset ss to 0
  May  4 06:44:49.014: INFO: Waiting for statefulset status.replicas updated to 0
  May  4 06:44:49.017: INFO: Deleting statefulset ss
  May  4 06:44:49.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3732" for this suite. @ 05/04/23 06:44:49.042
• [20.383 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 05/04/23 06:44:49.076
  May  4 06:44:49.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename secrets @ 05/04/23 06:44:49.078
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:44:49.117
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:44:49.121
  STEP: Creating secret with name secret-test-86421303-6ddf-4e34-8d8d-ad8280360a9a @ 05/04/23 06:44:49.149
  STEP: Creating a pod to test consume secrets @ 05/04/23 06:44:49.163
  STEP: Saw pod success @ 05/04/23 06:44:53.21
  May  4 06:44:53.230: INFO: Trying to get logs from node k8s-node2 pod pod-secrets-b047c08a-98cc-42dd-a37f-d57f415dde8e container secret-env-test: <nil>
  STEP: delete the pod @ 05/04/23 06:44:53.242
  May  4 06:44:53.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5942" for this suite. @ 05/04/23 06:44:53.33
• [4.272 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 05/04/23 06:44:53.35
  May  4 06:44:53.350: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename emptydir @ 05/04/23 06:44:53.353
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:44:53.421
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:44:53.428
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 05/04/23 06:44:53.435
  STEP: Saw pod success @ 05/04/23 06:44:57.478
  May  4 06:44:57.482: INFO: Trying to get logs from node k8s-node2 pod pod-5ac951fe-ea89-406e-bc90-9376fffdf4df container test-container: <nil>
  STEP: delete the pod @ 05/04/23 06:44:57.488
  May  4 06:44:57.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9262" for this suite. @ 05/04/23 06:44:57.569
• [4.235 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 05/04/23 06:44:57.586
  May  4 06:44:57.586: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename disruption @ 05/04/23 06:44:57.587
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:44:57.648
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:44:57.653
  STEP: Waiting for the pdb to be processed @ 05/04/23 06:44:57.688
  STEP: Waiting for all pods to be running @ 05/04/23 06:44:59.8
  May  4 06:44:59.834: INFO: running pods: 0 < 3
  May  4 06:45:01.849: INFO: running pods: 0 < 3
  May  4 06:45:03.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-7833" for this suite. @ 05/04/23 06:45:03.87
• [6.314 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 05/04/23 06:45:03.901
  May  4 06:45:03.901: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 06:45:03.902
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:45:04.025
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:45:04.029
  STEP: Creating a pod to test downward API volume plugin @ 05/04/23 06:45:04.032
  STEP: Saw pod success @ 05/04/23 06:45:08.129
  May  4 06:45:08.132: INFO: Trying to get logs from node k8s-node1 pod downwardapi-volume-8c2a4d2d-21dd-402c-800e-4e471b0d511a container client-container: <nil>
  STEP: delete the pod @ 05/04/23 06:45:08.155
  May  4 06:45:08.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8598" for this suite. @ 05/04/23 06:45:08.224
• [4.364 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 05/04/23 06:45:08.266
  May  4 06:45:08.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 06:45:08.267
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:45:08.301
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:45:08.306
  STEP: Creating secret with name projected-secret-test-cf9efaed-ef19-4f51-9e17-338a132fcc11 @ 05/04/23 06:45:08.321
  STEP: Creating a pod to test consume secrets @ 05/04/23 06:45:08.357
  STEP: Saw pod success @ 05/04/23 06:45:14.428
  May  4 06:45:14.431: INFO: Trying to get logs from node k8s-node2 pod pod-projected-secrets-cc79df37-4960-41c5-addb-856cf210c54f container secret-volume-test: <nil>
  STEP: delete the pod @ 05/04/23 06:45:14.437
  May  4 06:45:14.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7771" for this suite. @ 05/04/23 06:45:14.484
• [6.254 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 05/04/23 06:45:14.521
  May  4 06:45:14.521: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/04/23 06:45:14.522
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:45:14.578
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:45:14.582
  May  4 06:45:14.586: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 05/04/23 06:45:17.143
  May  4 06:45:17.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-6740 --namespace=crd-publish-openapi-6740 create -f -'
  May  4 06:45:18.163: INFO: stderr: ""
  May  4 06:45:18.163: INFO: stdout: "e2e-test-crd-publish-openapi-6648-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  May  4 06:45:18.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-6740 --namespace=crd-publish-openapi-6740 delete e2e-test-crd-publish-openapi-6648-crds test-foo'
  May  4 06:45:18.292: INFO: stderr: ""
  May  4 06:45:18.292: INFO: stdout: "e2e-test-crd-publish-openapi-6648-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  May  4 06:45:18.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-6740 --namespace=crd-publish-openapi-6740 apply -f -'
  May  4 06:45:18.704: INFO: stderr: ""
  May  4 06:45:18.705: INFO: stdout: "e2e-test-crd-publish-openapi-6648-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  May  4 06:45:18.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-6740 --namespace=crd-publish-openapi-6740 delete e2e-test-crd-publish-openapi-6648-crds test-foo'
  May  4 06:45:18.830: INFO: stderr: ""
  May  4 06:45:18.830: INFO: stdout: "e2e-test-crd-publish-openapi-6648-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 05/04/23 06:45:18.83
  May  4 06:45:18.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-6740 --namespace=crd-publish-openapi-6740 create -f -'
  May  4 06:45:19.197: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 05/04/23 06:45:19.197
  May  4 06:45:19.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-6740 --namespace=crd-publish-openapi-6740 create -f -'
  May  4 06:45:19.567: INFO: rc: 1
  May  4 06:45:19.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-6740 --namespace=crd-publish-openapi-6740 apply -f -'
  May  4 06:45:19.976: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 05/04/23 06:45:19.976
  May  4 06:45:19.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-6740 --namespace=crd-publish-openapi-6740 create -f -'
  May  4 06:45:20.339: INFO: rc: 1
  May  4 06:45:20.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-6740 --namespace=crd-publish-openapi-6740 apply -f -'
  May  4 06:45:20.688: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 05/04/23 06:45:20.688
  May  4 06:45:20.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-6740 explain e2e-test-crd-publish-openapi-6648-crds'
  May  4 06:45:21.064: INFO: stderr: ""
  May  4 06:45:21.064: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-6648-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 05/04/23 06:45:21.065
  May  4 06:45:21.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-6740 explain e2e-test-crd-publish-openapi-6648-crds.metadata'
  May  4 06:45:21.462: INFO: stderr: ""
  May  4 06:45:21.462: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-6648-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  May  4 06:45:21.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-6740 explain e2e-test-crd-publish-openapi-6648-crds.spec'
  May  4 06:45:21.832: INFO: stderr: ""
  May  4 06:45:21.832: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-6648-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  May  4 06:45:21.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-6740 explain e2e-test-crd-publish-openapi-6648-crds.spec.bars'
  May  4 06:45:22.204: INFO: stderr: ""
  May  4 06:45:22.205: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-6648-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 05/04/23 06:45:22.205
  May  4 06:45:22.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-6740 explain e2e-test-crd-publish-openapi-6648-crds.spec.bars2'
  May  4 06:45:22.551: INFO: rc: 1
  May  4 06:45:24.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6740" for this suite. @ 05/04/23 06:45:24.107
• [9.600 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 05/04/23 06:45:24.122
  May  4 06:45:24.122: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename daemonsets @ 05/04/23 06:45:24.123
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:45:24.188
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:45:24.194
  STEP: Creating a simple DaemonSet "daemon-set" @ 05/04/23 06:45:24.236
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/04/23 06:45:24.25
  May  4 06:45:24.254: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 06:45:24.275: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 06:45:24.275: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  May  4 06:45:25.334: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 06:45:25.343: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 06:45:25.343: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  May  4 06:45:26.281: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 06:45:26.284: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 06:45:26.284: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  May  4 06:45:27.290: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 06:45:27.294: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  4 06:45:27.294: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 05/04/23 06:45:27.297
  May  4 06:45:27.370: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 06:45:27.416: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  4 06:45:27.416: INFO: Node k8s-node2 is running 0 daemon pod, expected 1
  May  4 06:45:28.431: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 06:45:28.435: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  4 06:45:28.435: INFO: Node k8s-node2 is running 0 daemon pod, expected 1
  May  4 06:45:29.440: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 06:45:29.443: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  4 06:45:29.443: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 05/04/23 06:45:29.443
  STEP: Deleting DaemonSet "daemon-set" @ 05/04/23 06:45:29.466
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5435, will wait for the garbage collector to delete the pods @ 05/04/23 06:45:29.467
  May  4 06:45:29.534: INFO: Deleting DaemonSet.extensions daemon-set took: 13.795729ms
  May  4 06:45:29.634: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.152527ms
  May  4 06:45:32.538: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 06:45:32.538: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May  4 06:45:32.541: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"64436"},"items":null}

  May  4 06:45:32.544: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"64436"},"items":null}

  May  4 06:45:32.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5435" for this suite. @ 05/04/23 06:45:32.579
• [8.484 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 05/04/23 06:45:32.607
  May  4 06:45:32.607: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename certificates @ 05/04/23 06:45:32.608
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:45:32.649
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:45:32.653
  STEP: getting /apis @ 05/04/23 06:45:34.353
  STEP: getting /apis/certificates.k8s.io @ 05/04/23 06:45:34.359
  STEP: getting /apis/certificates.k8s.io/v1 @ 05/04/23 06:45:34.361
  STEP: creating @ 05/04/23 06:45:34.362
  STEP: getting @ 05/04/23 06:45:34.45
  STEP: listing @ 05/04/23 06:45:34.453
  STEP: watching @ 05/04/23 06:45:34.456
  May  4 06:45:34.456: INFO: starting watch
  STEP: patching @ 05/04/23 06:45:34.458
  STEP: updating @ 05/04/23 06:45:34.491
  May  4 06:45:34.533: INFO: waiting for watch events with expected annotations
  May  4 06:45:34.533: INFO: saw patched and updated annotations
  STEP: getting /approval @ 05/04/23 06:45:34.534
  STEP: patching /approval @ 05/04/23 06:45:34.537
  STEP: updating /approval @ 05/04/23 06:45:34.569
  STEP: getting /status @ 05/04/23 06:45:34.586
  STEP: patching /status @ 05/04/23 06:45:34.589
  STEP: updating /status @ 05/04/23 06:45:34.62
  STEP: deleting @ 05/04/23 06:45:34.661
  STEP: deleting a collection @ 05/04/23 06:45:34.685
  May  4 06:45:34.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-2605" for this suite. @ 05/04/23 06:45:34.725
• [2.135 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 05/04/23 06:45:34.742
  May  4 06:45:34.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename sched-preemption @ 05/04/23 06:45:34.744
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:45:34.81
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:45:34.814
  May  4 06:45:34.882: INFO: Waiting up to 1m0s for all nodes to be ready
  May  4 06:46:34.933: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 05/04/23 06:46:34.939
  May  4 06:46:35.007: INFO: Created pod: pod0-0-sched-preemption-low-priority
  May  4 06:46:35.024: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  May  4 06:46:35.138: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  May  4 06:46:35.189: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 05/04/23 06:46:35.189
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 05/04/23 06:46:39.255
  May  4 06:46:45.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-9039" for this suite. @ 05/04/23 06:46:45.477
• [70.751 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 05/04/23 06:46:45.494
  May  4 06:46:45.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename daemonsets @ 05/04/23 06:46:45.495
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:46:45.537
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:46:45.541
  STEP: Creating simple DaemonSet "daemon-set" @ 05/04/23 06:46:45.597
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/04/23 06:46:45.622
  May  4 06:46:45.626: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 06:46:45.637: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 06:46:45.637: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  May  4 06:46:46.647: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 06:46:46.654: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 06:46:46.654: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  May  4 06:46:47.643: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 06:46:47.647: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  4 06:46:47.647: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  May  4 06:46:48.642: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 06:46:48.646: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  4 06:46:48.646: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Getting /status @ 05/04/23 06:46:48.649
  May  4 06:46:48.652: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 05/04/23 06:46:48.652
  May  4 06:46:48.676: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 05/04/23 06:46:48.676
  May  4 06:46:48.678: INFO: Observed &DaemonSet event: ADDED
  May  4 06:46:48.679: INFO: Observed &DaemonSet event: MODIFIED
  May  4 06:46:48.679: INFO: Observed &DaemonSet event: MODIFIED
  May  4 06:46:48.679: INFO: Observed &DaemonSet event: MODIFIED
  May  4 06:46:48.679: INFO: Found daemon set daemon-set in namespace daemonsets-2214 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May  4 06:46:48.679: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 05/04/23 06:46:48.679
  STEP: watching for the daemon set status to be patched @ 05/04/23 06:46:48.705
  May  4 06:46:48.708: INFO: Observed &DaemonSet event: ADDED
  May  4 06:46:48.708: INFO: Observed &DaemonSet event: MODIFIED
  May  4 06:46:48.708: INFO: Observed &DaemonSet event: MODIFIED
  May  4 06:46:48.709: INFO: Observed &DaemonSet event: MODIFIED
  May  4 06:46:48.709: INFO: Observed daemon set daemon-set in namespace daemonsets-2214 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May  4 06:46:48.709: INFO: Observed &DaemonSet event: MODIFIED
  May  4 06:46:48.709: INFO: Found daemon set daemon-set in namespace daemonsets-2214 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  May  4 06:46:48.709: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 05/04/23 06:46:48.735
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2214, will wait for the garbage collector to delete the pods @ 05/04/23 06:46:48.735
  May  4 06:46:48.805: INFO: Deleting DaemonSet.extensions daemon-set took: 15.338368ms
  May  4 06:46:48.906: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.100535ms
  May  4 06:46:51.012: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 06:46:51.012: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May  4 06:46:51.016: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"64750"},"items":null}

  May  4 06:46:51.019: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"64750"},"items":null}

  May  4 06:46:51.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-2214" for this suite. @ 05/04/23 06:46:51.121
• [5.668 seconds]
------------------------------
SSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 05/04/23 06:46:51.163
  May  4 06:46:51.163: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename runtimeclass @ 05/04/23 06:46:51.164
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:46:51.283
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:46:51.296
  May  4 06:46:51.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3520" for this suite. @ 05/04/23 06:46:51.584
• [0.446 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 05/04/23 06:46:51.609
  May  4 06:46:51.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename configmap @ 05/04/23 06:46:51.61
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:46:51.707
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:46:51.713
  STEP: Creating configMap with name configmap-test-upd-fe8d76e1-91fe-419a-8bef-b44151b76c9a @ 05/04/23 06:46:51.768
  STEP: Creating the pod @ 05/04/23 06:46:51.801
  STEP: Waiting for pod with text data @ 05/04/23 06:46:55.92
  STEP: Waiting for pod with binary data @ 05/04/23 06:46:55.941
  May  4 06:46:55.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5617" for this suite. @ 05/04/23 06:46:55.952
• [4.357 seconds]
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 05/04/23 06:46:55.966
  May  4 06:46:55.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename limitrange @ 05/04/23 06:46:55.968
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:46:56.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:46:56.038
  STEP: Creating LimitRange "e2e-limitrange-9dm5v" in namespace "limitrange-7303" @ 05/04/23 06:46:56.042
  STEP: Creating another limitRange in another namespace @ 05/04/23 06:46:56.068
  May  4 06:46:56.103: INFO: Namespace "e2e-limitrange-9dm5v-5154" created
  May  4 06:46:56.103: INFO: Creating LimitRange "e2e-limitrange-9dm5v" in namespace "e2e-limitrange-9dm5v-5154"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-9dm5v" @ 05/04/23 06:46:56.141
  May  4 06:46:56.145: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-9dm5v" in "limitrange-7303" namespace @ 05/04/23 06:46:56.145
  May  4 06:46:56.159: INFO: LimitRange "e2e-limitrange-9dm5v" has been patched
  STEP: Delete LimitRange "e2e-limitrange-9dm5v" by Collection with labelSelector: "e2e-limitrange-9dm5v=patched" @ 05/04/23 06:46:56.159
  STEP: Confirm that the limitRange "e2e-limitrange-9dm5v" has been deleted @ 05/04/23 06:46:56.174
  May  4 06:46:56.174: INFO: Requesting list of LimitRange to confirm quantity
  May  4 06:46:56.177: INFO: Found 0 LimitRange with label "e2e-limitrange-9dm5v=patched"
  May  4 06:46:56.177: INFO: LimitRange "e2e-limitrange-9dm5v" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-9dm5v" @ 05/04/23 06:46:56.177
  May  4 06:46:56.180: INFO: Found 1 limitRange
  May  4 06:46:56.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-7303" for this suite. @ 05/04/23 06:46:56.186
  STEP: Destroying namespace "e2e-limitrange-9dm5v-5154" for this suite. @ 05/04/23 06:46:56.201
• [0.252 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 05/04/23 06:46:56.221
  May  4 06:46:56.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename services @ 05/04/23 06:46:56.222
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:46:56.289
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:46:56.293
  STEP: creating service endpoint-test2 in namespace services-9761 @ 05/04/23 06:46:56.3
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9761 to expose endpoints map[] @ 05/04/23 06:46:56.339
  May  4 06:46:56.398: INFO: successfully validated that service endpoint-test2 in namespace services-9761 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-9761 @ 05/04/23 06:46:56.398
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9761 to expose endpoints map[pod1:[80]] @ 05/04/23 06:47:00.452
  May  4 06:47:00.462: INFO: successfully validated that service endpoint-test2 in namespace services-9761 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 05/04/23 06:47:00.462
  May  4 06:47:00.462: INFO: Creating new exec pod
  May  4 06:47:05.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-9761 exec execpod5c84d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May  4 06:47:05.697: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May  4 06:47:05.697: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  4 06:47:05.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-9761 exec execpod5c84d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.112.52 80'
  May  4 06:47:05.901: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.112.52 80\nConnection to 10.103.112.52 80 port [tcp/http] succeeded!\n"
  May  4 06:47:05.901: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-9761 @ 05/04/23 06:47:05.901
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9761 to expose endpoints map[pod1:[80] pod2:[80]] @ 05/04/23 06:47:07.937
  May  4 06:47:07.949: INFO: successfully validated that service endpoint-test2 in namespace services-9761 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 05/04/23 06:47:07.949
  May  4 06:47:08.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-9761 exec execpod5c84d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May  4 06:47:09.159: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May  4 06:47:09.159: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  4 06:47:09.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-9761 exec execpod5c84d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.112.52 80'
  May  4 06:47:09.356: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.112.52 80\nConnection to 10.103.112.52 80 port [tcp/http] succeeded!\n"
  May  4 06:47:09.356: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-9761 @ 05/04/23 06:47:09.356
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9761 to expose endpoints map[pod2:[80]] @ 05/04/23 06:47:09.404
  May  4 06:47:09.464: INFO: successfully validated that service endpoint-test2 in namespace services-9761 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 05/04/23 06:47:09.464
  May  4 06:47:10.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-9761 exec execpod5c84d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May  4 06:47:10.662: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May  4 06:47:10.662: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  4 06:47:10.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-9761 exec execpod5c84d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.112.52 80'
  May  4 06:47:10.867: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.112.52 80\nConnection to 10.103.112.52 80 port [tcp/http] succeeded!\n"
  May  4 06:47:10.867: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-9761 @ 05/04/23 06:47:10.867
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9761 to expose endpoints map[] @ 05/04/23 06:47:10.901
  May  4 06:47:10.960: INFO: successfully validated that service endpoint-test2 in namespace services-9761 exposes endpoints map[]
  May  4 06:47:10.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9761" for this suite. @ 05/04/23 06:47:11.04
• [14.887 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 05/04/23 06:47:11.11
  May  4 06:47:11.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename configmap @ 05/04/23 06:47:11.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:47:11.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:47:11.174
  STEP: Creating configMap with name configmap-test-upd-70427b26-4dbd-4aea-b403-e9178e3428c2 @ 05/04/23 06:47:11.184
  STEP: Creating the pod @ 05/04/23 06:47:11.233
  STEP: Updating configmap configmap-test-upd-70427b26-4dbd-4aea-b403-e9178e3428c2 @ 05/04/23 06:47:15.305
  STEP: waiting to observe update in volume @ 05/04/23 06:47:15.318
  May  4 06:48:41.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3879" for this suite. @ 05/04/23 06:48:41.791
• [90.699 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 05/04/23 06:48:41.81
  May  4 06:48:41.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/04/23 06:48:41.812
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:48:41.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:48:41.871
  STEP: create the container to handle the HTTPGet hook request. @ 05/04/23 06:48:41.89
  STEP: create the pod with lifecycle hook @ 05/04/23 06:48:45.925
  STEP: delete the pod with lifecycle hook @ 05/04/23 06:48:47.986
  STEP: check prestop hook @ 05/04/23 06:48:52.042
  May  4 06:48:52.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-1060" for this suite. @ 05/04/23 06:48:52.069
• [10.273 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 05/04/23 06:48:52.085
  May  4 06:48:52.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename containers @ 05/04/23 06:48:52.087
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:48:52.14
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:48:52.144
  STEP: Creating a pod to test override command @ 05/04/23 06:48:52.148
  STEP: Saw pod success @ 05/04/23 06:48:56.224
  May  4 06:48:56.226: INFO: Trying to get logs from node k8s-node2 pod client-containers-2d298349-72f2-4841-b916-83405b8ef5a5 container agnhost-container: <nil>
  STEP: delete the pod @ 05/04/23 06:48:56.233
  May  4 06:48:56.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-5767" for this suite. @ 05/04/23 06:48:56.28
• [4.209 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 05/04/23 06:48:56.297
  May  4 06:48:56.297: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename events @ 05/04/23 06:48:56.298
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:48:56.37
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:48:56.374
  STEP: creating a test event @ 05/04/23 06:48:56.378
  STEP: listing all events in all namespaces @ 05/04/23 06:48:56.391
  STEP: patching the test event @ 05/04/23 06:48:56.398
  STEP: fetching the test event @ 05/04/23 06:48:56.415
  STEP: updating the test event @ 05/04/23 06:48:56.418
  STEP: getting the test event @ 05/04/23 06:48:56.444
  STEP: deleting the test event @ 05/04/23 06:48:56.466
  STEP: listing all events in all namespaces @ 05/04/23 06:48:56.481
  May  4 06:48:56.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-1003" for this suite. @ 05/04/23 06:48:56.494
• [0.214 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 05/04/23 06:48:56.515
  May  4 06:48:56.515: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename webhook @ 05/04/23 06:48:56.516
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:48:56.593
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:48:56.599
  STEP: Setting up server cert @ 05/04/23 06:48:56.646
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/04/23 06:48:57.668
  STEP: Deploying the webhook pod @ 05/04/23 06:48:57.712
  STEP: Wait for the deployment to be ready @ 05/04/23 06:48:57.778
  May  4 06:48:57.819: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  May  4 06:48:59.830: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 6, 48, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 48, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 6, 48, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 6, 48, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 05/04/23 06:49:01.835
  STEP: Verifying the service has paired with the endpoint @ 05/04/23 06:49:01.913
  May  4 06:49:02.913: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 05/04/23 06:49:03.422
  STEP: Creating a configMap that should be mutated @ 05/04/23 06:49:03.458
  STEP: Deleting the collection of validation webhooks @ 05/04/23 06:49:03.56
  STEP: Creating a configMap that should not be mutated @ 05/04/23 06:49:03.761
  May  4 06:49:03.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2330" for this suite. @ 05/04/23 06:49:04.113
  STEP: Destroying namespace "webhook-markers-4831" for this suite. @ 05/04/23 06:49:04.159
• [7.692 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 05/04/23 06:49:04.207
  May  4 06:49:04.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename replicaset @ 05/04/23 06:49:04.209
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:49:04.34
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:49:04.344
  May  4 06:49:04.419: INFO: Pod name sample-pod: Found 0 pods out of 1
  May  4 06:49:09.429: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/04/23 06:49:09.429
  STEP: Scaling up "test-rs" replicaset  @ 05/04/23 06:49:09.429
  May  4 06:49:09.475: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 05/04/23 06:49:09.476
  W0504 06:49:09.520102      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May  4 06:49:09.522: INFO: observed ReplicaSet test-rs in namespace replicaset-1835 with ReadyReplicas 1, AvailableReplicas 1
  May  4 06:49:09.644: INFO: observed ReplicaSet test-rs in namespace replicaset-1835 with ReadyReplicas 1, AvailableReplicas 1
  May  4 06:49:09.745: INFO: observed ReplicaSet test-rs in namespace replicaset-1835 with ReadyReplicas 1, AvailableReplicas 1
  May  4 06:49:09.798: INFO: observed ReplicaSet test-rs in namespace replicaset-1835 with ReadyReplicas 1, AvailableReplicas 1
  May  4 06:49:12.068: INFO: observed ReplicaSet test-rs in namespace replicaset-1835 with ReadyReplicas 2, AvailableReplicas 2
  May  4 06:49:12.263: INFO: observed Replicaset test-rs in namespace replicaset-1835 with ReadyReplicas 3 found true
  May  4 06:49:12.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1835" for this suite. @ 05/04/23 06:49:12.268
• [8.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 05/04/23 06:49:12.285
  May  4 06:49:12.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename field-validation @ 05/04/23 06:49:12.286
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:49:12.316
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:49:12.32
  May  4 06:49:12.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  W0504 06:49:12.335746      20 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc00473c0d0 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  W0504 06:49:14.960206      20 warnings.go:70] unknown field "alpha"
  W0504 06:49:14.960266      20 warnings.go:70] unknown field "beta"
  W0504 06:49:14.960324      20 warnings.go:70] unknown field "delta"
  W0504 06:49:14.960351      20 warnings.go:70] unknown field "epsilon"
  W0504 06:49:14.960380      20 warnings.go:70] unknown field "gamma"
  May  4 06:49:14.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5743" for this suite. @ 05/04/23 06:49:15.071
• [2.809 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 05/04/23 06:49:15.095
  May  4 06:49:15.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename container-probe @ 05/04/23 06:49:15.097
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:49:15.128
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:49:15.132
  STEP: Creating pod test-grpc-91283b42-7ab4-4430-b408-5ffc09fffa2a in namespace container-probe-5209 @ 05/04/23 06:49:15.15
  May  4 06:49:17.209: INFO: Started pod test-grpc-91283b42-7ab4-4430-b408-5ffc09fffa2a in namespace container-probe-5209
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/04/23 06:49:17.209
  May  4 06:49:17.212: INFO: Initial restart count of pod test-grpc-91283b42-7ab4-4430-b408-5ffc09fffa2a is 0
  May  4 06:53:17.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/04/23 06:53:17.953
  STEP: Destroying namespace "container-probe-5209" for this suite. @ 05/04/23 06:53:18.027
• [242.963 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 05/04/23 06:53:18.065
  May  4 06:53:18.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename services @ 05/04/23 06:53:18.067
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:53:18.181
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:53:18.186
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-8229 @ 05/04/23 06:53:18.19
  STEP: changing the ExternalName service to type=NodePort @ 05/04/23 06:53:18.231
  STEP: creating replication controller externalname-service in namespace services-8229 @ 05/04/23 06:53:18.349
  I0504 06:53:18.408677      20 runners.go:194] Created replication controller with name: externalname-service, namespace: services-8229, replica count: 2
  I0504 06:53:21.460447      20 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  4 06:53:21.460: INFO: Creating new exec pod
  May  4 06:53:26.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-8229 exec execpodq77cg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May  4 06:53:26.703: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May  4 06:53:26.703: INFO: stdout: ""
  May  4 06:53:27.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-8229 exec execpodq77cg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May  4 06:53:27.939: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May  4 06:53:27.939: INFO: stdout: ""
  May  4 06:53:28.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-8229 exec execpodq77cg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May  4 06:53:28.905: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May  4 06:53:28.905: INFO: stdout: "externalname-service-5s5vn"
  May  4 06:53:28.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-8229 exec execpodq77cg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.127.183 80'
  May  4 06:53:29.114: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.127.183 80\nConnection to 10.106.127.183 80 port [tcp/http] succeeded!\n"
  May  4 06:53:29.114: INFO: stdout: "externalname-service-5s5vn"
  May  4 06:53:29.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-8229 exec execpodq77cg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.156 32423'
  May  4 06:53:29.309: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.156 32423\nConnection to 192.168.0.156 32423 port [tcp/*] succeeded!\n"
  May  4 06:53:29.309: INFO: stdout: "externalname-service-b77p8"
  May  4 06:53:29.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-8229 exec execpodq77cg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.157 32423'
  May  4 06:53:29.506: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.157 32423\nConnection to 192.168.0.157 32423 port [tcp/*] succeeded!\n"
  May  4 06:53:29.507: INFO: stdout: "externalname-service-5s5vn"
  May  4 06:53:29.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  4 06:53:29.521: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-8229" for this suite. @ 05/04/23 06:53:29.64
• [11.621 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 05/04/23 06:53:29.687
  May  4 06:53:29.687: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename resourcequota @ 05/04/23 06:53:29.69
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:53:29.761
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:53:29.768
  STEP: Creating a ResourceQuota @ 05/04/23 06:53:29.773
  STEP: Getting a ResourceQuota @ 05/04/23 06:53:29.788
  STEP: Listing all ResourceQuotas with LabelSelector @ 05/04/23 06:53:29.791
  STEP: Patching the ResourceQuota @ 05/04/23 06:53:29.803
  STEP: Deleting a Collection of ResourceQuotas @ 05/04/23 06:53:29.819
  STEP: Verifying the deleted ResourceQuota @ 05/04/23 06:53:29.887
  May  4 06:53:29.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9711" for this suite. @ 05/04/23 06:53:29.894
• [0.222 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 05/04/23 06:53:29.91
  May  4 06:53:29.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename downward-api @ 05/04/23 06:53:29.911
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:53:29.957
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:53:29.961
  STEP: Creating a pod to test downward API volume plugin @ 05/04/23 06:53:29.965
  STEP: Saw pod success @ 05/04/23 06:53:34.013
  May  4 06:53:34.017: INFO: Trying to get logs from node k8s-node2 pod downwardapi-volume-28096db3-85fc-414d-8b3c-2057c4f010a1 container client-container: <nil>
  STEP: delete the pod @ 05/04/23 06:53:34.047
  May  4 06:53:34.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6573" for this suite. @ 05/04/23 06:53:34.135
• [4.255 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 05/04/23 06:53:34.167
  May  4 06:53:34.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename deployment @ 05/04/23 06:53:34.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:53:34.201
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:53:34.205
  May  4 06:53:34.253: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  May  4 06:53:39.267: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/04/23 06:53:39.267
  May  4 06:53:39.267: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 05/04/23 06:53:39.302
  May  4 06:53:43.349: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5029  bd2a1f86-25b0-42ba-9857-f7ce69cf8637 66158 1 2023-05-04 06:53:39 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-04 06:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 06:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005047508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-04 06:53:39 +0000 UTC,LastTransitionTime:2023-05-04 06:53:39 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-68b75d69f8" has successfully progressed.,LastUpdateTime:2023-05-04 06:53:41 +0000 UTC,LastTransitionTime:2023-05-04 06:53:39 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May  4 06:53:43.353: INFO: New ReplicaSet "test-cleanup-deployment-68b75d69f8" of Deployment "test-cleanup-deployment":
  &ReplicaSet{ObjectMeta:{test-cleanup-deployment-68b75d69f8  deployment-5029  345912e5-e1cc-4769-9321-0cbf97f14e98 66147 1 2023-05-04 06:53:39 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment bd2a1f86-25b0-42ba-9857-f7ce69cf8637 0xc004d59597 0xc004d59598}] [] [{kube-controller-manager Update apps/v1 2023-05-04 06:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd2a1f86-25b0-42ba-9857-f7ce69cf8637\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 06:53:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 68b75d69f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d596e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May  4 06:53:43.357: INFO: Pod "test-cleanup-deployment-68b75d69f8-fwqkw" is available:
  &Pod{ObjectMeta:{test-cleanup-deployment-68b75d69f8-fwqkw test-cleanup-deployment-68b75d69f8- deployment-5029  3b3b549b-a4c6-4ee2-9890-58d3412608f6 66146 0 2023-05-04 06:53:39 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[cni.projectcalico.org/containerID:9756ebaefbacd331267c391e6200d5713e997debadde7c8b848bd7fb72e5d183 cni.projectcalico.org/podIP:172.16.169.183/32 cni.projectcalico.org/podIPs:172.16.169.183/32] [{apps/v1 ReplicaSet test-cleanup-deployment-68b75d69f8 345912e5-e1cc-4769-9321-0cbf97f14e98 0xc004d59a87 0xc004d59a88}] [] [{kube-controller-manager Update v1 2023-05-04 06:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"345912e5-e1cc-4769-9321-0cbf97f14e98\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 06:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 06:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.169.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pkhjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pkhjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 06:53:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 06:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 06:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 06:53:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.157,PodIP:172.16.169.183,StartTime:2023-05-04 06:53:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 06:53:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:sha256:30e3d1b869f4d327bd612179ed37850611b462c8415691b3de9eb55787f81e71,ContainerID:containerd://ddf7b7507127e707892e26dea67fc0b6e4990f8c9d3421d8ba0abd4e2de9a155,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.169.183,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 06:53:43.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5029" for this suite. @ 05/04/23 06:53:43.364
• [9.216 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 05/04/23 06:53:43.385
  May  4 06:53:43.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename endpointslice @ 05/04/23 06:53:43.386
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:53:43.429
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:53:43.434
  STEP: referencing a single matching pod @ 05/04/23 06:53:48.819
  STEP: referencing matching pods with named port @ 05/04/23 06:53:53.827
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 05/04/23 06:53:58.836
  STEP: recreating EndpointSlices after they've been deleted @ 05/04/23 06:54:03.848
  May  4 06:54:03.888: INFO: EndpointSlice for Service endpointslice-1289/example-named-port not found
  May  4 06:54:13.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-1289" for this suite. @ 05/04/23 06:54:13.903
• [30.534 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 05/04/23 06:54:13.927
  May  4 06:54:13.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/04/23 06:54:13.928
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:54:14.01
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:54:14.016
  STEP: create the container to handle the HTTPGet hook request. @ 05/04/23 06:54:14.031
  STEP: create the pod with lifecycle hook @ 05/04/23 06:54:16.069
  STEP: check poststart hook @ 05/04/23 06:54:18.105
  STEP: delete the pod with lifecycle hook @ 05/04/23 06:54:18.132
  May  4 06:54:22.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-4215" for this suite. @ 05/04/23 06:54:22.201
• [8.288 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 05/04/23 06:54:22.216
  May  4 06:54:22.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename emptydir @ 05/04/23 06:54:22.217
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:54:22.291
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:54:22.295
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 05/04/23 06:54:22.299
  STEP: Saw pod success @ 05/04/23 06:54:28.344
  May  4 06:54:28.347: INFO: Trying to get logs from node k8s-node2 pod pod-bb6a858c-ae13-4486-ac49-073da3fe2a49 container test-container: <nil>
  STEP: delete the pod @ 05/04/23 06:54:28.353
  May  4 06:54:28.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1732" for this suite. @ 05/04/23 06:54:28.414
• [6.225 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 05/04/23 06:54:28.442
  May  4 06:54:28.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename pods @ 05/04/23 06:54:28.444
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:54:28.508
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:54:28.512
  STEP: creating the pod @ 05/04/23 06:54:28.516
  STEP: submitting the pod to kubernetes @ 05/04/23 06:54:28.516
  W0504 06:54:28.550499      20 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: verifying the pod is in kubernetes @ 05/04/23 06:54:32.566
  STEP: updating the pod @ 05/04/23 06:54:32.569
  May  4 06:54:33.112: INFO: Successfully updated pod "pod-update-activedeadlineseconds-066e59dd-0365-4f4a-835f-8c3adcf61021"
  May  4 06:54:37.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8977" for this suite. @ 05/04/23 06:54:37.131
• [8.713 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 05/04/23 06:54:37.158
  May  4 06:54:37.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename lease-test @ 05/04/23 06:54:37.16
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:54:37.204
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:54:37.208
  May  4 06:54:37.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-3988" for this suite. @ 05/04/23 06:54:37.361
• [0.217 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 05/04/23 06:54:37.376
  May  4 06:54:37.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 06:54:37.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:54:37.446
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:54:37.45
  STEP: Creating secret with name s-test-opt-del-1396db5e-d8cc-4c1f-929c-ccac3abbfaf9 @ 05/04/23 06:54:37.457
  STEP: Creating secret with name s-test-opt-upd-243369fd-0748-42d5-9fe4-008b4a89d3ff @ 05/04/23 06:54:37.471
  STEP: Creating the pod @ 05/04/23 06:54:37.486
  STEP: Deleting secret s-test-opt-del-1396db5e-d8cc-4c1f-929c-ccac3abbfaf9 @ 05/04/23 06:54:41.562
  STEP: Updating secret s-test-opt-upd-243369fd-0748-42d5-9fe4-008b4a89d3ff @ 05/04/23 06:54:41.577
  STEP: Creating secret with name s-test-opt-create-90721998-d410-42fd-b0f6-3f0ed0ee6949 @ 05/04/23 06:54:41.59
  STEP: waiting to observe update in volume @ 05/04/23 06:54:41.623
  May  4 06:55:48.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6973" for this suite. @ 05/04/23 06:55:48.061
• [70.721 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 05/04/23 06:55:48.099
  May  4 06:55:48.099: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename svcaccounts @ 05/04/23 06:55:48.101
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:55:48.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:55:48.138
  May  4 06:55:48.199: INFO: created pod pod-service-account-defaultsa
  May  4 06:55:48.199: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  May  4 06:55:48.249: INFO: created pod pod-service-account-mountsa
  May  4 06:55:48.249: INFO: pod pod-service-account-mountsa service account token volume mount: true
  May  4 06:55:48.277: INFO: created pod pod-service-account-nomountsa
  May  4 06:55:48.277: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  May  4 06:55:48.321: INFO: created pod pod-service-account-defaultsa-mountspec
  May  4 06:55:48.321: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  May  4 06:55:48.360: INFO: created pod pod-service-account-mountsa-mountspec
  May  4 06:55:48.360: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  May  4 06:55:48.424: INFO: created pod pod-service-account-nomountsa-mountspec
  May  4 06:55:48.424: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  May  4 06:55:48.515: INFO: created pod pod-service-account-defaultsa-nomountspec
  May  4 06:55:48.515: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  May  4 06:55:48.573: INFO: created pod pod-service-account-mountsa-nomountspec
  May  4 06:55:48.573: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  May  4 06:55:48.690: INFO: created pod pod-service-account-nomountsa-nomountspec
  May  4 06:55:48.690: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  May  4 06:55:48.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5594" for this suite. @ 05/04/23 06:55:48.76
• [0.728 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 05/04/23 06:55:48.828
  May  4 06:55:48.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename secrets @ 05/04/23 06:55:48.829
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:55:48.993
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:55:48.997
  STEP: Creating secret with name secret-test-9a244ef6-3b66-4312-8071-afc42d4685b7 @ 05/04/23 06:55:49.155
  STEP: Creating a pod to test consume secrets @ 05/04/23 06:55:49.173
  STEP: Saw pod success @ 05/04/23 06:55:55.39
  May  4 06:55:55.420: INFO: Trying to get logs from node k8s-node2 pod pod-secrets-dae06746-35d1-4596-a720-f611986a3677 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/04/23 06:55:55.533
  May  4 06:55:55.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8028" for this suite. @ 05/04/23 06:55:55.775
  STEP: Destroying namespace "secret-namespace-8674" for this suite. @ 05/04/23 06:55:55.827
• [7.023 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 05/04/23 06:55:55.853
  May  4 06:55:55.853: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/04/23 06:55:55.855
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:55:55.95
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:55:55.966
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 05/04/23 06:55:56.009
  May  4 06:55:56.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 06:55:57.687: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 06:56:04.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-904" for this suite. @ 05/04/23 06:56:04.767
• [8.928 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 05/04/23 06:56:04.782
  May  4 06:56:04.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename downward-api @ 05/04/23 06:56:04.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:56:04.842
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:56:04.846
  STEP: Creating a pod to test downward api env vars @ 05/04/23 06:56:04.849
  STEP: Saw pod success @ 05/04/23 06:56:08.881
  May  4 06:56:08.884: INFO: Trying to get logs from node k8s-node2 pod downward-api-d92ddcb7-31f7-4c90-b0be-98d19da9b402 container dapi-container: <nil>
  STEP: delete the pod @ 05/04/23 06:56:08.892
  May  4 06:56:09.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1048" for this suite. @ 05/04/23 06:56:09.011
• [4.271 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 05/04/23 06:56:09.057
  May  4 06:56:09.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename configmap @ 05/04/23 06:56:09.059
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:56:09.097
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:56:09.101
  STEP: Creating configMap with name configmap-test-volume-map-756f9e29-748c-48bc-a369-a2f2bfcf28cb @ 05/04/23 06:56:09.108
  STEP: Creating a pod to test consume configMaps @ 05/04/23 06:56:09.137
  STEP: Saw pod success @ 05/04/23 06:56:13.197
  May  4 06:56:13.255: INFO: Trying to get logs from node k8s-node2 pod pod-configmaps-169f9a7f-cfa5-46e6-9f61-7f730b8c114e container agnhost-container: <nil>
  STEP: delete the pod @ 05/04/23 06:56:13.266
  May  4 06:56:13.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5127" for this suite. @ 05/04/23 06:56:13.41
• [4.374 seconds]
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 05/04/23 06:56:13.432
  May  4 06:56:13.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename statefulset @ 05/04/23 06:56:13.435
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:56:13.487
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:56:13.491
  STEP: Creating service test in namespace statefulset-4613 @ 05/04/23 06:56:13.528
  STEP: Creating a new StatefulSet @ 05/04/23 06:56:13.545
  May  4 06:56:13.589: INFO: Found 0 stateful pods, waiting for 3
  May  4 06:56:23.593: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May  4 06:56:23.593: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May  4 06:56:23.593: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 05/04/23 06:56:23.602
  May  4 06:56:23.631: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 05/04/23 06:56:23.631
  STEP: Not applying an update when the partition is greater than the number of replicas @ 05/04/23 06:56:33.688
  STEP: Performing a canary update @ 05/04/23 06:56:33.688
  May  4 06:56:33.716: INFO: Updating stateful set ss2
  May  4 06:56:33.745: INFO: Waiting for Pod statefulset-4613/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 05/04/23 06:56:43.754
  May  4 06:56:44.041: INFO: Found 2 stateful pods, waiting for 3
  May  4 06:56:54.048: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May  4 06:56:54.048: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May  4 06:56:54.048: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 05/04/23 06:56:54.055
  May  4 06:56:54.112: INFO: Updating stateful set ss2
  May  4 06:56:54.119: INFO: Waiting for Pod statefulset-4613/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  May  4 06:57:04.156: INFO: Updating stateful set ss2
  May  4 06:57:04.163: INFO: Waiting for StatefulSet statefulset-4613/ss2 to complete update
  May  4 06:57:04.163: INFO: Waiting for Pod statefulset-4613/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  May  4 06:57:14.171: INFO: Deleting all statefulset in ns statefulset-4613
  May  4 06:57:14.174: INFO: Scaling statefulset ss2 to 0
  May  4 06:57:24.201: INFO: Waiting for statefulset status.replicas updated to 0
  May  4 06:57:24.204: INFO: Deleting statefulset ss2
  May  4 06:57:24.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4613" for this suite. @ 05/04/23 06:57:24.235
• [70.819 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 05/04/23 06:57:24.253
  May  4 06:57:24.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename resourcequota @ 05/04/23 06:57:24.255
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:57:24.346
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:57:24.35
  STEP: Creating a ResourceQuota @ 05/04/23 06:57:24.353
  STEP: Getting a ResourceQuota @ 05/04/23 06:57:24.376
  STEP: Updating a ResourceQuota @ 05/04/23 06:57:24.405
  STEP: Verifying a ResourceQuota was modified @ 05/04/23 06:57:24.47
  STEP: Deleting a ResourceQuota @ 05/04/23 06:57:24.487
  STEP: Verifying the deleted ResourceQuota @ 05/04/23 06:57:24.512
  May  4 06:57:24.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5872" for this suite. @ 05/04/23 06:57:24.52
• [0.279 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 05/04/23 06:57:24.534
  May  4 06:57:24.534: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename namespaces @ 05/04/23 06:57:24.535
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:57:24.604
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:57:24.609
  STEP: Read namespace status @ 05/04/23 06:57:24.613
  May  4 06:57:24.616: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 05/04/23 06:57:24.616
  May  4 06:57:24.632: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 05/04/23 06:57:24.632
  May  4 06:57:24.664: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  May  4 06:57:24.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3630" for this suite. @ 05/04/23 06:57:24.67
• [0.151 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 05/04/23 06:57:24.686
  May  4 06:57:24.686: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename kubectl @ 05/04/23 06:57:24.688
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:57:24.76
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:57:24.764
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/04/23 06:57:24.768
  May  4 06:57:24.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-8772 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  May  4 06:57:24.902: INFO: stderr: ""
  May  4 06:57:24.902: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 05/04/23 06:57:24.902
  STEP: verifying the pod e2e-test-httpd-pod was created @ 05/04/23 06:57:29.953
  May  4 06:57:29.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-8772 get pod e2e-test-httpd-pod -o json'
  May  4 06:57:30.089: INFO: stderr: ""
  May  4 06:57:30.089: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"70a40339d5442c386b76e0271ab4204be0f3853d8ace7b5818f94feabb591bf5\",\n            \"cni.projectcalico.org/podIP\": \"172.16.169.166/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.16.169.166/32\"\n        },\n        \"creationTimestamp\": \"2023-05-04T06:57:24Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8772\",\n        \"resourceVersion\": \"67447\",\n        \"uid\": \"e26f60ef-4f42-4de5-9faa-f0648005471a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-mfxtd\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-node2\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-mfxtd\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-04T06:57:24Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-04T06:57:26Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-04T06:57:26Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-04T06:57:24Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://f0145a0f767113c9328334fc6dec324e62dfbc1e12f8f556c5495b4884baed94\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-04T06:57:26Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.0.157\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.169.166\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.16.169.166\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-04T06:57:24Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 05/04/23 06:57:30.089
  May  4 06:57:30.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-8772 replace -f -'
  May  4 06:57:31.402: INFO: stderr: ""
  May  4 06:57:31.402: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 05/04/23 06:57:31.402
  May  4 06:57:31.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-8772 delete pods e2e-test-httpd-pod'
  May  4 06:57:33.648: INFO: stderr: ""
  May  4 06:57:33.648: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May  4 06:57:33.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8772" for this suite. @ 05/04/23 06:57:33.653
• [8.982 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 05/04/23 06:57:33.675
  May  4 06:57:33.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/04/23 06:57:33.676
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:57:33.734
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:57:33.738
  STEP: create the container to handle the HTTPGet hook request. @ 05/04/23 06:57:33.748
  STEP: create the pod with lifecycle hook @ 05/04/23 06:57:35.79
  STEP: check poststart hook @ 05/04/23 06:57:37.816
  STEP: delete the pod with lifecycle hook @ 05/04/23 06:57:37.839
  May  4 06:57:41.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-9452" for this suite. @ 05/04/23 06:57:41.894
• [8.250 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 05/04/23 06:57:41.925
  May  4 06:57:41.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename resourcequota @ 05/04/23 06:57:41.926
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:57:41.975
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:57:41.981
  STEP: Counting existing ResourceQuota @ 05/04/23 06:57:41.986
  STEP: Creating a ResourceQuota @ 05/04/23 06:57:46.992
  STEP: Ensuring resource quota status is calculated @ 05/04/23 06:57:47.011
  STEP: Creating a ReplicationController @ 05/04/23 06:57:49.016
  STEP: Ensuring resource quota status captures replication controller creation @ 05/04/23 06:57:49.055
  STEP: Deleting a ReplicationController @ 05/04/23 06:57:51.06
  STEP: Ensuring resource quota status released usage @ 05/04/23 06:57:51.075
  May  4 06:57:53.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-690" for this suite. @ 05/04/23 06:57:53.085
• [11.186 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 05/04/23 06:57:53.113
  May  4 06:57:53.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename replication-controller @ 05/04/23 06:57:53.115
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:57:53.157
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:57:53.163
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 05/04/23 06:57:53.206
  STEP: When a replication controller with a matching selector is created @ 05/04/23 06:57:57.257
  STEP: Then the orphan pod is adopted @ 05/04/23 06:57:57.272
  May  4 06:57:58.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-8473" for this suite. @ 05/04/23 06:57:58.285
• [5.188 seconds]
------------------------------
SS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 05/04/23 06:57:58.301
  May  4 06:57:58.301: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename secrets @ 05/04/23 06:57:58.303
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:57:58.359
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:57:58.366
  STEP: Creating secret with name s-test-opt-del-1850940a-731f-49fa-91e1-721b1b597464 @ 05/04/23 06:57:58.399
  STEP: Creating secret with name s-test-opt-upd-edc79d84-0d15-4fa1-920a-dc6f9b8285b8 @ 05/04/23 06:57:58.414
  STEP: Creating the pod @ 05/04/23 06:57:58.429
  STEP: Deleting secret s-test-opt-del-1850940a-731f-49fa-91e1-721b1b597464 @ 05/04/23 06:58:02.515
  STEP: Updating secret s-test-opt-upd-edc79d84-0d15-4fa1-920a-dc6f9b8285b8 @ 05/04/23 06:58:02.531
  STEP: Creating secret with name s-test-opt-create-191f91c4-072e-47cb-8f1a-0a7563a89bbb @ 05/04/23 06:58:02.546
  STEP: waiting to observe update in volume @ 05/04/23 06:58:02.583
  May  4 06:59:27.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4392" for this suite. @ 05/04/23 06:59:27.039
• [88.753 seconds]
------------------------------
SS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 05/04/23 06:59:27.055
  May  4 06:59:27.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename security-context @ 05/04/23 06:59:27.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:59:27.098
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:59:27.104
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 05/04/23 06:59:27.108
  STEP: Saw pod success @ 05/04/23 06:59:31.173
  May  4 06:59:31.176: INFO: Trying to get logs from node k8s-node1 pod security-context-fbdcdcda-fd96-46c9-bd19-af8965fd22b7 container test-container: <nil>
  STEP: delete the pod @ 05/04/23 06:59:31.202
  May  4 06:59:31.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-6758" for this suite. @ 05/04/23 06:59:31.259
• [4.223 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 05/04/23 06:59:31.28
  May  4 06:59:31.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename cronjob @ 05/04/23 06:59:31.281
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 06:59:31.341
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 06:59:31.345
  STEP: Creating a ForbidConcurrent cronjob @ 05/04/23 06:59:31.355
  STEP: Ensuring a job is scheduled @ 05/04/23 06:59:31.387
  STEP: Ensuring exactly one is scheduled @ 05/04/23 07:00:01.392
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 05/04/23 07:00:01.397
  STEP: Ensuring no more jobs are scheduled @ 05/04/23 07:00:01.401
  STEP: Removing cronjob @ 05/04/23 07:05:01.408
  May  4 07:05:01.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-539" for this suite. @ 05/04/23 07:05:01.429
• [330.164 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 05/04/23 07:05:01.445
  May  4 07:05:01.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename downward-api @ 05/04/23 07:05:01.446
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:05:01.528
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:05:01.532
  STEP: Creating a pod to test downward api env vars @ 05/04/23 07:05:01.561
  STEP: Saw pod success @ 05/04/23 07:05:05.602
  May  4 07:05:05.605: INFO: Trying to get logs from node k8s-node2 pod downward-api-d159f6ce-fdbb-460f-8af7-3d963a30547e container dapi-container: <nil>
  STEP: delete the pod @ 05/04/23 07:05:05.628
  May  4 07:05:05.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1592" for this suite. @ 05/04/23 07:05:05.71
• [4.302 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 05/04/23 07:05:05.747
  May  4 07:05:05.747: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename watch @ 05/04/23 07:05:05.748
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:05:05.802
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:05:05.806
  STEP: creating a watch on configmaps with label A @ 05/04/23 07:05:05.809
  STEP: creating a watch on configmaps with label B @ 05/04/23 07:05:05.811
  STEP: creating a watch on configmaps with label A or B @ 05/04/23 07:05:05.812
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 05/04/23 07:05:05.814
  May  4 07:05:05.827: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3815  527fd4d8-70a0-4b15-866f-9ddba6593621 68493 0 2023-05-04 07:05:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 07:05:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May  4 07:05:05.828: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3815  527fd4d8-70a0-4b15-866f-9ddba6593621 68493 0 2023-05-04 07:05:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 07:05:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 05/04/23 07:05:05.828
  May  4 07:05:05.848: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3815  527fd4d8-70a0-4b15-866f-9ddba6593621 68494 0 2023-05-04 07:05:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 07:05:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  4 07:05:05.848: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3815  527fd4d8-70a0-4b15-866f-9ddba6593621 68494 0 2023-05-04 07:05:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 07:05:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 05/04/23 07:05:05.848
  May  4 07:05:05.884: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3815  527fd4d8-70a0-4b15-866f-9ddba6593621 68495 0 2023-05-04 07:05:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 07:05:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  4 07:05:05.884: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3815  527fd4d8-70a0-4b15-866f-9ddba6593621 68495 0 2023-05-04 07:05:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 07:05:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 05/04/23 07:05:05.884
  May  4 07:05:05.899: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3815  527fd4d8-70a0-4b15-866f-9ddba6593621 68496 0 2023-05-04 07:05:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 07:05:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  4 07:05:05.899: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3815  527fd4d8-70a0-4b15-866f-9ddba6593621 68496 0 2023-05-04 07:05:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-04 07:05:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 05/04/23 07:05:05.899
  May  4 07:05:05.913: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3815  e1a5c33d-0219-4b02-9d0d-5ae4c459a942 68497 0 2023-05-04 07:05:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-04 07:05:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May  4 07:05:05.913: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3815  e1a5c33d-0219-4b02-9d0d-5ae4c459a942 68497 0 2023-05-04 07:05:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-04 07:05:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 05/04/23 07:05:15.913
  May  4 07:05:15.955: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3815  e1a5c33d-0219-4b02-9d0d-5ae4c459a942 68532 0 2023-05-04 07:05:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-04 07:05:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May  4 07:05:15.956: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3815  e1a5c33d-0219-4b02-9d0d-5ae4c459a942 68532 0 2023-05-04 07:05:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-04 07:05:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May  4 07:05:25.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-3815" for this suite. @ 05/04/23 07:05:25.965
• [20.236 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 05/04/23 07:05:25.984
  May  4 07:05:25.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename namespaces @ 05/04/23 07:05:25.985
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:05:26.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:05:26.038
  STEP: Creating namespace "e2e-ns-vf74b" @ 05/04/23 07:05:26.046
  May  4 07:05:26.154: INFO: Namespace "e2e-ns-vf74b-6538" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-vf74b-6538" @ 05/04/23 07:05:26.154
  May  4 07:05:26.202: INFO: Namespace "e2e-ns-vf74b-6538" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-vf74b-6538" @ 05/04/23 07:05:26.202
  May  4 07:05:26.252: INFO: Namespace "e2e-ns-vf74b-6538" has []v1.FinalizerName{"kubernetes"}
  May  4 07:05:26.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5819" for this suite. @ 05/04/23 07:05:26.256
  STEP: Destroying namespace "e2e-ns-vf74b-6538" for this suite. @ 05/04/23 07:05:26.276
• [0.326 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 05/04/23 07:05:26.312
  May  4 07:05:26.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 07:05:26.313
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:05:26.386
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:05:26.39
  STEP: Creating projection with secret that has name projected-secret-test-eec7b63b-31bc-4bd7-90d1-8d34eef21e20 @ 05/04/23 07:05:26.394
  STEP: Creating a pod to test consume secrets @ 05/04/23 07:05:26.412
  STEP: Saw pod success @ 05/04/23 07:05:30.455
  May  4 07:05:30.458: INFO: Trying to get logs from node k8s-node2 pod pod-projected-secrets-41114112-1bd3-4bb3-8b7a-475dfab02132 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/04/23 07:05:30.464
  May  4 07:05:30.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6435" for this suite. @ 05/04/23 07:05:30.517
• [4.276 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 05/04/23 07:05:30.59
  May  4 07:05:30.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename podtemplate @ 05/04/23 07:05:30.591
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:05:30.633
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:05:30.639
  STEP: Create a pod template @ 05/04/23 07:05:30.642
  STEP: Replace a pod template @ 05/04/23 07:05:30.666
  May  4 07:05:30.682: INFO: Found updated podtemplate annotation: "true"

  May  4 07:05:30.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-4529" for this suite. @ 05/04/23 07:05:30.688
• [0.128 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 05/04/23 07:05:30.721
  May  4 07:05:30.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename resourcequota @ 05/04/23 07:05:30.723
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:05:30.752
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:05:30.757
  STEP: Counting existing ResourceQuota @ 05/04/23 07:05:30.776
  STEP: Creating a ResourceQuota @ 05/04/23 07:05:35.81
  STEP: Ensuring resource quota status is calculated @ 05/04/23 07:05:35.853
  STEP: Creating a Pod that fits quota @ 05/04/23 07:05:37.876
  STEP: Ensuring ResourceQuota status captures the pod usage @ 05/04/23 07:05:37.917
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 05/04/23 07:05:39.922
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 05/04/23 07:05:39.925
  STEP: Ensuring a pod cannot update its resource requirements @ 05/04/23 07:05:39.928
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 05/04/23 07:05:39.932
  STEP: Deleting the pod @ 05/04/23 07:05:41.936
  STEP: Ensuring resource quota status released the pod usage @ 05/04/23 07:05:42
  May  4 07:05:44.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2404" for this suite. @ 05/04/23 07:05:44.009
• [13.302 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 05/04/23 07:05:44.026
  May  4 07:05:44.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename pod-network-test @ 05/04/23 07:05:44.028
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:05:44.109
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:05:44.112
  STEP: Performing setup for networking test in namespace pod-network-test-4436 @ 05/04/23 07:05:44.116
  STEP: creating a selector @ 05/04/23 07:05:44.116
  STEP: Creating the service pods in kubernetes @ 05/04/23 07:05:44.116
  May  4 07:05:44.116: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 05/04/23 07:06:06.291
  May  4 07:06:08.315: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  May  4 07:06:08.315: INFO: Breadth first check of 172.16.36.66 on host 192.168.0.156...
  May  4 07:06:08.318: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.169.143:9080/dial?request=hostname&protocol=udp&host=172.16.36.66&port=8081&tries=1'] Namespace:pod-network-test-4436 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 07:06:08.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 07:06:08.319: INFO: ExecWithOptions: Clientset creation
  May  4 07:06:08.319: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4436/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.169.143%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.16.36.66%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May  4 07:06:08.414: INFO: Waiting for responses: map[]
  May  4 07:06:08.414: INFO: reached 172.16.36.66 after 0/1 tries
  May  4 07:06:08.414: INFO: Breadth first check of 172.16.169.139 on host 192.168.0.157...
  May  4 07:06:08.418: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.169.143:9080/dial?request=hostname&protocol=udp&host=172.16.169.139&port=8081&tries=1'] Namespace:pod-network-test-4436 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 07:06:08.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 07:06:08.419: INFO: ExecWithOptions: Clientset creation
  May  4 07:06:08.419: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4436/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.169.143%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.16.169.139%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May  4 07:06:08.505: INFO: Waiting for responses: map[]
  May  4 07:06:08.506: INFO: reached 172.16.169.139 after 0/1 tries
  May  4 07:06:08.506: INFO: Going to retry 0 out of 2 pods....
  May  4 07:06:08.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-4436" for this suite. @ 05/04/23 07:06:08.511
• [24.499 seconds]
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 05/04/23 07:06:08.526
  May  4 07:06:08.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename kubelet-test @ 05/04/23 07:06:08.528
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:06:08.599
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:06:08.603
  May  4 07:06:12.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-3056" for this suite. @ 05/04/23 07:06:12.638
• [4.126 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 05/04/23 07:06:12.655
  May  4 07:06:12.655: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename watch @ 05/04/23 07:06:12.657
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:06:12.696
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:06:12.699
  STEP: creating a new configmap @ 05/04/23 07:06:12.705
  STEP: modifying the configmap once @ 05/04/23 07:06:12.737
  STEP: modifying the configmap a second time @ 05/04/23 07:06:12.754
  STEP: deleting the configmap @ 05/04/23 07:06:12.783
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 05/04/23 07:06:12.799
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 05/04/23 07:06:12.801
  May  4 07:06:12.801: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8365  8bb6ee15-e671-43b4-b18a-e0434e1b6b54 68768 0 2023-05-04 07:06:12 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-04 07:06:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  4 07:06:12.801: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8365  8bb6ee15-e671-43b4-b18a-e0434e1b6b54 68769 0 2023-05-04 07:06:12 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-04 07:06:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  4 07:06:12.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-8365" for this suite. @ 05/04/23 07:06:12.807
• [0.211 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 05/04/23 07:06:12.868
  May  4 07:06:12.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename kubectl @ 05/04/23 07:06:12.87
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:06:12.909
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:06:12.915
  STEP: creating a replication controller @ 05/04/23 07:06:12.919
  May  4 07:06:12.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 create -f -'
  May  4 07:06:13.515: INFO: stderr: ""
  May  4 07:06:13.515: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/04/23 07:06:13.515
  May  4 07:06:13.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May  4 07:06:13.664: INFO: stderr: ""
  May  4 07:06:13.664: INFO: stdout: "update-demo-nautilus-4tfqj update-demo-nautilus-8vrvw "
  May  4 07:06:13.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 get pods update-demo-nautilus-4tfqj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  4 07:06:13.838: INFO: stderr: ""
  May  4 07:06:13.839: INFO: stdout: ""
  May  4 07:06:13.839: INFO: update-demo-nautilus-4tfqj is created but not running
  May  4 07:06:18.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May  4 07:06:18.956: INFO: stderr: ""
  May  4 07:06:18.956: INFO: stdout: "update-demo-nautilus-4tfqj update-demo-nautilus-8vrvw "
  May  4 07:06:18.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 get pods update-demo-nautilus-4tfqj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  4 07:06:19.071: INFO: stderr: ""
  May  4 07:06:19.071: INFO: stdout: "true"
  May  4 07:06:19.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 get pods update-demo-nautilus-4tfqj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May  4 07:06:19.197: INFO: stderr: ""
  May  4 07:06:19.197: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May  4 07:06:19.197: INFO: validating pod update-demo-nautilus-4tfqj
  May  4 07:06:19.204: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May  4 07:06:19.204: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May  4 07:06:19.204: INFO: update-demo-nautilus-4tfqj is verified up and running
  May  4 07:06:19.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 get pods update-demo-nautilus-8vrvw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  4 07:06:19.326: INFO: stderr: ""
  May  4 07:06:19.326: INFO: stdout: "true"
  May  4 07:06:19.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 get pods update-demo-nautilus-8vrvw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May  4 07:06:19.444: INFO: stderr: ""
  May  4 07:06:19.444: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May  4 07:06:19.444: INFO: validating pod update-demo-nautilus-8vrvw
  May  4 07:06:19.457: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May  4 07:06:19.457: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May  4 07:06:19.457: INFO: update-demo-nautilus-8vrvw is verified up and running
  STEP: scaling down the replication controller @ 05/04/23 07:06:19.457
  May  4 07:06:19.461: INFO: scanned /root for discovery docs: <nil>
  May  4 07:06:19.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  May  4 07:06:20.624: INFO: stderr: ""
  May  4 07:06:20.624: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/04/23 07:06:20.624
  May  4 07:06:20.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May  4 07:06:20.744: INFO: stderr: ""
  May  4 07:06:20.744: INFO: stdout: "update-demo-nautilus-4tfqj update-demo-nautilus-8vrvw "
  STEP: Replicas for name=update-demo: expected=1 actual=2 @ 05/04/23 07:06:20.744
  May  4 07:06:25.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May  4 07:06:25.870: INFO: stderr: ""
  May  4 07:06:25.870: INFO: stdout: "update-demo-nautilus-8vrvw "
  May  4 07:06:25.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 get pods update-demo-nautilus-8vrvw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  4 07:06:25.982: INFO: stderr: ""
  May  4 07:06:25.982: INFO: stdout: "true"
  May  4 07:06:25.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 get pods update-demo-nautilus-8vrvw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May  4 07:06:26.099: INFO: stderr: ""
  May  4 07:06:26.099: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May  4 07:06:26.099: INFO: validating pod update-demo-nautilus-8vrvw
  May  4 07:06:26.103: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May  4 07:06:26.103: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May  4 07:06:26.103: INFO: update-demo-nautilus-8vrvw is verified up and running
  STEP: scaling up the replication controller @ 05/04/23 07:06:26.103
  May  4 07:06:26.106: INFO: scanned /root for discovery docs: <nil>
  May  4 07:06:26.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  May  4 07:06:27.277: INFO: stderr: ""
  May  4 07:06:27.277: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/04/23 07:06:27.277
  May  4 07:06:27.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May  4 07:06:27.429: INFO: stderr: ""
  May  4 07:06:27.429: INFO: stdout: "update-demo-nautilus-8vrvw update-demo-nautilus-mrtmk "
  May  4 07:06:27.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 get pods update-demo-nautilus-8vrvw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  4 07:06:27.565: INFO: stderr: ""
  May  4 07:06:27.565: INFO: stdout: "true"
  May  4 07:06:27.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 get pods update-demo-nautilus-8vrvw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May  4 07:06:27.701: INFO: stderr: ""
  May  4 07:06:27.701: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May  4 07:06:27.701: INFO: validating pod update-demo-nautilus-8vrvw
  May  4 07:06:27.706: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May  4 07:06:27.706: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May  4 07:06:27.706: INFO: update-demo-nautilus-8vrvw is verified up and running
  May  4 07:06:27.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 get pods update-demo-nautilus-mrtmk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  4 07:06:27.836: INFO: stderr: ""
  May  4 07:06:27.836: INFO: stdout: ""
  May  4 07:06:27.836: INFO: update-demo-nautilus-mrtmk is created but not running
  May  4 07:06:32.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May  4 07:06:32.990: INFO: stderr: ""
  May  4 07:06:32.991: INFO: stdout: "update-demo-nautilus-8vrvw update-demo-nautilus-mrtmk "
  May  4 07:06:32.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 get pods update-demo-nautilus-8vrvw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  4 07:06:33.116: INFO: stderr: ""
  May  4 07:06:33.116: INFO: stdout: "true"
  May  4 07:06:33.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 get pods update-demo-nautilus-8vrvw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May  4 07:06:33.250: INFO: stderr: ""
  May  4 07:06:33.250: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May  4 07:06:33.250: INFO: validating pod update-demo-nautilus-8vrvw
  May  4 07:06:33.254: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May  4 07:06:33.254: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May  4 07:06:33.254: INFO: update-demo-nautilus-8vrvw is verified up and running
  May  4 07:06:33.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 get pods update-demo-nautilus-mrtmk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  4 07:06:33.404: INFO: stderr: ""
  May  4 07:06:33.404: INFO: stdout: "true"
  May  4 07:06:33.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 get pods update-demo-nautilus-mrtmk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May  4 07:06:33.567: INFO: stderr: ""
  May  4 07:06:33.567: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May  4 07:06:33.567: INFO: validating pod update-demo-nautilus-mrtmk
  May  4 07:06:33.573: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May  4 07:06:33.573: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May  4 07:06:33.573: INFO: update-demo-nautilus-mrtmk is verified up and running
  STEP: using delete to clean up resources @ 05/04/23 07:06:33.573
  May  4 07:06:33.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 delete --grace-period=0 --force -f -'
  May  4 07:06:33.693: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  4 07:06:33.693: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  May  4 07:06:33.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 get rc,svc -l name=update-demo --no-headers'
  May  4 07:06:33.855: INFO: stderr: "No resources found in kubectl-843 namespace.\n"
  May  4 07:06:33.855: INFO: stdout: ""
  May  4 07:06:33.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-843 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May  4 07:06:34.035: INFO: stderr: ""
  May  4 07:06:34.035: INFO: stdout: ""
  May  4 07:06:34.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-843" for this suite. @ 05/04/23 07:06:34.051
• [21.208 seconds]
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 05/04/23 07:06:34.077
  May  4 07:06:34.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/04/23 07:06:34.079
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:06:34.146
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:06:34.151
  STEP: fetching the /apis discovery document @ 05/04/23 07:06:34.174
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 05/04/23 07:06:34.176
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 05/04/23 07:06:34.176
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 05/04/23 07:06:34.176
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 05/04/23 07:06:34.178
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 05/04/23 07:06:34.178
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 05/04/23 07:06:34.18
  May  4 07:06:34.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-9312" for this suite. @ 05/04/23 07:06:34.186
• [0.126 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 05/04/23 07:06:34.204
  May  4 07:06:34.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename containers @ 05/04/23 07:06:34.205
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:06:34.256
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:06:34.262
  STEP: Creating a pod to test override arguments @ 05/04/23 07:06:34.265
  STEP: Saw pod success @ 05/04/23 07:06:40.361
  May  4 07:06:40.364: INFO: Trying to get logs from node k8s-node2 pod client-containers-4b239120-f156-4df8-9075-5f321dc9426b container agnhost-container: <nil>
  STEP: delete the pod @ 05/04/23 07:06:40.376
  May  4 07:06:40.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-1613" for this suite. @ 05/04/23 07:06:40.422
• [6.251 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 05/04/23 07:06:40.463
  May  4 07:06:40.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename cronjob @ 05/04/23 07:06:40.464
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:06:40.508
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:06:40.515
  STEP: Creating a cronjob @ 05/04/23 07:06:40.519
  STEP: Ensuring more than one job is running at a time @ 05/04/23 07:06:40.534
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 05/04/23 07:08:00.539
  STEP: Removing cronjob @ 05/04/23 07:08:00.543
  May  4 07:08:00.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-101" for this suite. @ 05/04/23 07:08:00.562
• [80.121 seconds]
------------------------------
S
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 05/04/23 07:08:00.584
  May  4 07:08:00.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename security-context-test @ 05/04/23 07:08:00.585
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:08:00.748
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:08:00.753
  May  4 07:08:04.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-4165" for this suite. @ 05/04/23 07:08:04.877
• [4.308 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 05/04/23 07:08:04.894
  May  4 07:08:04.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename init-container @ 05/04/23 07:08:04.895
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:08:04.98
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:08:04.985
  STEP: creating the pod @ 05/04/23 07:08:04.99
  May  4 07:08:04.990: INFO: PodSpec: initContainers in spec.initContainers
  May  4 07:08:53.336: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-4f5c35f1-c31b-4312-b9ab-3b9d20727b5a", GenerateName:"", Namespace:"init-container-358", SelfLink:"", UID:"3ba8a04d-cffe-463a-a339-f2afd3fe78c3", ResourceVersion:"69356", Generation:0, CreationTimestamp:time.Date(2023, time.May, 4, 7, 8, 4, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"990080125"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"cdd4f5ab582c6de8f78c8de240bc490e3aa02bf2ef4ac471234547d863fddf65", "cni.projectcalico.org/podIP":"172.16.169.168/32", "cni.projectcalico.org/podIPs":"172.16.169.168/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 4, 7, 8, 4, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000d28690), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 4, 7, 8, 5, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000d28720), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 4, 7, 8, 53, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000d28768), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-gqw6x", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000698500), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-gqw6x", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-gqw6x", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-gqw6x", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004f326c0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-node2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0033d25b0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004f32740)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004f32760)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004f32768), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004f3276c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000972210), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 4, 7, 8, 5, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 4, 7, 8, 5, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 4, 7, 8, 5, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 4, 7, 8, 5, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.0.157", PodIP:"172.16.169.168", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.16.169.168"}}, StartTime:time.Date(2023, time.May, 4, 7, 8, 5, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0033d2690)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0033d2700)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"sha256:d59c675982d8692814ec9e1486d4c645cd86ad825ef33975a5db196cf2801592", ContainerID:"containerd://e3960ac8188d88f135cff34e5624cee1cbc87fe3d65329b67b6f2cfdca55d41e", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0006986e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000698620), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004f327f4), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  May  4 07:08:53.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-358" for this suite. @ 05/04/23 07:08:53.361
• [48.517 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 05/04/23 07:08:53.416
  May  4 07:08:53.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename containers @ 05/04/23 07:08:53.418
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:08:53.467
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:08:53.472
  May  4 07:08:55.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-1485" for this suite. @ 05/04/23 07:08:55.543
• [2.142 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 05/04/23 07:08:55.559
  May  4 07:08:55.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename deployment @ 05/04/23 07:08:55.56
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:08:55.616
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:08:55.622
  May  4 07:08:55.625: INFO: Creating deployment "webserver-deployment"
  May  4 07:08:55.642: INFO: Waiting for observed generation 1
  May  4 07:08:57.680: INFO: Waiting for all required pods to come up
  May  4 07:08:57.730: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 05/04/23 07:08:57.73
  May  4 07:09:02.102: INFO: Waiting for deployment "webserver-deployment" to complete
  May  4 07:09:02.151: INFO: Updating deployment "webserver-deployment" with a non-existent image
  May  4 07:09:02.264: INFO: Updating deployment webserver-deployment
  May  4 07:09:02.265: INFO: Waiting for observed generation 2
  May  4 07:09:04.304: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  May  4 07:09:04.330: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  May  4 07:09:04.334: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  May  4 07:09:04.346: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  May  4 07:09:04.346: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  May  4 07:09:04.352: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  May  4 07:09:04.381: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  May  4 07:09:04.381: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  May  4 07:09:04.434: INFO: Updating deployment webserver-deployment
  May  4 07:09:04.434: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  May  4 07:09:04.498: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  May  4 07:09:06.597: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  May  4 07:09:06.605: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-3383  73b3642f-5a9d-487f-9f6a-b3552ed659fa 69729 3 2023-05-04 07:08:55 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 07:09:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ba12b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-04 07:09:04 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-05-04 07:09:04 +0000 UTC,LastTransitionTime:2023-05-04 07:08:55 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  May  4 07:09:06.610: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-3383  a0951faf-6cb4-4603-91f7-8e09912e2ce5 69724 3 2023-05-04 07:09:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 73b3642f-5a9d-487f-9f6a-b3552ed659fa 0xc0054cc8c7 0xc0054cc8c8}] [] [{kube-controller-manager Update apps/v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"73b3642f-5a9d-487f-9f6a-b3552ed659fa\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0054cc968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May  4 07:09:06.610: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  May  4 07:09:06.610: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-3383  4f9cd8e8-2860-4a66-b630-eeb8c5e3d299 69727 3 2023-05-04 07:08:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 73b3642f-5a9d-487f-9f6a-b3552ed659fa 0xc0054cc7d7 0xc0054cc7d8}] [] [{kube-controller-manager Update apps/v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"73b3642f-5a9d-487f-9f6a-b3552ed659fa\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0054cc868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  May  4 07:09:06.623: INFO: Pod "webserver-deployment-67bd4bf6dc-2gqt9" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-2gqt9 webserver-deployment-67bd4bf6dc- deployment-3383  c2f06233-7369-4237-8b49-70bc435d5633 69541 0 2023-05-04 07:08:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:b83f0c05af48ff9eea2722dc0f40302bf2a1776bb3bd967001d46dc201bc4f98 cni.projectcalico.org/podIP:172.16.36.94/32 cni.projectcalico.org/podIPs:172.16.36.94/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4f9cd8e8-2860-4a66-b630-eeb8c5e3d299 0xc005e2d517 0xc005e2d518}] [] [{kube-controller-manager Update v1 2023-05-04 07:08:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f9cd8e8-2860-4a66-b630-eeb8c5e3d299\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 07:08:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 07:09:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.36.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p79gs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p79gs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.156,PodIP:172.16.36.94,StartTime:2023-05-04 07:08:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 07:09:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://e4a6fcd144aae5a2002a3732e0822c604023898cd3e2b886cd91644f28fbb60f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.36.94,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.623: INFO: Pod "webserver-deployment-67bd4bf6dc-6zrck" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6zrck webserver-deployment-67bd4bf6dc- deployment-3383  10fa4f88-a033-4a3d-93a3-f0131707d7cb 69742 0 2023-05-04 07:09:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4f9cd8e8-2860-4a66-b630-eeb8c5e3d299 0xc005e2d747 0xc005e2d748}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f9cd8e8-2860-4a66-b630-eeb8c5e3d299\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-clhc5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-clhc5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.156,PodIP:,StartTime:2023-05-04 07:09:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.624: INFO: Pod "webserver-deployment-67bd4bf6dc-8pnwq" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-8pnwq webserver-deployment-67bd4bf6dc- deployment-3383  f6ffd57a-e045-4120-8950-73195f3d5d1a 69517 0 2023-05-04 07:08:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:101ad001f5d2ce614232de4a5287981aa726c25497418bae35a9cc61c8fab4df cni.projectcalico.org/podIP:172.16.36.109/32 cni.projectcalico.org/podIPs:172.16.36.109/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4f9cd8e8-2860-4a66-b630-eeb8c5e3d299 0xc005e2d947 0xc005e2d948}] [] [{kube-controller-manager Update v1 2023-05-04 07:08:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f9cd8e8-2860-4a66-b630-eeb8c5e3d299\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 07:08:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 07:09:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.36.109\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mhbpn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mhbpn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.156,PodIP:172.16.36.109,StartTime:2023-05-04 07:08:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 07:08:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://535a2d00a99bcbccf9ce5f9540516aaae970ed34b41b016e4d515138ff46659b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.36.109,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.624: INFO: Pod "webserver-deployment-67bd4bf6dc-96ssb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-96ssb webserver-deployment-67bd4bf6dc- deployment-3383  07aed11f-0410-4747-acbd-6b0497fdc1f9 69710 0 2023-05-04 07:09:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4f9cd8e8-2860-4a66-b630-eeb8c5e3d299 0xc005e2db97 0xc005e2db98}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f9cd8e8-2860-4a66-b630-eeb8c5e3d299\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-464lr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-464lr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.157,PodIP:,StartTime:2023-05-04 07:09:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.625: INFO: Pod "webserver-deployment-67bd4bf6dc-9m7lx" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-9m7lx webserver-deployment-67bd4bf6dc- deployment-3383  6241d1a4-a7e8-49f5-b84d-74fcd1e98685 69548 0 2023-05-04 07:08:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:cfeeb30cff8ca604eb3233cba1cc0be614f4acad24dab41843f136cb843a4c15 cni.projectcalico.org/podIP:172.16.36.95/32 cni.projectcalico.org/podIPs:172.16.36.95/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4f9cd8e8-2860-4a66-b630-eeb8c5e3d299 0xc005e2dda7 0xc005e2dda8}] [] [{kube-controller-manager Update v1 2023-05-04 07:08:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f9cd8e8-2860-4a66-b630-eeb8c5e3d299\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 07:09:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 07:09:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.36.95\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-shhn4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-shhn4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.156,PodIP:172.16.36.95,StartTime:2023-05-04 07:08:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 07:09:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://b549cad183a734d95a132c7463ccf1ea503a455a7e5abc088b3ccb94edf7fcc3,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.36.95,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.625: INFO: Pod "webserver-deployment-67bd4bf6dc-9sbt5" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-9sbt5 webserver-deployment-67bd4bf6dc- deployment-3383  1d55fed7-840a-4321-aa5c-f6d9379f8ea2 69753 0 2023-05-04 07:09:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4f9cd8e8-2860-4a66-b630-eeb8c5e3d299 0xc005e2dfc7 0xc005e2dfc8}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f9cd8e8-2860-4a66-b630-eeb8c5e3d299\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bm44f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bm44f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.157,PodIP:,StartTime:2023-05-04 07:09:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.625: INFO: Pod "webserver-deployment-67bd4bf6dc-bgpkz" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-bgpkz webserver-deployment-67bd4bf6dc- deployment-3383  ff4a9d63-a513-4983-974d-08059a9604af 69500 0 2023-05-04 07:08:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:f01dd593c603f279f43e067cdd31fe96c62ac8baec7fa78e98c07d932cd75397 cni.projectcalico.org/podIP:172.16.169.163/32 cni.projectcalico.org/podIPs:172.16.169.163/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4f9cd8e8-2860-4a66-b630-eeb8c5e3d299 0xc00510e197 0xc00510e198}] [] [{kube-controller-manager Update v1 2023-05-04 07:08:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f9cd8e8-2860-4a66-b630-eeb8c5e3d299\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 07:08:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 07:08:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.169.163\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7xh5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7xh5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.157,PodIP:172.16.169.163,StartTime:2023-05-04 07:08:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 07:08:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://5918ba5dbec111b1733a7024ec89bd01e5f2bc13a18f81f23beef19dbca694af,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.169.163,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.626: INFO: Pod "webserver-deployment-67bd4bf6dc-c5xs8" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-c5xs8 webserver-deployment-67bd4bf6dc- deployment-3383  90420bda-f92f-4cdf-a7ff-7e2b657d3a6a 69522 0 2023-05-04 07:08:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:6091e2c3408ce38306f98040afe847b9ae5971d591ebe1c3fa842d70061f9102 cni.projectcalico.org/podIP:172.16.169.186/32 cni.projectcalico.org/podIPs:172.16.169.186/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4f9cd8e8-2860-4a66-b630-eeb8c5e3d299 0xc00510e3a7 0xc00510e3a8}] [] [{kube-controller-manager Update v1 2023-05-04 07:08:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f9cd8e8-2860-4a66-b630-eeb8c5e3d299\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 07:08:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 07:09:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.169.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2ttkr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2ttkr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.157,PodIP:172.16.169.186,StartTime:2023-05-04 07:08:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 07:09:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://17aa8eaa64f71805b4e5c1076ceacf5852054326a870db867434d12410f54508,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.169.186,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.626: INFO: Pod "webserver-deployment-67bd4bf6dc-cjhcv" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-cjhcv webserver-deployment-67bd4bf6dc- deployment-3383  8b4b4995-3129-40f0-9c47-ed895f0120b2 69763 0 2023-05-04 07:09:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4f9cd8e8-2860-4a66-b630-eeb8c5e3d299 0xc00510e5b7 0xc00510e5b8}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f9cd8e8-2860-4a66-b630-eeb8c5e3d299\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zpllz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zpllz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.157,PodIP:,StartTime:2023-05-04 07:09:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.627: INFO: Pod "webserver-deployment-67bd4bf6dc-frv8q" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-frv8q webserver-deployment-67bd4bf6dc- deployment-3383  c214aa27-7298-4c22-baad-80b53b6915d0 69492 0 2023-05-04 07:08:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:4821b02043c5e056d73691a4c3eee18cafbcbc7c1938e526a7b0b2048a27ad9e cni.projectcalico.org/podIP:172.16.36.89/32 cni.projectcalico.org/podIPs:172.16.36.89/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4f9cd8e8-2860-4a66-b630-eeb8c5e3d299 0xc00510e7c7 0xc00510e7c8}] [] [{kube-controller-manager Update v1 2023-05-04 07:08:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f9cd8e8-2860-4a66-b630-eeb8c5e3d299\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 07:08:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 07:08:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.36.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-742gz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-742gz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.156,PodIP:172.16.36.89,StartTime:2023-05-04 07:08:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 07:08:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://91d6a64d2a270a67e7b7d502002f06873660f251c366e76406ef6e26a3b584fb,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.36.89,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.627: INFO: Pod "webserver-deployment-67bd4bf6dc-j544n" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-j544n webserver-deployment-67bd4bf6dc- deployment-3383  1dc5da27-3a0c-4a5d-bb70-6ddd8db83db5 69738 0 2023-05-04 07:09:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4f9cd8e8-2860-4a66-b630-eeb8c5e3d299 0xc00510e9d7 0xc00510e9d8}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f9cd8e8-2860-4a66-b630-eeb8c5e3d299\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-79xfv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-79xfv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.156,PodIP:,StartTime:2023-05-04 07:09:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.627: INFO: Pod "webserver-deployment-67bd4bf6dc-l84b7" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-l84b7 webserver-deployment-67bd4bf6dc- deployment-3383  3a86bbeb-11a3-4933-94ba-be5041bd7633 69760 0 2023-05-04 07:09:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4f9cd8e8-2860-4a66-b630-eeb8c5e3d299 0xc00510eba7 0xc00510eba8}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f9cd8e8-2860-4a66-b630-eeb8c5e3d299\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sq5hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sq5hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.157,PodIP:,StartTime:2023-05-04 07:09:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.628: INFO: Pod "webserver-deployment-67bd4bf6dc-ljcqk" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-ljcqk webserver-deployment-67bd4bf6dc- deployment-3383  bba69da8-56f8-4096-9c17-ac9c4284c062 69747 0 2023-05-04 07:09:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4f9cd8e8-2860-4a66-b630-eeb8c5e3d299 0xc00510ed77 0xc00510ed78}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f9cd8e8-2860-4a66-b630-eeb8c5e3d299\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-29c4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-29c4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.157,PodIP:,StartTime:2023-05-04 07:09:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.628: INFO: Pod "webserver-deployment-67bd4bf6dc-mvqqs" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-mvqqs webserver-deployment-67bd4bf6dc- deployment-3383  7a4b4cb8-468e-4732-981e-7d9984c88d38 69490 0 2023-05-04 07:08:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:80eae3bb837090e1c6900a55ce669b58f6dc337f50ca9686cc6129dd40e650bd cni.projectcalico.org/podIP:172.16.169.161/32 cni.projectcalico.org/podIPs:172.16.169.161/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4f9cd8e8-2860-4a66-b630-eeb8c5e3d299 0xc00510ef47 0xc00510ef48}] [] [{kube-controller-manager Update v1 2023-05-04 07:08:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f9cd8e8-2860-4a66-b630-eeb8c5e3d299\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 07:08:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 07:08:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.169.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7t9g7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7t9g7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.157,PodIP:172.16.169.161,StartTime:2023-05-04 07:08:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 07:08:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://3548a9d7c54da467d525d18a3ec89d5cdb7e47a390060bb06c627324ad5d8552,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.169.161,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.629: INFO: Pod "webserver-deployment-67bd4bf6dc-rqgf9" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rqgf9 webserver-deployment-67bd4bf6dc- deployment-3383  e90305d2-28d5-4f78-bf45-a400e6ab925a 69731 0 2023-05-04 07:09:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4f9cd8e8-2860-4a66-b630-eeb8c5e3d299 0xc00510f167 0xc00510f168}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f9cd8e8-2860-4a66-b630-eeb8c5e3d299\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-68nnm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-68nnm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.156,PodIP:,StartTime:2023-05-04 07:09:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.629: INFO: Pod "webserver-deployment-67bd4bf6dc-rqq9j" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rqq9j webserver-deployment-67bd4bf6dc- deployment-3383  0b0a19f4-8a0b-4e25-b77f-3678d1ea36af 69750 0 2023-05-04 07:09:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4f9cd8e8-2860-4a66-b630-eeb8c5e3d299 0xc00510f337 0xc00510f338}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f9cd8e8-2860-4a66-b630-eeb8c5e3d299\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-clgk4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-clgk4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.156,PodIP:,StartTime:2023-05-04 07:09:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.629: INFO: Pod "webserver-deployment-67bd4bf6dc-twh2k" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-twh2k webserver-deployment-67bd4bf6dc- deployment-3383  943605f7-1e54-466c-9c91-a6a2ddd5847e 69728 0 2023-05-04 07:09:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4f9cd8e8-2860-4a66-b630-eeb8c5e3d299 0xc00510f507 0xc00510f508}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f9cd8e8-2860-4a66-b630-eeb8c5e3d299\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ggz94,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ggz94,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.157,PodIP:,StartTime:2023-05-04 07:09:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.630: INFO: Pod "webserver-deployment-67bd4bf6dc-vkcrh" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vkcrh webserver-deployment-67bd4bf6dc- deployment-3383  71627646-4275-4333-b37b-ac568b78bd96 69732 0 2023-05-04 07:09:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4f9cd8e8-2860-4a66-b630-eeb8c5e3d299 0xc00510f6d7 0xc00510f6d8}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f9cd8e8-2860-4a66-b630-eeb8c5e3d299\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qdnth,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qdnth,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.157,PodIP:,StartTime:2023-05-04 07:09:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.630: INFO: Pod "webserver-deployment-67bd4bf6dc-x4r2b" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-x4r2b webserver-deployment-67bd4bf6dc- deployment-3383  d0119534-8529-4b3c-9bd0-57e90c2e61c7 69501 0 2023-05-04 07:08:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:1723f3fe3d62ce6926f6abd87ae60e5ecf45edf36d9153182e999bc8d3ab05df cni.projectcalico.org/podIP:172.16.36.90/32 cni.projectcalico.org/podIPs:172.16.36.90/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4f9cd8e8-2860-4a66-b630-eeb8c5e3d299 0xc00510f8c7 0xc00510f8c8}] [] [{kube-controller-manager Update v1 2023-05-04 07:08:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f9cd8e8-2860-4a66-b630-eeb8c5e3d299\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 07:08:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 07:08:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.36.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sdv5l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sdv5l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:08:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.156,PodIP:172.16.36.90,StartTime:2023-05-04 07:08:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 07:08:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089,ContainerID:containerd://b65e6efd6ccd0ad50073c0a419c16f72f20b8f07a1a4608c33719fc71165651d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.36.90,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.631: INFO: Pod "webserver-deployment-67bd4bf6dc-xv2t4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xv2t4 webserver-deployment-67bd4bf6dc- deployment-3383  99315936-2d64-467e-bda0-81514f249400 69734 0 2023-05-04 07:09:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 4f9cd8e8-2860-4a66-b630-eeb8c5e3d299 0xc00510fae7 0xc00510fae8}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f9cd8e8-2860-4a66-b630-eeb8c5e3d299\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vwq98,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vwq98,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.156,PodIP:,StartTime:2023-05-04 07:09:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.631: INFO: Pod "webserver-deployment-7b75d79cf5-56s4x" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-56s4x webserver-deployment-7b75d79cf5- deployment-3383  3e60fc0f-c7e5-48a2-b7b6-8f613c816709 69766 0 2023-05-04 07:09:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a0951faf-6cb4-4603-91f7-8e09912e2ce5 0xc00510fcb7 0xc00510fcb8}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0951faf-6cb4-4603-91f7-8e09912e2ce5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5dxmf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5dxmf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.157,PodIP:,StartTime:2023-05-04 07:09:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.631: INFO: Pod "webserver-deployment-7b75d79cf5-6mqlh" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-6mqlh webserver-deployment-7b75d79cf5- deployment-3383  efd77e46-320e-4b04-a4e9-615bc4fc9bd4 69755 0 2023-05-04 07:09:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a0951faf-6cb4-4603-91f7-8e09912e2ce5 0xc00510fea7 0xc00510fea8}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0951faf-6cb4-4603-91f7-8e09912e2ce5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dcwfg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dcwfg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.156,PodIP:,StartTime:2023-05-04 07:09:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.631: INFO: Pod "webserver-deployment-7b75d79cf5-cfjms" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-cfjms webserver-deployment-7b75d79cf5- deployment-3383  a7a8f19c-2ceb-4687-94fe-352aef0010c8 69737 0 2023-05-04 07:09:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a0951faf-6cb4-4603-91f7-8e09912e2ce5 0xc005b2e077 0xc005b2e078}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0951faf-6cb4-4603-91f7-8e09912e2ce5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kcggj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kcggj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.157,PodIP:,StartTime:2023-05-04 07:09:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.632: INFO: Pod "webserver-deployment-7b75d79cf5-lphgn" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-lphgn webserver-deployment-7b75d79cf5- deployment-3383  7a2fa9ce-2a5f-47f7-8510-e1343deec792 69637 0 2023-05-04 07:09:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:dbaa0067e53da23f2ae820dc868bedc664d13d79124449b094e7bb91193855bd cni.projectcalico.org/podIP:172.16.36.103/32 cni.projectcalico.org/podIPs:172.16.36.103/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a0951faf-6cb4-4603-91f7-8e09912e2ce5 0xc005b2e287 0xc005b2e288}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0951faf-6cb4-4603-91f7-8e09912e2ce5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xqsdw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xqsdw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.156,PodIP:,StartTime:2023-05-04 07:09:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.632: INFO: Pod "webserver-deployment-7b75d79cf5-mxcws" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-mxcws webserver-deployment-7b75d79cf5- deployment-3383  87feb4a8-0145-4c12-a48a-fa28ca518f5e 69743 0 2023-05-04 07:09:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a0951faf-6cb4-4603-91f7-8e09912e2ce5 0xc005b2e4a7 0xc005b2e4a8}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0951faf-6cb4-4603-91f7-8e09912e2ce5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jldx4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jldx4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.157,PodIP:,StartTime:2023-05-04 07:09:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.632: INFO: Pod "webserver-deployment-7b75d79cf5-pgmk4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-pgmk4 webserver-deployment-7b75d79cf5- deployment-3383  7ae737b0-db76-4ecc-9a05-e6dc9a523b49 69689 0 2023-05-04 07:09:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a0951faf-6cb4-4603-91f7-8e09912e2ce5 0xc005b2e6a7 0xc005b2e6a8}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0951faf-6cb4-4603-91f7-8e09912e2ce5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2sbcf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2sbcf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.156,PodIP:,StartTime:2023-05-04 07:09:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.633: INFO: Pod "webserver-deployment-7b75d79cf5-qc85f" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-qc85f webserver-deployment-7b75d79cf5- deployment-3383  17d43fa8-93a5-43ce-b3c0-b5eed7fddb19 69644 0 2023-05-04 07:09:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:2b4eba1c123ec30f4111eb4309261caec186273454c7721adc5cee4a3a4af94d cni.projectcalico.org/podIP:172.16.36.101/32 cni.projectcalico.org/podIPs:172.16.36.101/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a0951faf-6cb4-4603-91f7-8e09912e2ce5 0xc005b2e8b7 0xc005b2e8b8}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0951faf-6cb4-4603-91f7-8e09912e2ce5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rlggk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rlggk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.156,PodIP:,StartTime:2023-05-04 07:09:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.633: INFO: Pod "webserver-deployment-7b75d79cf5-qdqbp" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-qdqbp webserver-deployment-7b75d79cf5- deployment-3383  b9063a6a-729e-441f-b391-53d3fb6f2d0f 69758 0 2023-05-04 07:09:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a0951faf-6cb4-4603-91f7-8e09912e2ce5 0xc005b2eac7 0xc005b2eac8}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0951faf-6cb4-4603-91f7-8e09912e2ce5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hrtzl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hrtzl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.157,PodIP:,StartTime:2023-05-04 07:09:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.633: INFO: Pod "webserver-deployment-7b75d79cf5-qfxtj" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-qfxtj webserver-deployment-7b75d79cf5- deployment-3383  2fce4cbf-c94a-41d3-bc65-25333c198a48 69715 0 2023-05-04 07:09:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a0951faf-6cb4-4603-91f7-8e09912e2ce5 0xc005b2ecb7 0xc005b2ecb8}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0951faf-6cb4-4603-91f7-8e09912e2ce5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-plf8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-plf8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.156,PodIP:,StartTime:2023-05-04 07:09:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.633: INFO: Pod "webserver-deployment-7b75d79cf5-qnv99" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-qnv99 webserver-deployment-7b75d79cf5- deployment-3383  d030e016-f31b-4150-b43a-cbb0ffe70656 69749 0 2023-05-04 07:09:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:50f2146f4f6a20367e7eca17673aa8ca70ad93a39d9c0691faf4cd9133e0b939 cni.projectcalico.org/podIP:172.16.169.130/32 cni.projectcalico.org/podIPs:172.16.169.130/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a0951faf-6cb4-4603-91f7-8e09912e2ce5 0xc005b2eeb7 0xc005b2eeb8}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0951faf-6cb4-4603-91f7-8e09912e2ce5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-04 07:09:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4tvdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4tvdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.157,PodIP:,StartTime:2023-05-04 07:09:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.634: INFO: Pod "webserver-deployment-7b75d79cf5-s2kzn" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-s2kzn webserver-deployment-7b75d79cf5- deployment-3383  891180ee-4ea0-4962-84f8-c4d7ce2aeb7d 69746 0 2023-05-04 07:09:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a0951faf-6cb4-4603-91f7-8e09912e2ce5 0xc005b2f0c7 0xc005b2f0c8}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0951faf-6cb4-4603-91f7-8e09912e2ce5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4mfvw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4mfvw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.156,PodIP:,StartTime:2023-05-04 07:09:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.634: INFO: Pod "webserver-deployment-7b75d79cf5-v2ppk" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-v2ppk webserver-deployment-7b75d79cf5- deployment-3383  2d55c8de-b216-4484-97d6-714a899c4538 69635 0 2023-05-04 07:09:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:45d52575a1b6d5f0c35a53f9c100e4edbd9a1e3e884dafcf1445d49a4d62cd2a cni.projectcalico.org/podIP:172.16.169.159/32 cni.projectcalico.org/podIPs:172.16.169.159/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a0951faf-6cb4-4603-91f7-8e09912e2ce5 0xc005b2f2b7 0xc005b2f2b8}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0951faf-6cb4-4603-91f7-8e09912e2ce5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tpw7x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tpw7x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.157,PodIP:,StartTime:2023-05-04 07:09:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.634: INFO: Pod "webserver-deployment-7b75d79cf5-zs8hs" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-zs8hs webserver-deployment-7b75d79cf5- deployment-3383  eb2703b1-cc3c-4a16-b691-dcffc51a639c 69708 0 2023-05-04 07:09:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:8b5e54f34631f6c0cf7adbc6ac5fff17ebaf49119cc81fa5f017f0281ab5d727 cni.projectcalico.org/podIP:172.16.169.129/32 cni.projectcalico.org/podIPs:172.16.169.129/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a0951faf-6cb4-4603-91f7-8e09912e2ce5 0xc005b2f4d7 0xc005b2f4d8}] [] [{kube-controller-manager Update v1 2023-05-04 07:09:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0951faf-6cb4-4603-91f7-8e09912e2ce5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-04 07:09:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-04 07:09:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tnvxg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tnvxg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:09:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.157,PodIP:,StartTime:2023-05-04 07:09:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:09:06.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3383" for this suite. @ 05/04/23 07:09:06.675
• [11.180 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 05/04/23 07:09:06.74
  May  4 07:09:06.740: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename crd-webhook @ 05/04/23 07:09:06.741
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:09:06.825
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:09:06.829
  STEP: Setting up server cert @ 05/04/23 07:09:06.833
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 05/04/23 07:09:09.618
  STEP: Deploying the custom resource conversion webhook pod @ 05/04/23 07:09:09.782
  STEP: Wait for the deployment to be ready @ 05/04/23 07:09:09.925
  May  4 07:09:10.103: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  May  4 07:09:12.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 7, 9, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 9, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 9, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 9, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  4 07:09:14.194: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 7, 9, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 9, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 9, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 9, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 05/04/23 07:09:16.35
  STEP: Verifying the service has paired with the endpoint @ 05/04/23 07:09:16.788
  May  4 07:09:17.790: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  May  4 07:09:17.875: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Creating a v1 custom resource @ 05/04/23 07:09:20.651
  STEP: v2 custom resource should be converted @ 05/04/23 07:09:20.748
  May  4 07:09:20.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-7477" for this suite. @ 05/04/23 07:09:21.591
• [14.921 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 05/04/23 07:09:21.664
  May  4 07:09:21.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename kubectl @ 05/04/23 07:09:21.666
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:09:21.826
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:09:21.831
  STEP: validating api versions @ 05/04/23 07:09:21.835
  May  4 07:09:21.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-4070 api-versions'
  May  4 07:09:22.040: INFO: stderr: ""
  May  4 07:09:22.040: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  May  4 07:09:22.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4070" for this suite. @ 05/04/23 07:09:22.084
• [0.437 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 05/04/23 07:09:22.102
  May  4 07:09:22.102: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename job @ 05/04/23 07:09:22.104
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:09:22.152
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:09:22.156
  STEP: Creating a job @ 05/04/23 07:09:22.16
  STEP: Ensuring active pods == parallelism @ 05/04/23 07:09:22.212
  STEP: delete a job @ 05/04/23 07:09:26.218
  STEP: deleting Job.batch foo in namespace job-6847, will wait for the garbage collector to delete the pods @ 05/04/23 07:09:26.218
  May  4 07:09:26.285: INFO: Deleting Job.batch foo took: 14.753031ms
  May  4 07:09:26.487: INFO: Terminating Job.batch foo pods took: 201.148248ms
  STEP: Ensuring job was deleted @ 05/04/23 07:09:58.488
  May  4 07:09:58.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6847" for this suite. @ 05/04/23 07:09:58.496
• [36.410 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 05/04/23 07:09:58.513
  May  4 07:09:58.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 07:09:58.515
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:09:58.625
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:09:58.63
  STEP: Creating a pod to test downward API volume plugin @ 05/04/23 07:09:58.634
  STEP: Saw pod success @ 05/04/23 07:10:02.687
  May  4 07:10:02.690: INFO: Trying to get logs from node k8s-node2 pod downwardapi-volume-a596c7d6-967c-4d74-b5ff-da901e8d89fd container client-container: <nil>
  STEP: delete the pod @ 05/04/23 07:10:02.696
  May  4 07:10:02.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2044" for this suite. @ 05/04/23 07:10:02.766
• [4.280 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 05/04/23 07:10:02.793
  May  4 07:10:02.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename resourcequota @ 05/04/23 07:10:02.795
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:10:02.839
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:10:02.843
  STEP: Creating a ResourceQuota with best effort scope @ 05/04/23 07:10:02.847
  STEP: Ensuring ResourceQuota status is calculated @ 05/04/23 07:10:02.885
  STEP: Creating a ResourceQuota with not best effort scope @ 05/04/23 07:10:04.89
  STEP: Ensuring ResourceQuota status is calculated @ 05/04/23 07:10:04.907
  STEP: Creating a best-effort pod @ 05/04/23 07:10:06.91
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 05/04/23 07:10:06.95
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 05/04/23 07:10:08.956
  STEP: Deleting the pod @ 05/04/23 07:10:10.96
  STEP: Ensuring resource quota status released the pod usage @ 05/04/23 07:10:11.025
  STEP: Creating a not best-effort pod @ 05/04/23 07:10:13.029
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 05/04/23 07:10:13.062
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 05/04/23 07:10:15.067
  STEP: Deleting the pod @ 05/04/23 07:10:17.07
  STEP: Ensuring resource quota status released the pod usage @ 05/04/23 07:10:17.156
  May  4 07:10:19.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5033" for this suite. @ 05/04/23 07:10:19.165
• [16.397 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 05/04/23 07:10:19.191
  May  4 07:10:19.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename gc @ 05/04/23 07:10:19.193
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:10:19.253
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:10:19.257
  STEP: create the deployment @ 05/04/23 07:10:19.261
  W0504 07:10:19.296802      20 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 05/04/23 07:10:19.296
  STEP: delete the deployment @ 05/04/23 07:10:19.804
  STEP: wait for all rs to be garbage collected @ 05/04/23 07:10:19.827
  STEP: expected 0 rs, got 1 rs @ 05/04/23 07:10:19.834
  STEP: expected 0 pods, got 2 pods @ 05/04/23 07:10:19.837
  STEP: Gathering metrics @ 05/04/23 07:10:20.349
  May  4 07:10:20.518: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May  4 07:10:20.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-489" for this suite. @ 05/04/23 07:10:20.524
• [1.351 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 05/04/23 07:10:20.543
  May  4 07:10:20.543: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename init-container @ 05/04/23 07:10:20.544
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:10:20.59
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:10:20.594
  STEP: creating the pod @ 05/04/23 07:10:20.6
  May  4 07:10:20.600: INFO: PodSpec: initContainers in spec.initContainers
  May  4 07:10:25.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-4254" for this suite. @ 05/04/23 07:10:25.163
• [4.637 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 05/04/23 07:10:25.181
  May  4 07:10:25.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename crd-watch @ 05/04/23 07:10:25.182
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:10:25.247
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:10:25.251
  May  4 07:10:25.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Creating first CR  @ 05/04/23 07:10:27.852
  May  4 07:10:27.878: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-04T07:10:27Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-04T07:10:27Z]] name:name1 resourceVersion:70549 uid:26fea56b-2bbb-48b6-a832-4350009a7339] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Creating second CR @ 05/04/23 07:10:37.879
  May  4 07:10:37.919: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-04T07:10:37Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-04T07:10:37Z]] name:name2 resourceVersion:70589 uid:2b449afe-362e-41dd-8ae2-4ddc2f30ba7d] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying first CR @ 05/04/23 07:10:47.921
  May  4 07:10:47.939: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-04T07:10:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-04T07:10:47Z]] name:name1 resourceVersion:70605 uid:26fea56b-2bbb-48b6-a832-4350009a7339] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying second CR @ 05/04/23 07:10:57.94
  May  4 07:10:57.957: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-04T07:10:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-04T07:10:57Z]] name:name2 resourceVersion:70621 uid:2b449afe-362e-41dd-8ae2-4ddc2f30ba7d] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting first CR @ 05/04/23 07:11:07.958
  May  4 07:11:07.975: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-04T07:10:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-04T07:10:47Z]] name:name1 resourceVersion:70637 uid:26fea56b-2bbb-48b6-a832-4350009a7339] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting second CR @ 05/04/23 07:11:17.975
  May  4 07:11:17.994: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-04T07:10:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-04T07:10:57Z]] name:name2 resourceVersion:70653 uid:2b449afe-362e-41dd-8ae2-4ddc2f30ba7d] num:map[num1:9223372036854775807 num2:1000000]]}
  May  4 07:11:28.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-5134" for this suite. @ 05/04/23 07:11:28.536
• [63.395 seconds]
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 05/04/23 07:11:28.576
  May  4 07:11:28.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename proxy @ 05/04/23 07:11:28.578
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:11:28.667
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:11:28.67
  STEP: starting an echo server on multiple ports @ 05/04/23 07:11:28.711
  STEP: creating replication controller proxy-service-w9lvc in namespace proxy-9184 @ 05/04/23 07:11:28.711
  I0504 07:11:28.815076      20 runners.go:194] Created replication controller with name: proxy-service-w9lvc, namespace: proxy-9184, replica count: 1
  I0504 07:11:29.866525      20 runners.go:194] proxy-service-w9lvc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0504 07:11:30.867000      20 runners.go:194] proxy-service-w9lvc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0504 07:11:31.867380      20 runners.go:194] proxy-service-w9lvc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  4 07:11:31.870: INFO: setup took 3.196275302s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 05/04/23 07:11:31.87
  May  4 07:11:31.878: INFO: (0) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">test<... (200; 7.752758ms)
  May  4 07:11:31.878: INFO: (0) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/rewriteme">test</a> (200; 7.305179ms)
  May  4 07:11:31.878: INFO: (0) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 7.132809ms)
  May  4 07:11:31.878: INFO: (0) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 6.9758ms)
  May  4 07:11:31.884: INFO: (0) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 11.891622ms)
  May  4 07:11:31.884: INFO: (0) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname1/proxy/: foo (200; 12.017142ms)
  May  4 07:11:31.885: INFO: (0) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">... (200; 12.149414ms)
  May  4 07:11:31.886: INFO: (0) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname2/proxy/: bar (200; 15.930758ms)
  May  4 07:11:31.887: INFO: (0) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname1/proxy/: foo (200; 13.488011ms)
  May  4 07:11:31.887: INFO: (0) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 14.870672ms)
  May  4 07:11:31.892: INFO: (0) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname2/proxy/: bar (200; 20.693604ms)
  May  4 07:11:31.899: INFO: (0) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:462/proxy/: tls qux (200; 25.856515ms)
  May  4 07:11:31.899: INFO: (0) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname2/proxy/: tls qux (200; 26.345209ms)
  May  4 07:11:31.899: INFO: (0) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/tlsrewritem... (200; 27.532957ms)
  May  4 07:11:31.900: INFO: (0) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:460/proxy/: tls baz (200; 27.391703ms)
  May  4 07:11:31.900: INFO: (0) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname1/proxy/: tls baz (200; 29.025252ms)
  May  4 07:11:31.914: INFO: (1) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 11.711469ms)
  May  4 07:11:31.914: INFO: (1) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:462/proxy/: tls qux (200; 12.71686ms)
  May  4 07:11:31.914: INFO: (1) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 13.474295ms)
  May  4 07:11:31.914: INFO: (1) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">test<... (200; 12.567278ms)
  May  4 07:11:31.914: INFO: (1) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/tlsrewritem... (200; 11.895332ms)
  May  4 07:11:31.914: INFO: (1) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/rewriteme">test</a> (200; 12.276186ms)
  May  4 07:11:31.914: INFO: (1) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:460/proxy/: tls baz (200; 13.520876ms)
  May  4 07:11:31.914: INFO: (1) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 12.213051ms)
  May  4 07:11:31.914: INFO: (1) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname1/proxy/: foo (200; 13.461073ms)
  May  4 07:11:31.916: INFO: (1) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 15.487679ms)
  May  4 07:11:31.916: INFO: (1) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">... (200; 14.78793ms)
  May  4 07:11:31.916: INFO: (1) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname2/proxy/: bar (200; 15.755332ms)
  May  4 07:11:31.916: INFO: (1) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname1/proxy/: tls baz (200; 14.184311ms)
  May  4 07:11:31.916: INFO: (1) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname1/proxy/: foo (200; 14.648978ms)
  May  4 07:11:31.916: INFO: (1) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname2/proxy/: tls qux (200; 15.086688ms)
  May  4 07:11:31.916: INFO: (1) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname2/proxy/: bar (200; 14.423491ms)
  May  4 07:11:31.925: INFO: (2) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">... (200; 8.487361ms)
  May  4 07:11:31.925: INFO: (2) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:460/proxy/: tls baz (200; 8.984811ms)
  May  4 07:11:31.927: INFO: (2) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 10.118785ms)
  May  4 07:11:31.928: INFO: (2) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 11.852879ms)
  May  4 07:11:31.928: INFO: (2) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 11.699796ms)
  May  4 07:11:31.928: INFO: (2) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 10.692805ms)
  May  4 07:11:31.929: INFO: (2) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname2/proxy/: bar (200; 12.603933ms)
  May  4 07:11:31.929: INFO: (2) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname1/proxy/: foo (200; 12.329621ms)
  May  4 07:11:31.929: INFO: (2) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/rewriteme">test</a> (200; 12.068288ms)
  May  4 07:11:31.930: INFO: (2) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">test<... (200; 13.011359ms)
  May  4 07:11:31.930: INFO: (2) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname1/proxy/: tls baz (200; 12.93854ms)
  May  4 07:11:31.930: INFO: (2) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/tlsrewritem... (200; 12.656636ms)
  May  4 07:11:31.931: INFO: (2) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname2/proxy/: bar (200; 14.802681ms)
  May  4 07:11:31.931: INFO: (2) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname2/proxy/: tls qux (200; 14.065169ms)
  May  4 07:11:31.931: INFO: (2) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname1/proxy/: foo (200; 14.211383ms)
  May  4 07:11:31.931: INFO: (2) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:462/proxy/: tls qux (200; 13.911543ms)
  May  4 07:11:31.941: INFO: (3) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">test<... (200; 9.273299ms)
  May  4 07:11:31.941: INFO: (3) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 10.079735ms)
  May  4 07:11:31.941: INFO: (3) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 10.559036ms)
  May  4 07:11:31.943: INFO: (3) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:460/proxy/: tls baz (200; 11.093537ms)
  May  4 07:11:31.943: INFO: (3) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 10.564956ms)
  May  4 07:11:31.943: INFO: (3) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:462/proxy/: tls qux (200; 10.710887ms)
  May  4 07:11:31.943: INFO: (3) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 12.227371ms)
  May  4 07:11:31.943: INFO: (3) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/tlsrewritem... (200; 11.655144ms)
  May  4 07:11:31.943: INFO: (3) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/rewriteme">test</a> (200; 11.456631ms)
  May  4 07:11:31.944: INFO: (3) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">... (200; 11.60623ms)
  May  4 07:11:31.944: INFO: (3) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname2/proxy/: bar (200; 11.688384ms)
  May  4 07:11:31.944: INFO: (3) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname2/proxy/: tls qux (200; 11.984373ms)
  May  4 07:11:31.945: INFO: (3) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname1/proxy/: foo (200; 13.117419ms)
  May  4 07:11:31.945: INFO: (3) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname1/proxy/: foo (200; 14.024725ms)
  May  4 07:11:31.945: INFO: (3) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname1/proxy/: tls baz (200; 13.293249ms)
  May  4 07:11:31.945: INFO: (3) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname2/proxy/: bar (200; 13.179458ms)
  May  4 07:11:31.954: INFO: (4) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 8.4116ms)
  May  4 07:11:31.956: INFO: (4) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/tlsrewritem... (200; 10.741408ms)
  May  4 07:11:31.957: INFO: (4) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">test<... (200; 11.203097ms)
  May  4 07:11:31.959: INFO: (4) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname1/proxy/: tls baz (200; 13.415357ms)
  May  4 07:11:31.959: INFO: (4) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:460/proxy/: tls baz (200; 13.301301ms)
  May  4 07:11:31.959: INFO: (4) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname2/proxy/: bar (200; 13.555526ms)
  May  4 07:11:31.959: INFO: (4) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname2/proxy/: tls qux (200; 13.186536ms)
  May  4 07:11:31.959: INFO: (4) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:462/proxy/: tls qux (200; 13.816322ms)
  May  4 07:11:31.960: INFO: (4) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">... (200; 13.685477ms)
  May  4 07:11:31.960: INFO: (4) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname2/proxy/: bar (200; 14.221015ms)
  May  4 07:11:31.960: INFO: (4) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname1/proxy/: foo (200; 14.457501ms)
  May  4 07:11:31.960: INFO: (4) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 14.067234ms)
  May  4 07:11:31.960: INFO: (4) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/rewriteme">test</a> (200; 14.252122ms)
  May  4 07:11:31.961: INFO: (4) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname1/proxy/: foo (200; 14.549871ms)
  May  4 07:11:31.961: INFO: (4) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 14.634396ms)
  May  4 07:11:31.961: INFO: (4) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 14.703294ms)
  May  4 07:11:31.981: INFO: (5) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname2/proxy/: bar (200; 19.395404ms)
  May  4 07:11:31.981: INFO: (5) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">test<... (200; 19.555691ms)
  May  4 07:11:31.981: INFO: (5) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">... (200; 19.807557ms)
  May  4 07:11:31.981: INFO: (5) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname1/proxy/: foo (200; 20.085237ms)
  May  4 07:11:31.981: INFO: (5) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/rewriteme">test</a> (200; 19.605278ms)
  May  4 07:11:31.981: INFO: (5) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname2/proxy/: tls qux (200; 20.035382ms)
  May  4 07:11:31.981: INFO: (5) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname1/proxy/: foo (200; 20.141566ms)
  May  4 07:11:31.981: INFO: (5) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 20.279676ms)
  May  4 07:11:31.981: INFO: (5) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:460/proxy/: tls baz (200; 20.240448ms)
  May  4 07:11:31.981: INFO: (5) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 20.317396ms)
  May  4 07:11:31.981: INFO: (5) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname1/proxy/: tls baz (200; 19.812545ms)
  May  4 07:11:31.982: INFO: (5) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/tlsrewritem... (200; 20.773823ms)
  May  4 07:11:31.982: INFO: (5) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname2/proxy/: bar (200; 21.032873ms)
  May  4 07:11:31.982: INFO: (5) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 20.722984ms)
  May  4 07:11:31.982: INFO: (5) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:462/proxy/: tls qux (200; 21.48204ms)
  May  4 07:11:31.982: INFO: (5) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 21.795381ms)
  May  4 07:11:31.997: INFO: (6) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">test<... (200; 14.70044ms)
  May  4 07:11:32.002: INFO: (6) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:462/proxy/: tls qux (200; 18.114911ms)
  May  4 07:11:32.005: INFO: (6) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/rewriteme">test</a> (200; 21.928535ms)
  May  4 07:11:32.005: INFO: (6) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 21.767648ms)
  May  4 07:11:32.005: INFO: (6) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname1/proxy/: foo (200; 21.043559ms)
  May  4 07:11:32.005: INFO: (6) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 21.372675ms)
  May  4 07:11:32.005: INFO: (6) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:460/proxy/: tls baz (200; 21.225322ms)
  May  4 07:11:32.005: INFO: (6) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/tlsrewritem... (200; 21.875986ms)
  May  4 07:11:32.006: INFO: (6) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname2/proxy/: bar (200; 22.914267ms)
  May  4 07:11:32.006: INFO: (6) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 21.729585ms)
  May  4 07:11:32.006: INFO: (6) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">... (200; 21.60344ms)
  May  4 07:11:32.007: INFO: (6) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname1/proxy/: tls baz (200; 23.594632ms)
  May  4 07:11:32.007: INFO: (6) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 23.373734ms)
  May  4 07:11:32.007: INFO: (6) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname2/proxy/: tls qux (200; 22.709669ms)
  May  4 07:11:32.007: INFO: (6) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname1/proxy/: foo (200; 23.009631ms)
  May  4 07:11:32.007: INFO: (6) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname2/proxy/: bar (200; 23.983812ms)
  May  4 07:11:32.024: INFO: (7) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 16.370213ms)
  May  4 07:11:32.024: INFO: (7) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 16.832068ms)
  May  4 07:11:32.024: INFO: (7) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 15.27331ms)
  May  4 07:11:32.025: INFO: (7) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:462/proxy/: tls qux (200; 16.733287ms)
  May  4 07:11:32.025: INFO: (7) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">... (200; 16.930155ms)
  May  4 07:11:32.025: INFO: (7) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/tlsrewritem... (200; 16.050652ms)
  May  4 07:11:32.025: INFO: (7) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">test<... (200; 16.542956ms)
  May  4 07:11:32.025: INFO: (7) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 15.929447ms)
  May  4 07:11:32.025: INFO: (7) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/rewriteme">test</a> (200; 16.360121ms)
  May  4 07:11:32.026: INFO: (7) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:460/proxy/: tls baz (200; 18.374229ms)
  May  4 07:11:32.028: INFO: (7) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname1/proxy/: tls baz (200; 19.426236ms)
  May  4 07:11:32.028: INFO: (7) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname1/proxy/: foo (200; 20.420975ms)
  May  4 07:11:32.028: INFO: (7) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname2/proxy/: tls qux (200; 19.721387ms)
  May  4 07:11:32.028: INFO: (7) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname1/proxy/: foo (200; 19.876907ms)
  May  4 07:11:32.028: INFO: (7) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname2/proxy/: bar (200; 21.007929ms)
  May  4 07:11:32.028: INFO: (7) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname2/proxy/: bar (200; 19.07983ms)
  May  4 07:11:32.038: INFO: (8) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:460/proxy/: tls baz (200; 9.34302ms)
  May  4 07:11:32.038: INFO: (8) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 9.659579ms)
  May  4 07:11:32.038: INFO: (8) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">test<... (200; 10.231014ms)
  May  4 07:11:32.039: INFO: (8) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 10.723375ms)
  May  4 07:11:32.039: INFO: (8) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/rewriteme">test</a> (200; 11.230493ms)
  May  4 07:11:32.039: INFO: (8) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 10.917875ms)
  May  4 07:11:32.039: INFO: (8) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">... (200; 10.363713ms)
  May  4 07:11:32.039: INFO: (8) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:462/proxy/: tls qux (200; 11.315702ms)
  May  4 07:11:32.039: INFO: (8) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 10.912751ms)
  May  4 07:11:32.061: INFO: (8) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname2/proxy/: bar (200; 32.514964ms)
  May  4 07:11:32.061: INFO: (8) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname2/proxy/: tls qux (200; 31.917553ms)
  May  4 07:11:32.061: INFO: (8) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname1/proxy/: foo (200; 32.149806ms)
  May  4 07:11:32.061: INFO: (8) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/tlsrewritem... (200; 32.637442ms)
  May  4 07:11:32.061: INFO: (8) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname1/proxy/: foo (200; 32.323691ms)
  May  4 07:11:32.061: INFO: (8) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname1/proxy/: tls baz (200; 32.474464ms)
  May  4 07:11:32.061: INFO: (8) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname2/proxy/: bar (200; 32.649358ms)
  May  4 07:11:32.068: INFO: (9) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/rewriteme">test</a> (200; 6.419143ms)
  May  4 07:11:32.072: INFO: (9) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname1/proxy/: foo (200; 10.821079ms)
  May  4 07:11:32.073: INFO: (9) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/tlsrewritem... (200; 11.102703ms)
  May  4 07:11:32.074: INFO: (9) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">... (200; 12.071279ms)
  May  4 07:11:32.074: INFO: (9) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:462/proxy/: tls qux (200; 11.997793ms)
  May  4 07:11:32.074: INFO: (9) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname2/proxy/: tls qux (200; 12.191587ms)
  May  4 07:11:32.074: INFO: (9) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 12.484787ms)
  May  4 07:11:32.074: INFO: (9) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 12.304615ms)
  May  4 07:11:32.074: INFO: (9) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 12.424867ms)
  May  4 07:11:32.074: INFO: (9) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">test<... (200; 12.934976ms)
  May  4 07:11:32.074: INFO: (9) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname1/proxy/: tls baz (200; 12.933853ms)
  May  4 07:11:32.074: INFO: (9) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 12.809504ms)
  May  4 07:11:32.074: INFO: (9) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:460/proxy/: tls baz (200; 12.684447ms)
  May  4 07:11:32.075: INFO: (9) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname2/proxy/: bar (200; 13.943024ms)
  May  4 07:11:32.075: INFO: (9) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname1/proxy/: foo (200; 13.508673ms)
  May  4 07:11:32.075: INFO: (9) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname2/proxy/: bar (200; 13.833573ms)
  May  4 07:11:32.086: INFO: (10) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:462/proxy/: tls qux (200; 10.868562ms)
  May  4 07:11:32.086: INFO: (10) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">test<... (200; 10.7679ms)
  May  4 07:11:32.086: INFO: (10) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/tlsrewritem... (200; 10.885513ms)
  May  4 07:11:32.087: INFO: (10) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 12.010974ms)
  May  4 07:11:32.088: INFO: (10) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname1/proxy/: foo (200; 12.055706ms)
  May  4 07:11:32.089: INFO: (10) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname2/proxy/: bar (200; 14.049275ms)
  May  4 07:11:32.089: INFO: (10) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname2/proxy/: bar (200; 14.120287ms)
  May  4 07:11:32.089: INFO: (10) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">... (200; 13.543195ms)
  May  4 07:11:32.089: INFO: (10) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname1/proxy/: foo (200; 13.81469ms)
  May  4 07:11:32.089: INFO: (10) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:460/proxy/: tls baz (200; 13.937607ms)
  May  4 07:11:32.089: INFO: (10) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 14.018963ms)
  May  4 07:11:32.090: INFO: (10) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/rewriteme">test</a> (200; 14.126168ms)
  May  4 07:11:32.089: INFO: (10) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname2/proxy/: tls qux (200; 13.716961ms)
  May  4 07:11:32.089: INFO: (10) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 13.847999ms)
  May  4 07:11:32.090: INFO: (10) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 14.358718ms)
  May  4 07:11:32.090: INFO: (10) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname1/proxy/: tls baz (200; 14.685319ms)
  May  4 07:11:32.095: INFO: (11) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 5.104384ms)
  May  4 07:11:32.099: INFO: (11) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname1/proxy/: foo (200; 8.363866ms)
  May  4 07:11:32.100: INFO: (11) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 9.791431ms)
  May  4 07:11:32.101: INFO: (11) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:460/proxy/: tls baz (200; 10.740634ms)
  May  4 07:11:32.102: INFO: (11) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">test<... (200; 11.212805ms)
  May  4 07:11:32.103: INFO: (11) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname1/proxy/: tls baz (200; 11.725179ms)
  May  4 07:11:32.103: INFO: (11) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 12.168285ms)
  May  4 07:11:32.103: INFO: (11) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname2/proxy/: tls qux (200; 12.5074ms)
  May  4 07:11:32.103: INFO: (11) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/rewriteme">test</a> (200; 12.14819ms)
  May  4 07:11:32.104: INFO: (11) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname2/proxy/: bar (200; 13.447265ms)
  May  4 07:11:32.104: INFO: (11) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname1/proxy/: foo (200; 12.363545ms)
  May  4 07:11:32.104: INFO: (11) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">... (200; 12.414091ms)
  May  4 07:11:32.104: INFO: (11) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 12.294479ms)
  May  4 07:11:32.104: INFO: (11) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/tlsrewritem... (200; 12.58184ms)
  May  4 07:11:32.104: INFO: (11) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname2/proxy/: bar (200; 12.901272ms)
  May  4 07:11:32.104: INFO: (11) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:462/proxy/: tls qux (200; 12.893868ms)
  May  4 07:11:32.112: INFO: (12) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:460/proxy/: tls baz (200; 7.558396ms)
  May  4 07:11:32.112: INFO: (12) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">test<... (200; 8.128146ms)
  May  4 07:11:32.112: INFO: (12) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">... (200; 7.541286ms)
  May  4 07:11:32.114: INFO: (12) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/rewriteme">test</a> (200; 10.001642ms)
  May  4 07:11:32.115: INFO: (12) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 10.065451ms)
  May  4 07:11:32.115: INFO: (12) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 10.456617ms)
  May  4 07:11:32.116: INFO: (12) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 11.527423ms)
  May  4 07:11:32.116: INFO: (12) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:462/proxy/: tls qux (200; 11.269141ms)
  May  4 07:11:32.116: INFO: (12) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname2/proxy/: bar (200; 11.761494ms)
  May  4 07:11:32.116: INFO: (12) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname1/proxy/: foo (200; 12.064264ms)
  May  4 07:11:32.116: INFO: (12) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/tlsrewritem... (200; 11.942614ms)
  May  4 07:11:32.116: INFO: (12) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname1/proxy/: tls baz (200; 11.826031ms)
  May  4 07:11:32.116: INFO: (12) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname2/proxy/: tls qux (200; 11.441359ms)
  May  4 07:11:32.116: INFO: (12) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname1/proxy/: foo (200; 11.774558ms)
  May  4 07:11:32.116: INFO: (12) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname2/proxy/: bar (200; 11.96332ms)
  May  4 07:11:32.116: INFO: (12) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 12.089934ms)
  May  4 07:11:32.123: INFO: (13) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:462/proxy/: tls qux (200; 6.795672ms)
  May  4 07:11:32.124: INFO: (13) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/tlsrewritem... (200; 7.309785ms)
  May  4 07:11:32.124: INFO: (13) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">... (200; 6.974203ms)
  May  4 07:11:32.124: INFO: (13) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 7.859466ms)
  May  4 07:11:32.125: INFO: (13) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 8.284915ms)
  May  4 07:11:32.125: INFO: (13) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:460/proxy/: tls baz (200; 8.48284ms)
  May  4 07:11:32.125: INFO: (13) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">test<... (200; 8.257202ms)
  May  4 07:11:32.129: INFO: (13) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname1/proxy/: foo (200; 11.593697ms)
  May  4 07:11:32.129: INFO: (13) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname2/proxy/: bar (200; 11.653684ms)
  May  4 07:11:32.129: INFO: (13) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 11.798269ms)
  May  4 07:11:32.129: INFO: (13) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname2/proxy/: bar (200; 12.1162ms)
  May  4 07:11:32.129: INFO: (13) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/rewriteme">test</a> (200; 11.885661ms)
  May  4 07:11:32.130: INFO: (13) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname1/proxy/: foo (200; 12.843587ms)
  May  4 07:11:32.130: INFO: (13) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 12.223535ms)
  May  4 07:11:32.130: INFO: (13) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname1/proxy/: tls baz (200; 12.683506ms)
  May  4 07:11:32.130: INFO: (13) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname2/proxy/: tls qux (200; 12.471789ms)
  May  4 07:11:32.139: INFO: (14) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 8.004066ms)
  May  4 07:11:32.139: INFO: (14) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:462/proxy/: tls qux (200; 9.293996ms)
  May  4 07:11:32.139: INFO: (14) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 8.807702ms)
  May  4 07:11:32.141: INFO: (14) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 11.492823ms)
  May  4 07:11:32.141: INFO: (14) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 10.326412ms)
  May  4 07:11:32.141: INFO: (14) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:460/proxy/: tls baz (200; 10.293699ms)
  May  4 07:11:32.141: INFO: (14) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">test<... (200; 11.526878ms)
  May  4 07:11:32.142: INFO: (14) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname1/proxy/: foo (200; 12.305337ms)
  May  4 07:11:32.142: INFO: (14) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/tlsrewritem... (200; 12.312363ms)
  May  4 07:11:32.142: INFO: (14) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname2/proxy/: tls qux (200; 11.247418ms)
  May  4 07:11:32.143: INFO: (14) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname2/proxy/: bar (200; 12.487311ms)
  May  4 07:11:32.143: INFO: (14) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname1/proxy/: foo (200; 12.466105ms)
  May  4 07:11:32.143: INFO: (14) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/rewriteme">test</a> (200; 13.054443ms)
  May  4 07:11:32.143: INFO: (14) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">... (200; 12.360278ms)
  May  4 07:11:32.143: INFO: (14) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname1/proxy/: tls baz (200; 13.277209ms)
  May  4 07:11:32.143: INFO: (14) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname2/proxy/: bar (200; 12.896336ms)
  May  4 07:11:32.153: INFO: (15) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:460/proxy/: tls baz (200; 9.356292ms)
  May  4 07:11:32.156: INFO: (15) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 11.536029ms)
  May  4 07:11:32.156: INFO: (15) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname1/proxy/: tls baz (200; 12.232753ms)
  May  4 07:11:32.157: INFO: (15) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 12.496839ms)
  May  4 07:11:32.157: INFO: (15) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">test<... (200; 12.924915ms)
  May  4 07:11:32.157: INFO: (15) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/rewriteme">test</a> (200; 12.991371ms)
  May  4 07:11:32.157: INFO: (15) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">... (200; 12.626785ms)
  May  4 07:11:32.157: INFO: (15) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname1/proxy/: foo (200; 13.647133ms)
  May  4 07:11:32.158: INFO: (15) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:462/proxy/: tls qux (200; 13.714234ms)
  May  4 07:11:32.159: INFO: (15) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/tlsrewritem... (200; 14.431903ms)
  May  4 07:11:32.159: INFO: (15) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 14.80184ms)
  May  4 07:11:32.159: INFO: (15) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 14.577236ms)
  May  4 07:11:32.159: INFO: (15) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname1/proxy/: foo (200; 15.174808ms)
  May  4 07:11:32.160: INFO: (15) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname2/proxy/: bar (200; 16.289493ms)
  May  4 07:11:32.160: INFO: (15) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname2/proxy/: bar (200; 16.239494ms)
  May  4 07:11:32.161: INFO: (15) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname2/proxy/: tls qux (200; 17.033312ms)
  May  4 07:11:32.168: INFO: (16) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">test<... (200; 6.796928ms)
  May  4 07:11:32.168: INFO: (16) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/rewriteme">test</a> (200; 6.761027ms)
  May  4 07:11:32.169: INFO: (16) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 6.639257ms)
  May  4 07:11:32.169: INFO: (16) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 7.426752ms)
  May  4 07:11:32.172: INFO: (16) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 9.35955ms)
  May  4 07:11:32.173: INFO: (16) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:460/proxy/: tls baz (200; 9.689424ms)
  May  4 07:11:32.173: INFO: (16) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 9.293527ms)
  May  4 07:11:32.173: INFO: (16) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:462/proxy/: tls qux (200; 9.138535ms)
  May  4 07:11:32.174: INFO: (16) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/tlsrewritem... (200; 11.691188ms)
  May  4 07:11:32.174: INFO: (16) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">... (200; 10.355762ms)
  May  4 07:11:32.177: INFO: (16) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname2/proxy/: bar (200; 15.047751ms)
  May  4 07:11:32.177: INFO: (16) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname2/proxy/: tls qux (200; 13.688434ms)
  May  4 07:11:32.177: INFO: (16) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname1/proxy/: foo (200; 13.875174ms)
  May  4 07:11:32.178: INFO: (16) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname1/proxy/: tls baz (200; 15.31152ms)
  May  4 07:11:32.178: INFO: (16) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname2/proxy/: bar (200; 15.509608ms)
  May  4 07:11:32.178: INFO: (16) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname1/proxy/: foo (200; 15.024683ms)
  May  4 07:11:32.185: INFO: (17) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">... (200; 6.443722ms)
  May  4 07:11:32.185: INFO: (17) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:460/proxy/: tls baz (200; 6.943077ms)
  May  4 07:11:32.185: INFO: (17) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:462/proxy/: tls qux (200; 6.335363ms)
  May  4 07:11:32.185: INFO: (17) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">test<... (200; 6.148099ms)
  May  4 07:11:32.189: INFO: (17) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 8.749646ms)
  May  4 07:11:32.190: INFO: (17) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 10.730073ms)
  May  4 07:11:32.190: INFO: (17) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname2/proxy/: tls qux (200; 11.532288ms)
  May  4 07:11:32.190: INFO: (17) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/tlsrewritem... (200; 10.736991ms)
  May  4 07:11:32.190: INFO: (17) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 10.040451ms)
  May  4 07:11:32.190: INFO: (17) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/rewriteme">test</a> (200; 10.217357ms)
  May  4 07:11:32.190: INFO: (17) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 9.857049ms)
  May  4 07:11:32.191: INFO: (17) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname1/proxy/: foo (200; 13.066949ms)
  May  4 07:11:32.193: INFO: (17) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname2/proxy/: bar (200; 13.443208ms)
  May  4 07:11:32.193: INFO: (17) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname2/proxy/: bar (200; 13.402426ms)
  May  4 07:11:32.193: INFO: (17) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname1/proxy/: foo (200; 12.871364ms)
  May  4 07:11:32.193: INFO: (17) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname1/proxy/: tls baz (200; 13.253378ms)
  May  4 07:11:32.198: INFO: (18) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 4.629538ms)
  May  4 07:11:32.203: INFO: (18) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 8.902098ms)
  May  4 07:11:32.203: INFO: (18) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/tlsrewritem... (200; 9.588092ms)
  May  4 07:11:32.203: INFO: (18) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 10.003723ms)
  May  4 07:11:32.203: INFO: (18) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">... (200; 9.819924ms)
  May  4 07:11:32.207: INFO: (18) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname1/proxy/: foo (200; 14.185871ms)
  May  4 07:11:32.207: INFO: (18) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 14.081147ms)
  May  4 07:11:32.208: INFO: (18) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/rewriteme">test</a> (200; 14.230117ms)
  May  4 07:11:32.208: INFO: (18) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname2/proxy/: tls qux (200; 14.798133ms)
  May  4 07:11:32.208: INFO: (18) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname2/proxy/: bar (200; 14.388628ms)
  May  4 07:11:32.208: INFO: (18) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:462/proxy/: tls qux (200; 14.676198ms)
  May  4 07:11:32.208: INFO: (18) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">test<... (200; 14.633703ms)
  May  4 07:11:32.208: INFO: (18) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname2/proxy/: bar (200; 14.515967ms)
  May  4 07:11:32.208: INFO: (18) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname1/proxy/: foo (200; 15.125879ms)
  May  4 07:11:32.208: INFO: (18) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname1/proxy/: tls baz (200; 14.620177ms)
  May  4 07:11:32.208: INFO: (18) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:460/proxy/: tls baz (200; 15.062271ms)
  May  4 07:11:32.219: INFO: (19) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 9.958203ms)
  May  4 07:11:32.221: INFO: (19) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 12.441057ms)
  May  4 07:11:32.224: INFO: (19) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">... (200; 14.377506ms)
  May  4 07:11:32.224: INFO: (19) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:160/proxy/: foo (200; 14.991222ms)
  May  4 07:11:32.224: INFO: (19) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2/proxy/rewriteme">test</a> (200; 15.115619ms)
  May  4 07:11:32.225: INFO: (19) /api/v1/namespaces/proxy-9184/pods/http:proxy-service-w9lvc-cdzq2:162/proxy/: bar (200; 15.924979ms)
  May  4 07:11:32.225: INFO: (19) /api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/proxy-service-w9lvc-cdzq2:1080/proxy/rewriteme">test<... (200; 16.211707ms)
  May  4 07:11:32.225: INFO: (19) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:460/proxy/: tls baz (200; 15.683606ms)
  May  4 07:11:32.225: INFO: (19) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/: <a href="/api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:443/proxy/tlsrewritem... (200; 15.97555ms)
  May  4 07:11:32.225: INFO: (19) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname2/proxy/: bar (200; 16.521369ms)
  May  4 07:11:32.225: INFO: (19) /api/v1/namespaces/proxy-9184/services/http:proxy-service-w9lvc:portname1/proxy/: foo (200; 16.88952ms)
  May  4 07:11:32.225: INFO: (19) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname1/proxy/: foo (200; 16.341501ms)
  May  4 07:11:32.226: INFO: (19) /api/v1/namespaces/proxy-9184/pods/https:proxy-service-w9lvc-cdzq2:462/proxy/: tls qux (200; 17.587733ms)
  May  4 07:11:32.226: INFO: (19) /api/v1/namespaces/proxy-9184/services/proxy-service-w9lvc:portname2/proxy/: bar (200; 17.30138ms)
  May  4 07:11:32.228: INFO: (19) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname2/proxy/: tls qux (200; 18.474432ms)
  May  4 07:11:32.228: INFO: (19) /api/v1/namespaces/proxy-9184/services/https:proxy-service-w9lvc:tlsportname1/proxy/: tls baz (200; 18.674197ms)
  May  4 07:11:32.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-w9lvc in namespace proxy-9184, will wait for the garbage collector to delete the pods @ 05/04/23 07:11:32.232
  May  4 07:11:32.300: INFO: Deleting ReplicationController proxy-service-w9lvc took: 13.941162ms
  May  4 07:11:32.402: INFO: Terminating ReplicationController proxy-service-w9lvc pods took: 101.16734ms
  STEP: Destroying namespace "proxy-9184" for this suite. @ 05/04/23 07:11:37.503
• [8.951 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 05/04/23 07:11:37.528
  May  4 07:11:37.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename replicaset @ 05/04/23 07:11:37.53
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:11:37.598
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:11:37.602
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 05/04/23 07:11:37.605
  May  4 07:11:37.636: INFO: Pod name sample-pod: Found 0 pods out of 1
  May  4 07:11:42.646: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/04/23 07:11:42.646
  STEP: getting scale subresource @ 05/04/23 07:11:42.646
  STEP: updating a scale subresource @ 05/04/23 07:11:42.662
  STEP: verifying the replicaset Spec.Replicas was modified @ 05/04/23 07:11:42.689
  STEP: Patch a scale subresource @ 05/04/23 07:11:42.747
  May  4 07:11:42.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-6301" for this suite. @ 05/04/23 07:11:42.849
• [5.405 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 05/04/23 07:11:42.934
  May  4 07:11:42.934: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 07:11:42.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:11:43.106
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:11:43.112
  STEP: Creating the pod @ 05/04/23 07:11:43.117
  May  4 07:11:47.759: INFO: Successfully updated pod "labelsupdated9099d5d-31fa-4843-9a0e-cea48b7447a6"
  May  4 07:11:49.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2990" for this suite. @ 05/04/23 07:11:49.783
• [6.864 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 05/04/23 07:11:49.8
  May  4 07:11:49.800: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename pods @ 05/04/23 07:11:49.802
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:11:49.844
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:11:49.848
  STEP: creating the pod @ 05/04/23 07:11:49.852
  STEP: submitting the pod to kubernetes @ 05/04/23 07:11:49.852
  STEP: verifying QOS class is set on the pod @ 05/04/23 07:11:49.888
  May  4 07:11:49.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4518" for this suite. @ 05/04/23 07:11:49.918
• [0.144 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 05/04/23 07:11:49.947
  May  4 07:11:49.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename kubectl @ 05/04/23 07:11:49.949
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:11:50.032
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:11:50.035
  STEP: Starting the proxy @ 05/04/23 07:11:50.039
  May  4 07:11:50.040: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-2322 proxy --unix-socket=/tmp/kubectl-proxy-unix3921392214/test'
  STEP: retrieving proxy /api/ output @ 05/04/23 07:11:50.122
  May  4 07:11:50.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2322" for this suite. @ 05/04/23 07:11:50.143
• [0.210 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 05/04/23 07:11:50.16
  May  4 07:11:50.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename gc @ 05/04/23 07:11:50.161
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:11:50.192
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:11:50.197
  STEP: create the rc @ 05/04/23 07:11:50.217
  W0504 07:11:50.233248      20 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 05/04/23 07:11:56.239
  STEP: wait for the rc to be deleted @ 05/04/23 07:11:56.332
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 05/04/23 07:12:01.373
  STEP: Gathering metrics @ 05/04/23 07:12:31.396
  May  4 07:12:31.536: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May  4 07:12:31.536: INFO: Deleting pod "simpletest.rc-2bs6d" in namespace "gc-6837"
  May  4 07:12:31.575: INFO: Deleting pod "simpletest.rc-2ss82" in namespace "gc-6837"
  May  4 07:12:31.641: INFO: Deleting pod "simpletest.rc-49tqt" in namespace "gc-6837"
  May  4 07:12:31.736: INFO: Deleting pod "simpletest.rc-4bn9j" in namespace "gc-6837"
  May  4 07:12:31.842: INFO: Deleting pod "simpletest.rc-4r572" in namespace "gc-6837"
  May  4 07:12:31.999: INFO: Deleting pod "simpletest.rc-4tmf5" in namespace "gc-6837"
  May  4 07:12:32.284: INFO: Deleting pod "simpletest.rc-4vf28" in namespace "gc-6837"
  May  4 07:12:32.507: INFO: Deleting pod "simpletest.rc-5692w" in namespace "gc-6837"
  May  4 07:12:32.687: INFO: Deleting pod "simpletest.rc-59hlf" in namespace "gc-6837"
  May  4 07:12:32.839: INFO: Deleting pod "simpletest.rc-5b57z" in namespace "gc-6837"
  May  4 07:12:33.121: INFO: Deleting pod "simpletest.rc-5bgcv" in namespace "gc-6837"
  May  4 07:12:33.333: INFO: Deleting pod "simpletest.rc-5fwkj" in namespace "gc-6837"
  May  4 07:12:33.520: INFO: Deleting pod "simpletest.rc-5tqzn" in namespace "gc-6837"
  May  4 07:12:33.736: INFO: Deleting pod "simpletest.rc-5w2f7" in namespace "gc-6837"
  May  4 07:12:33.951: INFO: Deleting pod "simpletest.rc-65n9v" in namespace "gc-6837"
  May  4 07:12:34.108: INFO: Deleting pod "simpletest.rc-6d7fh" in namespace "gc-6837"
  May  4 07:12:34.334: INFO: Deleting pod "simpletest.rc-6ff6c" in namespace "gc-6837"
  May  4 07:12:34.519: INFO: Deleting pod "simpletest.rc-6n7hv" in namespace "gc-6837"
  May  4 07:12:34.694: INFO: Deleting pod "simpletest.rc-6rzzs" in namespace "gc-6837"
  May  4 07:12:34.838: INFO: Deleting pod "simpletest.rc-6shxk" in namespace "gc-6837"
  May  4 07:12:34.957: INFO: Deleting pod "simpletest.rc-6sj9r" in namespace "gc-6837"
  May  4 07:12:35.045: INFO: Deleting pod "simpletest.rc-7jfqw" in namespace "gc-6837"
  May  4 07:12:35.181: INFO: Deleting pod "simpletest.rc-7lxjv" in namespace "gc-6837"
  May  4 07:12:35.414: INFO: Deleting pod "simpletest.rc-7m6l8" in namespace "gc-6837"
  May  4 07:12:35.587: INFO: Deleting pod "simpletest.rc-7pchc" in namespace "gc-6837"
  May  4 07:12:35.732: INFO: Deleting pod "simpletest.rc-7pgnb" in namespace "gc-6837"
  May  4 07:12:35.895: INFO: Deleting pod "simpletest.rc-7ztst" in namespace "gc-6837"
  May  4 07:12:35.979: INFO: Deleting pod "simpletest.rc-8fbz7" in namespace "gc-6837"
  May  4 07:12:36.230: INFO: Deleting pod "simpletest.rc-8h8w4" in namespace "gc-6837"
  May  4 07:12:36.438: INFO: Deleting pod "simpletest.rc-8hr7g" in namespace "gc-6837"
  May  4 07:12:36.718: INFO: Deleting pod "simpletest.rc-8mphh" in namespace "gc-6837"
  May  4 07:12:36.804: INFO: Deleting pod "simpletest.rc-8pddk" in namespace "gc-6837"
  May  4 07:12:37.100: INFO: Deleting pod "simpletest.rc-9kv4t" in namespace "gc-6837"
  May  4 07:12:37.350: INFO: Deleting pod "simpletest.rc-b9phw" in namespace "gc-6837"
  May  4 07:12:37.529: INFO: Deleting pod "simpletest.rc-bdqhz" in namespace "gc-6837"
  May  4 07:12:37.682: INFO: Deleting pod "simpletest.rc-bqh24" in namespace "gc-6837"
  May  4 07:12:37.816: INFO: Deleting pod "simpletest.rc-brdkp" in namespace "gc-6837"
  May  4 07:12:38.035: INFO: Deleting pod "simpletest.rc-bwrrm" in namespace "gc-6837"
  May  4 07:12:38.156: INFO: Deleting pod "simpletest.rc-bznvm" in namespace "gc-6837"
  May  4 07:12:38.287: INFO: Deleting pod "simpletest.rc-c2j9k" in namespace "gc-6837"
  May  4 07:12:38.458: INFO: Deleting pod "simpletest.rc-cj278" in namespace "gc-6837"
  May  4 07:12:38.610: INFO: Deleting pod "simpletest.rc-cpnq2" in namespace "gc-6837"
  May  4 07:12:38.713: INFO: Deleting pod "simpletest.rc-csf6m" in namespace "gc-6837"
  May  4 07:12:38.875: INFO: Deleting pod "simpletest.rc-cwcp5" in namespace "gc-6837"
  May  4 07:12:39.078: INFO: Deleting pod "simpletest.rc-d6jtj" in namespace "gc-6837"
  May  4 07:12:39.328: INFO: Deleting pod "simpletest.rc-dfsgx" in namespace "gc-6837"
  May  4 07:12:39.485: INFO: Deleting pod "simpletest.rc-f4m4d" in namespace "gc-6837"
  May  4 07:12:39.620: INFO: Deleting pod "simpletest.rc-fdj26" in namespace "gc-6837"
  May  4 07:12:39.775: INFO: Deleting pod "simpletest.rc-fgh92" in namespace "gc-6837"
  May  4 07:12:39.954: INFO: Deleting pod "simpletest.rc-fh5rq" in namespace "gc-6837"
  May  4 07:12:40.069: INFO: Deleting pod "simpletest.rc-fv7p7" in namespace "gc-6837"
  May  4 07:12:40.283: INFO: Deleting pod "simpletest.rc-fwj46" in namespace "gc-6837"
  May  4 07:12:40.393: INFO: Deleting pod "simpletest.rc-g6llk" in namespace "gc-6837"
  May  4 07:12:40.698: INFO: Deleting pod "simpletest.rc-gmvd5" in namespace "gc-6837"
  May  4 07:12:40.866: INFO: Deleting pod "simpletest.rc-gts99" in namespace "gc-6837"
  May  4 07:12:40.981: INFO: Deleting pod "simpletest.rc-gvzm9" in namespace "gc-6837"
  May  4 07:12:41.105: INFO: Deleting pod "simpletest.rc-h26ps" in namespace "gc-6837"
  May  4 07:12:41.249: INFO: Deleting pod "simpletest.rc-hrlwx" in namespace "gc-6837"
  May  4 07:12:41.392: INFO: Deleting pod "simpletest.rc-k4hd7" in namespace "gc-6837"
  May  4 07:12:41.582: INFO: Deleting pod "simpletest.rc-kmdvf" in namespace "gc-6837"
  May  4 07:12:41.780: INFO: Deleting pod "simpletest.rc-kwkr8" in namespace "gc-6837"
  May  4 07:12:41.876: INFO: Deleting pod "simpletest.rc-l4xl8" in namespace "gc-6837"
  May  4 07:12:42.073: INFO: Deleting pod "simpletest.rc-l6w9f" in namespace "gc-6837"
  May  4 07:12:42.185: INFO: Deleting pod "simpletest.rc-l7zc4" in namespace "gc-6837"
  May  4 07:12:42.436: INFO: Deleting pod "simpletest.rc-lbvnv" in namespace "gc-6837"
  May  4 07:12:42.622: INFO: Deleting pod "simpletest.rc-m9q58" in namespace "gc-6837"
  May  4 07:12:42.900: INFO: Deleting pod "simpletest.rc-mpxnl" in namespace "gc-6837"
  May  4 07:12:43.084: INFO: Deleting pod "simpletest.rc-pdgcg" in namespace "gc-6837"
  May  4 07:12:43.226: INFO: Deleting pod "simpletest.rc-pzzjb" in namespace "gc-6837"
  May  4 07:12:43.485: INFO: Deleting pod "simpletest.rc-qcvfn" in namespace "gc-6837"
  May  4 07:12:43.660: INFO: Deleting pod "simpletest.rc-qkdk5" in namespace "gc-6837"
  May  4 07:12:43.735: INFO: Deleting pod "simpletest.rc-qttf5" in namespace "gc-6837"
  May  4 07:12:43.944: INFO: Deleting pod "simpletest.rc-qxdxr" in namespace "gc-6837"
  May  4 07:12:44.127: INFO: Deleting pod "simpletest.rc-r28b2" in namespace "gc-6837"
  May  4 07:12:44.320: INFO: Deleting pod "simpletest.rc-rrvv9" in namespace "gc-6837"
  May  4 07:12:44.542: INFO: Deleting pod "simpletest.rc-rs5hh" in namespace "gc-6837"
  May  4 07:12:44.690: INFO: Deleting pod "simpletest.rc-spgn9" in namespace "gc-6837"
  May  4 07:12:44.795: INFO: Deleting pod "simpletest.rc-spxbx" in namespace "gc-6837"
  May  4 07:12:45.039: INFO: Deleting pod "simpletest.rc-t7rd7" in namespace "gc-6837"
  May  4 07:12:45.224: INFO: Deleting pod "simpletest.rc-tkm6h" in namespace "gc-6837"
  May  4 07:12:45.355: INFO: Deleting pod "simpletest.rc-tljrz" in namespace "gc-6837"
  May  4 07:12:45.608: INFO: Deleting pod "simpletest.rc-twnqd" in namespace "gc-6837"
  May  4 07:12:45.729: INFO: Deleting pod "simpletest.rc-tz2v5" in namespace "gc-6837"
  May  4 07:12:45.911: INFO: Deleting pod "simpletest.rc-vbsrp" in namespace "gc-6837"
  May  4 07:12:46.119: INFO: Deleting pod "simpletest.rc-vhvwg" in namespace "gc-6837"
  May  4 07:12:46.283: INFO: Deleting pod "simpletest.rc-vkfvn" in namespace "gc-6837"
  May  4 07:12:46.440: INFO: Deleting pod "simpletest.rc-vmc87" in namespace "gc-6837"
  May  4 07:12:46.546: INFO: Deleting pod "simpletest.rc-vwt8b" in namespace "gc-6837"
  May  4 07:12:46.669: INFO: Deleting pod "simpletest.rc-w75b9" in namespace "gc-6837"
  May  4 07:12:46.874: INFO: Deleting pod "simpletest.rc-w7rpm" in namespace "gc-6837"
  May  4 07:12:47.131: INFO: Deleting pod "simpletest.rc-wjpt9" in namespace "gc-6837"
  May  4 07:12:47.310: INFO: Deleting pod "simpletest.rc-wmhwq" in namespace "gc-6837"
  May  4 07:12:47.430: INFO: Deleting pod "simpletest.rc-wwgx2" in namespace "gc-6837"
  May  4 07:12:47.583: INFO: Deleting pod "simpletest.rc-x4hfh" in namespace "gc-6837"
  May  4 07:12:47.893: INFO: Deleting pod "simpletest.rc-xdqvx" in namespace "gc-6837"
  May  4 07:12:48.013: INFO: Deleting pod "simpletest.rc-xtxsz" in namespace "gc-6837"
  May  4 07:12:48.211: INFO: Deleting pod "simpletest.rc-z54rn" in namespace "gc-6837"
  May  4 07:12:48.303: INFO: Deleting pod "simpletest.rc-z7cbs" in namespace "gc-6837"
  May  4 07:12:48.556: INFO: Deleting pod "simpletest.rc-z855g" in namespace "gc-6837"
  May  4 07:12:48.733: INFO: Deleting pod "simpletest.rc-zdq2v" in namespace "gc-6837"
  May  4 07:12:48.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6837" for this suite. @ 05/04/23 07:12:48.938
• [58.863 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 05/04/23 07:12:49.048
  May  4 07:12:49.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename kubelet-test @ 05/04/23 07:12:49.05
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:12:49.299
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:12:49.312
  May  4 07:12:55.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-4518" for this suite. @ 05/04/23 07:12:55.69
• [6.719 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 05/04/23 07:12:55.768
  May  4 07:12:55.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 07:12:55.77
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:12:55.832
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:12:55.851
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-9b9f38c2-e2a5-4fbe-807d-71066779f690 @ 05/04/23 07:12:55.9
  STEP: Creating the pod @ 05/04/23 07:12:55.921
  STEP: Updating configmap projected-configmap-test-upd-9b9f38c2-e2a5-4fbe-807d-71066779f690 @ 05/04/23 07:13:00.102
  STEP: waiting to observe update in volume @ 05/04/23 07:13:00.12
  May  4 07:14:14.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1937" for this suite. @ 05/04/23 07:14:14.853
• [79.102 seconds]
------------------------------
SSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 05/04/23 07:14:14.87
  May  4 07:14:14.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename security-context @ 05/04/23 07:14:14.872
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:14:14.981
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:14:14.986
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 05/04/23 07:14:14.992
  STEP: Saw pod success @ 05/04/23 07:14:19.125
  May  4 07:14:19.130: INFO: Trying to get logs from node k8s-node2 pod security-context-9f56f447-df6f-4b24-b1e1-f9c3248e7814 container test-container: <nil>
  STEP: delete the pod @ 05/04/23 07:14:19.178
  May  4 07:14:19.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-9085" for this suite. @ 05/04/23 07:14:19.253
• [4.451 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 05/04/23 07:14:19.327
  May  4 07:14:19.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/04/23 07:14:19.328
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:14:19.394
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:14:19.399
  May  4 07:14:19.404: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 07:14:20.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-5169" for this suite. @ 05/04/23 07:14:20.228
• [0.932 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 05/04/23 07:14:20.26
  May  4 07:14:20.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename downward-api @ 05/04/23 07:14:20.262
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:14:20.367
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:14:20.371
  STEP: Creating a pod to test downward API volume plugin @ 05/04/23 07:14:20.375
  STEP: Saw pod success @ 05/04/23 07:14:26.422
  May  4 07:14:26.425: INFO: Trying to get logs from node k8s-node2 pod downwardapi-volume-f4148b64-c2da-4a95-897f-6b3966bb31eb container client-container: <nil>
  STEP: delete the pod @ 05/04/23 07:14:26.431
  May  4 07:14:26.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1254" for this suite. @ 05/04/23 07:14:26.514
• [6.268 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 05/04/23 07:14:26.53
  May  4 07:14:26.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename container-probe @ 05/04/23 07:14:26.531
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:14:26.582
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:14:26.586
  STEP: Creating pod liveness-ac31cecf-5393-4e4e-83b4-0050014348b5 in namespace container-probe-4313 @ 05/04/23 07:14:26.59
  May  4 07:14:28.634: INFO: Started pod liveness-ac31cecf-5393-4e4e-83b4-0050014348b5 in namespace container-probe-4313
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/04/23 07:14:28.634
  May  4 07:14:28.638: INFO: Initial restart count of pod liveness-ac31cecf-5393-4e4e-83b4-0050014348b5 is 0
  May  4 07:18:29.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/04/23 07:18:29.295
  STEP: Destroying namespace "container-probe-4313" for this suite. @ 05/04/23 07:18:29.376
• [242.901 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 05/04/23 07:18:29.435
  May  4 07:18:29.435: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename webhook @ 05/04/23 07:18:29.436
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:18:29.53
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:18:29.534
  STEP: Setting up server cert @ 05/04/23 07:18:29.598
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/04/23 07:18:30.42
  STEP: Deploying the webhook pod @ 05/04/23 07:18:30.437
  STEP: Wait for the deployment to be ready @ 05/04/23 07:18:30.49
  May  4 07:18:30.537: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  May  4 07:18:32.546: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 7, 18, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 18, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 18, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 18, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 05/04/23 07:18:34.559
  STEP: Verifying the service has paired with the endpoint @ 05/04/23 07:18:34.611
  May  4 07:18:35.611: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May  4 07:18:35.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1617-crds.webhook.example.com via the AdmissionRegistration API @ 05/04/23 07:18:36.165
  STEP: Creating a custom resource while v1 is storage version @ 05/04/23 07:18:36.211
  STEP: Patching Custom Resource Definition to set v2 as storage @ 05/04/23 07:18:38.274
  STEP: Patching the custom resource while v2 is storage version @ 05/04/23 07:18:38.303
  May  4 07:18:38.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5833" for this suite. @ 05/04/23 07:18:39.235
  STEP: Destroying namespace "webhook-markers-2594" for this suite. @ 05/04/23 07:18:39.256
• [9.858 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 05/04/23 07:18:39.295
  May  4 07:18:39.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename disruption @ 05/04/23 07:18:39.296
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:18:39.333
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:18:39.338
  STEP: creating the pdb @ 05/04/23 07:18:39.363
  STEP: Waiting for the pdb to be processed @ 05/04/23 07:18:39.432
  STEP: updating the pdb @ 05/04/23 07:18:41.451
  STEP: Waiting for the pdb to be processed @ 05/04/23 07:18:41.48
  STEP: patching the pdb @ 05/04/23 07:18:41.494
  STEP: Waiting for the pdb to be processed @ 05/04/23 07:18:41.561
  STEP: Waiting for the pdb to be deleted @ 05/04/23 07:18:43.59
  May  4 07:18:43.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9586" for this suite. @ 05/04/23 07:18:43.598
• [4.339 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 05/04/23 07:18:43.635
  May  4 07:18:43.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename configmap @ 05/04/23 07:18:43.636
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:18:43.697
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:18:43.704
  STEP: Creating configMap with name configmap-test-volume-4d2a8d67-4299-48f0-b8db-6bc70a64d608 @ 05/04/23 07:18:43.708
  STEP: Creating a pod to test consume configMaps @ 05/04/23 07:18:43.721
  STEP: Saw pod success @ 05/04/23 07:18:47.786
  May  4 07:18:47.789: INFO: Trying to get logs from node k8s-node2 pod pod-configmaps-94ca91f6-c498-4698-a932-b43633610336 container agnhost-container: <nil>
  STEP: delete the pod @ 05/04/23 07:18:47.813
  May  4 07:18:47.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-34" for this suite. @ 05/04/23 07:18:47.924
• [4.323 seconds]
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 05/04/23 07:18:47.958
  May  4 07:18:47.958: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename webhook @ 05/04/23 07:18:47.96
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:18:48.02
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:18:48.024
  STEP: Setting up server cert @ 05/04/23 07:18:48.099
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/04/23 07:18:48.722
  STEP: Deploying the webhook pod @ 05/04/23 07:18:48.746
  STEP: Wait for the deployment to be ready @ 05/04/23 07:18:48.788
  May  4 07:18:48.817: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/04/23 07:18:50.827
  STEP: Verifying the service has paired with the endpoint @ 05/04/23 07:18:50.875
  May  4 07:18:51.875: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 05/04/23 07:18:52.18
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/04/23 07:18:52.24
  STEP: Deleting the collection of validation webhooks @ 05/04/23 07:18:52.286
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/04/23 07:18:52.443
  May  4 07:18:52.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9143" for this suite. @ 05/04/23 07:18:52.64
  STEP: Destroying namespace "webhook-markers-9348" for this suite. @ 05/04/23 07:18:52.674
• [4.784 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 05/04/23 07:18:52.743
  May  4 07:18:52.743: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename secrets @ 05/04/23 07:18:52.744
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:18:52.778
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:18:52.781
  STEP: Creating secret with name secret-test-map-9f561664-5086-4e8d-91c8-d156d5f0c482 @ 05/04/23 07:18:52.799
  STEP: Creating a pod to test consume secrets @ 05/04/23 07:18:52.812
  STEP: Saw pod success @ 05/04/23 07:18:56.84
  May  4 07:18:56.843: INFO: Trying to get logs from node k8s-node2 pod pod-secrets-541623d0-40dc-441b-81eb-0a3bf643ff89 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/04/23 07:18:56.85
  May  4 07:18:56.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1742" for this suite. @ 05/04/23 07:18:56.923
• [4.206 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 05/04/23 07:18:56.951
  May  4 07:18:56.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename webhook @ 05/04/23 07:18:56.952
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:18:56.984
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:18:56.988
  STEP: Setting up server cert @ 05/04/23 07:18:57.08
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/04/23 07:18:58.157
  STEP: Deploying the webhook pod @ 05/04/23 07:18:58.189
  STEP: Wait for the deployment to be ready @ 05/04/23 07:18:58.245
  May  4 07:18:58.259: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  May  4 07:19:00.290: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 7, 18, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 18, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 18, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 18, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 05/04/23 07:19:02.308
  STEP: Verifying the service has paired with the endpoint @ 05/04/23 07:19:02.385
  May  4 07:19:03.385: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 05/04/23 07:19:03.39
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 05/04/23 07:19:03.392
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 05/04/23 07:19:03.392
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 05/04/23 07:19:03.392
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 05/04/23 07:19:03.393
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 05/04/23 07:19:03.394
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 05/04/23 07:19:03.395
  May  4 07:19:03.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1117" for this suite. @ 05/04/23 07:19:03.601
  STEP: Destroying namespace "webhook-markers-8338" for this suite. @ 05/04/23 07:19:03.622
• [6.740 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 05/04/23 07:19:03.694
  May  4 07:19:03.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename container-probe @ 05/04/23 07:19:03.696
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:19:03.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:19:03.773
  STEP: Creating pod busybox-e576bb04-30ac-4864-9d49-370bcf9ecc23 in namespace container-probe-7555 @ 05/04/23 07:19:03.778
  May  4 07:19:05.820: INFO: Started pod busybox-e576bb04-30ac-4864-9d49-370bcf9ecc23 in namespace container-probe-7555
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/04/23 07:19:05.82
  May  4 07:19:05.823: INFO: Initial restart count of pod busybox-e576bb04-30ac-4864-9d49-370bcf9ecc23 is 0
  May  4 07:23:06.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/04/23 07:23:06.452
  STEP: Destroying namespace "container-probe-7555" for this suite. @ 05/04/23 07:23:06.512
• [242.859 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 05/04/23 07:23:06.555
  May  4 07:23:06.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/04/23 07:23:06.557
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:23:06.611
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:23:06.616
  May  4 07:23:06.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/04/23 07:23:09.262
  May  4 07:23:09.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-9546 --namespace=crd-publish-openapi-9546 create -f -'
  May  4 07:23:10.544: INFO: stderr: ""
  May  4 07:23:10.544: INFO: stdout: "e2e-test-crd-publish-openapi-2771-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  May  4 07:23:10.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-9546 --namespace=crd-publish-openapi-9546 delete e2e-test-crd-publish-openapi-2771-crds test-cr'
  May  4 07:23:10.684: INFO: stderr: ""
  May  4 07:23:10.684: INFO: stdout: "e2e-test-crd-publish-openapi-2771-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  May  4 07:23:10.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-9546 --namespace=crd-publish-openapi-9546 apply -f -'
  May  4 07:23:11.131: INFO: stderr: ""
  May  4 07:23:11.131: INFO: stdout: "e2e-test-crd-publish-openapi-2771-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  May  4 07:23:11.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-9546 --namespace=crd-publish-openapi-9546 delete e2e-test-crd-publish-openapi-2771-crds test-cr'
  May  4 07:23:11.275: INFO: stderr: ""
  May  4 07:23:11.275: INFO: stdout: "e2e-test-crd-publish-openapi-2771-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 05/04/23 07:23:11.275
  May  4 07:23:11.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-9546 explain e2e-test-crd-publish-openapi-2771-crds'
  May  4 07:23:11.731: INFO: stderr: ""
  May  4 07:23:11.731: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-2771-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  May  4 07:23:13.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9546" for this suite. @ 05/04/23 07:23:13.415
• [6.883 seconds]
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 05/04/23 07:23:13.443
  May  4 07:23:13.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 07:23:13.445
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:23:13.513
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:23:13.518
  STEP: Creating projection with secret that has name projected-secret-test-5fb452b9-44b8-4cdb-a0d3-7842cee38a05 @ 05/04/23 07:23:13.521
  STEP: Creating a pod to test consume secrets @ 05/04/23 07:23:13.536
  STEP: Saw pod success @ 05/04/23 07:23:17.567
  May  4 07:23:17.570: INFO: Trying to get logs from node k8s-node2 pod pod-projected-secrets-ba4d6531-918d-43f9-a08d-e834d0944e09 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/04/23 07:23:17.59
  May  4 07:23:17.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9659" for this suite. @ 05/04/23 07:23:17.673
• [4.254 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 05/04/23 07:23:17.699
  May  4 07:23:17.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename container-probe @ 05/04/23 07:23:17.7
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:23:17.74
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:23:17.744
  STEP: Creating pod busybox-e8bf39ba-db1b-45aa-8002-406c09bc2fb9 in namespace container-probe-123 @ 05/04/23 07:23:17.75
  May  4 07:23:19.799: INFO: Started pod busybox-e8bf39ba-db1b-45aa-8002-406c09bc2fb9 in namespace container-probe-123
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/04/23 07:23:19.799
  May  4 07:23:19.802: INFO: Initial restart count of pod busybox-e8bf39ba-db1b-45aa-8002-406c09bc2fb9 is 0
  May  4 07:24:09.992: INFO: Restart count of pod container-probe-123/busybox-e8bf39ba-db1b-45aa-8002-406c09bc2fb9 is now 1 (50.189768663s elapsed)
  May  4 07:24:09.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/04/23 07:24:10
  STEP: Destroying namespace "container-probe-123" for this suite. @ 05/04/23 07:24:10.06
• [52.377 seconds]
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 05/04/23 07:24:10.077
  May  4 07:24:10.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename podtemplate @ 05/04/23 07:24:10.079
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:24:10.118
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:24:10.122
  STEP: Create set of pod templates @ 05/04/23 07:24:10.178
  May  4 07:24:10.227: INFO: created test-podtemplate-1
  May  4 07:24:10.241: INFO: created test-podtemplate-2
  May  4 07:24:10.255: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 05/04/23 07:24:10.255
  STEP: delete collection of pod templates @ 05/04/23 07:24:10.265
  May  4 07:24:10.265: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 05/04/23 07:24:10.347
  May  4 07:24:10.347: INFO: requesting list of pod templates to confirm quantity
  May  4 07:24:10.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-1296" for this suite. @ 05/04/23 07:24:10.356
• [0.295 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 05/04/23 07:24:10.373
  May  4 07:24:10.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 05/04/23 07:24:10.375
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:24:10.435
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:24:10.439
  STEP: creating a target pod @ 05/04/23 07:24:10.442
  STEP: adding an ephemeral container @ 05/04/23 07:24:14.476
  STEP: checking pod container endpoints @ 05/04/23 07:24:16.513
  May  4 07:24:16.513: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-790 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 07:24:16.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 07:24:16.513: INFO: ExecWithOptions: Clientset creation
  May  4 07:24:16.514: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-790/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  May  4 07:24:16.600: INFO: Exec stderr: ""
  May  4 07:24:16.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-790" for this suite. @ 05/04/23 07:24:16.615
• [6.256 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 05/04/23 07:24:16.635
  May  4 07:24:16.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename events @ 05/04/23 07:24:16.636
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:24:16.702
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:24:16.707
  STEP: creating a test event @ 05/04/23 07:24:16.712
  STEP: listing events in all namespaces @ 05/04/23 07:24:16.739
  STEP: listing events in test namespace @ 05/04/23 07:24:16.747
  STEP: listing events with field selection filtering on source @ 05/04/23 07:24:16.762
  STEP: listing events with field selection filtering on reportingController @ 05/04/23 07:24:16.773
  STEP: getting the test event @ 05/04/23 07:24:16.804
  STEP: patching the test event @ 05/04/23 07:24:16.814
  STEP: getting the test event @ 05/04/23 07:24:16.848
  STEP: updating the test event @ 05/04/23 07:24:16.853
  STEP: getting the test event @ 05/04/23 07:24:16.891
  STEP: deleting the test event @ 05/04/23 07:24:16.895
  STEP: listing events in all namespaces @ 05/04/23 07:24:16.93
  STEP: listing events in test namespace @ 05/04/23 07:24:16.934
  May  4 07:24:16.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-8731" for this suite. @ 05/04/23 07:24:16.942
• [0.324 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 05/04/23 07:24:16.964
  May  4 07:24:16.964: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename runtimeclass @ 05/04/23 07:24:16.965
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:24:17.022
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:24:17.026
  STEP: getting /apis @ 05/04/23 07:24:17.033
  STEP: getting /apis/node.k8s.io @ 05/04/23 07:24:17.039
  STEP: getting /apis/node.k8s.io/v1 @ 05/04/23 07:24:17.041
  STEP: creating @ 05/04/23 07:24:17.042
  STEP: watching @ 05/04/23 07:24:17.103
  May  4 07:24:17.103: INFO: starting watch
  STEP: getting @ 05/04/23 07:24:17.148
  STEP: listing @ 05/04/23 07:24:17.151
  STEP: patching @ 05/04/23 07:24:17.153
  STEP: updating @ 05/04/23 07:24:17.195
  May  4 07:24:17.215: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 05/04/23 07:24:17.215
  STEP: deleting a collection @ 05/04/23 07:24:17.237
  May  4 07:24:17.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-4038" for this suite. @ 05/04/23 07:24:17.274
• [0.332 seconds]
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 05/04/23 07:24:17.296
  May  4 07:24:17.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename var-expansion @ 05/04/23 07:24:17.299
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:24:17.352
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:24:17.356
  STEP: Creating a pod to test env composition @ 05/04/23 07:24:17.36
  STEP: Saw pod success @ 05/04/23 07:24:21.389
  May  4 07:24:21.393: INFO: Trying to get logs from node k8s-node2 pod var-expansion-93dbe5a9-6e21-4a3c-a29a-828a9251a4db container dapi-container: <nil>
  STEP: delete the pod @ 05/04/23 07:24:21.401
  May  4 07:24:21.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3729" for this suite. @ 05/04/23 07:24:21.446
• [4.164 seconds]
------------------------------
SS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 05/04/23 07:24:21.46
  May  4 07:24:21.461: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename job @ 05/04/23 07:24:21.462
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:24:21.517
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:24:21.523
  STEP: Creating a job @ 05/04/23 07:24:21.549
  STEP: Ensure pods equal to parallelism count is attached to the job @ 05/04/23 07:24:21.576
  STEP: patching /status @ 05/04/23 07:24:25.582
  STEP: updating /status @ 05/04/23 07:24:25.621
  STEP: get /status @ 05/04/23 07:24:25.634
  May  4 07:24:25.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6430" for this suite. @ 05/04/23 07:24:25.67
• [4.227 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 05/04/23 07:24:25.689
  May  4 07:24:25.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename services @ 05/04/23 07:24:25.691
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:24:25.807
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:24:25.812
  STEP: fetching services @ 05/04/23 07:24:25.816
  May  4 07:24:25.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2159" for this suite. @ 05/04/23 07:24:25.825
• [0.155 seconds]
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 05/04/23 07:24:25.844
  May  4 07:24:25.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename configmap @ 05/04/23 07:24:25.846
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:24:25.942
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:24:25.947
  May  4 07:24:26.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4589" for this suite. @ 05/04/23 07:24:26.104
• [0.281 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 05/04/23 07:24:26.126
  May  4 07:24:26.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/04/23 07:24:26.127
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:24:26.221
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:24:26.225
  May  4 07:24:26.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 07:24:29.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2061" for this suite. @ 05/04/23 07:24:29.426
• [3.346 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 05/04/23 07:24:29.477
  May  4 07:24:29.477: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename daemonsets @ 05/04/23 07:24:29.478
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:24:29.537
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:24:29.542
  May  4 07:24:29.605: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/04/23 07:24:29.635
  May  4 07:24:29.640: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:24:29.642: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 07:24:29.642: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  May  4 07:24:30.659: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:24:30.676: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 07:24:30.676: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  May  4 07:24:31.655: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:24:31.682: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 07:24:31.682: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  May  4 07:24:32.648: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:24:32.669: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  4 07:24:32.669: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Update daemon pods image. @ 05/04/23 07:24:32.681
  STEP: Check that daemon pods images are updated. @ 05/04/23 07:24:32.721
  May  4 07:24:32.746: INFO: Wrong image for pod: daemon-set-l5tk4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May  4 07:24:32.746: INFO: Wrong image for pod: daemon-set-q9cxd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May  4 07:24:32.750: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:24:33.755: INFO: Wrong image for pod: daemon-set-l5tk4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May  4 07:24:33.759: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:24:34.755: INFO: Wrong image for pod: daemon-set-l5tk4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May  4 07:24:34.759: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:24:35.758: INFO: Pod daemon-set-4hddp is not available
  May  4 07:24:35.758: INFO: Wrong image for pod: daemon-set-l5tk4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May  4 07:24:35.764: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:24:36.755: INFO: Pod daemon-set-4hddp is not available
  May  4 07:24:36.755: INFO: Wrong image for pod: daemon-set-l5tk4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May  4 07:24:36.759: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:24:37.760: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:24:38.763: INFO: Pod daemon-set-lvmr2 is not available
  May  4 07:24:38.769: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Check that daemon pods are still running on every node of the cluster. @ 05/04/23 07:24:38.769
  May  4 07:24:38.775: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:24:38.779: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  4 07:24:38.780: INFO: Node k8s-node2 is running 0 daemon pod, expected 1
  May  4 07:24:39.786: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:24:39.790: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  4 07:24:39.790: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/04/23 07:24:39.813
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-917, will wait for the garbage collector to delete the pods @ 05/04/23 07:24:39.813
  May  4 07:24:39.880: INFO: Deleting DaemonSet.extensions daemon-set took: 13.854051ms
  May  4 07:24:39.981: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.833979ms
  May  4 07:24:42.785: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 07:24:42.785: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May  4 07:24:42.788: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"75332"},"items":null}

  May  4 07:24:42.790: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"75332"},"items":null}

  May  4 07:24:42.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-917" for this suite. @ 05/04/23 07:24:42.805
• [13.343 seconds]
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 05/04/23 07:24:42.819
  May  4 07:24:42.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename var-expansion @ 05/04/23 07:24:42.821
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:24:42.895
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:24:42.899
  May  4 07:24:44.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  4 07:24:44.935: INFO: Deleting pod "var-expansion-7c7ae851-0268-430d-9764-09ba5673619a" in namespace "var-expansion-6455"
  May  4 07:24:44.955: INFO: Wait up to 5m0s for pod "var-expansion-7c7ae851-0268-430d-9764-09ba5673619a" to be fully deleted
  STEP: Destroying namespace "var-expansion-6455" for this suite. @ 05/04/23 07:24:48.981
• [6.176 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 05/04/23 07:24:48.997
  May  4 07:24:48.997: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename events @ 05/04/23 07:24:48.999
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:24:49.031
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:24:49.037
  STEP: Create set of events @ 05/04/23 07:24:49.071
  May  4 07:24:49.084: INFO: created test-event-1
  May  4 07:24:49.097: INFO: created test-event-2
  May  4 07:24:49.110: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 05/04/23 07:24:49.11
  STEP: delete collection of events @ 05/04/23 07:24:49.113
  May  4 07:24:49.113: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 05/04/23 07:24:49.19
  May  4 07:24:49.191: INFO: requesting list of events to confirm quantity
  May  4 07:24:49.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-3765" for this suite. @ 05/04/23 07:24:49.199
• [0.215 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 05/04/23 07:24:49.214
  May  4 07:24:49.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename container-runtime @ 05/04/23 07:24:49.215
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:24:49.288
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:24:49.292
  STEP: create the container @ 05/04/23 07:24:49.295
  W0504 07:24:49.356938      20 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/04/23 07:24:49.357
  STEP: get the container status @ 05/04/23 07:24:54.396
  STEP: the container should be terminated @ 05/04/23 07:24:54.399
  STEP: the termination message should be set @ 05/04/23 07:24:54.399
  May  4 07:24:54.399: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 05/04/23 07:24:54.399
  May  4 07:24:54.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-1439" for this suite. @ 05/04/23 07:24:54.527
• [5.345 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 05/04/23 07:24:54.561
  May  4 07:24:54.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename dns @ 05/04/23 07:24:54.562
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:24:54.598
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:24:54.603
  STEP: Creating a test headless service @ 05/04/23 07:24:54.611
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7743.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7743.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 05/04/23 07:24:54.651
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7743.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7743.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 05/04/23 07:24:54.651
  STEP: creating a pod to probe DNS @ 05/04/23 07:24:54.651
  STEP: submitting the pod to kubernetes @ 05/04/23 07:24:54.651
  STEP: retrieving the pod @ 05/04/23 07:24:58.727
  STEP: looking for the results for each expected name from probers @ 05/04/23 07:24:58.73
  May  4 07:24:58.746: INFO: DNS probes using dns-7743/dns-test-0aadcb5d-3107-4b88-a59e-7809035b7764 succeeded

  May  4 07:24:58.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/04/23 07:24:58.751
  STEP: deleting the test headless service @ 05/04/23 07:24:58.81
  STEP: Destroying namespace "dns-7743" for this suite. @ 05/04/23 07:24:58.931
• [4.414 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 05/04/23 07:24:58.975
  May  4 07:24:58.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename webhook @ 05/04/23 07:24:58.976
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:24:59.014
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:24:59.018
  STEP: Setting up server cert @ 05/04/23 07:24:59.159
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/04/23 07:25:00.366
  STEP: Deploying the webhook pod @ 05/04/23 07:25:00.383
  STEP: Wait for the deployment to be ready @ 05/04/23 07:25:00.434
  May  4 07:25:00.469: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  May  4 07:25:02.488: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 7, 25, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 25, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 25, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 25, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 05/04/23 07:25:04.493
  STEP: Verifying the service has paired with the endpoint @ 05/04/23 07:25:04.539
  May  4 07:25:05.540: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 05/04/23 07:25:05.544
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/04/23 07:25:05.586
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 05/04/23 07:25:05.599
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/04/23 07:25:05.628
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 05/04/23 07:25:05.679
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/04/23 07:25:05.705
  May  4 07:25:05.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6608" for this suite. @ 05/04/23 07:25:05.876
  STEP: Destroying namespace "webhook-markers-6420" for this suite. @ 05/04/23 07:25:05.95
• [7.008 seconds]
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 05/04/23 07:25:05.984
  May  4 07:25:05.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename subpath @ 05/04/23 07:25:05.985
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:25:06.088
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:25:06.092
  STEP: Setting up data @ 05/04/23 07:25:06.095
  STEP: Creating pod pod-subpath-test-configmap-rvp4 @ 05/04/23 07:25:06.131
  STEP: Creating a pod to test atomic-volume-subpath @ 05/04/23 07:25:06.131
  STEP: Saw pod success @ 05/04/23 07:25:32.215
  May  4 07:25:32.218: INFO: Trying to get logs from node k8s-node2 pod pod-subpath-test-configmap-rvp4 container test-container-subpath-configmap-rvp4: <nil>
  STEP: delete the pod @ 05/04/23 07:25:32.227
  STEP: Deleting pod pod-subpath-test-configmap-rvp4 @ 05/04/23 07:25:32.282
  May  4 07:25:32.282: INFO: Deleting pod "pod-subpath-test-configmap-rvp4" in namespace "subpath-5539"
  May  4 07:25:32.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5539" for this suite. @ 05/04/23 07:25:32.29
• [26.322 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 05/04/23 07:25:32.307
  May  4 07:25:32.307: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename secrets @ 05/04/23 07:25:32.308
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:25:32.375
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:25:32.378
  STEP: Creating secret with name secret-test-map-0617a27a-d6d4-49f8-b68b-7b4e67e98375 @ 05/04/23 07:25:32.382
  STEP: Creating a pod to test consume secrets @ 05/04/23 07:25:32.399
  STEP: Saw pod success @ 05/04/23 07:25:36.435
  May  4 07:25:36.441: INFO: Trying to get logs from node k8s-node2 pod pod-secrets-78831b53-00f1-478a-9af4-7b74c4d03787 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/04/23 07:25:36.447
  May  4 07:25:36.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1052" for this suite. @ 05/04/23 07:25:36.491
• [4.197 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 05/04/23 07:25:36.508
  May  4 07:25:36.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename field-validation @ 05/04/23 07:25:36.509
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:25:36.587
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:25:36.591
  May  4 07:25:36.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 07:25:39.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5919" for this suite. @ 05/04/23 07:25:39.265
• [2.778 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 05/04/23 07:25:39.29
  May  4 07:25:39.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 07:25:39.291
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:25:39.322
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:25:39.326
  STEP: Creating configMap with name projected-configmap-test-volume-map-899b8ea7-19ae-4240-afd5-708f39947665 @ 05/04/23 07:25:39.357
  STEP: Creating a pod to test consume configMaps @ 05/04/23 07:25:39.372
  STEP: Saw pod success @ 05/04/23 07:25:43.408
  May  4 07:25:43.413: INFO: Trying to get logs from node k8s-node2 pod pod-projected-configmaps-edf7ae49-b29b-4dd9-becc-d14e0e532da4 container agnhost-container: <nil>
  STEP: delete the pod @ 05/04/23 07:25:43.425
  May  4 07:25:43.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3553" for this suite. @ 05/04/23 07:25:43.487
• [4.233 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 05/04/23 07:25:43.525
  May  4 07:25:43.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename ingressclass @ 05/04/23 07:25:43.527
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:25:43.561
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:25:43.565
  STEP: getting /apis @ 05/04/23 07:25:43.586
  STEP: getting /apis/networking.k8s.io @ 05/04/23 07:25:43.593
  STEP: getting /apis/networking.k8s.iov1 @ 05/04/23 07:25:43.594
  STEP: creating @ 05/04/23 07:25:43.595
  STEP: getting @ 05/04/23 07:25:43.658
  STEP: listing @ 05/04/23 07:25:43.661
  STEP: watching @ 05/04/23 07:25:43.663
  May  4 07:25:43.663: INFO: starting watch
  STEP: patching @ 05/04/23 07:25:43.665
  STEP: updating @ 05/04/23 07:25:43.689
  May  4 07:25:43.702: INFO: waiting for watch events with expected annotations
  May  4 07:25:43.702: INFO: saw patched and updated annotations
  STEP: deleting @ 05/04/23 07:25:43.702
  STEP: deleting a collection @ 05/04/23 07:25:43.72
  May  4 07:25:43.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-9407" for this suite. @ 05/04/23 07:25:43.772
• [0.263 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 05/04/23 07:25:43.79
  May  4 07:25:43.790: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename webhook @ 05/04/23 07:25:43.791
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:25:43.824
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:25:43.828
  STEP: Setting up server cert @ 05/04/23 07:25:43.913
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/04/23 07:25:45.137
  STEP: Deploying the webhook pod @ 05/04/23 07:25:45.152
  STEP: Wait for the deployment to be ready @ 05/04/23 07:25:45.218
  May  4 07:25:45.246: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/04/23 07:25:47.257
  STEP: Verifying the service has paired with the endpoint @ 05/04/23 07:25:47.319
  May  4 07:25:48.320: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 05/04/23 07:25:48.323
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 05/04/23 07:25:48.372
  STEP: Creating a dummy validating-webhook-configuration object @ 05/04/23 07:25:48.422
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 05/04/23 07:25:48.441
  STEP: Creating a dummy mutating-webhook-configuration object @ 05/04/23 07:25:48.456
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 05/04/23 07:25:48.487
  May  4 07:25:48.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1594" for this suite. @ 05/04/23 07:25:48.81
  STEP: Destroying namespace "webhook-markers-8756" for this suite. @ 05/04/23 07:25:48.834
• [5.075 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 05/04/23 07:25:48.868
  May  4 07:25:48.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename endpointslice @ 05/04/23 07:25:48.87
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:25:48.946
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:25:48.952
  STEP: getting /apis @ 05/04/23 07:25:48.957
  STEP: getting /apis/discovery.k8s.io @ 05/04/23 07:25:48.964
  STEP: getting /apis/discovery.k8s.iov1 @ 05/04/23 07:25:48.966
  STEP: creating @ 05/04/23 07:25:48.968
  STEP: getting @ 05/04/23 07:25:49.046
  STEP: listing @ 05/04/23 07:25:49.057
  STEP: watching @ 05/04/23 07:25:49.061
  May  4 07:25:49.061: INFO: starting watch
  STEP: cluster-wide listing @ 05/04/23 07:25:49.063
  STEP: cluster-wide watching @ 05/04/23 07:25:49.068
  May  4 07:25:49.068: INFO: starting watch
  STEP: patching @ 05/04/23 07:25:49.07
  STEP: updating @ 05/04/23 07:25:49.088
  May  4 07:25:49.108: INFO: waiting for watch events with expected annotations
  May  4 07:25:49.108: INFO: saw patched and updated annotations
  STEP: deleting @ 05/04/23 07:25:49.108
  STEP: deleting a collection @ 05/04/23 07:25:49.133
  May  4 07:25:49.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-7854" for this suite. @ 05/04/23 07:25:49.198
• [0.389 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 05/04/23 07:25:49.267
  May  4 07:25:49.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename svcaccounts @ 05/04/23 07:25:49.268
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:25:49.321
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:25:49.324
  STEP: creating a ServiceAccount @ 05/04/23 07:25:49.353
  STEP: watching for the ServiceAccount to be added @ 05/04/23 07:25:49.376
  STEP: patching the ServiceAccount @ 05/04/23 07:25:49.38
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 05/04/23 07:25:49.417
  STEP: deleting the ServiceAccount @ 05/04/23 07:25:49.442
  May  4 07:25:49.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9414" for this suite. @ 05/04/23 07:25:49.499
• [0.274 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 05/04/23 07:25:49.541
  May  4 07:25:49.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename emptydir @ 05/04/23 07:25:49.542
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:25:49.585
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:25:49.588
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 05/04/23 07:25:49.606
  STEP: Saw pod success @ 05/04/23 07:25:53.636
  May  4 07:25:53.639: INFO: Trying to get logs from node k8s-node2 pod pod-81c87085-4367-41e7-a238-c8299d18dbfe container test-container: <nil>
  STEP: delete the pod @ 05/04/23 07:25:53.646
  May  4 07:25:53.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-986" for this suite. @ 05/04/23 07:25:53.707
• [4.181 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 05/04/23 07:25:53.723
  May  4 07:25:53.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 05/04/23 07:25:53.725
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:25:53.789
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:25:53.793
  STEP: Setting up the test @ 05/04/23 07:25:53.799
  STEP: Creating hostNetwork=false pod @ 05/04/23 07:25:53.799
  STEP: Creating hostNetwork=true pod @ 05/04/23 07:25:57.9
  STEP: Running the test @ 05/04/23 07:26:01.945
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 05/04/23 07:26:01.945
  May  4 07:26:01.945: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7597 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 07:26:01.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 07:26:01.946: INFO: ExecWithOptions: Clientset creation
  May  4 07:26:01.946: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7597/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May  4 07:26:02.029: INFO: Exec stderr: ""
  May  4 07:26:02.029: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7597 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 07:26:02.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 07:26:02.030: INFO: ExecWithOptions: Clientset creation
  May  4 07:26:02.030: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7597/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May  4 07:26:02.109: INFO: Exec stderr: ""
  May  4 07:26:02.109: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7597 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 07:26:02.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 07:26:02.110: INFO: ExecWithOptions: Clientset creation
  May  4 07:26:02.110: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7597/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May  4 07:26:02.192: INFO: Exec stderr: ""
  May  4 07:26:02.192: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7597 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 07:26:02.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 07:26:02.193: INFO: ExecWithOptions: Clientset creation
  May  4 07:26:02.193: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7597/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May  4 07:26:02.267: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 05/04/23 07:26:02.267
  May  4 07:26:02.267: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7597 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 07:26:02.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 07:26:02.268: INFO: ExecWithOptions: Clientset creation
  May  4 07:26:02.268: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7597/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  May  4 07:26:02.349: INFO: Exec stderr: ""
  May  4 07:26:02.349: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7597 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 07:26:02.349: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 07:26:02.350: INFO: ExecWithOptions: Clientset creation
  May  4 07:26:02.350: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7597/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  May  4 07:26:02.432: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 05/04/23 07:26:02.432
  May  4 07:26:02.432: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7597 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 07:26:02.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 07:26:02.433: INFO: ExecWithOptions: Clientset creation
  May  4 07:26:02.433: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7597/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May  4 07:26:02.514: INFO: Exec stderr: ""
  May  4 07:26:02.514: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7597 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 07:26:02.514: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 07:26:02.515: INFO: ExecWithOptions: Clientset creation
  May  4 07:26:02.515: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7597/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May  4 07:26:02.593: INFO: Exec stderr: ""
  May  4 07:26:02.593: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7597 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 07:26:02.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 07:26:02.594: INFO: ExecWithOptions: Clientset creation
  May  4 07:26:02.594: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7597/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May  4 07:26:02.676: INFO: Exec stderr: ""
  May  4 07:26:02.676: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7597 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 07:26:02.676: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 07:26:02.677: INFO: ExecWithOptions: Clientset creation
  May  4 07:26:02.677: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7597/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May  4 07:26:02.762: INFO: Exec stderr: ""
  May  4 07:26:02.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-7597" for this suite. @ 05/04/23 07:26:02.767
• [9.058 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 05/04/23 07:26:02.782
  May  4 07:26:02.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename webhook @ 05/04/23 07:26:02.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:26:02.834
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:26:02.838
  STEP: Setting up server cert @ 05/04/23 07:26:02.935
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/04/23 07:26:04.202
  STEP: Deploying the webhook pod @ 05/04/23 07:26:04.224
  STEP: Wait for the deployment to be ready @ 05/04/23 07:26:04.277
  May  4 07:26:04.318: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/04/23 07:26:06.328
  STEP: Verifying the service has paired with the endpoint @ 05/04/23 07:26:06.376
  May  4 07:26:07.376: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 05/04/23 07:26:07.382
  STEP: create a configmap that should be updated by the webhook @ 05/04/23 07:26:07.424
  May  4 07:26:07.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-476" for this suite. @ 05/04/23 07:26:07.68
  STEP: Destroying namespace "webhook-markers-6628" for this suite. @ 05/04/23 07:26:07.722
• [4.957 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 05/04/23 07:26:07.742
  May  4 07:26:07.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename emptydir-wrapper @ 05/04/23 07:26:07.744
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:26:07.78
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:26:07.784
  May  4 07:26:11.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 05/04/23 07:26:11.902
  STEP: Cleaning up the configmap @ 05/04/23 07:26:11.927
  STEP: Cleaning up the pod @ 05/04/23 07:26:11.942
  STEP: Destroying namespace "emptydir-wrapper-4299" for this suite. @ 05/04/23 07:26:11.988
• [4.297 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 05/04/23 07:26:12.045
  May  4 07:26:12.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename disruption @ 05/04/23 07:26:12.046
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:26:12.125
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:26:12.129
  STEP: Creating a kubernetes client @ 05/04/23 07:26:12.133
  May  4 07:26:12.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename disruption-2 @ 05/04/23 07:26:12.134
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:26:12.217
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:26:12.222
  STEP: Waiting for the pdb to be processed @ 05/04/23 07:26:12.254
  STEP: Waiting for the pdb to be processed @ 05/04/23 07:26:14.274
  STEP: Waiting for the pdb to be processed @ 05/04/23 07:26:16.307
  STEP: listing a collection of PDBs across all namespaces @ 05/04/23 07:26:18.345
  STEP: listing a collection of PDBs in namespace disruption-2094 @ 05/04/23 07:26:18.356
  STEP: deleting a collection of PDBs @ 05/04/23 07:26:18.359
  STEP: Waiting for the PDB collection to be deleted @ 05/04/23 07:26:18.386
  May  4 07:26:18.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  4 07:26:18.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-9932" for this suite. @ 05/04/23 07:26:18.399
  STEP: Destroying namespace "disruption-2094" for this suite. @ 05/04/23 07:26:18.416
• [6.436 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 05/04/23 07:26:18.482
  May  4 07:26:18.482: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename emptydir @ 05/04/23 07:26:18.484
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:26:18.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:26:18.529
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 05/04/23 07:26:18.532
  STEP: Saw pod success @ 05/04/23 07:26:22.603
  May  4 07:26:22.606: INFO: Trying to get logs from node k8s-node2 pod pod-aaca7f58-d553-4be0-93ee-53664ba19df1 container test-container: <nil>
  STEP: delete the pod @ 05/04/23 07:26:22.613
  May  4 07:26:22.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9535" for this suite. @ 05/04/23 07:26:22.651
• [4.195 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 05/04/23 07:26:22.678
  May  4 07:26:22.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 07:26:22.679
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:26:22.711
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:26:22.715
  STEP: Creating projection with secret that has name projected-secret-test-7d68747b-11b3-478d-a8bf-49029832ae81 @ 05/04/23 07:26:22.72
  STEP: Creating a pod to test consume secrets @ 05/04/23 07:26:22.753
  STEP: Saw pod success @ 05/04/23 07:26:26.784
  May  4 07:26:26.787: INFO: Trying to get logs from node k8s-node2 pod pod-projected-secrets-37e1fa7a-a708-4212-a999-d6bb620ecde1 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/04/23 07:26:26.793
  May  4 07:26:26.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6900" for this suite. @ 05/04/23 07:26:26.837
• [4.218 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 05/04/23 07:26:26.897
  May  4 07:26:26.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename var-expansion @ 05/04/23 07:26:26.898
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:26:26.958
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:26:26.962
  STEP: Creating a pod to test substitution in container's command @ 05/04/23 07:26:26.966
  STEP: Saw pod success @ 05/04/23 07:26:31.048
  May  4 07:26:31.052: INFO: Trying to get logs from node k8s-node2 pod var-expansion-00535652-7640-4126-b426-372259ca38dd container dapi-container: <nil>
  STEP: delete the pod @ 05/04/23 07:26:31.059
  May  4 07:26:31.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1504" for this suite. @ 05/04/23 07:26:31.191
• [4.312 seconds]
------------------------------
SS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 05/04/23 07:26:31.209
  May  4 07:26:31.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename downward-api @ 05/04/23 07:26:31.21
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:26:31.252
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:26:31.257
  STEP: Creating a pod to test downward api env vars @ 05/04/23 07:26:31.29
  STEP: Saw pod success @ 05/04/23 07:26:37.337
  May  4 07:26:37.339: INFO: Trying to get logs from node k8s-node2 pod downward-api-1cb7b16c-86a3-4386-ae3c-2362468125af container dapi-container: <nil>
  STEP: delete the pod @ 05/04/23 07:26:37.348
  May  4 07:26:37.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4673" for this suite. @ 05/04/23 07:26:37.399
• [6.205 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 05/04/23 07:26:37.415
  May  4 07:26:37.415: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename sysctl @ 05/04/23 07:26:37.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:26:37.455
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:26:37.459
  STEP: Creating a pod with one valid and two invalid sysctls @ 05/04/23 07:26:37.465
  May  4 07:26:37.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-6341" for this suite. @ 05/04/23 07:26:37.491
• [0.089 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 05/04/23 07:26:37.507
  May  4 07:26:37.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename emptydir @ 05/04/23 07:26:37.51
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:26:37.565
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:26:37.569
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 05/04/23 07:26:37.574
  STEP: Saw pod success @ 05/04/23 07:26:41.644
  May  4 07:26:41.647: INFO: Trying to get logs from node k8s-node2 pod pod-8a590aa9-5430-45c3-b241-481e9d4f479b container test-container: <nil>
  STEP: delete the pod @ 05/04/23 07:26:41.654
  May  4 07:26:41.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1681" for this suite. @ 05/04/23 07:26:41.718
• [4.226 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 05/04/23 07:26:41.735
  May  4 07:26:41.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename webhook @ 05/04/23 07:26:41.736
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:26:41.827
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:26:41.831
  STEP: Setting up server cert @ 05/04/23 07:26:41.907
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/04/23 07:26:42.643
  STEP: Deploying the webhook pod @ 05/04/23 07:26:42.667
  STEP: Wait for the deployment to be ready @ 05/04/23 07:26:42.736
  May  4 07:26:42.753: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  May  4 07:26:44.764: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 7, 26, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 26, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 26, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 26, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 05/04/23 07:26:46.776
  STEP: Verifying the service has paired with the endpoint @ 05/04/23 07:26:46.852
  May  4 07:26:47.852: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 05/04/23 07:26:47.856
  STEP: Creating a custom resource definition that should be denied by the webhook @ 05/04/23 07:26:47.902
  May  4 07:26:47.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 07:26:47.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1875" for this suite. @ 05/04/23 07:26:48.118
  STEP: Destroying namespace "webhook-markers-7451" for this suite. @ 05/04/23 07:26:48.153
• [6.451 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 05/04/23 07:26:48.188
  May  4 07:26:48.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename job @ 05/04/23 07:26:48.189
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:26:48.267
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:26:48.272
  STEP: Creating Indexed job @ 05/04/23 07:26:48.294
  STEP: Ensuring job reaches completions @ 05/04/23 07:26:48.314
  STEP: Ensuring pods with index for job exist @ 05/04/23 07:27:00.319
  May  4 07:27:00.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2855" for this suite. @ 05/04/23 07:27:00.329
• [12.155 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 05/04/23 07:27:00.344
  May  4 07:27:00.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename services @ 05/04/23 07:27:00.345
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:27:00.393
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:27:00.398
  STEP: creating a Service @ 05/04/23 07:27:00.405
  STEP: watching for the Service to be added @ 05/04/23 07:27:00.458
  May  4 07:27:00.461: INFO: Found Service test-service-7tdwp in namespace services-1255 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  May  4 07:27:00.461: INFO: Service test-service-7tdwp created
  STEP: Getting /status @ 05/04/23 07:27:00.461
  May  4 07:27:00.470: INFO: Service test-service-7tdwp has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 05/04/23 07:27:00.47
  STEP: watching for the Service to be patched @ 05/04/23 07:27:00.486
  May  4 07:27:00.489: INFO: observed Service test-service-7tdwp in namespace services-1255 with annotations: map[] & LoadBalancer: {[]}
  May  4 07:27:00.490: INFO: Found Service test-service-7tdwp in namespace services-1255 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  May  4 07:27:00.490: INFO: Service test-service-7tdwp has service status patched
  STEP: updating the ServiceStatus @ 05/04/23 07:27:00.49
  May  4 07:27:00.520: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 05/04/23 07:27:00.52
  May  4 07:27:00.523: INFO: Observed Service test-service-7tdwp in namespace services-1255 with annotations: map[] & Conditions: {[]}
  May  4 07:27:00.523: INFO: Observed event: &Service{ObjectMeta:{test-service-7tdwp  services-1255  d8267aba-01a8-40d1-912e-4f8ef9da5fb0 76587 0 2023-05-04 07:27:00 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-04 07:27:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-04 07:27:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.102.14.50,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.102.14.50],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  May  4 07:27:00.523: INFO: Found Service test-service-7tdwp in namespace services-1255 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May  4 07:27:00.523: INFO: Service test-service-7tdwp has service status updated
  STEP: patching the service @ 05/04/23 07:27:00.523
  STEP: watching for the Service to be patched @ 05/04/23 07:27:00.538
  May  4 07:27:00.542: INFO: observed Service test-service-7tdwp in namespace services-1255 with labels: map[test-service-static:true]
  May  4 07:27:00.542: INFO: observed Service test-service-7tdwp in namespace services-1255 with labels: map[test-service-static:true]
  May  4 07:27:00.542: INFO: observed Service test-service-7tdwp in namespace services-1255 with labels: map[test-service-static:true]
  May  4 07:27:00.542: INFO: Found Service test-service-7tdwp in namespace services-1255 with labels: map[test-service:patched test-service-static:true]
  May  4 07:27:00.542: INFO: Service test-service-7tdwp patched
  STEP: deleting the service @ 05/04/23 07:27:00.542
  STEP: watching for the Service to be deleted @ 05/04/23 07:27:00.601
  May  4 07:27:00.603: INFO: Observed event: ADDED
  May  4 07:27:00.603: INFO: Observed event: MODIFIED
  May  4 07:27:00.603: INFO: Observed event: MODIFIED
  May  4 07:27:00.603: INFO: Observed event: MODIFIED
  May  4 07:27:00.603: INFO: Found Service test-service-7tdwp in namespace services-1255 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  May  4 07:27:00.603: INFO: Service test-service-7tdwp deleted
  May  4 07:27:00.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1255" for this suite. @ 05/04/23 07:27:00.607
• [0.278 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 05/04/23 07:27:00.624
  May  4 07:27:00.627: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename var-expansion @ 05/04/23 07:27:00.628
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:27:00.69
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:27:00.694
  STEP: creating the pod with failed condition @ 05/04/23 07:27:00.698
  STEP: updating the pod @ 05/04/23 07:29:00.727
  May  4 07:29:01.265: INFO: Successfully updated pod "var-expansion-92054fb0-cd1d-4249-a14f-f734ff83e43c"
  STEP: waiting for pod running @ 05/04/23 07:29:01.265
  STEP: deleting the pod gracefully @ 05/04/23 07:29:03.276
  May  4 07:29:03.276: INFO: Deleting pod "var-expansion-92054fb0-cd1d-4249-a14f-f734ff83e43c" in namespace "var-expansion-7346"
  May  4 07:29:03.295: INFO: Wait up to 5m0s for pod "var-expansion-92054fb0-cd1d-4249-a14f-f734ff83e43c" to be fully deleted
  May  4 07:29:35.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7346" for this suite. @ 05/04/23 07:29:35.39
• [154.785 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 05/04/23 07:29:35.411
  May  4 07:29:35.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename watch @ 05/04/23 07:29:35.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:29:35.448
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:29:35.452
  STEP: creating a watch on configmaps @ 05/04/23 07:29:35.458
  STEP: creating a new configmap @ 05/04/23 07:29:35.46
  STEP: modifying the configmap once @ 05/04/23 07:29:35.486
  STEP: closing the watch once it receives two notifications @ 05/04/23 07:29:35.502
  May  4 07:29:35.502: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7787  ac03bf2b-9212-4bd3-936a-5fe2017383ae 76919 0 2023-05-04 07:29:35 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-04 07:29:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May  4 07:29:35.502: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7787  ac03bf2b-9212-4bd3-936a-5fe2017383ae 76920 0 2023-05-04 07:29:35 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-04 07:29:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 05/04/23 07:29:35.502
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 05/04/23 07:29:35.54
  STEP: deleting the configmap @ 05/04/23 07:29:35.541
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 05/04/23 07:29:35.555
  May  4 07:29:35.556: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7787  ac03bf2b-9212-4bd3-936a-5fe2017383ae 76921 0 2023-05-04 07:29:35 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-04 07:29:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  4 07:29:35.556: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7787  ac03bf2b-9212-4bd3-936a-5fe2017383ae 76922 0 2023-05-04 07:29:35 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-04 07:29:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  4 07:29:35.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7787" for this suite. @ 05/04/23 07:29:35.56
• [0.163 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 05/04/23 07:29:35.575
  May  4 07:29:35.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename replication-controller @ 05/04/23 07:29:35.577
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:29:35.608
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:29:35.611
  STEP: Creating ReplicationController "e2e-rc-hfwhn" @ 05/04/23 07:29:35.646
  May  4 07:29:35.662: INFO: Get Replication Controller "e2e-rc-hfwhn" to confirm replicas
  May  4 07:29:36.666: INFO: Get Replication Controller "e2e-rc-hfwhn" to confirm replicas
  May  4 07:29:36.670: INFO: Found 1 replicas for "e2e-rc-hfwhn" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-hfwhn" @ 05/04/23 07:29:36.67
  STEP: Updating a scale subresource @ 05/04/23 07:29:36.673
  STEP: Verifying replicas where modified for replication controller "e2e-rc-hfwhn" @ 05/04/23 07:29:36.688
  May  4 07:29:36.688: INFO: Get Replication Controller "e2e-rc-hfwhn" to confirm replicas
  May  4 07:29:37.704: INFO: Get Replication Controller "e2e-rc-hfwhn" to confirm replicas
  May  4 07:29:37.709: INFO: Found 2 replicas for "e2e-rc-hfwhn" replication controller
  May  4 07:29:37.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-2901" for this suite. @ 05/04/23 07:29:37.714
• [2.157 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 05/04/23 07:29:37.733
  May  4 07:29:37.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename secrets @ 05/04/23 07:29:37.735
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:29:37.81
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:29:37.815
  May  4 07:29:38.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-814" for this suite. @ 05/04/23 07:29:38.011
• [0.301 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 05/04/23 07:29:38.035
  May  4 07:29:38.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename resourcequota @ 05/04/23 07:29:38.037
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:29:38.097
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:29:38.102
  STEP: Counting existing ResourceQuota @ 05/04/23 07:29:38.107
  STEP: Creating a ResourceQuota @ 05/04/23 07:29:43.112
  STEP: Ensuring resource quota status is calculated @ 05/04/23 07:29:43.132
  May  4 07:29:45.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2396" for this suite. @ 05/04/23 07:29:45.141
• [7.121 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 05/04/23 07:29:45.156
  May  4 07:29:45.157: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 07:29:45.158
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:29:45.211
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:29:45.215
  STEP: Creating configMap with name projected-configmap-test-volume-map-57fe9d07-f367-4b06-b3e5-e5bef17fb8b8 @ 05/04/23 07:29:45.219
  STEP: Creating a pod to test consume configMaps @ 05/04/23 07:29:45.233
  STEP: Saw pod success @ 05/04/23 07:29:49.3
  May  4 07:29:49.303: INFO: Trying to get logs from node k8s-node2 pod pod-projected-configmaps-94bd1d37-7b39-4bcf-a3aa-d431f010ca5e container agnhost-container: <nil>
  STEP: delete the pod @ 05/04/23 07:29:49.328
  May  4 07:29:49.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6905" for this suite. @ 05/04/23 07:29:49.388
• [4.267 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 05/04/23 07:29:49.425
  May  4 07:29:49.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename sched-pred @ 05/04/23 07:29:49.426
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:29:49.478
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:29:49.482
  May  4 07:29:49.485: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May  4 07:29:49.493: INFO: Waiting for terminating namespaces to be deleted...
  May  4 07:29:49.496: INFO: 
  Logging pods the apiserver thinks is on node k8s-node1 before test
  May  4 07:29:49.504: INFO: calico-kube-controllers-6849cf9bcf-6tzqn from kube-system started at 2023-05-04 06:32:24 +0000 UTC (1 container statuses recorded)
  May  4 07:29:49.504: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May  4 07:29:49.504: INFO: calico-node-5st2f from kube-system started at 2023-05-04 01:43:32 +0000 UTC (1 container statuses recorded)
  May  4 07:29:49.504: INFO: 	Container calico-node ready: true, restart count 0
  May  4 07:29:49.504: INFO: coredns-7bdc4cb885-57mgw from kube-system started at 2023-05-04 06:32:24 +0000 UTC (1 container statuses recorded)
  May  4 07:29:49.504: INFO: 	Container coredns ready: true, restart count 0
  May  4 07:29:49.504: INFO: coredns-7bdc4cb885-7bzz7 from kube-system started at 2023-05-04 06:32:24 +0000 UTC (1 container statuses recorded)
  May  4 07:29:49.504: INFO: 	Container coredns ready: true, restart count 0
  May  4 07:29:49.504: INFO: kube-proxy-b6n46 from kube-system started at 2023-05-04 01:43:02 +0000 UTC (1 container statuses recorded)
  May  4 07:29:49.504: INFO: 	Container kube-proxy ready: true, restart count 0
  May  4 07:29:49.504: INFO: sonobuoy from sonobuoy started at 2023-05-04 06:11:43 +0000 UTC (1 container statuses recorded)
  May  4 07:29:49.504: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May  4 07:29:49.504: INFO: sonobuoy-e2e-job-5e11a169cb044f18 from sonobuoy started at 2023-05-04 06:11:45 +0000 UTC (2 container statuses recorded)
  May  4 07:29:49.504: INFO: 	Container e2e ready: true, restart count 0
  May  4 07:29:49.504: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  4 07:29:49.504: INFO: sonobuoy-systemd-logs-daemon-set-1f5c24134e834838-h8cct from sonobuoy started at 2023-05-04 06:11:45 +0000 UTC (2 container statuses recorded)
  May  4 07:29:49.504: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  4 07:29:49.504: INFO: 	Container systemd-logs ready: true, restart count 0
  May  4 07:29:49.504: INFO: 
  Logging pods the apiserver thinks is on node k8s-node2 before test
  May  4 07:29:49.512: INFO: calico-kube-controllers-6849cf9bcf-9j56g from kube-system started at 2023-05-04 01:44:12 +0000 UTC (1 container statuses recorded)
  May  4 07:29:49.512: INFO: 	Container calico-kube-controllers ready: false, restart count 0
  May  4 07:29:49.512: INFO: calico-node-ssrv4 from kube-system started at 2023-05-04 01:43:32 +0000 UTC (1 container statuses recorded)
  May  4 07:29:49.512: INFO: 	Container calico-node ready: true, restart count 0
  May  4 07:29:49.512: INFO: coredns-7bdc4cb885-7knh9 from kube-system started at 2023-05-04 01:44:12 +0000 UTC (1 container statuses recorded)
  May  4 07:29:49.512: INFO: 	Container coredns ready: false, restart count 0
  May  4 07:29:49.512: INFO: coredns-7bdc4cb885-pdxsn from kube-system started at 2023-05-04 01:44:12 +0000 UTC (1 container statuses recorded)
  May  4 07:29:49.512: INFO: 	Container coredns ready: false, restart count 0
  May  4 07:29:49.512: INFO: kube-proxy-zxvtj from kube-system started at 2023-05-04 01:43:07 +0000 UTC (1 container statuses recorded)
  May  4 07:29:49.512: INFO: 	Container kube-proxy ready: true, restart count 0
  May  4 07:29:49.512: INFO: sonobuoy-systemd-logs-daemon-set-1f5c24134e834838-dqrng from sonobuoy started at 2023-05-04 06:11:45 +0000 UTC (2 container statuses recorded)
  May  4 07:29:49.512: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  4 07:29:49.512: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/04/23 07:29:49.512
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/04/23 07:29:53.565
  STEP: Trying to apply a random label on the found node. @ 05/04/23 07:29:53.606
  STEP: verifying the node has the label kubernetes.io/e2e-6192d3a3-b988-4999-a8c4-277d6e66ba18 42 @ 05/04/23 07:29:53.645
  STEP: Trying to relaunch the pod, now with labels. @ 05/04/23 07:29:53.665
  STEP: removing the label kubernetes.io/e2e-6192d3a3-b988-4999-a8c4-277d6e66ba18 off the node k8s-node2 @ 05/04/23 07:29:55.722
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-6192d3a3-b988-4999-a8c4-277d6e66ba18 @ 05/04/23 07:29:55.76
  May  4 07:29:55.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-9808" for this suite. @ 05/04/23 07:29:55.777
• [6.371 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 05/04/23 07:29:55.798
  May  4 07:29:55.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename containers @ 05/04/23 07:29:55.8
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:29:55.835
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:29:55.842
  STEP: Creating a pod to test override all @ 05/04/23 07:29:55.868
  STEP: Saw pod success @ 05/04/23 07:29:59.926
  May  4 07:29:59.929: INFO: Trying to get logs from node k8s-node2 pod client-containers-710a8872-c711-4036-abc1-4b27e48221f2 container agnhost-container: <nil>
  STEP: delete the pod @ 05/04/23 07:29:59.935
  May  4 07:29:59.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-7274" for this suite. @ 05/04/23 07:29:59.987
• [4.207 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 05/04/23 07:30:00.01
  May  4 07:30:00.010: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename downward-api @ 05/04/23 07:30:00.011
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:30:00.085
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:30:00.089
  STEP: Creating a pod to test downward API volume plugin @ 05/04/23 07:30:00.093
  STEP: Saw pod success @ 05/04/23 07:30:06.17
  May  4 07:30:06.173: INFO: Trying to get logs from node k8s-node2 pod downwardapi-volume-53ab8df0-d7e4-4e5b-b6f2-c6dcb885ed32 container client-container: <nil>
  STEP: delete the pod @ 05/04/23 07:30:06.181
  May  4 07:30:06.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2395" for this suite. @ 05/04/23 07:30:06.239
• [6.311 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 05/04/23 07:30:06.323
  May  4 07:30:06.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename server-version @ 05/04/23 07:30:06.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:30:06.366
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:30:06.37
  STEP: Request ServerVersion @ 05/04/23 07:30:06.379
  STEP: Confirm major version @ 05/04/23 07:30:06.38
  May  4 07:30:06.380: INFO: Major version: 1
  STEP: Confirm minor version @ 05/04/23 07:30:06.38
  May  4 07:30:06.380: INFO: cleanMinorVersion: 27
  May  4 07:30:06.380: INFO: Minor version: 27
  May  4 07:30:06.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-45" for this suite. @ 05/04/23 07:30:06.385
• [0.091 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 05/04/23 07:30:06.415
  May  4 07:30:06.415: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename statefulset @ 05/04/23 07:30:06.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:30:06.473
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:30:06.477
  STEP: Creating service test in namespace statefulset-2242 @ 05/04/23 07:30:06.483
  STEP: Creating statefulset ss in namespace statefulset-2242 @ 05/04/23 07:30:06.52
  May  4 07:30:06.584: INFO: Found 0 stateful pods, waiting for 1
  May  4 07:30:16.591: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 05/04/23 07:30:16.596
  STEP: updating a scale subresource @ 05/04/23 07:30:16.598
  STEP: verifying the statefulset Spec.Replicas was modified @ 05/04/23 07:30:16.613
  STEP: Patch a scale subresource @ 05/04/23 07:30:16.617
  STEP: verifying the statefulset Spec.Replicas was modified @ 05/04/23 07:30:16.653
  May  4 07:30:16.678: INFO: Deleting all statefulset in ns statefulset-2242
  May  4 07:30:16.683: INFO: Scaling statefulset ss to 0
  May  4 07:30:26.772: INFO: Waiting for statefulset status.replicas updated to 0
  May  4 07:30:26.776: INFO: Deleting statefulset ss
  May  4 07:30:26.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2242" for this suite. @ 05/04/23 07:30:26.802
• [20.445 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 05/04/23 07:30:26.862
  May  4 07:30:26.862: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename services @ 05/04/23 07:30:26.863
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:30:26.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:30:26.905
  STEP: creating a collection of services @ 05/04/23 07:30:26.909
  May  4 07:30:26.909: INFO: Creating e2e-svc-a-kfbqq
  May  4 07:30:26.970: INFO: Creating e2e-svc-b-b7m2f
  May  4 07:30:27.017: INFO: Creating e2e-svc-c-8rvbw
  STEP: deleting service collection @ 05/04/23 07:30:27.115
  May  4 07:30:27.252: INFO: Collection of services has been deleted
  May  4 07:30:27.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8371" for this suite. @ 05/04/23 07:30:27.259
• [0.420 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 05/04/23 07:30:27.283
  May  4 07:30:27.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename deployment @ 05/04/23 07:30:27.285
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:30:27.373
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:30:27.377
  STEP: creating a Deployment @ 05/04/23 07:30:27.384
  STEP: waiting for Deployment to be created @ 05/04/23 07:30:27.404
  STEP: waiting for all Replicas to be Ready @ 05/04/23 07:30:27.406
  May  4 07:30:27.408: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May  4 07:30:27.408: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May  4 07:30:27.502: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May  4 07:30:27.502: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May  4 07:30:27.598: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May  4 07:30:27.598: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May  4 07:30:27.733: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May  4 07:30:27.733: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May  4 07:30:29.784: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  May  4 07:30:29.784: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  May  4 07:30:29.833: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 05/04/23 07:30:29.833
  W0504 07:30:29.861876      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May  4 07:30:29.866: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 05/04/23 07:30:29.866
  May  4 07:30:29.869: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 0
  May  4 07:30:29.869: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 0
  May  4 07:30:29.869: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 0
  May  4 07:30:29.869: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 0
  May  4 07:30:29.869: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 0
  May  4 07:30:29.869: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 0
  May  4 07:30:29.869: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 0
  May  4 07:30:29.869: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 0
  May  4 07:30:29.870: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 1
  May  4 07:30:29.870: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 1
  May  4 07:30:29.870: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 2
  May  4 07:30:29.870: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 2
  May  4 07:30:29.870: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 2
  May  4 07:30:29.870: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 2
  May  4 07:30:29.902: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 2
  May  4 07:30:29.902: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 2
  May  4 07:30:30.005: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 2
  May  4 07:30:30.005: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 2
  May  4 07:30:30.079: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 1
  May  4 07:30:30.079: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 1
  May  4 07:30:30.127: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 1
  May  4 07:30:30.127: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 1
  May  4 07:30:31.887: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 2
  May  4 07:30:31.887: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 2
  May  4 07:30:32.022: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 1
  STEP: listing Deployments @ 05/04/23 07:30:32.022
  May  4 07:30:32.028: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 05/04/23 07:30:32.028
  May  4 07:30:32.063: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 05/04/23 07:30:32.063
  May  4 07:30:32.120: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May  4 07:30:32.154: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May  4 07:30:32.266: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May  4 07:30:32.371: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May  4 07:30:32.393: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May  4 07:30:34.836: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May  4 07:30:34.986: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  May  4 07:30:35.278: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May  4 07:30:35.492: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May  4 07:30:37.887: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 05/04/23 07:30:38.052
  STEP: fetching the DeploymentStatus @ 05/04/23 07:30:38.061
  May  4 07:30:38.068: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 1
  May  4 07:30:38.068: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 1
  May  4 07:30:38.068: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 1
  May  4 07:30:38.069: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 1
  May  4 07:30:38.069: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 1
  May  4 07:30:38.069: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 2
  May  4 07:30:38.069: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 3
  May  4 07:30:38.069: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 2
  May  4 07:30:38.069: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 2
  May  4 07:30:38.069: INFO: observed Deployment test-deployment in namespace deployment-4394 with ReadyReplicas 3
  STEP: deleting the Deployment @ 05/04/23 07:30:38.069
  May  4 07:30:38.091: INFO: observed event type MODIFIED
  May  4 07:30:38.091: INFO: observed event type MODIFIED
  May  4 07:30:38.091: INFO: observed event type MODIFIED
  May  4 07:30:38.092: INFO: observed event type MODIFIED
  May  4 07:30:38.092: INFO: observed event type MODIFIED
  May  4 07:30:38.092: INFO: observed event type MODIFIED
  May  4 07:30:38.092: INFO: observed event type MODIFIED
  May  4 07:30:38.092: INFO: observed event type MODIFIED
  May  4 07:30:38.092: INFO: observed event type MODIFIED
  May  4 07:30:38.093: INFO: observed event type MODIFIED
  May  4 07:30:38.093: INFO: observed event type MODIFIED
  May  4 07:30:38.098: INFO: Log out all the ReplicaSets if there is no deployment created
  May  4 07:30:38.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4394" for this suite. @ 05/04/23 07:30:38.17
• [10.947 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 05/04/23 07:30:38.231
  May  4 07:30:38.232: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename var-expansion @ 05/04/23 07:30:38.234
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:30:38.318
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:30:38.33
  May  4 07:30:40.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  4 07:30:40.426: INFO: Deleting pod "var-expansion-a54bb567-5ae8-4fe9-b0d8-f224b92e9197" in namespace "var-expansion-5816"
  May  4 07:30:40.468: INFO: Wait up to 5m0s for pod "var-expansion-a54bb567-5ae8-4fe9-b0d8-f224b92e9197" to be fully deleted
  STEP: Destroying namespace "var-expansion-5816" for this suite. @ 05/04/23 07:30:44.483
• [6.267 seconds]
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 05/04/23 07:30:44.498
  May  4 07:30:44.498: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename prestop @ 05/04/23 07:30:44.5
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:30:44.577
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:30:44.58
  STEP: Creating server pod server in namespace prestop-2467 @ 05/04/23 07:30:44.584
  STEP: Waiting for pods to come up. @ 05/04/23 07:30:44.612
  STEP: Creating tester pod tester in namespace prestop-2467 @ 05/04/23 07:30:48.628
  STEP: Deleting pre-stop pod @ 05/04/23 07:30:52.656
  May  4 07:30:57.692: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  May  4 07:30:57.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 05/04/23 07:30:57.698
  STEP: Destroying namespace "prestop-2467" for this suite. @ 05/04/23 07:30:57.743
• [13.286 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 05/04/23 07:30:57.785
  May  4 07:30:57.785: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename kubectl @ 05/04/23 07:30:57.787
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:30:57.884
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:30:57.889
  STEP: creating a replication controller @ 05/04/23 07:30:57.893
  May  4 07:30:57.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-6678 create -f -'
  May  4 07:30:59.389: INFO: stderr: ""
  May  4 07:30:59.389: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/04/23 07:30:59.39
  May  4 07:30:59.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-6678 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May  4 07:30:59.551: INFO: stderr: ""
  May  4 07:30:59.551: INFO: stdout: "update-demo-nautilus-bqckh update-demo-nautilus-f9phl "
  May  4 07:30:59.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-6678 get pods update-demo-nautilus-bqckh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  4 07:30:59.668: INFO: stderr: ""
  May  4 07:30:59.668: INFO: stdout: ""
  May  4 07:30:59.668: INFO: update-demo-nautilus-bqckh is created but not running
  May  4 07:31:04.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-6678 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May  4 07:31:04.791: INFO: stderr: ""
  May  4 07:31:04.791: INFO: stdout: "update-demo-nautilus-bqckh update-demo-nautilus-f9phl "
  May  4 07:31:04.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-6678 get pods update-demo-nautilus-bqckh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  4 07:31:04.927: INFO: stderr: ""
  May  4 07:31:04.927: INFO: stdout: "true"
  May  4 07:31:04.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-6678 get pods update-demo-nautilus-bqckh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May  4 07:31:05.047: INFO: stderr: ""
  May  4 07:31:05.047: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May  4 07:31:05.047: INFO: validating pod update-demo-nautilus-bqckh
  May  4 07:31:05.053: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May  4 07:31:05.053: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May  4 07:31:05.053: INFO: update-demo-nautilus-bqckh is verified up and running
  May  4 07:31:05.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-6678 get pods update-demo-nautilus-f9phl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  4 07:31:05.170: INFO: stderr: ""
  May  4 07:31:05.170: INFO: stdout: "true"
  May  4 07:31:05.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-6678 get pods update-demo-nautilus-f9phl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May  4 07:31:05.307: INFO: stderr: ""
  May  4 07:31:05.307: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May  4 07:31:05.307: INFO: validating pod update-demo-nautilus-f9phl
  May  4 07:31:05.314: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May  4 07:31:05.314: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May  4 07:31:05.314: INFO: update-demo-nautilus-f9phl is verified up and running
  STEP: using delete to clean up resources @ 05/04/23 07:31:05.314
  May  4 07:31:05.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-6678 delete --grace-period=0 --force -f -'
  May  4 07:31:05.443: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  4 07:31:05.443: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  May  4 07:31:05.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-6678 get rc,svc -l name=update-demo --no-headers'
  May  4 07:31:05.595: INFO: stderr: "No resources found in kubectl-6678 namespace.\n"
  May  4 07:31:05.595: INFO: stdout: ""
  May  4 07:31:05.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-6678 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May  4 07:31:05.727: INFO: stderr: ""
  May  4 07:31:05.727: INFO: stdout: ""
  May  4 07:31:05.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6678" for this suite. @ 05/04/23 07:31:05.733
• [7.968 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 05/04/23 07:31:05.765
  May  4 07:31:05.765: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename controllerrevisions @ 05/04/23 07:31:05.766
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:31:05.892
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:31:05.896
  STEP: Creating DaemonSet "e2e-xzh68-daemon-set" @ 05/04/23 07:31:05.993
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/04/23 07:31:06.015
  May  4 07:31:06.083: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:31:06.089: INFO: Number of nodes with available pods controlled by daemonset e2e-xzh68-daemon-set: 0
  May  4 07:31:06.089: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  May  4 07:31:07.144: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:31:07.174: INFO: Number of nodes with available pods controlled by daemonset e2e-xzh68-daemon-set: 0
  May  4 07:31:07.174: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  May  4 07:31:08.104: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:31:08.108: INFO: Number of nodes with available pods controlled by daemonset e2e-xzh68-daemon-set: 0
  May  4 07:31:08.108: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  May  4 07:31:09.098: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:31:09.102: INFO: Number of nodes with available pods controlled by daemonset e2e-xzh68-daemon-set: 2
  May  4 07:31:09.102: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-xzh68-daemon-set
  STEP: Confirm DaemonSet "e2e-xzh68-daemon-set" successfully created with "daemonset-name=e2e-xzh68-daemon-set" label @ 05/04/23 07:31:09.106
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-xzh68-daemon-set" @ 05/04/23 07:31:09.112
  May  4 07:31:09.115: INFO: Located ControllerRevision: "e2e-xzh68-daemon-set-65bf7454df"
  STEP: Patching ControllerRevision "e2e-xzh68-daemon-set-65bf7454df" @ 05/04/23 07:31:09.118
  May  4 07:31:09.151: INFO: e2e-xzh68-daemon-set-65bf7454df has been patched
  STEP: Create a new ControllerRevision @ 05/04/23 07:31:09.151
  May  4 07:31:09.169: INFO: Created ControllerRevision: e2e-xzh68-daemon-set-9f894f44f
  STEP: Confirm that there are two ControllerRevisions @ 05/04/23 07:31:09.169
  May  4 07:31:09.169: INFO: Requesting list of ControllerRevisions to confirm quantity
  May  4 07:31:09.172: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-xzh68-daemon-set-65bf7454df" @ 05/04/23 07:31:09.173
  STEP: Confirm that there is only one ControllerRevision @ 05/04/23 07:31:09.196
  May  4 07:31:09.196: INFO: Requesting list of ControllerRevisions to confirm quantity
  May  4 07:31:09.199: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-xzh68-daemon-set-9f894f44f" @ 05/04/23 07:31:09.202
  May  4 07:31:09.277: INFO: e2e-xzh68-daemon-set-9f894f44f has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 05/04/23 07:31:09.277
  W0504 07:31:09.296324      20 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 05/04/23 07:31:09.296
  May  4 07:31:09.296: INFO: Requesting list of ControllerRevisions to confirm quantity
  May  4 07:31:10.300: INFO: Requesting list of ControllerRevisions to confirm quantity
  May  4 07:31:10.304: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-xzh68-daemon-set-9f894f44f=updated" @ 05/04/23 07:31:10.304
  STEP: Confirm that there is only one ControllerRevision @ 05/04/23 07:31:10.332
  May  4 07:31:10.332: INFO: Requesting list of ControllerRevisions to confirm quantity
  May  4 07:31:10.336: INFO: Found 1 ControllerRevisions
  May  4 07:31:10.339: INFO: ControllerRevision "e2e-xzh68-daemon-set-78585d59d8" has revision 3
  STEP: Deleting DaemonSet "e2e-xzh68-daemon-set" @ 05/04/23 07:31:10.342
  STEP: deleting DaemonSet.extensions e2e-xzh68-daemon-set in namespace controllerrevisions-6803, will wait for the garbage collector to delete the pods @ 05/04/23 07:31:10.342
  May  4 07:31:10.440: INFO: Deleting DaemonSet.extensions e2e-xzh68-daemon-set took: 43.812384ms
  May  4 07:31:10.541: INFO: Terminating DaemonSet.extensions e2e-xzh68-daemon-set pods took: 100.322075ms
  May  4 07:31:12.970: INFO: Number of nodes with available pods controlled by daemonset e2e-xzh68-daemon-set: 0
  May  4 07:31:12.970: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-xzh68-daemon-set
  May  4 07:31:12.973: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"77926"},"items":null}

  May  4 07:31:12.976: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"77926"},"items":null}

  May  4 07:31:12.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-6803" for this suite. @ 05/04/23 07:31:12.99
• [7.241 seconds]
------------------------------
SSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 05/04/23 07:31:13.007
  May  4 07:31:13.007: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename dns @ 05/04/23 07:31:13.009
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:31:13.045
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:31:13.051
  STEP: Creating a test headless service @ 05/04/23 07:31:13.064
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3821.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3821.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3821.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3821.svc.cluster.local;sleep 1; done
   @ 05/04/23 07:31:13.103
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3821.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3821.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3821.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3821.svc.cluster.local;sleep 1; done
   @ 05/04/23 07:31:13.103
  STEP: creating a pod to probe DNS @ 05/04/23 07:31:13.103
  STEP: submitting the pod to kubernetes @ 05/04/23 07:31:13.103
  STEP: retrieving the pod @ 05/04/23 07:31:17.191
  STEP: looking for the results for each expected name from probers @ 05/04/23 07:31:17.195
  May  4 07:31:17.202: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:17.209: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:17.220: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:17.225: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:17.229: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:17.233: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:17.237: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:17.237: INFO: Lookups using dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3821.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local jessie_udp@dns-test-service-2.dns-3821.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3821.svc.cluster.local]

  May  4 07:31:22.243: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:22.246: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:22.257: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:22.261: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:22.269: INFO: Lookups using dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local]

  May  4 07:31:27.241: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:27.245: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:27.256: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:27.259: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:27.267: INFO: Lookups using dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local]

  May  4 07:31:32.242: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:32.268: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:32.279: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:32.283: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:32.290: INFO: Lookups using dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local]

  May  4 07:31:37.242: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:37.246: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:37.258: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:37.261: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:37.268: INFO: Lookups using dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local]

  May  4 07:31:42.242: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:42.246: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:42.279: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:42.283: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local from pod dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f: the server could not find the requested resource (get pods dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f)
  May  4 07:31:42.291: INFO: Lookups using dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3821.svc.cluster.local]

  May  4 07:31:47.275: INFO: DNS probes using dns-3821/dns-test-9c76d795-4eac-465d-8587-d8bcb854c58f succeeded

  May  4 07:31:47.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/04/23 07:31:47.281
  STEP: deleting the test headless service @ 05/04/23 07:31:47.362
  STEP: Destroying namespace "dns-3821" for this suite. @ 05/04/23 07:31:47.458
• [34.493 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 05/04/23 07:31:47.501
  May  4 07:31:47.501: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename downward-api @ 05/04/23 07:31:47.502
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:31:47.583
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:31:47.588
  STEP: Creating a pod to test downward api env vars @ 05/04/23 07:31:47.593
  STEP: Saw pod success @ 05/04/23 07:31:51.629
  May  4 07:31:51.635: INFO: Trying to get logs from node k8s-node2 pod downward-api-46e7e1dc-7cde-4ad8-9725-a01f59977e78 container dapi-container: <nil>
  STEP: delete the pod @ 05/04/23 07:31:51.659
  May  4 07:31:51.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4759" for this suite. @ 05/04/23 07:31:51.726
• [4.266 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 05/04/23 07:31:51.768
  May  4 07:31:51.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename webhook @ 05/04/23 07:31:51.769
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:31:51.824
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:31:51.829
  STEP: Setting up server cert @ 05/04/23 07:31:51.913
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/04/23 07:31:52.67
  STEP: Deploying the webhook pod @ 05/04/23 07:31:52.705
  STEP: Wait for the deployment to be ready @ 05/04/23 07:31:52.802
  May  4 07:31:52.850: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  May  4 07:31:54.861: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 7, 31, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 31, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 31, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 31, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 05/04/23 07:31:56.895
  STEP: Verifying the service has paired with the endpoint @ 05/04/23 07:31:56.976
  May  4 07:31:57.981: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May  4 07:31:58.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9968-crds.webhook.example.com via the AdmissionRegistration API @ 05/04/23 07:31:58.541
  STEP: Creating a custom resource that should be mutated by the webhook @ 05/04/23 07:31:58.601
  May  4 07:32:00.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7307" for this suite. @ 05/04/23 07:32:01.452
  STEP: Destroying namespace "webhook-markers-7514" for this suite. @ 05/04/23 07:32:01.47
• [9.742 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 05/04/23 07:32:01.517
  May  4 07:32:01.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename subpath @ 05/04/23 07:32:01.518
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:32:01.614
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:32:01.621
  STEP: Setting up data @ 05/04/23 07:32:01.627
  STEP: Creating pod pod-subpath-test-configmap-928g @ 05/04/23 07:32:01.665
  STEP: Creating a pod to test atomic-volume-subpath @ 05/04/23 07:32:01.665
  STEP: Saw pod success @ 05/04/23 07:32:25.798
  May  4 07:32:25.802: INFO: Trying to get logs from node k8s-node2 pod pod-subpath-test-configmap-928g container test-container-subpath-configmap-928g: <nil>
  STEP: delete the pod @ 05/04/23 07:32:25.81
  STEP: Deleting pod pod-subpath-test-configmap-928g @ 05/04/23 07:32:25.863
  May  4 07:32:25.863: INFO: Deleting pod "pod-subpath-test-configmap-928g" in namespace "subpath-5155"
  May  4 07:32:25.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5155" for this suite. @ 05/04/23 07:32:25.871
• [24.386 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 05/04/23 07:32:25.904
  May  4 07:32:25.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename emptydir @ 05/04/23 07:32:25.906
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:32:25.936
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:32:25.949
  STEP: Creating Pod @ 05/04/23 07:32:25.958
  STEP: Reading file content from the nginx-container @ 05/04/23 07:32:29.989
  May  4 07:32:29.989: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9690 PodName:pod-sharedvolume-acb62afc-dbd7-451d-a232-aaee033646ea ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 07:32:29.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 07:32:29.990: INFO: ExecWithOptions: Clientset creation
  May  4 07:32:29.991: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-9690/pods/pod-sharedvolume-acb62afc-dbd7-451d-a232-aaee033646ea/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  May  4 07:32:30.084: INFO: Exec stderr: ""
  May  4 07:32:30.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9690" for this suite. @ 05/04/23 07:32:30.089
• [4.202 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 05/04/23 07:32:30.109
  May  4 07:32:30.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename deployment @ 05/04/23 07:32:30.111
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:32:30.164
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:32:30.168
  May  4 07:32:30.172: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  May  4 07:32:30.211: INFO: Pod name sample-pod: Found 0 pods out of 1
  May  4 07:32:35.219: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/04/23 07:32:35.219
  May  4 07:32:35.219: INFO: Creating deployment "test-rolling-update-deployment"
  May  4 07:32:35.239: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  May  4 07:32:35.285: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  May  4 07:32:37.292: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  May  4 07:32:37.295: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 32, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 32, 35, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 32, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 32, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-656d657cd8\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  4 07:32:39.300: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  May  4 07:32:39.309: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-491  c18d4e78-f520-49a1-ba85-6a66021e13ff 78372 1 2023-05-04 07:32:35 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-04 07:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 07:32:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a1ee938 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-04 07:32:35 +0000 UTC,LastTransitionTime:2023-05-04 07:32:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-05-04 07:32:38 +0000 UTC,LastTransitionTime:2023-05-04 07:32:35 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May  4 07:32:39.313: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-491  30ecb145-e090-4db7-a1c5-36ee649afe73 78361 1 2023-05-04 07:32:35 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment c18d4e78-f520-49a1-ba85-6a66021e13ff 0xc0013004e7 0xc0013004e8}] [] [{kube-controller-manager Update apps/v1 2023-05-04 07:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c18d4e78-f520-49a1-ba85-6a66021e13ff\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 07:32:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001300598 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May  4 07:32:39.313: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  May  4 07:32:39.313: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-491  f0522ed7-e803-4ecd-89d9-e7a329db346f 78371 2 2023-05-04 07:32:30 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment c18d4e78-f520-49a1-ba85-6a66021e13ff 0xc0013003b7 0xc0013003b8}] [] [{e2e.test Update apps/v1 2023-05-04 07:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 07:32:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c18d4e78-f520-49a1-ba85-6a66021e13ff\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-04 07:32:38 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001300478 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May  4 07:32:39.317: INFO: Pod "test-rolling-update-deployment-656d657cd8-rkw9w" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-rkw9w test-rolling-update-deployment-656d657cd8- deployment-491  e8e43dfa-2ecf-492e-b864-0a555c374271 78360 0 2023-05-04 07:32:35 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[cni.projectcalico.org/containerID:6d424e399897b977a2d6dbe4aa19da2bec8a35ba55b6a9f7c2cb04378a773ad2 cni.projectcalico.org/podIP:172.16.169.146/32 cni.projectcalico.org/podIPs:172.16.169.146/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 30ecb145-e090-4db7-a1c5-36ee649afe73 0xc005b5cd37 0xc005b5cd38}] [] [{kube-controller-manager Update v1 2023-05-04 07:32:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"30ecb145-e090-4db7-a1c5-36ee649afe73\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 07:32:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 07:32:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.169.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n9vjs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n9vjs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:32:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:32:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:32:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:32:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.157,PodIP:172.16.169.146,StartTime:2023-05-04 07:32:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 07:32:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:sha256:30e3d1b869f4d327bd612179ed37850611b462c8415691b3de9eb55787f81e71,ContainerID:containerd://57793ec01be4161d15c2a603759de68940da6fb4f004816a7519417b58b5f506,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.169.146,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:32:39.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-491" for this suite. @ 05/04/23 07:32:39.322
• [9.228 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 05/04/23 07:32:39.339
  May  4 07:32:39.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename webhook @ 05/04/23 07:32:39.341
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:32:39.424
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:32:39.428
  STEP: Setting up server cert @ 05/04/23 07:32:39.515
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/04/23 07:32:40.537
  STEP: Deploying the webhook pod @ 05/04/23 07:32:40.553
  STEP: Wait for the deployment to be ready @ 05/04/23 07:32:40.59
  May  4 07:32:40.615: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  May  4 07:32:42.626: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 7, 32, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 32, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 32, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 32, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 05/04/23 07:32:44.632
  STEP: Verifying the service has paired with the endpoint @ 05/04/23 07:32:44.673
  May  4 07:32:45.674: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 05/04/23 07:32:45.68
  STEP: create a namespace for the webhook @ 05/04/23 07:32:45.721
  STEP: create a configmap should be unconditionally rejected by the webhook @ 05/04/23 07:32:45.771
  May  4 07:32:45.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5037" for this suite. @ 05/04/23 07:32:46.011
  STEP: Destroying namespace "webhook-markers-5995" for this suite. @ 05/04/23 07:32:46.028
  STEP: Destroying namespace "fail-closed-namespace-284" for this suite. @ 05/04/23 07:32:46.068
• [6.745 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 05/04/23 07:32:46.086
  May  4 07:32:46.086: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename kubectl @ 05/04/23 07:32:46.087
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:32:46.147
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:32:46.155
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/04/23 07:32:46.175
  May  4 07:32:46.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-9646 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  May  4 07:32:46.343: INFO: stderr: ""
  May  4 07:32:46.343: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 05/04/23 07:32:46.343
  May  4 07:32:46.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-9646 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  May  4 07:32:46.491: INFO: stderr: ""
  May  4 07:32:46.491: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/04/23 07:32:46.491
  May  4 07:32:46.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-9646 delete pods e2e-test-httpd-pod'
  May  4 07:32:51.385: INFO: stderr: ""
  May  4 07:32:51.386: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May  4 07:32:51.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9646" for this suite. @ 05/04/23 07:32:51.396
• [5.349 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 05/04/23 07:32:51.435
  May  4 07:32:51.435: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename emptydir @ 05/04/23 07:32:51.437
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:32:51.502
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:32:51.507
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 05/04/23 07:32:51.511
  STEP: Saw pod success @ 05/04/23 07:32:55.598
  May  4 07:32:55.601: INFO: Trying to get logs from node k8s-node2 pod pod-d5f159f1-91e5-45e5-8b8f-55ac6bbace0f container test-container: <nil>
  STEP: delete the pod @ 05/04/23 07:32:55.607
  May  4 07:32:55.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7495" for this suite. @ 05/04/23 07:32:55.656
• [4.236 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 05/04/23 07:32:55.672
  May  4 07:32:55.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 07:32:55.673
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:32:55.731
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:32:55.735
  STEP: Creating a pod to test downward API volume plugin @ 05/04/23 07:32:55.738
  STEP: Saw pod success @ 05/04/23 07:32:59.836
  May  4 07:32:59.840: INFO: Trying to get logs from node k8s-node2 pod downwardapi-volume-9e963ee3-ced2-47bd-ac47-8431444a7d37 container client-container: <nil>
  STEP: delete the pod @ 05/04/23 07:32:59.876
  May  4 07:32:59.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2393" for this suite. @ 05/04/23 07:32:59.93
• [4.275 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 05/04/23 07:32:59.95
  May  4 07:32:59.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename namespaces @ 05/04/23 07:32:59.951
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:33:00.03
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:33:00.035
  STEP: Creating a test namespace @ 05/04/23 07:33:00.06
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:33:00.15
  STEP: Creating a service in the namespace @ 05/04/23 07:33:00.155
  STEP: Deleting the namespace @ 05/04/23 07:33:00.202
  STEP: Waiting for the namespace to be removed. @ 05/04/23 07:33:00.28
  STEP: Recreating the namespace @ 05/04/23 07:33:06.285
  STEP: Verifying there is no service in the namespace @ 05/04/23 07:33:06.37
  May  4 07:33:06.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-2643" for this suite. @ 05/04/23 07:33:06.378
  STEP: Destroying namespace "nsdeletetest-6691" for this suite. @ 05/04/23 07:33:06.395
  May  4 07:33:06.399: INFO: Namespace nsdeletetest-6691 was already deleted
  STEP: Destroying namespace "nsdeletetest-2193" for this suite. @ 05/04/23 07:33:06.399
• [6.480 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 05/04/23 07:33:06.431
  May  4 07:33:06.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename services @ 05/04/23 07:33:06.432
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:33:06.469
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:33:06.475
  STEP: creating service in namespace services-3091 @ 05/04/23 07:33:06.516
  STEP: creating service affinity-nodeport in namespace services-3091 @ 05/04/23 07:33:06.516
  STEP: creating replication controller affinity-nodeport in namespace services-3091 @ 05/04/23 07:33:06.61
  I0504 07:33:06.721993      20 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-3091, replica count: 3
  I0504 07:33:09.773533      20 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  4 07:33:09.785: INFO: Creating new exec pod
  May  4 07:33:12.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-3091 exec execpod-affinityjx2kk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  May  4 07:33:13.018: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  May  4 07:33:13.018: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  4 07:33:13.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-3091 exec execpod-affinityjx2kk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.56.189 80'
  May  4 07:33:13.272: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.56.189 80\nConnection to 10.100.56.189 80 port [tcp/http] succeeded!\n"
  May  4 07:33:13.272: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  4 07:33:13.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-3091 exec execpod-affinityjx2kk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.156 30721'
  May  4 07:33:13.530: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.156 30721\nConnection to 192.168.0.156 30721 port [tcp/*] succeeded!\n"
  May  4 07:33:13.530: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  4 07:33:13.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-3091 exec execpod-affinityjx2kk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.157 30721'
  May  4 07:33:13.736: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.157 30721\nConnection to 192.168.0.157 30721 port [tcp/*] succeeded!\n"
  May  4 07:33:13.736: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  4 07:33:13.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-3091 exec execpod-affinityjx2kk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.0.156:30721/ ; done'
  May  4 07:33:14.048: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30721/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.0.156:30721/\n"
  May  4 07:33:14.049: INFO: stdout: "\naffinity-nodeport-rttgj\naffinity-nodeport-rttgj\naffinity-nodeport-rttgj\naffinity-nodeport-rttgj\naffinity-nodeport-rttgj\naffinity-nodeport-rttgj\naffinity-nodeport-rttgj\naffinity-nodeport-rttgj\naffinity-nodeport-rttgj\naffinity-nodeport-rttgj\naffinity-nodeport-rttgj\naffinity-nodeport-rttgj\naffinity-nodeport-rttgj\naffinity-nodeport-rttgj\naffinity-nodeport-rttgj\naffinity-nodeport-rttgj"
  May  4 07:33:14.049: INFO: Received response from host: affinity-nodeport-rttgj
  May  4 07:33:14.049: INFO: Received response from host: affinity-nodeport-rttgj
  May  4 07:33:14.049: INFO: Received response from host: affinity-nodeport-rttgj
  May  4 07:33:14.049: INFO: Received response from host: affinity-nodeport-rttgj
  May  4 07:33:14.049: INFO: Received response from host: affinity-nodeport-rttgj
  May  4 07:33:14.049: INFO: Received response from host: affinity-nodeport-rttgj
  May  4 07:33:14.049: INFO: Received response from host: affinity-nodeport-rttgj
  May  4 07:33:14.049: INFO: Received response from host: affinity-nodeport-rttgj
  May  4 07:33:14.049: INFO: Received response from host: affinity-nodeport-rttgj
  May  4 07:33:14.049: INFO: Received response from host: affinity-nodeport-rttgj
  May  4 07:33:14.049: INFO: Received response from host: affinity-nodeport-rttgj
  May  4 07:33:14.049: INFO: Received response from host: affinity-nodeport-rttgj
  May  4 07:33:14.049: INFO: Received response from host: affinity-nodeport-rttgj
  May  4 07:33:14.049: INFO: Received response from host: affinity-nodeport-rttgj
  May  4 07:33:14.049: INFO: Received response from host: affinity-nodeport-rttgj
  May  4 07:33:14.049: INFO: Received response from host: affinity-nodeport-rttgj
  May  4 07:33:14.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  4 07:33:14.054: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-3091, will wait for the garbage collector to delete the pods @ 05/04/23 07:33:14.147
  May  4 07:33:14.219: INFO: Deleting ReplicationController affinity-nodeport took: 16.759406ms
  May  4 07:33:14.320: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.22269ms
  STEP: Destroying namespace "services-3091" for this suite. @ 05/04/23 07:33:17.617
• [11.221 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 05/04/23 07:33:17.653
  May  4 07:33:17.653: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename kubelet-test @ 05/04/23 07:33:17.654
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:33:17.71
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:33:17.714
  May  4 07:33:17.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-9284" for this suite. @ 05/04/23 07:33:17.823
• [0.184 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 05/04/23 07:33:17.838
  May  4 07:33:17.838: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename webhook @ 05/04/23 07:33:17.839
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:33:17.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:33:17.884
  STEP: Setting up server cert @ 05/04/23 07:33:17.991
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/04/23 07:33:18.929
  STEP: Deploying the webhook pod @ 05/04/23 07:33:18.945
  STEP: Wait for the deployment to be ready @ 05/04/23 07:33:19.003
  May  4 07:33:19.029: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  May  4 07:33:21.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 7, 33, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 33, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 33, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 33, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 05/04/23 07:33:23.068
  STEP: Verifying the service has paired with the endpoint @ 05/04/23 07:33:23.122
  May  4 07:33:24.123: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 05/04/23 07:33:24.126
  STEP: create a pod @ 05/04/23 07:33:24.17
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 05/04/23 07:33:28.209
  May  4 07:33:28.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=webhook-8152 attach --namespace=webhook-8152 to-be-attached-pod -i -c=container1'
  May  4 07:33:28.351: INFO: rc: 1
  May  4 07:33:28.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8152" for this suite. @ 05/04/23 07:33:28.558
  STEP: Destroying namespace "webhook-markers-3058" for this suite. @ 05/04/23 07:33:28.654
• [10.834 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 05/04/23 07:33:28.673
  May  4 07:33:28.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename configmap @ 05/04/23 07:33:28.674
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:33:28.772
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:33:28.776
  STEP: creating a ConfigMap @ 05/04/23 07:33:28.78
  STEP: fetching the ConfigMap @ 05/04/23 07:33:28.801
  STEP: patching the ConfigMap @ 05/04/23 07:33:28.805
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 05/04/23 07:33:28.82
  STEP: deleting the ConfigMap by collection with a label selector @ 05/04/23 07:33:28.825
  STEP: listing all ConfigMaps in test namespace @ 05/04/23 07:33:28.848
  May  4 07:33:28.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6701" for this suite. @ 05/04/23 07:33:28.856
• [0.256 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 05/04/23 07:33:28.931
  May  4 07:33:28.931: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename kubelet-test @ 05/04/23 07:33:28.932
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:33:28.979
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:33:28.984
  May  4 07:33:33.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-3002" for this suite. @ 05/04/23 07:33:33.047
• [4.136 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 05/04/23 07:33:33.071
  May  4 07:33:33.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename configmap @ 05/04/23 07:33:33.073
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:33:33.182
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:33:33.187
  STEP: Creating configMap configmap-5645/configmap-test-2b19f81e-ebce-4cb3-a5c5-27ff8650d883 @ 05/04/23 07:33:33.192
  STEP: Creating a pod to test consume configMaps @ 05/04/23 07:33:33.208
  STEP: Saw pod success @ 05/04/23 07:33:37.339
  May  4 07:33:37.342: INFO: Trying to get logs from node k8s-node2 pod pod-configmaps-69a95e76-4ecc-4c4b-b519-9361165e79d2 container env-test: <nil>
  STEP: delete the pod @ 05/04/23 07:33:37.35
  May  4 07:33:37.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5645" for this suite. @ 05/04/23 07:33:37.427
• [4.384 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 05/04/23 07:33:37.457
  May  4 07:33:37.457: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename ingress @ 05/04/23 07:33:37.458
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:33:37.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:33:37.504
  STEP: getting /apis @ 05/04/23 07:33:37.508
  STEP: getting /apis/networking.k8s.io @ 05/04/23 07:33:37.514
  STEP: getting /apis/networking.k8s.iov1 @ 05/04/23 07:33:37.516
  STEP: creating @ 05/04/23 07:33:37.517
  STEP: getting @ 05/04/23 07:33:37.58
  STEP: listing @ 05/04/23 07:33:37.583
  STEP: watching @ 05/04/23 07:33:37.586
  May  4 07:33:37.586: INFO: starting watch
  STEP: cluster-wide listing @ 05/04/23 07:33:37.588
  STEP: cluster-wide watching @ 05/04/23 07:33:37.591
  May  4 07:33:37.591: INFO: starting watch
  STEP: patching @ 05/04/23 07:33:37.592
  STEP: updating @ 05/04/23 07:33:37.606
  May  4 07:33:37.622: INFO: waiting for watch events with expected annotations
  May  4 07:33:37.622: INFO: saw patched and updated annotations
  STEP: patching /status @ 05/04/23 07:33:37.622
  STEP: updating /status @ 05/04/23 07:33:37.646
  STEP: get /status @ 05/04/23 07:33:37.684
  STEP: deleting @ 05/04/23 07:33:37.688
  STEP: deleting a collection @ 05/04/23 07:33:37.709
  May  4 07:33:37.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-7219" for this suite. @ 05/04/23 07:33:37.745
• [0.303 seconds]
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 05/04/23 07:33:37.761
  May  4 07:33:37.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename svcaccounts @ 05/04/23 07:33:37.762
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:33:37.837
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:33:37.841
  STEP: Creating a pod to test service account token:  @ 05/04/23 07:33:37.85
  STEP: Saw pod success @ 05/04/23 07:33:41.892
  May  4 07:33:41.894: INFO: Trying to get logs from node k8s-node2 pod test-pod-7417fe4f-0546-4cc2-88e1-6b2990a168a9 container agnhost-container: <nil>
  STEP: delete the pod @ 05/04/23 07:33:41.901
  May  4 07:33:41.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4308" for this suite. @ 05/04/23 07:33:41.946
• [4.200 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 05/04/23 07:33:41.963
  May  4 07:33:41.963: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/04/23 07:33:41.964
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:33:42.02
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:33:42.025
  May  4 07:33:42.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 07:33:48.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2043" for this suite. @ 05/04/23 07:33:48.722
• [6.789 seconds]
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 05/04/23 07:33:48.752
  May  4 07:33:48.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename sched-pred @ 05/04/23 07:33:48.753
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:33:48.835
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:33:48.841
  May  4 07:33:48.847: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May  4 07:33:48.858: INFO: Waiting for terminating namespaces to be deleted...
  May  4 07:33:48.862: INFO: 
  Logging pods the apiserver thinks is on node k8s-node1 before test
  May  4 07:33:48.871: INFO: calico-kube-controllers-6849cf9bcf-6tzqn from kube-system started at 2023-05-04 06:32:24 +0000 UTC (1 container statuses recorded)
  May  4 07:33:48.871: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May  4 07:33:48.871: INFO: calico-node-5st2f from kube-system started at 2023-05-04 01:43:32 +0000 UTC (1 container statuses recorded)
  May  4 07:33:48.872: INFO: 	Container calico-node ready: true, restart count 0
  May  4 07:33:48.872: INFO: coredns-7bdc4cb885-57mgw from kube-system started at 2023-05-04 06:32:24 +0000 UTC (1 container statuses recorded)
  May  4 07:33:48.872: INFO: 	Container coredns ready: true, restart count 0
  May  4 07:33:48.872: INFO: coredns-7bdc4cb885-7bzz7 from kube-system started at 2023-05-04 06:32:24 +0000 UTC (1 container statuses recorded)
  May  4 07:33:48.872: INFO: 	Container coredns ready: true, restart count 0
  May  4 07:33:48.872: INFO: kube-proxy-b6n46 from kube-system started at 2023-05-04 01:43:02 +0000 UTC (1 container statuses recorded)
  May  4 07:33:48.872: INFO: 	Container kube-proxy ready: true, restart count 0
  May  4 07:33:48.872: INFO: sonobuoy from sonobuoy started at 2023-05-04 06:11:43 +0000 UTC (1 container statuses recorded)
  May  4 07:33:48.872: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May  4 07:33:48.872: INFO: sonobuoy-e2e-job-5e11a169cb044f18 from sonobuoy started at 2023-05-04 06:11:45 +0000 UTC (2 container statuses recorded)
  May  4 07:33:48.872: INFO: 	Container e2e ready: true, restart count 0
  May  4 07:33:48.872: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  4 07:33:48.872: INFO: sonobuoy-systemd-logs-daemon-set-1f5c24134e834838-h8cct from sonobuoy started at 2023-05-04 06:11:45 +0000 UTC (2 container statuses recorded)
  May  4 07:33:48.872: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  4 07:33:48.872: INFO: 	Container systemd-logs ready: true, restart count 0
  May  4 07:33:48.872: INFO: 
  Logging pods the apiserver thinks is on node k8s-node2 before test
  May  4 07:33:48.883: INFO: calico-kube-controllers-6849cf9bcf-9j56g from kube-system started at 2023-05-04 01:44:12 +0000 UTC (1 container statuses recorded)
  May  4 07:33:48.883: INFO: 	Container calico-kube-controllers ready: false, restart count 0
  May  4 07:33:48.883: INFO: calico-node-ssrv4 from kube-system started at 2023-05-04 01:43:32 +0000 UTC (1 container statuses recorded)
  May  4 07:33:48.883: INFO: 	Container calico-node ready: true, restart count 0
  May  4 07:33:48.883: INFO: coredns-7bdc4cb885-7knh9 from kube-system started at 2023-05-04 01:44:12 +0000 UTC (1 container statuses recorded)
  May  4 07:33:48.883: INFO: 	Container coredns ready: false, restart count 0
  May  4 07:33:48.883: INFO: coredns-7bdc4cb885-pdxsn from kube-system started at 2023-05-04 01:44:12 +0000 UTC (1 container statuses recorded)
  May  4 07:33:48.883: INFO: 	Container coredns ready: false, restart count 0
  May  4 07:33:48.883: INFO: kube-proxy-zxvtj from kube-system started at 2023-05-04 01:43:07 +0000 UTC (1 container statuses recorded)
  May  4 07:33:48.883: INFO: 	Container kube-proxy ready: true, restart count 0
  May  4 07:33:48.883: INFO: busybox-scheduling-525f0a8a-f5f6-4b9b-8795-9b5ce88568e9 from kubelet-test-3002 started at 2023-05-04 07:33:29 +0000 UTC (1 container statuses recorded)
  May  4 07:33:48.883: INFO: 	Container busybox-scheduling-525f0a8a-f5f6-4b9b-8795-9b5ce88568e9 ready: true, restart count 0
  May  4 07:33:48.883: INFO: sonobuoy-systemd-logs-daemon-set-1f5c24134e834838-dqrng from sonobuoy started at 2023-05-04 06:11:45 +0000 UTC (2 container statuses recorded)
  May  4 07:33:48.883: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  4 07:33:48.883: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node k8s-node1 @ 05/04/23 07:33:48.933
  STEP: verifying the node has the label node k8s-node2 @ 05/04/23 07:33:49.013
  May  4 07:33:49.041: INFO: Pod calico-kube-controllers-6849cf9bcf-6tzqn requesting resource cpu=0m on Node k8s-node1
  May  4 07:33:49.041: INFO: Pod calico-kube-controllers-6849cf9bcf-9j56g requesting resource cpu=0m on Node k8s-node2
  May  4 07:33:49.041: INFO: Pod calico-node-5st2f requesting resource cpu=250m on Node k8s-node1
  May  4 07:33:49.041: INFO: Pod calico-node-ssrv4 requesting resource cpu=250m on Node k8s-node2
  May  4 07:33:49.041: INFO: Pod coredns-7bdc4cb885-57mgw requesting resource cpu=100m on Node k8s-node1
  May  4 07:33:49.041: INFO: Pod coredns-7bdc4cb885-7bzz7 requesting resource cpu=100m on Node k8s-node1
  May  4 07:33:49.041: INFO: Pod coredns-7bdc4cb885-7knh9 requesting resource cpu=100m on Node k8s-node2
  May  4 07:33:49.041: INFO: Pod coredns-7bdc4cb885-pdxsn requesting resource cpu=100m on Node k8s-node2
  May  4 07:33:49.041: INFO: Pod kube-proxy-b6n46 requesting resource cpu=0m on Node k8s-node1
  May  4 07:33:49.041: INFO: Pod kube-proxy-zxvtj requesting resource cpu=0m on Node k8s-node2
  May  4 07:33:49.041: INFO: Pod busybox-scheduling-525f0a8a-f5f6-4b9b-8795-9b5ce88568e9 requesting resource cpu=0m on Node k8s-node2
  May  4 07:33:49.041: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-node1
  May  4 07:33:49.041: INFO: Pod sonobuoy-e2e-job-5e11a169cb044f18 requesting resource cpu=0m on Node k8s-node1
  May  4 07:33:49.041: INFO: Pod sonobuoy-systemd-logs-daemon-set-1f5c24134e834838-dqrng requesting resource cpu=0m on Node k8s-node2
  May  4 07:33:49.041: INFO: Pod sonobuoy-systemd-logs-daemon-set-1f5c24134e834838-h8cct requesting resource cpu=0m on Node k8s-node1
  STEP: Starting Pods to consume most of the cluster CPU. @ 05/04/23 07:33:49.041
  May  4 07:33:49.041: INFO: Creating a pod which consumes cpu=1785m on Node k8s-node1
  May  4 07:33:49.086: INFO: Creating a pod which consumes cpu=1785m on Node k8s-node2
  STEP: Creating another pod that requires unavailable amount of CPU. @ 05/04/23 07:33:53.322
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-5f2a938c-c510-4f0a-b8f3-42268bccceea.175be06972f1efee], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9280/filler-pod-5f2a938c-c510-4f0a-b8f3-42268bccceea to k8s-node2] @ 05/04/23 07:33:53.329
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-5f2a938c-c510-4f0a-b8f3-42268bccceea.175be069bca3f4f6], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/04/23 07:33:53.329
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-5f2a938c-c510-4f0a-b8f3-42268bccceea.175be069cbd71a07], Reason = [Created], Message = [Created container filler-pod-5f2a938c-c510-4f0a-b8f3-42268bccceea] @ 05/04/23 07:33:53.329
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-5f2a938c-c510-4f0a-b8f3-42268bccceea.175be069d88e8c54], Reason = [Started], Message = [Started container filler-pod-5f2a938c-c510-4f0a-b8f3-42268bccceea] @ 05/04/23 07:33:53.329
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-8b121438-eb0e-4b1d-8ce2-1ce353d6e38b.175be0696a4a768c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9280/filler-pod-8b121438-eb0e-4b1d-8ce2-1ce353d6e38b to k8s-node1] @ 05/04/23 07:33:53.329
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-8b121438-eb0e-4b1d-8ce2-1ce353d6e38b.175be069b41a41df], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/04/23 07:33:53.329
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-8b121438-eb0e-4b1d-8ce2-1ce353d6e38b.175be069c323f4a3], Reason = [Created], Message = [Created container filler-pod-8b121438-eb0e-4b1d-8ce2-1ce353d6e38b] @ 05/04/23 07:33:53.329
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-8b121438-eb0e-4b1d-8ce2-1ce353d6e38b.175be069cf766e45], Reason = [Started], Message = [Started container filler-pod-8b121438-eb0e-4b1d-8ce2-1ce353d6e38b] @ 05/04/23 07:33:53.33
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.175be06a68270abe], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod..] @ 05/04/23 07:33:53.47
  STEP: removing the label node off the node k8s-node1 @ 05/04/23 07:33:54.38
  STEP: verifying the node doesn't have the label node @ 05/04/23 07:33:54.412
  STEP: removing the label node off the node k8s-node2 @ 05/04/23 07:33:54.419
  STEP: verifying the node doesn't have the label node @ 05/04/23 07:33:54.46
  May  4 07:33:54.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-9280" for this suite. @ 05/04/23 07:33:54.475
• [5.761 seconds]
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 05/04/23 07:33:54.513
  May  4 07:33:54.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename disruption @ 05/04/23 07:33:54.516
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:33:54.552
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:33:54.557
  STEP: Waiting for the pdb to be processed @ 05/04/23 07:33:54.591
  STEP: Updating PodDisruptionBudget status @ 05/04/23 07:33:56.6
  STEP: Waiting for all pods to be running @ 05/04/23 07:33:56.617
  May  4 07:33:56.620: INFO: running pods: 0 < 1
  STEP: locating a running pod @ 05/04/23 07:33:58.625
  STEP: Waiting for the pdb to be processed @ 05/04/23 07:33:58.646
  STEP: Patching PodDisruptionBudget status @ 05/04/23 07:33:58.672
  STEP: Waiting for the pdb to be processed @ 05/04/23 07:33:58.692
  May  4 07:33:58.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-7846" for this suite. @ 05/04/23 07:33:58.752
• [4.260 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 05/04/23 07:33:58.773
  May  4 07:33:58.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename services @ 05/04/23 07:33:58.775
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:33:58.84
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:33:58.847
  STEP: creating service in namespace services-6296 @ 05/04/23 07:33:58.851
  STEP: creating service affinity-clusterip in namespace services-6296 @ 05/04/23 07:33:58.851
  STEP: creating replication controller affinity-clusterip in namespace services-6296 @ 05/04/23 07:33:58.921
  I0504 07:33:58.960681      20 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-6296, replica count: 3
  I0504 07:34:02.012413      20 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0504 07:34:05.013941      20 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  4 07:34:05.020: INFO: Creating new exec pod
  May  4 07:34:08.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-6296 exec execpod-affinityswsrk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  May  4 07:34:08.256: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  May  4 07:34:08.256: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  4 07:34:08.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-6296 exec execpod-affinityswsrk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.196.127 80'
  May  4 07:34:08.479: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.196.127 80\nConnection to 10.101.196.127 80 port [tcp/http] succeeded!\n"
  May  4 07:34:08.479: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  4 07:34:08.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-6296 exec execpod-affinityswsrk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.101.196.127:80/ ; done'
  May  4 07:34:08.846: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.196.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.196.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.196.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.196.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.196.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.196.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.196.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.196.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.196.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.196.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.196.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.196.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.196.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.196.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.196.127:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.196.127:80/\n"
  May  4 07:34:08.846: INFO: stdout: "\naffinity-clusterip-d5h72\naffinity-clusterip-d5h72\naffinity-clusterip-d5h72\naffinity-clusterip-d5h72\naffinity-clusterip-d5h72\naffinity-clusterip-d5h72\naffinity-clusterip-d5h72\naffinity-clusterip-d5h72\naffinity-clusterip-d5h72\naffinity-clusterip-d5h72\naffinity-clusterip-d5h72\naffinity-clusterip-d5h72\naffinity-clusterip-d5h72\naffinity-clusterip-d5h72\naffinity-clusterip-d5h72\naffinity-clusterip-d5h72"
  May  4 07:34:08.846: INFO: Received response from host: affinity-clusterip-d5h72
  May  4 07:34:08.846: INFO: Received response from host: affinity-clusterip-d5h72
  May  4 07:34:08.846: INFO: Received response from host: affinity-clusterip-d5h72
  May  4 07:34:08.846: INFO: Received response from host: affinity-clusterip-d5h72
  May  4 07:34:08.846: INFO: Received response from host: affinity-clusterip-d5h72
  May  4 07:34:08.846: INFO: Received response from host: affinity-clusterip-d5h72
  May  4 07:34:08.846: INFO: Received response from host: affinity-clusterip-d5h72
  May  4 07:34:08.846: INFO: Received response from host: affinity-clusterip-d5h72
  May  4 07:34:08.846: INFO: Received response from host: affinity-clusterip-d5h72
  May  4 07:34:08.846: INFO: Received response from host: affinity-clusterip-d5h72
  May  4 07:34:08.846: INFO: Received response from host: affinity-clusterip-d5h72
  May  4 07:34:08.846: INFO: Received response from host: affinity-clusterip-d5h72
  May  4 07:34:08.846: INFO: Received response from host: affinity-clusterip-d5h72
  May  4 07:34:08.846: INFO: Received response from host: affinity-clusterip-d5h72
  May  4 07:34:08.846: INFO: Received response from host: affinity-clusterip-d5h72
  May  4 07:34:08.846: INFO: Received response from host: affinity-clusterip-d5h72
  May  4 07:34:08.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  4 07:34:08.875: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-6296, will wait for the garbage collector to delete the pods @ 05/04/23 07:34:08.923
  May  4 07:34:09.043: INFO: Deleting ReplicationController affinity-clusterip took: 64.220478ms
  May  4 07:34:09.144: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.09102ms
  STEP: Destroying namespace "services-6296" for this suite. @ 05/04/23 07:34:11.951
• [13.196 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 05/04/23 07:34:11.972
  May  4 07:34:11.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename watch @ 05/04/23 07:34:11.973
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:34:12.03
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:34:12.033
  STEP: getting a starting resourceVersion @ 05/04/23 07:34:12.037
  STEP: starting a background goroutine to produce watch events @ 05/04/23 07:34:12.039
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 05/04/23 07:34:12.04
  May  4 07:34:14.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-4954" for this suite. @ 05/04/23 07:34:14.828
• [2.917 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 05/04/23 07:34:14.89
  May  4 07:34:14.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename downward-api @ 05/04/23 07:34:14.892
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:34:14.922
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:34:14.926
  STEP: Creating a pod to test downward api env vars @ 05/04/23 07:34:14.952
  STEP: Saw pod success @ 05/04/23 07:34:18.986
  May  4 07:34:18.989: INFO: Trying to get logs from node k8s-node2 pod downward-api-12da060f-8ccc-4f8c-9fea-d18e285bb0c0 container dapi-container: <nil>
  STEP: delete the pod @ 05/04/23 07:34:18.996
  May  4 07:34:19.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1161" for this suite. @ 05/04/23 07:34:19.061
• [4.185 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 05/04/23 07:34:19.079
  May  4 07:34:19.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename statefulset @ 05/04/23 07:34:19.08
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:34:19.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:34:19.117
  STEP: Creating service test in namespace statefulset-1098 @ 05/04/23 07:34:19.133
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 05/04/23 07:34:19.15
  STEP: Creating stateful set ss in namespace statefulset-1098 @ 05/04/23 07:34:19.185
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1098 @ 05/04/23 07:34:19.214
  May  4 07:34:19.216: INFO: Found 0 stateful pods, waiting for 1
  May  4 07:34:29.221: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 05/04/23 07:34:29.221
  May  4 07:34:29.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=statefulset-1098 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  4 07:34:29.444: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  4 07:34:29.444: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  4 07:34:29.444: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  4 07:34:29.448: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  May  4 07:34:39.453: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May  4 07:34:39.453: INFO: Waiting for statefulset status.replicas updated to 0
  May  4 07:34:39.493: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999548s
  May  4 07:34:40.497: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996168007s
  May  4 07:34:41.502: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.99229754s
  May  4 07:34:42.505: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988110981s
  May  4 07:34:43.511: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.983422334s
  May  4 07:34:44.515: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.978796813s
  May  4 07:34:45.520: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.973590282s
  May  4 07:34:46.524: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.969817504s
  May  4 07:34:47.528: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.965604776s
  May  4 07:34:48.533: INFO: Verifying statefulset ss doesn't scale past 1 for another 960.876018ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1098 @ 05/04/23 07:34:49.533
  May  4 07:34:49.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=statefulset-1098 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  4 07:34:49.771: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May  4 07:34:49.771: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  4 07:34:49.771: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  4 07:34:49.776: INFO: Found 1 stateful pods, waiting for 3
  May  4 07:34:59.783: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May  4 07:34:59.783: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  May  4 07:34:59.783: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 05/04/23 07:34:59.784
  STEP: Scale down will halt with unhealthy stateful pod @ 05/04/23 07:34:59.784
  May  4 07:34:59.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=statefulset-1098 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  4 07:34:59.993: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  4 07:34:59.993: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  4 07:34:59.993: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  4 07:34:59.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=statefulset-1098 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  4 07:35:00.226: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  4 07:35:00.226: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  4 07:35:00.226: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  4 07:35:00.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=statefulset-1098 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  4 07:35:00.471: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  4 07:35:00.471: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  4 07:35:00.471: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  4 07:35:00.471: INFO: Waiting for statefulset status.replicas updated to 0
  May  4 07:35:00.475: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  May  4 07:35:10.483: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May  4 07:35:10.484: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  May  4 07:35:10.484: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  May  4 07:35:10.505: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999574s
  May  4 07:35:11.509: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996224925s
  May  4 07:35:12.513: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992133826s
  May  4 07:35:13.522: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986973601s
  May  4 07:35:14.526: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979241433s
  May  4 07:35:15.531: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.974408274s
  May  4 07:35:16.536: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969925603s
  May  4 07:35:17.540: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.965208158s
  May  4 07:35:18.545: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.960322933s
  May  4 07:35:19.549: INFO: Verifying statefulset ss doesn't scale past 3 for another 956.137469ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1098 @ 05/04/23 07:35:20.549
  May  4 07:35:20.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=statefulset-1098 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  4 07:35:20.755: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May  4 07:35:20.755: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  4 07:35:20.755: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  4 07:35:20.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=statefulset-1098 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  4 07:35:20.962: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May  4 07:35:20.962: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  4 07:35:20.962: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  4 07:35:20.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=statefulset-1098 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  4 07:35:21.157: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May  4 07:35:21.157: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  4 07:35:21.157: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  4 07:35:21.157: INFO: Scaling statefulset ss to 0
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 05/04/23 07:35:31.202
  May  4 07:35:31.202: INFO: Deleting all statefulset in ns statefulset-1098
  May  4 07:35:31.206: INFO: Scaling statefulset ss to 0
  May  4 07:35:31.219: INFO: Waiting for statefulset status.replicas updated to 0
  May  4 07:35:31.223: INFO: Deleting statefulset ss
  May  4 07:35:31.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1098" for this suite. @ 05/04/23 07:35:31.28
• [72.241 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 05/04/23 07:35:31.323
  May  4 07:35:31.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename container-runtime @ 05/04/23 07:35:31.325
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:35:31.411
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:35:31.417
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 05/04/23 07:35:31.442
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 05/04/23 07:35:52.61
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 05/04/23 07:35:52.613
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 05/04/23 07:35:52.62
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 05/04/23 07:35:52.62
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 05/04/23 07:35:52.739
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 05/04/23 07:35:56.76
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 05/04/23 07:35:57.768
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 05/04/23 07:35:57.778
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 05/04/23 07:35:57.778
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 05/04/23 07:35:57.86
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 05/04/23 07:35:58.898
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 05/04/23 07:36:01.959
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 05/04/23 07:36:01.966
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 05/04/23 07:36:01.966
  May  4 07:36:02.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-3612" for this suite. @ 05/04/23 07:36:02.057
• [30.751 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 05/04/23 07:36:02.075
  May  4 07:36:02.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename replication-controller @ 05/04/23 07:36:02.077
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:36:02.126
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:36:02.13
  STEP: creating a ReplicationController @ 05/04/23 07:36:02.166
  STEP: waiting for RC to be added @ 05/04/23 07:36:02.183
  STEP: waiting for available Replicas @ 05/04/23 07:36:02.183
  STEP: patching ReplicationController @ 05/04/23 07:36:04.967
  STEP: waiting for RC to be modified @ 05/04/23 07:36:04.997
  STEP: patching ReplicationController status @ 05/04/23 07:36:04.997
  STEP: waiting for RC to be modified @ 05/04/23 07:36:05.043
  STEP: waiting for available Replicas @ 05/04/23 07:36:05.043
  STEP: fetching ReplicationController status @ 05/04/23 07:36:05.059
  STEP: patching ReplicationController scale @ 05/04/23 07:36:05.062
  STEP: waiting for RC to be modified @ 05/04/23 07:36:05.094
  STEP: waiting for ReplicationController's scale to be the max amount @ 05/04/23 07:36:05.094
  STEP: fetching ReplicationController; ensuring that it's patched @ 05/04/23 07:36:07.674
  STEP: updating ReplicationController status @ 05/04/23 07:36:07.678
  STEP: waiting for RC to be modified @ 05/04/23 07:36:07.695
  STEP: listing all ReplicationControllers @ 05/04/23 07:36:07.695
  STEP: checking that ReplicationController has expected values @ 05/04/23 07:36:07.698
  STEP: deleting ReplicationControllers by collection @ 05/04/23 07:36:07.698
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 05/04/23 07:36:07.783
  May  4 07:36:07.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0504 07:36:07.948710      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-9283" for this suite. @ 05/04/23 07:36:07.95
• [5.920 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 05/04/23 07:36:07.999
  May  4 07:36:07.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename downward-api @ 05/04/23 07:36:08
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:36:08.039
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:36:08.042
  STEP: Creating a pod to test downward API volume plugin @ 05/04/23 07:36:08.046
  E0504 07:36:08.945313      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:09.945528      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:10.945700      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:11.946160      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/04/23 07:36:12.099
  May  4 07:36:12.102: INFO: Trying to get logs from node k8s-node2 pod downwardapi-volume-53fa9f2d-3793-47da-8ed9-385f28801912 container client-container: <nil>
  STEP: delete the pod @ 05/04/23 07:36:12.126
  May  4 07:36:12.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4963" for this suite. @ 05/04/23 07:36:12.176
• [4.211 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 05/04/23 07:36:12.211
  May  4 07:36:12.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename webhook @ 05/04/23 07:36:12.212
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:36:12.254
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:36:12.258
  STEP: Setting up server cert @ 05/04/23 07:36:12.35
  E0504 07:36:12.946339      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/04/23 07:36:13.216
  STEP: Deploying the webhook pod @ 05/04/23 07:36:13.275
  STEP: Wait for the deployment to be ready @ 05/04/23 07:36:13.404
  May  4 07:36:13.444: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0504 07:36:13.947336      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:14.947582      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:36:15.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 4, 7, 36, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 36, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 36, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 36, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0504 07:36:15.947852      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:16.948666      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/04/23 07:36:17.479
  STEP: Verifying the service has paired with the endpoint @ 05/04/23 07:36:17.53
  E0504 07:36:17.949764      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:36:18.531: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 05/04/23 07:36:18.535
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/04/23 07:36:18.535
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 05/04/23 07:36:18.579
  E0504 07:36:18.950682      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 05/04/23 07:36:19.604
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/04/23 07:36:19.604
  E0504 07:36:19.951306      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is longer than webhook latency @ 05/04/23 07:36:20.707
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/04/23 07:36:20.707
  E0504 07:36:20.952090      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:21.952645      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:22.953188      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:23.953971      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:24.954266      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 05/04/23 07:36:25.798
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/04/23 07:36:25.798
  E0504 07:36:25.955097      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:26.955255      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:27.955589      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:28.956501      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:29.956644      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:36:30.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0504 07:36:30.957813      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-7516" for this suite. @ 05/04/23 07:36:31.125
  STEP: Destroying namespace "webhook-markers-1417" for this suite. @ 05/04/23 07:36:31.184
• [19.002 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 05/04/23 07:36:31.217
  May  4 07:36:31.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename container-probe @ 05/04/23 07:36:31.219
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:36:31.292
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:36:31.298
  E0504 07:36:31.958024      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:32.958197      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:33.958901      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:34.959083      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:35.959233      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:36.959529      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:37.960638      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:38.960730      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:39.961450      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:40.961593      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:41.962368      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:42.962536      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:43.962897      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:44.963532      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:45.964060      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:46.964450      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:47.964833      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:48.965671      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:49.965898      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:50.966094      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:51.966547      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:52.966936      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:36:53.413: INFO: Container started at 2023-05-04 07:36:32 +0000 UTC, pod became ready at 2023-05-04 07:36:51 +0000 UTC
  May  4 07:36:53.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-9373" for this suite. @ 05/04/23 07:36:53.418
• [22.225 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 05/04/23 07:36:53.445
  May  4 07:36:53.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename kubectl @ 05/04/23 07:36:53.447
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:36:53.492
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:36:53.497
  STEP: creating the pod @ 05/04/23 07:36:53.519
  May  4 07:36:53.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-4135 create -f -'
  E0504 07:36:53.967147      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:36:54.245: INFO: stderr: ""
  May  4 07:36:54.245: INFO: stdout: "pod/pause created\n"
  E0504 07:36:54.967624      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:55.967884      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 05/04/23 07:36:56.273
  May  4 07:36:56.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-4135 label pods pause testing-label=testing-label-value'
  May  4 07:36:56.422: INFO: stderr: ""
  May  4 07:36:56.422: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 05/04/23 07:36:56.422
  May  4 07:36:56.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-4135 get pod pause -L testing-label'
  May  4 07:36:56.546: INFO: stderr: ""
  May  4 07:36:56.546: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 05/04/23 07:36:56.547
  May  4 07:36:56.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-4135 label pods pause testing-label-'
  May  4 07:36:56.694: INFO: stderr: ""
  May  4 07:36:56.694: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 05/04/23 07:36:56.695
  May  4 07:36:56.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-4135 get pod pause -L testing-label'
  May  4 07:36:56.817: INFO: stderr: ""
  May  4 07:36:56.818: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
  STEP: using delete to clean up resources @ 05/04/23 07:36:56.818
  May  4 07:36:56.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-4135 delete --grace-period=0 --force -f -'
  May  4 07:36:56.963: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  4 07:36:56.963: INFO: stdout: "pod \"pause\" force deleted\n"
  May  4 07:36:56.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-4135 get rc,svc -l name=pause --no-headers'
  E0504 07:36:56.968877      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:36:57.088: INFO: stderr: "No resources found in kubectl-4135 namespace.\n"
  May  4 07:36:57.088: INFO: stdout: ""
  May  4 07:36:57.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-4135 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May  4 07:36:57.209: INFO: stderr: ""
  May  4 07:36:57.210: INFO: stdout: ""
  May  4 07:36:57.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4135" for this suite. @ 05/04/23 07:36:57.215
• [3.799 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 05/04/23 07:36:57.254
  May  4 07:36:57.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename kubectl @ 05/04/23 07:36:57.255
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:36:57.312
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:36:57.317
  STEP: validating cluster-info @ 05/04/23 07:36:57.322
  May  4 07:36:57.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-6064 cluster-info'
  May  4 07:36:57.434: INFO: stderr: ""
  May  4 07:36:57.434: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  May  4 07:36:57.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6064" for this suite. @ 05/04/23 07:36:57.44
• [0.200 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 05/04/23 07:36:57.455
  May  4 07:36:57.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename secrets @ 05/04/23 07:36:57.457
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:36:57.527
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:36:57.532
  STEP: creating a secret @ 05/04/23 07:36:57.536
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 05/04/23 07:36:57.553
  STEP: patching the secret @ 05/04/23 07:36:57.556
  STEP: deleting the secret using a LabelSelector @ 05/04/23 07:36:57.585
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 05/04/23 07:36:57.601
  May  4 07:36:57.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7918" for this suite. @ 05/04/23 07:36:57.607
• [0.167 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 05/04/23 07:36:57.624
  May  4 07:36:57.624: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 07:36:57.625
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:36:57.697
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:36:57.701
  STEP: Creating a pod to test downward API volume plugin @ 05/04/23 07:36:57.705
  E0504 07:36:57.969743      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:58.970497      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:36:59.971505      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:00.971681      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/04/23 07:37:01.739
  May  4 07:37:01.742: INFO: Trying to get logs from node k8s-node2 pod downwardapi-volume-d7cdc43f-32c3-41cd-9498-5d7dec785735 container client-container: <nil>
  STEP: delete the pod @ 05/04/23 07:37:01.75
  May  4 07:37:01.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6082" for this suite. @ 05/04/23 07:37:01.799
• [4.209 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 05/04/23 07:37:01.835
  May  4 07:37:01.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 07:37:01.836
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:37:01.889
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:37:01.894
  STEP: Creating a pod to test downward API volume plugin @ 05/04/23 07:37:01.898
  E0504 07:37:01.972874      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:02.973112      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:03.974150      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:04.974480      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:05.974876      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:06.975368      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/04/23 07:37:07.93
  May  4 07:37:07.933: INFO: Trying to get logs from node k8s-node2 pod downwardapi-volume-e056a321-607f-4e4b-8212-b8a0f2ad3eb9 container client-container: <nil>
  STEP: delete the pod @ 05/04/23 07:37:07.939
  E0504 07:37:07.976194      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:37:07.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-259" for this suite. @ 05/04/23 07:37:07.996
• [6.178 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 05/04/23 07:37:08.016
  May  4 07:37:08.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename resourcequota @ 05/04/23 07:37:08.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:37:08.072
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:37:08.076
  STEP: Discovering how many secrets are in namespace by default @ 05/04/23 07:37:08.08
  E0504 07:37:08.976385      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:09.977282      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:10.977695      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:11.977871      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:12.978742      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 05/04/23 07:37:13.086
  E0504 07:37:13.979191      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:14.979350      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:15.979474      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:16.980356      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:17.980840      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/04/23 07:37:18.09
  STEP: Ensuring resource quota status is calculated @ 05/04/23 07:37:18.106
  E0504 07:37:18.981509      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:19.982214      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Secret @ 05/04/23 07:37:20.11
  STEP: Ensuring resource quota status captures secret creation @ 05/04/23 07:37:20.149
  E0504 07:37:20.982645      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:21.982887      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a secret @ 05/04/23 07:37:22.154
  STEP: Ensuring resource quota status released usage @ 05/04/23 07:37:22.169
  E0504 07:37:22.983186      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:23.983879      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:37:24.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6097" for this suite. @ 05/04/23 07:37:24.18
• [16.179 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 05/04/23 07:37:24.199
  May  4 07:37:24.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename taint-multiple-pods @ 05/04/23 07:37:24.2
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:37:24.235
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:37:24.239
  May  4 07:37:24.287: INFO: Waiting up to 1m0s for all nodes to be ready
  E0504 07:37:24.984630      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:25.985738      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:26.986908      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:27.987859      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:28.988128      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:29.988470      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:30.989198      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:31.990243      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:32.990658      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:33.991450      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:34.991726      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:35.992075      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:36.993093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:37.994120      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:38.994683      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:39.995089      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:40.995729      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:41.995927      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:42.996927      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:43.997820      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:44.997770      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:45.997975      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:46.998953      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:47.999906      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:49.000572      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:50.000781      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:51.001524      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:52.001633      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:53.001809      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:54.001933      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:55.002403      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:56.002895      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:57.003113      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:58.003684      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:37:59.003910      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:00.004134      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:01.004380      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:02.004509      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:03.004603      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:04.004768      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:05.005835      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:06.006211      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:07.006942      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:08.007617      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:09.007787      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:10.008010      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:11.008970      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:12.009231      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:13.009452      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:14.009605      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:15.009744      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:16.010386      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:17.011479      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:18.012396      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:19.012561      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:20.012873      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:21.013934      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:22.014339      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:23.015256      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:24.015997      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:38:24.315: INFO: Waiting for terminating namespaces to be deleted...
  May  4 07:38:24.319: INFO: Starting informer...
  STEP: Starting pods... @ 05/04/23 07:38:24.319
  May  4 07:38:24.560: INFO: Pod1 is running on k8s-node2. Tainting Node
  E0504 07:38:25.017466      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:26.017599      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:38:26.797: INFO: Pod2 is running on k8s-node2. Tainting Node
  STEP: Trying to apply a taint on the Node @ 05/04/23 07:38:26.797
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/04/23 07:38:26.833
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 05/04/23 07:38:26.846
  E0504 07:38:27.017780      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:28.018793      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:29.019333      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:30.019525      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:31.019788      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:32.020566      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:33.021255      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:38:33.454: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  E0504 07:38:34.021462      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:35.021630      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:36.021872      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:37.022017      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:38.023022      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:39.023392      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:40.023511      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:41.023821      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:42.024130      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:43.024887      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:44.025322      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:45.025675      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:46.026167      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:47.026518      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:48.027443      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:49.027815      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:50.028146      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:51.028402      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:52.029428      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:53.029970      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:38:53.563: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  May  4 07:38:53.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/04/23 07:38:53.61
  STEP: Destroying namespace "taint-multiple-pods-5592" for this suite. @ 05/04/23 07:38:53.62
• [89.440 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 05/04/23 07:38:53.641
  May  4 07:38:53.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename namespaces @ 05/04/23 07:38:53.644
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:38:53.711
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:38:53.715
  STEP: Updating Namespace "namespaces-159" @ 05/04/23 07:38:53.724
  May  4 07:38:53.758: INFO: Namespace "namespaces-159" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"03fb56be-d3a0-4cb6-b1ca-4a4094abfd3d", "kubernetes.io/metadata.name":"namespaces-159", "namespaces-159":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  May  4 07:38:53.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-159" for this suite. @ 05/04/23 07:38:53.764
• [0.149 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 05/04/23 07:38:53.793
  May  4 07:38:53.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename container-probe @ 05/04/23 07:38:53.795
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:38:53.888
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:38:53.893
  STEP: Creating pod test-webserver-c794b272-9251-4774-aa4d-070d6596d0cc in namespace container-probe-7888 @ 05/04/23 07:38:53.897
  E0504 07:38:54.031019      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:55.031448      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:38:55.926: INFO: Started pod test-webserver-c794b272-9251-4774-aa4d-070d6596d0cc in namespace container-probe-7888
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/04/23 07:38:55.926
  May  4 07:38:55.929: INFO: Initial restart count of pod test-webserver-c794b272-9251-4774-aa4d-070d6596d0cc is 0
  E0504 07:38:56.032473      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:57.032581      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:58.032877      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:38:59.033309      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:00.033496      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:01.033900      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:02.035020      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:03.035189      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:04.035214      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:05.035480      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:06.036116      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:07.036427      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:08.037532      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:09.037724      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:10.038315      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:11.038695      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:12.039208      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:13.039514      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:14.040054      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:15.040984      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:16.041072      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:17.041338      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:18.041702      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:19.042063      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:20.042903      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:21.043331      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:22.043680      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:23.044852      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:24.045818      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:25.046228      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:26.047179      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:27.047568      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:28.047784      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:29.048171      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:30.049240      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:31.049480      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:32.050448      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:33.051614      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:34.052423      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:35.052547      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:36.053493      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:37.053741      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:38.054618      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:39.054844      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:40.055599      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:41.056653      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:42.057395      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:43.058489      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:44.058568      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:45.058933      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:46.059883      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:47.060098      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:48.060907      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:49.061184      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:50.061649      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:51.061889      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:52.061994      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:53.062692      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:54.062871      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:55.063932      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:56.064265      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:57.064903      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:58.065755      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:39:59.065915      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:00.066364      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:01.066554      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:02.067603      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:03.068196      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:04.068268      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:05.068983      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:06.069148      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:07.070342      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:08.071135      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:09.071346      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:10.071498      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:11.071631      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:12.071847      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:13.072801      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:14.073105      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:15.073274      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:16.073657      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:17.073719      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:18.074634      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:19.075480      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:20.075875      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:21.076700      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:22.077073      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:23.078246      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:24.078473      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:25.078648      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:26.078872      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:27.079695      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:28.080529      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:29.081164      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:30.081561      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:31.082228      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:32.082780      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:33.083572      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:34.083922      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:35.084708      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:36.084983      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:37.085856      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:38.086793      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:39.087729      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:40.088472      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:41.088673      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:42.089074      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:43.089397      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:44.089514      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:45.090082      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:46.090450      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:47.091369      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:48.092222      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:49.092899      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:50.093362      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:51.093449      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:52.093849      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:53.094386      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:54.094885      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:55.095510      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:56.095859      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:57.095918      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:58.096646      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:40:59.097610      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:00.097788      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:01.098608      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:02.099212      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:03.100080      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:04.100753      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:05.100889      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:06.101244      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:07.101558      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:08.102375      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:09.103191      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:10.103536      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:11.104463      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:12.104943      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:13.105043      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:14.105230      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:15.105363      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:16.105677      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:17.106444      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:18.107554      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:19.107696      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:20.108254      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:21.108485      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:22.108839      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:23.109658      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:24.110488      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:25.111169      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:26.111614      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:27.112345      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:28.113210      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:29.114023      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:30.114439      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:31.114552      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:32.115048      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:33.115201      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:34.115613      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:35.116417      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:36.116566      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:37.117622      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:38.118262      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:39.118524      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:40.118738      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:41.118851      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:42.118972      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:43.119196      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:44.119346      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:45.119768      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:46.120399      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:47.121243      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:48.122024      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:49.122372      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:50.122534      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:51.123353      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:52.123489      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:53.124081      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:54.124476      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:55.124788      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:56.124979      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:57.125539      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:58.126629      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:41:59.126878      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:00.127036      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:01.127494      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:02.128636      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:03.129739      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:04.129914      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:05.130244      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:06.130493      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:07.131639      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:08.132229      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:09.132504      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:10.132523      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:11.132695      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:12.132893      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:13.133357      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:14.133492      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:15.134109      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:16.134617      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:17.135548      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:18.136269      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:19.136577      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:20.136752      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:21.137404      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:22.137771      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:23.138848      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:24.138942      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:25.139115      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:26.139330      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:27.140028      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:28.140878      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:29.142025      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:30.142657      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:31.143612      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:32.143778      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:33.144229      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:34.144432      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:35.144490      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:36.144820      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:37.144927      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:38.145896      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:39.146564      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:40.146786      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:41.147358      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:42.147535      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:43.148340      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:44.148545      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:45.148969      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:46.149257      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:47.149374      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:48.150204      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:49.150779      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:50.151149      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:51.151385      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:52.151593      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:53.152477      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:54.152870      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:55.153044      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:56.153534      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:42:56.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/04/23 07:42:56.518
  STEP: Destroying namespace "container-probe-7888" for this suite. @ 05/04/23 07:42:56.585
• [242.839 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 05/04/23 07:42:56.634
  May  4 07:42:56.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename kubectl @ 05/04/23 07:42:56.636
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:42:56.723
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:42:56.728
  May  4 07:42:56.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-7695 version'
  May  4 07:42:56.861: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  May  4 07:42:56.861: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:21:19Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:14:42Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  May  4 07:42:56.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7695" for this suite. @ 05/04/23 07:42:56.868
• [0.252 seconds]
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 05/04/23 07:42:56.887
  May  4 07:42:56.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename configmap @ 05/04/23 07:42:56.888
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:42:56.991
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:42:56.996
  STEP: Creating configMap that has name configmap-test-emptyKey-f097bb23-5621-43bd-aedf-f638225e7fb7 @ 05/04/23 07:42:57.001
  May  4 07:42:57.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5920" for this suite. @ 05/04/23 07:42:57.008
• [0.137 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 05/04/23 07:42:57.026
  May  4 07:42:57.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename replication-controller @ 05/04/23 07:42:57.028
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:42:57.089
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:42:57.098
  STEP: Given a ReplicationController is created @ 05/04/23 07:42:57.106
  STEP: When the matched label of one of its pods change @ 05/04/23 07:42:57.124
  May  4 07:42:57.128: INFO: Pod name pod-release: Found 0 pods out of 1
  E0504 07:42:57.154116      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:58.154950      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:42:59.155211      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:00.155428      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:01.155618      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:43:02.133: INFO: Pod name pod-release: Found 1 pods out of 1
  E0504 07:43:02.156451      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Then the pod is released @ 05/04/23 07:43:02.173
  E0504 07:43:03.157537      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:43:03.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-5291" for this suite. @ 05/04/23 07:43:03.235
• [6.227 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 05/04/23 07:43:03.254
  May  4 07:43:03.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename csiinlinevolumes @ 05/04/23 07:43:03.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:43:03.303
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:43:03.311
  STEP: creating @ 05/04/23 07:43:03.322
  STEP: getting @ 05/04/23 07:43:03.528
  STEP: listing in namespace @ 05/04/23 07:43:03.549
  STEP: patching @ 05/04/23 07:43:03.586
  STEP: deleting @ 05/04/23 07:43:03.666
  May  4 07:43:03.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-8799" for this suite. @ 05/04/23 07:43:03.694
• [0.479 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 05/04/23 07:43:03.735
  May  4 07:43:03.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename secrets @ 05/04/23 07:43:03.736
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:43:03.825
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:43:03.83
  STEP: Creating secret with name secret-test-fe189ac0-ff9c-495d-b1fd-3a25f8337329 @ 05/04/23 07:43:03.834
  STEP: Creating a pod to test consume secrets @ 05/04/23 07:43:03.887
  E0504 07:43:04.157613      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:05.158069      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:06.158213      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:07.158468      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:08.159221      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:09.159694      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/04/23 07:43:09.949
  May  4 07:43:09.952: INFO: Trying to get logs from node k8s-node1 pod pod-secrets-52b7d54c-faff-4128-be7d-e906342f3f8e container secret-volume-test: <nil>
  STEP: delete the pod @ 05/04/23 07:43:09.961
  May  4 07:43:10.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-985" for this suite. @ 05/04/23 07:43:10.016
• [6.305 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 05/04/23 07:43:10.044
  May  4 07:43:10.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename services @ 05/04/23 07:43:10.045
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:43:10.105
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:43:10.11
  STEP: creating service nodeport-test with type=NodePort in namespace services-4990 @ 05/04/23 07:43:10.128
  E0504 07:43:10.160834      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating replication controller nodeport-test in namespace services-4990 @ 05/04/23 07:43:10.217
  I0504 07:43:10.246695      20 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-4990, replica count: 2
  E0504 07:43:11.160895      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:12.161919      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:13.162667      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0504 07:43:13.298242      20 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  4 07:43:13.298: INFO: Creating new exec pod
  E0504 07:43:14.163744      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:15.163915      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:16.164845      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:43:16.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-4990 exec execpodzn7zg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  May  4 07:43:16.577: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  May  4 07:43:16.577: INFO: stdout: "nodeport-test-zbdpz"
  May  4 07:43:16.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-4990 exec execpodzn7zg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.180.244 80'
  May  4 07:43:16.786: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.180.244 80\nConnection to 10.98.180.244 80 port [tcp/http] succeeded!\n"
  May  4 07:43:16.786: INFO: stdout: ""
  E0504 07:43:17.164960      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:43:17.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-4990 exec execpodzn7zg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.180.244 80'
  May  4 07:43:17.997: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.180.244 80\nConnection to 10.98.180.244 80 port [tcp/http] succeeded!\n"
  May  4 07:43:17.997: INFO: stdout: ""
  E0504 07:43:18.166117      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:43:18.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-4990 exec execpodzn7zg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.180.244 80'
  May  4 07:43:18.990: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.180.244 80\nConnection to 10.98.180.244 80 port [tcp/http] succeeded!\n"
  May  4 07:43:18.990: INFO: stdout: ""
  E0504 07:43:19.166821      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:43:19.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-4990 exec execpodzn7zg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.180.244 80'
  May  4 07:43:19.996: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.180.244 80\nConnection to 10.98.180.244 80 port [tcp/http] succeeded!\n"
  May  4 07:43:19.996: INFO: stdout: ""
  E0504 07:43:20.167345      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:43:20.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-4990 exec execpodzn7zg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.180.244 80'
  May  4 07:43:21.037: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.180.244 80\nConnection to 10.98.180.244 80 port [tcp/http] succeeded!\n"
  May  4 07:43:21.037: INFO: stdout: ""
  E0504 07:43:21.168249      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:43:21.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-4990 exec execpodzn7zg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.180.244 80'
  May  4 07:43:22.003: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.180.244 80\nConnection to 10.98.180.244 80 port [tcp/http] succeeded!\n"
  May  4 07:43:22.003: INFO: stdout: ""
  E0504 07:43:22.168717      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:43:22.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-4990 exec execpodzn7zg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.180.244 80'
  May  4 07:43:22.987: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.180.244 80\nConnection to 10.98.180.244 80 port [tcp/http] succeeded!\n"
  May  4 07:43:22.987: INFO: stdout: "nodeport-test-zbdpz"
  May  4 07:43:22.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-4990 exec execpodzn7zg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.156 32573'
  E0504 07:43:23.169500      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:43:23.248: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.156 32573\nConnection to 192.168.0.156 32573 port [tcp/*] succeeded!\n"
  May  4 07:43:23.248: INFO: stdout: ""
  E0504 07:43:24.170469      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:43:24.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-4990 exec execpodzn7zg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.156 32573'
  May  4 07:43:24.493: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.156 32573\nConnection to 192.168.0.156 32573 port [tcp/*] succeeded!\n"
  May  4 07:43:24.493: INFO: stdout: "nodeport-test-zbdpz"
  May  4 07:43:24.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-4990 exec execpodzn7zg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.0.157 32573'
  May  4 07:43:24.707: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.0.157 32573\nConnection to 192.168.0.157 32573 port [tcp/*] succeeded!\n"
  May  4 07:43:24.707: INFO: stdout: "nodeport-test-p46sc"
  May  4 07:43:24.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4990" for this suite. @ 05/04/23 07:43:24.712
• [14.697 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 05/04/23 07:43:24.743
  May  4 07:43:24.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename svcaccounts @ 05/04/23 07:43:24.745
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:43:24.81
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:43:24.815
  May  4 07:43:24.872: INFO: created pod
  E0504 07:43:25.170525      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:26.170903      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:27.171644      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:28.172237      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/04/23 07:43:28.893
  E0504 07:43:29.172455      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:30.172872      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:31.173667      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:32.174029      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:33.175045      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:34.175333      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:35.175699      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:36.176149      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:37.176595      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:38.177401      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:39.177759      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:40.177936      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:41.178158      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:42.178402      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:43.179393      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:44.180414      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:45.180769      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:46.181224      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:47.181664      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:48.181904      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:49.182110      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:50.182460      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:51.182920      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:52.183180      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:53.186401      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:54.186421      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:55.186864      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:56.187272      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:57.187715      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:43:58.187759      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:43:58.893: INFO: polling logs
  May  4 07:43:58.904: INFO: Pod logs: 
  I0504 07:43:26.312686       1 log.go:198] OK: Got token
  I0504 07:43:26.312811       1 log.go:198] validating with in-cluster discovery
  I0504 07:43:26.313930       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
  I0504 07:43:26.314000       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5670:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683186805, NotBefore:1683186205, IssuedAt:1683186205, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5670", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"cefeef56-e097-467c-9fa2-fc83c6828ee7"}}}
  I0504 07:43:26.338585       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I0504 07:43:26.351003       1 log.go:198] OK: Validated signature on JWT
  I0504 07:43:26.351133       1 log.go:198] OK: Got valid claims from token!
  I0504 07:43:26.351178       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5670:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683186805, NotBefore:1683186205, IssuedAt:1683186205, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5670", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"cefeef56-e097-467c-9fa2-fc83c6828ee7"}}}

  May  4 07:43:58.904: INFO: completed pod
  May  4 07:43:58.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5670" for this suite. @ 05/04/23 07:43:58.925
• [34.196 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 05/04/23 07:43:58.943
  May  4 07:43:58.943: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename webhook @ 05/04/23 07:43:58.944
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:43:58.999
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:43:59.004
  STEP: Setting up server cert @ 05/04/23 07:43:59.066
  E0504 07:43:59.188890      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:00.189458      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/04/23 07:44:00.329
  STEP: Deploying the webhook pod @ 05/04/23 07:44:00.378
  STEP: Wait for the deployment to be ready @ 05/04/23 07:44:00.418
  May  4 07:44:00.446: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0504 07:44:01.189832      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:02.190864      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/04/23 07:44:02.456
  STEP: Verifying the service has paired with the endpoint @ 05/04/23 07:44:02.501
  E0504 07:44:03.190971      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:44:03.502: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 05/04/23 07:44:03.507
  STEP: create a pod that should be updated by the webhook @ 05/04/23 07:44:03.548
  May  4 07:44:03.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4203" for this suite. @ 05/04/23 07:44:03.88
  STEP: Destroying namespace "webhook-markers-1886" for this suite. @ 05/04/23 07:44:03.909
• [4.982 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 05/04/23 07:44:03.926
  May  4 07:44:03.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename field-validation @ 05/04/23 07:44:03.927
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:44:03.985
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:44:03.992
  STEP: apply creating a deployment @ 05/04/23 07:44:04.042
  May  4 07:44:04.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9613" for this suite. @ 05/04/23 07:44:04.111
• [0.212 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 05/04/23 07:44:04.139
  May  4 07:44:04.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename downward-api @ 05/04/23 07:44:04.141
  E0504 07:44:04.191873      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:44:04.24
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:44:04.247
  STEP: Creating a pod to test downward API volume plugin @ 05/04/23 07:44:04.252
  E0504 07:44:05.191919      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:06.192510      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:07.193345      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:08.194095      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/04/23 07:44:08.312
  May  4 07:44:08.316: INFO: Trying to get logs from node k8s-node2 pod downwardapi-volume-5e4696bb-5a06-49da-a941-705f1188feac container client-container: <nil>
  STEP: delete the pod @ 05/04/23 07:44:08.323
  May  4 07:44:08.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6312" for this suite. @ 05/04/23 07:44:08.381
• [4.271 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 05/04/23 07:44:08.414
  May  4 07:44:08.414: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 07:44:08.415
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:44:08.48
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:44:08.485
  STEP: Creating a pod to test downward API volume plugin @ 05/04/23 07:44:08.49
  E0504 07:44:09.194768      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:10.195103      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:11.195816      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:12.196164      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:13.196921      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:14.197085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/04/23 07:44:14.56
  May  4 07:44:14.581: INFO: Trying to get logs from node k8s-node2 pod downwardapi-volume-4e576d80-c8eb-40b3-9ab6-46a1d6cd3107 container client-container: <nil>
  STEP: delete the pod @ 05/04/23 07:44:14.588
  May  4 07:44:14.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-609" for this suite. @ 05/04/23 07:44:14.637
• [6.239 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 05/04/23 07:44:14.656
  May  4 07:44:14.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename pods @ 05/04/23 07:44:14.658
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:44:14.725
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:44:14.729
  May  4 07:44:14.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: creating the pod @ 05/04/23 07:44:14.734
  STEP: submitting the pod to kubernetes @ 05/04/23 07:44:14.734
  E0504 07:44:15.197527      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:16.197765      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:44:16.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5777" for this suite. @ 05/04/23 07:44:16.875
• [2.249 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 05/04/23 07:44:16.906
  May  4 07:44:16.906: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename secrets @ 05/04/23 07:44:16.908
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:44:16.942
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:44:16.947
  STEP: Creating projection with secret that has name secret-emptykey-test-e72750e1-b24d-493b-89c0-84c2264f5305 @ 05/04/23 07:44:16.952
  May  4 07:44:16.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3381" for this suite. @ 05/04/23 07:44:16.963
• [0.128 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 05/04/23 07:44:17.035
  May  4 07:44:17.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename replicaset @ 05/04/23 07:44:17.036
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:44:17.069
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:44:17.077
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 05/04/23 07:44:17.081
  E0504 07:44:17.198451      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:18.199488      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:19.199844      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:20.200234      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replicaset with a matching selector is created @ 05/04/23 07:44:21.123
  STEP: Then the orphan pod is adopted @ 05/04/23 07:44:21.147
  E0504 07:44:21.201237      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When the matched label of one of its pods change @ 05/04/23 07:44:22.156
  May  4 07:44:22.161: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  E0504 07:44:22.201879      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Then the pod is released @ 05/04/23 07:44:22.214
  May  4 07:44:22.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-9397" for this suite. @ 05/04/23 07:44:22.311
• [5.417 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 05/04/23 07:44:22.459
  May  4 07:44:22.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename statefulset @ 05/04/23 07:44:22.461
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:44:22.595
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:44:22.599
  STEP: Creating service test in namespace statefulset-5023 @ 05/04/23 07:44:22.612
  STEP: Creating stateful set ss in namespace statefulset-5023 @ 05/04/23 07:44:22.664
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5023 @ 05/04/23 07:44:22.709
  May  4 07:44:22.730: INFO: Found 0 stateful pods, waiting for 1
  E0504 07:44:23.202017      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:24.202595      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:25.202957      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:26.203206      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:27.203619      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:28.204325      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:29.204504      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:30.204629      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:31.204844      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:32.205216      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:44:32.734: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 05/04/23 07:44:32.735
  May  4 07:44:32.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=statefulset-5023 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  4 07:44:32.985: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  4 07:44:32.985: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  4 07:44:32.985: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  4 07:44:32.988: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0504 07:44:33.206314      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:34.206750      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:35.207146      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:36.207405      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:37.207926      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:38.208930      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:39.209070      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:40.209271      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:41.209680      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:42.210613      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:44:42.994: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May  4 07:44:42.994: INFO: Waiting for statefulset status.replicas updated to 0
  May  4 07:44:43.025: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
  May  4 07:44:43.025: INFO: ss-0  k8s-node1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:44:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:44:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:44:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:44:22 +0000 UTC  }]
  May  4 07:44:43.025: INFO: 
  May  4 07:44:43.025: INFO: StatefulSet ss has not reached scale 3, at 1
  E0504 07:44:43.211109      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:44:44.046: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993600488s
  E0504 07:44:44.211629      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:44:45.051: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.972051458s
  E0504 07:44:45.211968      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:44:46.056: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.967624931s
  E0504 07:44:46.212837      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:44:47.061: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.962838349s
  E0504 07:44:47.213399      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:44:48.066: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.957775242s
  E0504 07:44:48.213893      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:44:49.071: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.952741336s
  E0504 07:44:49.214392      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:44:50.077: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.94721621s
  E0504 07:44:50.214894      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:44:51.083: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.941781752s
  E0504 07:44:51.215451      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:44:52.088: INFO: Verifying statefulset ss doesn't scale past 3 for another 936.204326ms
  E0504 07:44:52.216567      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5023 @ 05/04/23 07:44:53.089
  May  4 07:44:53.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=statefulset-5023 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0504 07:44:53.217673      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:44:53.348: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May  4 07:44:53.348: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  4 07:44:53.348: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  4 07:44:53.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=statefulset-5023 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  4 07:44:53.667: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  May  4 07:44:53.667: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  4 07:44:53.667: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  4 07:44:53.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=statefulset-5023 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  4 07:44:53.882: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  May  4 07:44:53.882: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  4 07:44:53.882: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  4 07:44:53.886: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
  E0504 07:44:54.218036      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:55.218240      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:56.218637      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:57.219531      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:58.220434      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:44:59.220509      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:00.220835      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:01.221225      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:02.221585      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:03.222575      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:45:03.892: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May  4 07:45:03.892: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  May  4 07:45:03.892: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 05/04/23 07:45:03.892
  May  4 07:45:03.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=statefulset-5023 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  4 07:45:04.105: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  4 07:45:04.105: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  4 07:45:04.105: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  4 07:45:04.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=statefulset-5023 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0504 07:45:04.223383      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:45:04.442: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  4 07:45:04.442: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  4 07:45:04.442: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  4 07:45:04.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=statefulset-5023 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  4 07:45:04.734: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  4 07:45:04.734: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  4 07:45:04.734: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  4 07:45:04.734: INFO: Waiting for statefulset status.replicas updated to 0
  May  4 07:45:04.739: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
  E0504 07:45:05.224255      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:06.224447      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:07.224833      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:08.225770      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:09.226177      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:10.226433      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:11.232238      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:12.232693      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:13.233482      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:14.233757      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:45:14.747: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May  4 07:45:14.747: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  May  4 07:45:14.747: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  May  4 07:45:14.786: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
  May  4 07:45:14.786: INFO: ss-0  k8s-node1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:44:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:45:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:45:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:44:22 +0000 UTC  }]
  May  4 07:45:14.786: INFO: ss-1  k8s-node2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:44:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:45:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:45:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:44:43 +0000 UTC  }]
  May  4 07:45:14.786: INFO: ss-2  k8s-node2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:44:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:44:43 +0000 UTC  }]
  May  4 07:45:14.786: INFO: 
  May  4 07:45:14.786: INFO: StatefulSet ss has not reached scale 0, at 3
  E0504 07:45:15.239507      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:45:15.795: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
  May  4 07:45:15.795: INFO: ss-0  k8s-node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:44:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:45:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:45:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:44:22 +0000 UTC  }]
  May  4 07:45:15.795: INFO: ss-1  k8s-node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:44:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:45:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:45:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:44:43 +0000 UTC  }]
  May  4 07:45:15.795: INFO: ss-2  k8s-node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:44:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:45:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:44:43 +0000 UTC  }]
  May  4 07:45:15.795: INFO: 
  May  4 07:45:15.795: INFO: StatefulSet ss has not reached scale 0, at 3
  E0504 07:45:16.240487      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:45:16.799: INFO: POD   NODE       PHASE      GRACE  CONDITIONS
  May  4 07:45:16.799: INFO: ss-0  k8s-node1  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:44:22 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:45:04 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:45:04 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-04 07:44:22 +0000 UTC  }]
  May  4 07:45:16.799: INFO: 
  May  4 07:45:16.799: INFO: StatefulSet ss has not reached scale 0, at 1
  E0504 07:45:17.241459      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:45:17.803: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.982104515s
  E0504 07:45:18.242612      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:45:18.806: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.978818766s
  E0504 07:45:19.243116      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:45:19.810: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.975289249s
  E0504 07:45:20.243957      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:45:20.814: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.971533784s
  E0504 07:45:21.245013      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:45:21.817: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.967764384s
  E0504 07:45:22.245461      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:45:22.821: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.963998567s
  E0504 07:45:23.246414      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:45:23.827: INFO: Verifying statefulset ss doesn't scale past 0 for another 959.764714ms
  E0504 07:45:24.246471      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5023 @ 05/04/23 07:45:24.828
  May  4 07:45:24.832: INFO: Scaling statefulset ss to 0
  May  4 07:45:24.842: INFO: Waiting for statefulset status.replicas updated to 0
  May  4 07:45:24.846: INFO: Deleting all statefulset in ns statefulset-5023
  May  4 07:45:24.852: INFO: Scaling statefulset ss to 0
  May  4 07:45:24.867: INFO: Waiting for statefulset status.replicas updated to 0
  May  4 07:45:24.876: INFO: Deleting statefulset ss
  May  4 07:45:24.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5023" for this suite. @ 05/04/23 07:45:24.914
• [62.494 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 05/04/23 07:45:24.954
  May  4 07:45:24.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename resourcequota @ 05/04/23 07:45:24.955
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:45:25.014
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:45:25.018
  STEP: Creating resourceQuota "e2e-rq-status-v84vr" @ 05/04/23 07:45:25.025
  May  4 07:45:25.042: INFO: Resource quota "e2e-rq-status-v84vr" reports spec: hard cpu limit of 500m
  May  4 07:45:25.042: INFO: Resource quota "e2e-rq-status-v84vr" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-v84vr" /status @ 05/04/23 07:45:25.042
  STEP: Confirm /status for "e2e-rq-status-v84vr" resourceQuota via watch @ 05/04/23 07:45:25.087
  May  4 07:45:25.097: INFO: observed resourceQuota "e2e-rq-status-v84vr" in namespace "resourcequota-4586" with hard status: v1.ResourceList(nil)
  May  4 07:45:25.097: INFO: Found resourceQuota "e2e-rq-status-v84vr" in namespace "resourcequota-4586" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  May  4 07:45:25.098: INFO: ResourceQuota "e2e-rq-status-v84vr" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 05/04/23 07:45:25.103
  May  4 07:45:25.130: INFO: Resource quota "e2e-rq-status-v84vr" reports spec: hard cpu limit of 1
  May  4 07:45:25.130: INFO: Resource quota "e2e-rq-status-v84vr" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-v84vr" /status @ 05/04/23 07:45:25.13
  STEP: Confirm /status for "e2e-rq-status-v84vr" resourceQuota via watch @ 05/04/23 07:45:25.158
  May  4 07:45:25.160: INFO: observed resourceQuota "e2e-rq-status-v84vr" in namespace "resourcequota-4586" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  May  4 07:45:25.160: INFO: Found resourceQuota "e2e-rq-status-v84vr" in namespace "resourcequota-4586" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  May  4 07:45:25.160: INFO: ResourceQuota "e2e-rq-status-v84vr" /status was patched
  STEP: Get "e2e-rq-status-v84vr" /status @ 05/04/23 07:45:25.16
  May  4 07:45:25.165: INFO: Resourcequota "e2e-rq-status-v84vr" reports status: hard cpu of 1
  May  4 07:45:25.165: INFO: Resourcequota "e2e-rq-status-v84vr" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-v84vr" /status before checking Spec is unchanged @ 05/04/23 07:45:25.169
  May  4 07:45:25.203: INFO: Resourcequota "e2e-rq-status-v84vr" reports status: hard cpu of 2
  May  4 07:45:25.203: INFO: Resourcequota "e2e-rq-status-v84vr" reports status: hard memory of 2Gi
  May  4 07:45:25.205: INFO: Found resourceQuota "e2e-rq-status-v84vr" in namespace "resourcequota-4586" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  E0504 07:45:25.247315      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:26.247846      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:27.248222      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:28.249364      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:29.250160      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:30.251311      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:31.251692      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:32.251931      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:33.252436      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:34.252723      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:35.253803      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:36.254128      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:37.254501      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:38.255424      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:39.255561      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:40.256195      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:41.256494      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:42.256867      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:43.257493      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:44.257894      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:45.258964      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:46.259358      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:47.260197      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:48.261179      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:49.261499      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:50.262065      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:51.262466      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:52.262635      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:53.263245      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:54.263052      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:55.263561      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:56.263911      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:57.264332      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:58.265402      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:45:59.266384      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:00.267053      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:01.267415      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:02.267539      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:03.268075      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:04.268488      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:05.269546      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:06.270343      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:07.270794      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:08.271939      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:09.272477      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:10.272954      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:11.273158      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:12.273388      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:13.273905      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:14.274220      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:15.275168      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:16.275499      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:17.275927      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:18.276212      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:19.276507      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:20.277627      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:21.278037      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:22.278466      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:23.279463      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:24.280221      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:25.281022      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:26.281381      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:27.281824      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:28.282966      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:29.283400      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:30.283978      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:31.284336      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:32.284707      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:33.285683      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:34.286047      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:35.286241      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:36.286613      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:37.287047      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:38.287483      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:39.287976      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:40.288724      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:41.288941      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:42.289145      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:43.289464      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:44.289839      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:45.290682      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:46.291179      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:47.291605      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:48.292676      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:49.293039      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:50.293803      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:51.294345      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:52.294473      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:53.294768      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:54.294960      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:55.296033      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:56.296524      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:57.296815      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:58.297644      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:46:59.297999      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:00.298767      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:01.298961      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:02.299383      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:03.300165      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:04.300534      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:05.301228      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:06.301674      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:07.302023      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:08.302234      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:09.303048      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:10.303836      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:11.304212      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:12.304662      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:13.305499      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:14.305745      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:15.306124      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:16.306436      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:17.306683      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:18.307485      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:19.307863      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:20.308402      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:21.308577      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:22.308897      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:23.310029      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:24.310408      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:25.311164      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:26.311639      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:27.312090      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:28.312435      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:29.312501      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:30.312651      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:31.312889      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:32.313313      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:33.313518      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:34.313834      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:35.315042      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:36.315271      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:37.315774      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:38.316788      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:39.317005      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:40.317994      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:41.318346      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:42.319457      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:43.319634      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:44.319745      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:45.320452      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:46.320766      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:47.321787      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:48.321982      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:49.322247      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:50.323047      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:51.323432      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:52.323774      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:53.324449      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:54.324529      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:55.324909      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:56.325408      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:57.325536      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:58.326390      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:47:59.326573      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:00.327032      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:01.327330      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:02.327507      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:03.327874      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:04.328235      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:05.328971      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:06.329217      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:07.329320      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:08.329662      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:09.329965      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:10.330551      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:11.331439      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:12.331597      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:13.332579      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:14.332952      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:15.333525      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:16.333699      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:17.334183      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:18.334419      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:19.334744      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:48:20.212: INFO: ResourceQuota "e2e-rq-status-v84vr" Spec was unchanged and /status reset
  May  4 07:48:20.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4586" for this suite. @ 05/04/23 07:48:20.219
• [175.297 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 05/04/23 07:48:20.252
  May  4 07:48:20.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename container-runtime @ 05/04/23 07:48:20.254
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:48:20.322
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:48:20.326
  E0504 07:48:20.335775      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the container @ 05/04/23 07:48:20.337
  W0504 07:48:20.373129      20 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/04/23 07:48:20.373
  E0504 07:48:21.335987      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:22.336583      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:23.337621      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:24.338060      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/04/23 07:48:24.44
  STEP: the container should be terminated @ 05/04/23 07:48:24.444
  STEP: the termination message should be set @ 05/04/23 07:48:24.444
  May  4 07:48:24.444: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 05/04/23 07:48:24.444
  May  4 07:48:24.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-3163" for this suite. @ 05/04/23 07:48:24.523
• [4.285 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 05/04/23 07:48:24.542
  May  4 07:48:24.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename downward-api @ 05/04/23 07:48:24.543
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:48:24.616
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:48:24.62
  STEP: Creating a pod to test downward API volume plugin @ 05/04/23 07:48:24.624
  E0504 07:48:25.339256      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:26.339531      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:27.340315      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:28.341344      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/04/23 07:48:28.655
  May  4 07:48:28.658: INFO: Trying to get logs from node k8s-node2 pod downwardapi-volume-625d0d90-994f-4838-b759-7fa4dd92a4da container client-container: <nil>
  STEP: delete the pod @ 05/04/23 07:48:28.665
  May  4 07:48:28.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-387" for this suite. @ 05/04/23 07:48:28.725
• [4.199 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 05/04/23 07:48:28.741
  May  4 07:48:28.741: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename configmap @ 05/04/23 07:48:28.743
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:48:28.804
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:48:28.808
  STEP: Creating configMap with name configmap-test-volume-6c8bbcc3-9a07-428a-ad8c-6dd869b9fffd @ 05/04/23 07:48:28.813
  STEP: Creating a pod to test consume configMaps @ 05/04/23 07:48:28.826
  E0504 07:48:29.341594      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:30.342072      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:31.342447      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:32.342934      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/04/23 07:48:32.867
  May  4 07:48:32.870: INFO: Trying to get logs from node k8s-node2 pod pod-configmaps-16585dfc-dd58-4393-9deb-32500d2c044f container configmap-volume-test: <nil>
  STEP: delete the pod @ 05/04/23 07:48:32.876
  May  4 07:48:32.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7263" for this suite. @ 05/04/23 07:48:32.944
• [4.230 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 05/04/23 07:48:32.971
  May  4 07:48:32.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename sched-preemption @ 05/04/23 07:48:32.972
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:48:33.031
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:48:33.035
  May  4 07:48:33.156: INFO: Waiting up to 1m0s for all nodes to be ready
  E0504 07:48:33.343716      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:34.344163      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:35.344858      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:36.345818      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:37.346058      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:38.347150      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:39.347435      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:40.347824      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:41.348484      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:42.349357      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:43.349678      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:44.349877      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:45.350598      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:46.351069      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:47.351382      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:48.352194      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:49.352361      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:50.352704      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:51.353890      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:52.354678      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:53.355001      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:54.355420      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:55.355832      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:56.356245      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:57.356932      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:58.356991      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:48:59.357919      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:00.358199      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:01.358540      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:02.358887      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:03.359920      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:04.360598      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:05.361792      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:06.362826      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:07.363627      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:08.364755      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:09.365747      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:10.366121      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:11.366440      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:12.366886      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:13.367054      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:14.367516      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:15.368168      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:16.368497      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:17.369727      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:18.370560      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:19.371104      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:20.371535      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:21.371753      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:22.372087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:23.373022      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:24.373416      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:25.374186      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:26.374672      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:27.374806      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:28.375634      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:29.375784      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:30.376245      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:31.376500      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:32.376935      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:49:33.201: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 05/04/23 07:49:33.205
  May  4 07:49:33.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename sched-preemption-path @ 05/04/23 07:49:33.207
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:49:33.265
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:49:33.269
  May  4 07:49:33.343: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  May  4 07:49:33.349: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  E0504 07:49:33.377398      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:49:33.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  4 07:49:33.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-9131" for this suite. @ 05/04/23 07:49:33.661
  STEP: Destroying namespace "sched-preemption-2203" for this suite. @ 05/04/23 07:49:33.68
• [60.745 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 05/04/23 07:49:33.718
  May  4 07:49:33.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename gc @ 05/04/23 07:49:33.72
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:49:33.778
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:49:33.783
  STEP: create the rc1 @ 05/04/23 07:49:33.791
  STEP: create the rc2 @ 05/04/23 07:49:33.806
  E0504 07:49:34.378098      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:35.378145      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:36.379180      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:37.379399      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 05/04/23 07:49:37.961
  E0504 07:49:38.379477      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:39.380069      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:40.380923      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:41.381116      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:42.381851      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc simpletest-rc-to-be-deleted @ 05/04/23 07:49:42.81
  STEP: wait for the rc to be deleted @ 05/04/23 07:49:42.963
  E0504 07:49:43.382681      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:44.382820      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:45.383033      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:46.383530      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:47.385551      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:49:48.129: INFO: 71 pods remaining
  May  4 07:49:48.129: INFO: 71 pods has nil DeletionTimestamp
  May  4 07:49:48.129: INFO: 
  E0504 07:49:48.386452      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:49.386562      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:50.391652      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:51.397572      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:49:52.398731      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/04/23 07:49:53.333
  E0504 07:49:53.429173      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:49:54.180: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May  4 07:49:54.180: INFO: Deleting pod "simpletest-rc-to-be-deleted-2f4jr" in namespace "gc-3810"
  May  4 07:49:54.421: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jdp8" in namespace "gc-3810"
  E0504 07:49:54.433789      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:49:54.765: INFO: Deleting pod "simpletest-rc-to-be-deleted-49qwn" in namespace "gc-3810"
  May  4 07:49:55.074: INFO: Deleting pod "simpletest-rc-to-be-deleted-4j4l7" in namespace "gc-3810"
  May  4 07:49:55.357: INFO: Deleting pod "simpletest-rc-to-be-deleted-55d5s" in namespace "gc-3810"
  E0504 07:49:55.444122      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:49:55.565: INFO: Deleting pod "simpletest-rc-to-be-deleted-57zvk" in namespace "gc-3810"
  May  4 07:49:55.695: INFO: Deleting pod "simpletest-rc-to-be-deleted-5rxbw" in namespace "gc-3810"
  May  4 07:49:55.878: INFO: Deleting pod "simpletest-rc-to-be-deleted-5x9k2" in namespace "gc-3810"
  May  4 07:49:56.063: INFO: Deleting pod "simpletest-rc-to-be-deleted-6572q" in namespace "gc-3810"
  May  4 07:49:56.394: INFO: Deleting pod "simpletest-rc-to-be-deleted-66pbk" in namespace "gc-3810"
  E0504 07:49:56.444371      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:49:56.572: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gpc8" in namespace "gc-3810"
  May  4 07:49:56.775: INFO: Deleting pod "simpletest-rc-to-be-deleted-6nf8c" in namespace "gc-3810"
  May  4 07:49:57.117: INFO: Deleting pod "simpletest-rc-to-be-deleted-6q7r6" in namespace "gc-3810"
  May  4 07:49:57.430: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dd49" in namespace "gc-3810"
  E0504 07:49:57.444514      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:49:57.571: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jbv7" in namespace "gc-3810"
  May  4 07:49:57.635: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kq99" in namespace "gc-3810"
  May  4 07:49:57.807: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pgzm" in namespace "gc-3810"
  May  4 07:49:57.913: INFO: Deleting pod "simpletest-rc-to-be-deleted-7wpmq" in namespace "gc-3810"
  May  4 07:49:58.006: INFO: Deleting pod "simpletest-rc-to-be-deleted-88brd" in namespace "gc-3810"
  May  4 07:49:58.153: INFO: Deleting pod "simpletest-rc-to-be-deleted-8kmjl" in namespace "gc-3810"
  May  4 07:49:58.224: INFO: Deleting pod "simpletest-rc-to-be-deleted-8tmkw" in namespace "gc-3810"
  May  4 07:49:58.334: INFO: Deleting pod "simpletest-rc-to-be-deleted-94h4z" in namespace "gc-3810"
  E0504 07:49:58.444985      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:49:58.469: INFO: Deleting pod "simpletest-rc-to-be-deleted-9h89l" in namespace "gc-3810"
  May  4 07:49:58.625: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jksh" in namespace "gc-3810"
  May  4 07:49:58.793: INFO: Deleting pod "simpletest-rc-to-be-deleted-9llfg" in namespace "gc-3810"
  May  4 07:49:58.902: INFO: Deleting pod "simpletest-rc-to-be-deleted-9mjj9" in namespace "gc-3810"
  May  4 07:49:59.069: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vspj" in namespace "gc-3810"
  May  4 07:49:59.206: INFO: Deleting pod "simpletest-rc-to-be-deleted-b4k5n" in namespace "gc-3810"
  May  4 07:49:59.408: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgdl5" in namespace "gc-3810"
  E0504 07:49:59.445702      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:49:59.587: INFO: Deleting pod "simpletest-rc-to-be-deleted-bjhtz" in namespace "gc-3810"
  May  4 07:49:59.724: INFO: Deleting pod "simpletest-rc-to-be-deleted-bkwjz" in namespace "gc-3810"
  May  4 07:49:59.926: INFO: Deleting pod "simpletest-rc-to-be-deleted-c8knr" in namespace "gc-3810"
  May  4 07:50:00.089: INFO: Deleting pod "simpletest-rc-to-be-deleted-ccdhg" in namespace "gc-3810"
  May  4 07:50:00.274: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmvt7" in namespace "gc-3810"
  May  4 07:50:00.446: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnsw9" in namespace "gc-3810"
  E0504 07:50:00.446797      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:50:00.520: INFO: Deleting pod "simpletest-rc-to-be-deleted-cp2fk" in namespace "gc-3810"
  May  4 07:50:00.619: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxkcx" in namespace "gc-3810"
  May  4 07:50:00.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-dghhj" in namespace "gc-3810"
  May  4 07:50:00.996: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2xgw" in namespace "gc-3810"
  May  4 07:50:01.359: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7dgc" in namespace "gc-3810"
  E0504 07:50:01.447344      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:50:01.582: INFO: Deleting pod "simpletest-rc-to-be-deleted-fc6k2" in namespace "gc-3810"
  May  4 07:50:01.702: INFO: Deleting pod "simpletest-rc-to-be-deleted-fnd5b" in namespace "gc-3810"
  May  4 07:50:01.856: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2kfq" in namespace "gc-3810"
  May  4 07:50:02.020: INFO: Deleting pod "simpletest-rc-to-be-deleted-gbq2g" in namespace "gc-3810"
  May  4 07:50:02.191: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdmb8" in namespace "gc-3810"
  May  4 07:50:02.331: INFO: Deleting pod "simpletest-rc-to-be-deleted-gf4qv" in namespace "gc-3810"
  E0504 07:50:02.448464      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:50:02.498: INFO: Deleting pod "simpletest-rc-to-be-deleted-ghgs8" in namespace "gc-3810"
  May  4 07:50:02.555: INFO: Deleting pod "simpletest-rc-to-be-deleted-gs8ch" in namespace "gc-3810"
  May  4 07:50:02.653: INFO: Deleting pod "simpletest-rc-to-be-deleted-h4fdh" in namespace "gc-3810"
  May  4 07:50:02.752: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9n5t" in namespace "gc-3810"
  May  4 07:50:02.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3810" for this suite. @ 05/04/23 07:50:02.852
• [29.185 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 05/04/23 07:50:02.906
  May  4 07:50:02.906: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename replication-controller @ 05/04/23 07:50:02.911
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:50:03.103
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:50:03.112
  May  4 07:50:03.134: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 05/04/23 07:50:03.245
  STEP: Checking rc "condition-test" has the desired failure condition set @ 05/04/23 07:50:03.349
  E0504 07:50:03.449585      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 05/04/23 07:50:04.429
  E0504 07:50:04.450627      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:50:04.480: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 05/04/23 07:50:04.48
  May  4 07:50:04.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-6027" for this suite. @ 05/04/23 07:50:04.59
• [1.747 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 05/04/23 07:50:04.655
  May  4 07:50:04.655: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename podtemplate @ 05/04/23 07:50:04.657
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:50:04.762
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:50:04.772
  May  4 07:50:04.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-2193" for this suite. @ 05/04/23 07:50:05.01
• [0.383 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 05/04/23 07:50:05.039
  May  4 07:50:05.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename container-probe @ 05/04/23 07:50:05.04
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:50:05.128
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:50:05.145
  STEP: Creating pod test-grpc-cf6921d2-967b-4c8c-8c80-7f3903520449 in namespace container-probe-352 @ 05/04/23 07:50:05.153
  E0504 07:50:05.451626      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:06.452688      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:07.453161      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:08.453382      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:09.453949      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:10.454099      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:50:11.432: INFO: Started pod test-grpc-cf6921d2-967b-4c8c-8c80-7f3903520449 in namespace container-probe-352
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/04/23 07:50:11.432
  E0504 07:50:11.454956      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:50:11.458: INFO: Initial restart count of pod test-grpc-cf6921d2-967b-4c8c-8c80-7f3903520449 is 0
  E0504 07:50:12.456575      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:13.457931      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:14.458064      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:15.458544      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:16.459579      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:17.461684      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:18.462000      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:19.462172      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:20.462546      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:21.462729      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:22.463206      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:23.463635      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:24.463793      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:25.464181      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:26.464966      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:27.465347      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:28.466574      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:29.467506      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:30.468028      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:31.468568      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:32.468682      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:33.469572      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:34.469897      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:35.470600      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:36.470687      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:37.470934      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:38.471130      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:39.471695      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:40.472183      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:41.472463      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:42.472516      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:43.476581      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:44.477413      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:45.477769      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:46.478920      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:47.479384      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:48.480174      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:49.480511      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:50.480859      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:51.481183      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:52.482321      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:53.482610      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:54.482816      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:55.483366      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:56.484159      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:57.484541      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:58.485504      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:50:59.486449      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:00.486988      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:01.487470      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:02.487904      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:03.488619      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:04.489158      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:05.489385      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:06.489985      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:07.490442      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:08.490748      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:09.490964      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:10.491370      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:11.491768      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:12.492414      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:13.493209      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:14.493792      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:15.494176      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:16.494523      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:17.494904      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:18.495239      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:19.495550      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:20.496582      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:21.496989      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:22.497424      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:23.498260      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:51:23.800: INFO: Restart count of pod container-probe-352/test-grpc-cf6921d2-967b-4c8c-8c80-7f3903520449 is now 1 (1m12.341570995s elapsed)
  May  4 07:51:23.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/04/23 07:51:23.804
  STEP: Destroying namespace "container-probe-352" for this suite. @ 05/04/23 07:51:23.868
• [78.864 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 05/04/23 07:51:23.907
  May  4 07:51:23.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename downward-api @ 05/04/23 07:51:23.909
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:51:23.954
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:51:23.96
  STEP: Creating a pod to test downward API volume plugin @ 05/04/23 07:51:23.964
  E0504 07:51:24.498728      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:25.499358      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:26.499847      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:27.500081      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/04/23 07:51:28.021
  May  4 07:51:28.027: INFO: Trying to get logs from node k8s-node2 pod downwardapi-volume-d1fb3d51-9885-42e5-9e89-10f31ce74d53 container client-container: <nil>
  STEP: delete the pod @ 05/04/23 07:51:28.056
  May  4 07:51:28.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8594" for this suite. @ 05/04/23 07:51:28.119
• [4.231 seconds]
------------------------------
SSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 05/04/23 07:51:28.139
  May  4 07:51:28.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename cronjob @ 05/04/23 07:51:28.141
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:51:28.175
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:51:28.178
  STEP: Creating a cronjob @ 05/04/23 07:51:28.193
  STEP: creating @ 05/04/23 07:51:28.193
  STEP: getting @ 05/04/23 07:51:28.224
  STEP: listing @ 05/04/23 07:51:28.227
  STEP: watching @ 05/04/23 07:51:28.23
  May  4 07:51:28.230: INFO: starting watch
  STEP: cluster-wide listing @ 05/04/23 07:51:28.231
  STEP: cluster-wide watching @ 05/04/23 07:51:28.234
  May  4 07:51:28.234: INFO: starting watch
  STEP: patching @ 05/04/23 07:51:28.236
  STEP: updating @ 05/04/23 07:51:28.25
  May  4 07:51:28.277: INFO: waiting for watch events with expected annotations
  May  4 07:51:28.277: INFO: saw patched and updated annotations
  STEP: patching /status @ 05/04/23 07:51:28.277
  STEP: updating /status @ 05/04/23 07:51:28.296
  STEP: get /status @ 05/04/23 07:51:28.303
  STEP: deleting @ 05/04/23 07:51:28.306
  STEP: deleting a collection @ 05/04/23 07:51:28.372
  May  4 07:51:28.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8136" for this suite. @ 05/04/23 07:51:28.395
• [0.270 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 05/04/23 07:51:28.41
  May  4 07:51:28.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename kubectl @ 05/04/23 07:51:28.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:51:28.444
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:51:28.448
  STEP: create deployment with httpd image @ 05/04/23 07:51:28.484
  May  4 07:51:28.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-7504 create -f -'
  E0504 07:51:28.502714      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:51:28.941: INFO: stderr: ""
  May  4 07:51:28.941: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 05/04/23 07:51:28.941
  May  4 07:51:28.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-7504 diff -f -'
  May  4 07:51:29.388: INFO: rc: 1
  May  4 07:51:29.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-7504 delete -f -'
  E0504 07:51:29.503252      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:51:29.573: INFO: stderr: ""
  May  4 07:51:29.573: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  May  4 07:51:29.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7504" for this suite. @ 05/04/23 07:51:29.578
• [1.204 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 05/04/23 07:51:29.615
  May  4 07:51:29.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 07:51:29.617
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:51:29.711
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:51:29.716
  STEP: Creating configMap with name projected-configmap-test-volume-6eef184b-b221-4628-8b55-2032ac70b68e @ 05/04/23 07:51:29.729
  STEP: Creating a pod to test consume configMaps @ 05/04/23 07:51:29.746
  E0504 07:51:30.503760      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:31.504396      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:32.504630      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:33.507392      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:34.508118      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:35.508393      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/04/23 07:51:35.797
  May  4 07:51:35.800: INFO: Trying to get logs from node k8s-node2 pod pod-projected-configmaps-54e47ad1-50b1-4d11-9472-6b71048bee09 container agnhost-container: <nil>
  STEP: delete the pod @ 05/04/23 07:51:35.806
  May  4 07:51:35.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1215" for this suite. @ 05/04/23 07:51:35.871
• [6.273 seconds]
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 05/04/23 07:51:35.889
  May  4 07:51:35.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename kubectl @ 05/04/23 07:51:35.89
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:51:35.958
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:51:35.962
  STEP: creating Agnhost RC @ 05/04/23 07:51:35.966
  May  4 07:51:35.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-87 create -f -'
  May  4 07:51:36.480: INFO: stderr: ""
  May  4 07:51:36.480: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/04/23 07:51:36.48
  E0504 07:51:36.508562      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:51:37.505: INFO: Selector matched 1 pods for map[app:agnhost]
  May  4 07:51:37.505: INFO: Found 0 / 1
  E0504 07:51:37.508940      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:51:38.484: INFO: Selector matched 1 pods for map[app:agnhost]
  May  4 07:51:38.484: INFO: Found 1 / 1
  May  4 07:51:38.484: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 05/04/23 07:51:38.484
  May  4 07:51:38.487: INFO: Selector matched 1 pods for map[app:agnhost]
  May  4 07:51:38.487: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May  4 07:51:38.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-87 patch pod agnhost-primary-hnzml -p {"metadata":{"annotations":{"x":"y"}}}'
  E0504 07:51:38.509546      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:51:38.631: INFO: stderr: ""
  May  4 07:51:38.631: INFO: stdout: "pod/agnhost-primary-hnzml patched\n"
  STEP: checking annotations @ 05/04/23 07:51:38.631
  May  4 07:51:38.653: INFO: Selector matched 1 pods for map[app:agnhost]
  May  4 07:51:38.653: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May  4 07:51:38.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-87" for this suite. @ 05/04/23 07:51:38.658
• [2.784 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 05/04/23 07:51:38.675
  May  4 07:51:38.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename emptydir @ 05/04/23 07:51:38.676
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:51:38.707
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:51:38.711
  STEP: Creating a pod to test emptydir volume type on node default medium @ 05/04/23 07:51:38.728
  E0504 07:51:39.509763      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:40.510246      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:41.511082      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:42.511262      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/04/23 07:51:42.764
  May  4 07:51:42.768: INFO: Trying to get logs from node k8s-node2 pod pod-8930c4f4-3bac-4c51-9644-56caf1cb3d13 container test-container: <nil>
  STEP: delete the pod @ 05/04/23 07:51:42.774
  May  4 07:51:42.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1407" for this suite. @ 05/04/23 07:51:42.814
• [4.166 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 05/04/23 07:51:42.842
  May  4 07:51:42.842: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename emptydir @ 05/04/23 07:51:42.843
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:51:42.908
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:51:42.912
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 05/04/23 07:51:42.92
  E0504 07:51:43.512470      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:44.512820      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:45.513356      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:46.513792      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/04/23 07:51:46.97
  May  4 07:51:46.973: INFO: Trying to get logs from node k8s-node2 pod pod-0fc3c592-164d-4f8f-be63-f689afbb84c7 container test-container: <nil>
  STEP: delete the pod @ 05/04/23 07:51:46.98
  May  4 07:51:47.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1938" for this suite. @ 05/04/23 07:51:47.034
• [4.219 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 05/04/23 07:51:47.063
  May  4 07:51:47.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename kubectl @ 05/04/23 07:51:47.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:51:47.11
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:51:47.114
  STEP: starting the proxy server @ 05/04/23 07:51:47.118
  May  4 07:51:47.118: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-7212 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 05/04/23 07:51:47.213
  May  4 07:51:47.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7212" for this suite. @ 05/04/23 07:51:47.24
• [0.202 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 05/04/23 07:51:47.267
  May  4 07:51:47.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename var-expansion @ 05/04/23 07:51:47.268
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:51:47.333
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:51:47.337
  STEP: creating the pod @ 05/04/23 07:51:47.341
  STEP: waiting for pod running @ 05/04/23 07:51:47.392
  E0504 07:51:47.514771      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:48.515800      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a file in subpath @ 05/04/23 07:51:49.4
  May  4 07:51:49.404: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7946 PodName:var-expansion-01dd4148-8e61-44fa-b5f5-ac5cc569373f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 07:51:49.404: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 07:51:49.405: INFO: ExecWithOptions: Clientset creation
  May  4 07:51:49.405: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-7946/pods/var-expansion-01dd4148-8e61-44fa-b5f5-ac5cc569373f/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 05/04/23 07:51:49.512
  E0504 07:51:49.516573      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:51:49.516: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7946 PodName:var-expansion-01dd4148-8e61-44fa-b5f5-ac5cc569373f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 07:51:49.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 07:51:49.517: INFO: ExecWithOptions: Clientset creation
  May  4 07:51:49.517: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-7946/pods/var-expansion-01dd4148-8e61-44fa-b5f5-ac5cc569373f/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 05/04/23 07:51:49.63
  May  4 07:51:50.159: INFO: Successfully updated pod "var-expansion-01dd4148-8e61-44fa-b5f5-ac5cc569373f"
  STEP: waiting for annotated pod running @ 05/04/23 07:51:50.159
  STEP: deleting the pod gracefully @ 05/04/23 07:51:50.164
  May  4 07:51:50.164: INFO: Deleting pod "var-expansion-01dd4148-8e61-44fa-b5f5-ac5cc569373f" in namespace "var-expansion-7946"
  May  4 07:51:50.201: INFO: Wait up to 5m0s for pod "var-expansion-01dd4148-8e61-44fa-b5f5-ac5cc569373f" to be fully deleted
  E0504 07:51:50.517018      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:51.517390      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:52.517986      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:53.518267      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:54.518590      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:55.518939      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:56.519137      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:57.519467      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:58.520460      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:51:59.520843      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:00.521009      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:01.521218      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:02.522200      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:03.522811      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:04.523061      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:05.523443      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:06.523590      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:07.524046      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:08.524261      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:09.524503      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:10.524952      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:11.525364      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:12.525485      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:13.525755      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:14.526383      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:15.526785      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:16.527742      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:17.528124      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:18.528752      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:19.529830      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:20.530112      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:21.530705      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:22.530931      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:23.531326      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:52:24.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7946" for this suite. @ 05/04/23 07:52:24.288
• [37.037 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 05/04/23 07:52:24.305
  May  4 07:52:24.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename services @ 05/04/23 07:52:24.307
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:52:24.368
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:52:24.373
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-7535 @ 05/04/23 07:52:24.378
  STEP: changing the ExternalName service to type=ClusterIP @ 05/04/23 07:52:24.393
  STEP: creating replication controller externalname-service in namespace services-7535 @ 05/04/23 07:52:24.459
  I0504 07:52:24.496661      20 runners.go:194] Created replication controller with name: externalname-service, namespace: services-7535, replica count: 2
  E0504 07:52:24.531594      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:25.532463      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:26.532682      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:27.533764      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0504 07:52:27.547125      20 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  4 07:52:27.547: INFO: Creating new exec pod
  E0504 07:52:28.534666      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:29.535338      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:30.535618      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:52:30.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-7535 exec execpodnqplj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May  4 07:52:30.768: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May  4 07:52:30.768: INFO: stdout: "externalname-service-knqhr"
  May  4 07:52:30.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-7535 exec execpodnqplj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.107.14 80'
  May  4 07:52:30.967: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.107.14 80\nConnection to 10.110.107.14 80 port [tcp/http] succeeded!\n"
  May  4 07:52:30.968: INFO: stdout: ""
  E0504 07:52:31.535787      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:52:31.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-7535 exec execpodnqplj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.107.14 80'
  May  4 07:52:32.167: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.107.14 80\nConnection to 10.110.107.14 80 port [tcp/http] succeeded!\n"
  May  4 07:52:32.167: INFO: stdout: "externalname-service-cd8z8"
  May  4 07:52:32.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  4 07:52:32.173: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-7535" for this suite. @ 05/04/23 07:52:32.279
• [8.001 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 05/04/23 07:52:32.307
  May  4 07:52:32.307: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename downward-api @ 05/04/23 07:52:32.309
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:52:32.385
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:52:32.389
  STEP: Creating a pod to test downward API volume plugin @ 05/04/23 07:52:32.392
  E0504 07:52:32.535999      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:33.536613      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:34.537200      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:35.537603      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/04/23 07:52:36.429
  May  4 07:52:36.432: INFO: Trying to get logs from node k8s-node2 pod downwardapi-volume-0deec714-430b-41aa-9d21-770a1ca7d4ed container client-container: <nil>
  STEP: delete the pod @ 05/04/23 07:52:36.438
  May  4 07:52:36.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2979" for this suite. @ 05/04/23 07:52:36.506
• [4.213 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 05/04/23 07:52:36.521
  May  4 07:52:36.521: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename container-runtime @ 05/04/23 07:52:36.523
  E0504 07:52:36.538490      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:52:36.574
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:52:36.578
  STEP: create the container @ 05/04/23 07:52:36.583
  W0504 07:52:36.617421      20 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/04/23 07:52:36.617
  E0504 07:52:37.539658      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:38.539959      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:39.542523      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:40.542601      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/04/23 07:52:40.696
  STEP: the container should be terminated @ 05/04/23 07:52:40.698
  STEP: the termination message should be set @ 05/04/23 07:52:40.698
  May  4 07:52:40.698: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 05/04/23 07:52:40.699
  May  4 07:52:40.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-670" for this suite. @ 05/04/23 07:52:40.746
• [4.242 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 05/04/23 07:52:40.767
  May  4 07:52:40.767: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/04/23 07:52:40.769
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:52:40.885
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:52:40.89
  May  4 07:52:40.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  E0504 07:52:41.543567      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/04/23 07:52:42.51
  May  4 07:52:42.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-6672 --namespace=crd-publish-openapi-6672 create -f -'
  E0504 07:52:42.544121      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:43.546806      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:52:43.749: INFO: stderr: ""
  May  4 07:52:43.749: INFO: stdout: "e2e-test-crd-publish-openapi-6185-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  May  4 07:52:43.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-6672 --namespace=crd-publish-openapi-6672 delete e2e-test-crd-publish-openapi-6185-crds test-cr'
  May  4 07:52:43.879: INFO: stderr: ""
  May  4 07:52:43.879: INFO: stdout: "e2e-test-crd-publish-openapi-6185-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  May  4 07:52:43.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-6672 --namespace=crd-publish-openapi-6672 apply -f -'
  May  4 07:52:44.323: INFO: stderr: ""
  May  4 07:52:44.323: INFO: stdout: "e2e-test-crd-publish-openapi-6185-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  May  4 07:52:44.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-6672 --namespace=crd-publish-openapi-6672 delete e2e-test-crd-publish-openapi-6185-crds test-cr'
  May  4 07:52:44.466: INFO: stderr: ""
  May  4 07:52:44.467: INFO: stdout: "e2e-test-crd-publish-openapi-6185-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 05/04/23 07:52:44.467
  May  4 07:52:44.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=crd-publish-openapi-6672 explain e2e-test-crd-publish-openapi-6185-crds'
  E0504 07:52:44.547806      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:52:44.842: INFO: stderr: ""
  May  4 07:52:44.842: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-6185-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0504 07:52:45.548370      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:52:46.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6672" for this suite. @ 05/04/23 07:52:46.469
• [5.717 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 05/04/23 07:52:46.485
  May  4 07:52:46.485: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 07:52:46.487
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:52:46.548
  E0504 07:52:46.548396      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:52:46.552
  STEP: Creating projection with secret that has name projected-secret-test-map-f427241f-242f-4844-a3c5-c561512bef5c @ 05/04/23 07:52:46.555
  STEP: Creating a pod to test consume secrets @ 05/04/23 07:52:46.57
  E0504 07:52:47.549387      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:48.549826      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:49.550808      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:50.551162      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/04/23 07:52:50.647
  May  4 07:52:50.649: INFO: Trying to get logs from node k8s-node2 pod pod-projected-secrets-6a9bdf26-a2d7-4fbc-9b75-6d02fbaf72a3 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/04/23 07:52:50.655
  May  4 07:52:50.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6032" for this suite. @ 05/04/23 07:52:50.712
• [4.241 seconds]
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 05/04/23 07:52:50.726
  May  4 07:52:50.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename replicaset @ 05/04/23 07:52:50.728
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:52:50.8
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:52:50.804
  STEP: Create a Replicaset @ 05/04/23 07:52:50.812
  STEP: Verify that the required pods have come up. @ 05/04/23 07:52:50.828
  May  4 07:52:50.832: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0504 07:52:51.552235      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:52.553142      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:53.553747      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:54.554243      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:52:55.554602      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:52:55.848: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/04/23 07:52:55.848
  STEP: Getting /status @ 05/04/23 07:52:55.848
  May  4 07:52:55.857: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 05/04/23 07:52:55.857
  May  4 07:52:55.896: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 05/04/23 07:52:55.896
  May  4 07:52:55.898: INFO: Observed &ReplicaSet event: ADDED
  May  4 07:52:55.898: INFO: Observed &ReplicaSet event: MODIFIED
  May  4 07:52:55.898: INFO: Observed &ReplicaSet event: MODIFIED
  May  4 07:52:55.899: INFO: Observed &ReplicaSet event: MODIFIED
  May  4 07:52:55.899: INFO: Found replicaset test-rs in namespace replicaset-7634 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May  4 07:52:55.899: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 05/04/23 07:52:55.899
  May  4 07:52:55.899: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May  4 07:52:55.915: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 05/04/23 07:52:55.915
  May  4 07:52:55.918: INFO: Observed &ReplicaSet event: ADDED
  May  4 07:52:55.918: INFO: Observed &ReplicaSet event: MODIFIED
  May  4 07:52:55.918: INFO: Observed &ReplicaSet event: MODIFIED
  May  4 07:52:55.919: INFO: Observed &ReplicaSet event: MODIFIED
  May  4 07:52:55.919: INFO: Observed replicaset test-rs in namespace replicaset-7634 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May  4 07:52:55.919: INFO: Observed &ReplicaSet event: MODIFIED
  May  4 07:52:55.919: INFO: Found replicaset test-rs in namespace replicaset-7634 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  May  4 07:52:55.919: INFO: Replicaset test-rs has a patched status
  May  4 07:52:55.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7634" for this suite. @ 05/04/23 07:52:55.93
• [5.237 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 05/04/23 07:52:55.968
  May  4 07:52:55.968: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename svcaccounts @ 05/04/23 07:52:55.97
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:52:56.012
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:52:56.016
  STEP: Creating ServiceAccount "e2e-sa-6w5bv"  @ 05/04/23 07:52:56.022
  May  4 07:52:56.036: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-6w5bv"  @ 05/04/23 07:52:56.036
  May  4 07:52:56.077: INFO: AutomountServiceAccountToken: true
  May  4 07:52:56.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-2790" for this suite. @ 05/04/23 07:52:56.099
• [0.147 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 05/04/23 07:52:56.116
  May  4 07:52:56.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename daemonsets @ 05/04/23 07:52:56.118
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:52:56.173
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:52:56.177
  STEP: Creating simple DaemonSet "daemon-set" @ 05/04/23 07:52:56.218
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/04/23 07:52:56.233
  May  4 07:52:56.240: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:52:56.258: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 07:52:56.258: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  E0504 07:52:56.555341      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:52:57.362: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:52:57.367: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 07:52:57.367: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  E0504 07:52:57.555946      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:52:58.267: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:52:58.273: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 07:52:58.273: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  E0504 07:52:58.557062      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:52:59.263: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:52:59.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  4 07:52:59.266: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: listing all DaemonSets @ 05/04/23 07:52:59.269
  STEP: DeleteCollection of the DaemonSets @ 05/04/23 07:52:59.273
  STEP: Verify that ReplicaSets have been deleted @ 05/04/23 07:52:59.316
  May  4 07:52:59.326: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"85094"},"items":null}

  May  4 07:52:59.331: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"85094"},"items":[{"metadata":{"name":"daemon-set-j5l68","generateName":"daemon-set-","namespace":"daemonsets-1739","uid":"653bcb77-5195-49a5-8c86-499838d3b44f","resourceVersion":"85089","creationTimestamp":"2023-05-04T07:52:56Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"176b10d7ca469b14d63743adcab4f32ee1d0060ac1b2d49420af81f43ea853d1","cni.projectcalico.org/podIP":"172.16.169.186/32","cni.projectcalico.org/podIPs":"172.16.169.186/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4071ebda-40ff-4ba4-8052-f2bbcd0d017b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-04T07:52:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4071ebda-40ff-4ba4-8052-f2bbcd0d017b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-04T07:52:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-04T07:52:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.169.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-76sf4","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-76sf4","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-node2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-node2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T07:52:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T07:52:58Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T07:52:58Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T07:52:56Z"}],"hostIP":"192.168.0.157","podIP":"172.16.169.186","podIPs":[{"ip":"172.16.169.186"}],"startTime":"2023-05-04T07:52:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-04T07:52:58Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089","containerID":"containerd://b499fc7229b99d3300e783f68d459ce75b852505b7e7091c01f54f1097cc6241","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-kqqml","generateName":"daemon-set-","namespace":"daemonsets-1739","uid":"5d99cf12-0567-4e4a-bad3-0e7a04f59ccf","resourceVersion":"85091","creationTimestamp":"2023-05-04T07:52:56Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"5c992c7617da9592418aefdc1a3be0d0679d8ac67cbf94d99c11b1e90ee7a9a1","cni.projectcalico.org/podIP":"172.16.36.114/32","cni.projectcalico.org/podIPs":"172.16.36.114/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4071ebda-40ff-4ba4-8052-f2bbcd0d017b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-04T07:52:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4071ebda-40ff-4ba4-8052-f2bbcd0d017b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-04T07:52:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-04T07:52:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.36.114\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9t2tj","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9t2tj","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-node1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-node1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T07:52:56Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T07:52:58Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T07:52:58Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-04T07:52:56Z"}],"hostIP":"192.168.0.156","podIP":"172.16.36.114","podIPs":[{"ip":"172.16.36.114"}],"startTime":"2023-05-04T07:52:56Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-04T07:52:57Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"sha256:987dae5a853a43c663ab2f902708e65874bd2c0189aa0bc57d81ffb57187d089","containerID":"containerd://a4fe6a03a647482325b0b21ed39cf337469605c9b7f5015174d8843979f8c968","started":true}],"qosClass":"BestEffort"}}]}

  May  4 07:52:59.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1739" for this suite. @ 05/04/23 07:52:59.392
• [3.295 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 05/04/23 07:52:59.412
  May  4 07:52:59.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename pods @ 05/04/23 07:52:59.413
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:52:59.509
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:52:59.515
  E0504 07:52:59.557910      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:00.558997      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:01.559112      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:02.559976      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:03.560600      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:04.560756      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:05.561171      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:06.561492      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:07.561674      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/04/23 07:53:07.737
  May  4 07:53:07.740: INFO: Trying to get logs from node k8s-node2 pod client-envvars-97217b8e-05a1-4b9f-94e0-1af285223610 container env3cont: <nil>
  STEP: delete the pod @ 05/04/23 07:53:07.748
  May  4 07:53:07.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9586" for this suite. @ 05/04/23 07:53:07.827
• [8.431 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 05/04/23 07:53:07.845
  May  4 07:53:07.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 07:53:07.85
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:53:07.915
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:53:07.919
  STEP: Creating configMap with name projected-configmap-test-volume-map-7ff57efb-6c09-4068-a782-d24e1d647864 @ 05/04/23 07:53:07.923
  STEP: Creating a pod to test consume configMaps @ 05/04/23 07:53:07.953
  E0504 07:53:08.562091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:09.562450      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:10.562815      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:11.563261      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/04/23 07:53:11.987
  May  4 07:53:11.991: INFO: Trying to get logs from node k8s-node2 pod pod-projected-configmaps-fe8715e5-d542-4549-840d-fb9f34ec6ce2 container agnhost-container: <nil>
  STEP: delete the pod @ 05/04/23 07:53:11.998
  May  4 07:53:12.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2058" for this suite. @ 05/04/23 07:53:12.097
• [4.287 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 05/04/23 07:53:12.133
  May  4 07:53:12.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename services @ 05/04/23 07:53:12.134
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:53:12.194
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:53:12.198
  May  4 07:53:12.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4552" for this suite. @ 05/04/23 07:53:12.21
• [0.100 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 05/04/23 07:53:12.234
  May  4 07:53:12.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename container-probe @ 05/04/23 07:53:12.235
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:53:12.316
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:53:12.32
  STEP: Creating pod liveness-44452947-c392-4f7c-9eb5-396779680f80 in namespace container-probe-5223 @ 05/04/23 07:53:12.323
  E0504 07:53:12.563677      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:13.564243      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:14.565182      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:15.565635      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:53:16.374: INFO: Started pod liveness-44452947-c392-4f7c-9eb5-396779680f80 in namespace container-probe-5223
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/04/23 07:53:16.374
  May  4 07:53:16.377: INFO: Initial restart count of pod liveness-44452947-c392-4f7c-9eb5-396779680f80 is 0
  E0504 07:53:16.566381      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:17.566791      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:18.567727      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:19.567997      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:20.568046      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:21.568453      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:22.569512      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:23.570687      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:24.571873      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:25.572581      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:26.573648      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:27.574035      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:28.574623      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:29.575214      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:30.575747      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:31.576146      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:32.576641      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:33.577483      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:53:34.444: INFO: Restart count of pod container-probe-5223/liveness-44452947-c392-4f7c-9eb5-396779680f80 is now 1 (18.066930092s elapsed)
  E0504 07:53:34.578184      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:35.578572      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:36.579674      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:37.580816      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:38.581432      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:39.581952      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:40.582925      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:41.583442      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:42.584415      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:43.585362      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:44.585924      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:45.586359      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:46.587259      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:47.587876      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:48.588518      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:49.588655      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:50.588776      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:51.588859      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:52.589826      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:53.590802      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:53:54.487: INFO: Restart count of pod container-probe-5223/liveness-44452947-c392-4f7c-9eb5-396779680f80 is now 2 (38.109954352s elapsed)
  E0504 07:53:54.590987      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:55.591439      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:56.592556      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:57.593464      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:58.594549      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:53:59.594971      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:00.594987      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:01.595495      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:02.596096      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:03.597016      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:04.597603      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:05.598069      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:06.598518      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:07.598969      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:08.599364      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:09.599738      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:10.600315      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:11.601019      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:12.601709      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:13.602704      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:54:14.531: INFO: Restart count of pod container-probe-5223/liveness-44452947-c392-4f7c-9eb5-396779680f80 is now 3 (58.15419359s elapsed)
  E0504 07:54:14.603426      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:15.604419      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:16.605190      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:17.605617      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:18.606226      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:19.606648      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:20.606741      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:21.607136      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:22.607221      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:23.607856      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:24.608591      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:25.609014      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:26.609119      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:27.609489      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:28.610373      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:29.610767      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:30.611746      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:31.612230      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:32.612436      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:33.612809      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:54:34.574: INFO: Restart count of pod container-probe-5223/liveness-44452947-c392-4f7c-9eb5-396779680f80 is now 4 (1m18.196564101s elapsed)
  E0504 07:54:34.613547      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:35.614003      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:36.615099      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:37.615476      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:38.616392      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:39.616665      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:40.617441      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:41.617797      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:42.617913      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:43.619009      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:44.619891      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:45.620405      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:46.622626      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:47.623467      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:48.624230      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:49.624437      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:50.625371      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:51.625508      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:52.626192      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:53.626586      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:54.627738      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:55.628096      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:56.628841      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:57.629158      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:58.629850      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:54:59.630695      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:00.630908      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:01.631086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:02.631487      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:03.631885      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:04.632231      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:05.633274      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:06.633525      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:07.634501      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:08.634571      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:09.634922      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:10.635398      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:11.635688      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:12.635980      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:13.637001      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:14.637225      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:15.638354      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:16.638524      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:17.639333      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:18.640211      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:19.640771      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:20.641101      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:21.641937      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:22.642395      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:23.642735      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:24.643150      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:25.643523      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:26.643688      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:27.644612      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:28.645651      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:29.646740      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:30.647181      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:31.647495      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:32.647971      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:33.648679      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:34.648791      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:55:34.742: INFO: Restart count of pod container-probe-5223/liveness-44452947-c392-4f7c-9eb5-396779680f80 is now 5 (2m18.365017926s elapsed)
  May  4 07:55:34.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/04/23 07:55:34.749
  STEP: Destroying namespace "container-probe-5223" for this suite. @ 05/04/23 07:55:34.821
• [142.608 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 05/04/23 07:55:34.853
  May  4 07:55:34.853: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename container-runtime @ 05/04/23 07:55:34.854
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:55:34.95
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:55:34.963
  STEP: create the container @ 05/04/23 07:55:34.972
  W0504 07:55:34.998263      20 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 05/04/23 07:55:34.998
  E0504 07:55:35.649076      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:36.650018      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:37.650726      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:38.651613      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/04/23 07:55:39.059
  STEP: the container should be terminated @ 05/04/23 07:55:39.062
  STEP: the termination message should be set @ 05/04/23 07:55:39.062
  May  4 07:55:39.062: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 05/04/23 07:55:39.062
  May  4 07:55:39.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-1523" for this suite. @ 05/04/23 07:55:39.122
• [4.284 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 05/04/23 07:55:39.137
  May  4 07:55:39.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename configmap @ 05/04/23 07:55:39.139
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:55:39.198
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:55:39.201
  STEP: Creating configMap with name cm-test-opt-del-1be9440d-bf31-40d3-90f7-fed6597637fd @ 05/04/23 07:55:39.209
  STEP: Creating configMap with name cm-test-opt-upd-7c05e0e4-ad1a-4dd8-9d09-d5f3b694cade @ 05/04/23 07:55:39.222
  STEP: Creating the pod @ 05/04/23 07:55:39.251
  E0504 07:55:39.652689      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:40.653104      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:41.653962      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:42.654331      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-1be9440d-bf31-40d3-90f7-fed6597637fd @ 05/04/23 07:55:43.384
  STEP: Updating configmap cm-test-opt-upd-7c05e0e4-ad1a-4dd8-9d09-d5f3b694cade @ 05/04/23 07:55:43.402
  STEP: Creating configMap with name cm-test-opt-create-5cce88c7-5ae6-4452-bb67-844464c9ae82 @ 05/04/23 07:55:43.416
  STEP: waiting to observe update in volume @ 05/04/23 07:55:43.432
  E0504 07:55:43.655316      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:44.655648      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:45.656394      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:46.656611      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:47.657618      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:48.658822      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:49.659909      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:50.660381      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:51.661056      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:52.661498      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:53.661965      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:54.662246      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:55.662394      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:56.662771      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:57.664003      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:58.664758      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:55:59.664998      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:00.665219      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:01.665936      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:02.666317      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:03.667210      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:04.667617      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:05.668772      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:06.669126      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:07.669978      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:08.670234      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:09.671146      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:10.671530      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:11.671971      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:12.672155      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:13.673077      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:14.673455      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:15.674476      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:16.674889      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:17.675685      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:18.676843      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:19.678051      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:20.678495      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:21.679130      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:22.679528      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:23.680448      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:24.680710      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:25.681085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:26.681491      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:27.681901      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:28.683166      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:29.683405      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:30.683879      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:31.684306      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:32.684741      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:33.685390      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:34.686064      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:35.686313      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:36.686542      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:37.686943      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:38.687063      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:39.687667      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:40.688120      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:41.688521      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:42.688774      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:43.689823      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:44.690601      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:45.690989      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:46.691490      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:47.691954      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:48.692624      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:49.692865      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:50.693216      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:51.693495      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:52.694111      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:53.694452      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:54.694669      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:55.694921      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:56.695203      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:57.695525      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:58.695786      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:56:59.696170      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:00.697351      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:01.697567      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:02.698690      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:03.699460      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:04.700510      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:05.700763      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:06.701614      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:07.702066      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:08.702468      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:09.702829      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:10.703636      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:11.704157      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:12.704542      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:13.705685      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:57:13.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6368" for this suite. @ 05/04/23 07:57:13.968
• [94.846 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 05/04/23 07:57:13.984
  May  4 07:57:13.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename services @ 05/04/23 07:57:13.986
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:57:14.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:57:14.038
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5540 @ 05/04/23 07:57:14.068
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 05/04/23 07:57:14.109
  STEP: creating service externalsvc in namespace services-5540 @ 05/04/23 07:57:14.109
  STEP: creating replication controller externalsvc in namespace services-5540 @ 05/04/23 07:57:14.215
  I0504 07:57:14.275644      20 runners.go:194] Created replication controller with name: externalsvc, namespace: services-5540, replica count: 2
  E0504 07:57:14.706754      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:15.706952      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:16.707599      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0504 07:57:17.326461      20 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 05/04/23 07:57:17.337
  May  4 07:57:17.383: INFO: Creating new exec pod
  E0504 07:57:17.708566      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:18.709552      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:57:19.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=services-5540 exec execpodw4rlf -- /bin/sh -x -c nslookup clusterip-service.services-5540.svc.cluster.local'
  E0504 07:57:19.710191      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:57:19.756: INFO: stderr: "+ nslookup clusterip-service.services-5540.svc.cluster.local\n"
  May  4 07:57:19.756: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-5540.svc.cluster.local\tcanonical name = externalsvc.services-5540.svc.cluster.local.\nName:\texternalsvc.services-5540.svc.cluster.local\nAddress: 10.103.14.245\n\n"
  May  4 07:57:19.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-5540, will wait for the garbage collector to delete the pods @ 05/04/23 07:57:19.777
  May  4 07:57:19.849: INFO: Deleting ReplicationController externalsvc took: 16.796979ms
  May  4 07:57:19.950: INFO: Terminating ReplicationController externalsvc pods took: 100.671341ms
  E0504 07:57:20.711162      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:21.711244      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:22.712235      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:57:22.739: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-5540" for this suite. @ 05/04/23 07:57:22.797
• [8.841 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 05/04/23 07:57:22.826
  May  4 07:57:22.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/04/23 07:57:22.827
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:57:22.907
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:57:22.911
  May  4 07:57:22.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  E0504 07:57:23.712870      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:57:23.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-8462" for this suite. @ 05/04/23 07:57:23.979
• [1.172 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 05/04/23 07:57:23.999
  May  4 07:57:23.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename pods @ 05/04/23 07:57:24.001
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:57:24.04
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:57:24.045
  STEP: Create a pod @ 05/04/23 07:57:24.081
  E0504 07:57:24.713006      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:25.713449      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:26.713730      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:27.714645      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 05/04/23 07:57:28.119
  May  4 07:57:28.156: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  May  4 07:57:28.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8392" for this suite. @ 05/04/23 07:57:28.192
• [4.208 seconds]
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 05/04/23 07:57:28.207
  May  4 07:57:28.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename pod-network-test @ 05/04/23 07:57:28.21
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:57:28.242
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:57:28.246
  STEP: Performing setup for networking test in namespace pod-network-test-2477 @ 05/04/23 07:57:28.264
  STEP: creating a selector @ 05/04/23 07:57:28.264
  STEP: Creating the service pods in kubernetes @ 05/04/23 07:57:28.264
  May  4 07:57:28.264: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0504 07:57:28.714740      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:29.715718      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:30.716743      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:31.717198      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:32.718148      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:33.718564      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:34.719649      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:35.719880      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:36.720859      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:37.721211      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:38.721793      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:39.722040      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:40.722227      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:41.722516      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:42.722766      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:43.723502      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:44.723963      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:45.724702      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:46.724976      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:47.725190      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:48.725969      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:49.726194      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 05/04/23 07:57:50.465
  E0504 07:57:50.727068      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:51.727502      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:57:52.509: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  May  4 07:57:52.510: INFO: Breadth first check of 172.16.36.88 on host 192.168.0.156...
  May  4 07:57:52.512: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.169.152:9080/dial?request=hostname&protocol=http&host=172.16.36.88&port=8083&tries=1'] Namespace:pod-network-test-2477 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 07:57:52.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 07:57:52.514: INFO: ExecWithOptions: Clientset creation
  May  4 07:57:52.514: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2477/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.169.152%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.16.36.88%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May  4 07:57:52.602: INFO: Waiting for responses: map[]
  May  4 07:57:52.602: INFO: reached 172.16.36.88 after 0/1 tries
  May  4 07:57:52.602: INFO: Breadth first check of 172.16.169.157 on host 192.168.0.157...
  May  4 07:57:52.606: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.169.152:9080/dial?request=hostname&protocol=http&host=172.16.169.157&port=8083&tries=1'] Namespace:pod-network-test-2477 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  4 07:57:52.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  May  4 07:57:52.606: INFO: ExecWithOptions: Clientset creation
  May  4 07:57:52.606: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2477/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.169.152%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.16.169.157%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May  4 07:57:52.697: INFO: Waiting for responses: map[]
  May  4 07:57:52.698: INFO: reached 172.16.169.157 after 0/1 tries
  May  4 07:57:52.698: INFO: Going to retry 0 out of 2 pods....
  May  4 07:57:52.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-2477" for this suite. @ 05/04/23 07:57:52.704
• [24.513 seconds]
------------------------------
SS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 05/04/23 07:57:52.72
  May  4 07:57:52.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename dns @ 05/04/23 07:57:52.722
  E0504 07:57:52.727796      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:57:52.792
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:57:52.797
  STEP: Creating a test headless service @ 05/04/23 07:57:52.801
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-89.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-89.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-89.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-89.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-89.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-89.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-89.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-89.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-89.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-89.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 196.117.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.117.196_udp@PTR;check="$$(dig +tcp +noall +answer +search 196.117.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.117.196_tcp@PTR;sleep 1; done
   @ 05/04/23 07:57:52.883
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-89.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-89.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-89.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-89.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-89.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-89.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-89.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-89.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-89.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-89.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 196.117.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.117.196_udp@PTR;check="$$(dig +tcp +noall +answer +search 196.117.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.117.196_tcp@PTR;sleep 1; done
   @ 05/04/23 07:57:52.883
  STEP: creating a pod to probe DNS @ 05/04/23 07:57:52.883
  STEP: submitting the pod to kubernetes @ 05/04/23 07:57:52.883
  E0504 07:57:53.728084      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:54.728491      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:55.729054      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:56.729171      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/04/23 07:57:56.977
  STEP: looking for the results for each expected name from probers @ 05/04/23 07:57:56.98
  May  4 07:57:56.985: INFO: Unable to read wheezy_udp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:57:56.989: INFO: Unable to read wheezy_tcp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:57:56.993: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:57:56.996: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:57:57.015: INFO: Unable to read jessie_udp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:57:57.018: INFO: Unable to read jessie_tcp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:57:57.022: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:57:57.025: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:57:57.040: INFO: Lookups using dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990 failed for: [wheezy_udp@dns-test-service.dns-89.svc.cluster.local wheezy_tcp@dns-test-service.dns-89.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local jessie_udp@dns-test-service.dns-89.svc.cluster.local jessie_tcp@dns-test-service.dns-89.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local]

  E0504 07:57:57.730203      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:58.730556      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:57:59.731185      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:00.731637      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:01.732160      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:02.045: INFO: Unable to read wheezy_udp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:02.049: INFO: Unable to read wheezy_tcp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:02.053: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:02.057: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:02.076: INFO: Unable to read jessie_udp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:02.080: INFO: Unable to read jessie_tcp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:02.084: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:02.088: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:02.103: INFO: Lookups using dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990 failed for: [wheezy_udp@dns-test-service.dns-89.svc.cluster.local wheezy_tcp@dns-test-service.dns-89.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local jessie_udp@dns-test-service.dns-89.svc.cluster.local jessie_tcp@dns-test-service.dns-89.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local]

  E0504 07:58:02.732599      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:03.733067      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:04.733480      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:05.734313      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:06.734713      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:07.054: INFO: Unable to read wheezy_udp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:07.058: INFO: Unable to read wheezy_tcp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:07.061: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:07.065: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:07.083: INFO: Unable to read jessie_udp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:07.113: INFO: Unable to read jessie_tcp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:07.118: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:07.121: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:07.135: INFO: Lookups using dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990 failed for: [wheezy_udp@dns-test-service.dns-89.svc.cluster.local wheezy_tcp@dns-test-service.dns-89.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local jessie_udp@dns-test-service.dns-89.svc.cluster.local jessie_tcp@dns-test-service.dns-89.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local]

  E0504 07:58:07.735760      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:08.736866      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:09.737093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:10.737230      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:11.737528      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:12.045: INFO: Unable to read wheezy_udp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:12.050: INFO: Unable to read wheezy_tcp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:12.055: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:12.060: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:12.081: INFO: Unable to read jessie_udp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:12.098: INFO: Unable to read jessie_tcp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:12.102: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:12.106: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:12.155: INFO: Lookups using dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990 failed for: [wheezy_udp@dns-test-service.dns-89.svc.cluster.local wheezy_tcp@dns-test-service.dns-89.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local jessie_udp@dns-test-service.dns-89.svc.cluster.local jessie_tcp@dns-test-service.dns-89.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local]

  E0504 07:58:12.738327      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:13.738749      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:14.739094      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:15.739431      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:16.739598      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:17.046: INFO: Unable to read wheezy_udp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:17.051: INFO: Unable to read wheezy_tcp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:17.055: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:17.059: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:17.078: INFO: Unable to read jessie_udp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:17.082: INFO: Unable to read jessie_tcp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:17.085: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:17.089: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:17.108: INFO: Lookups using dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990 failed for: [wheezy_udp@dns-test-service.dns-89.svc.cluster.local wheezy_tcp@dns-test-service.dns-89.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local jessie_udp@dns-test-service.dns-89.svc.cluster.local jessie_tcp@dns-test-service.dns-89.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local]

  E0504 07:58:17.740558      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:18.741612      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:19.742060      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:20.742488      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:21.742741      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:22.045: INFO: Unable to read wheezy_udp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:22.050: INFO: Unable to read wheezy_tcp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:22.055: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:22.060: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:22.082: INFO: Unable to read jessie_udp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:22.086: INFO: Unable to read jessie_tcp@dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:22.090: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:22.094: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local from pod dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990: the server could not find the requested resource (get pods dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990)
  May  4 07:58:22.109: INFO: Lookups using dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990 failed for: [wheezy_udp@dns-test-service.dns-89.svc.cluster.local wheezy_tcp@dns-test-service.dns-89.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local jessie_udp@dns-test-service.dns-89.svc.cluster.local jessie_tcp@dns-test-service.dns-89.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-89.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-89.svc.cluster.local]

  E0504 07:58:22.743044      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:23.743982      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:24.744431      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:25.744939      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:26.745637      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:27.103: INFO: DNS probes using dns-89/dns-test-a9ad3865-ec20-4bc0-adce-e27cb666d990 succeeded

  May  4 07:58:27.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/04/23 07:58:27.108
  STEP: deleting the test service @ 05/04/23 07:58:27.193
  STEP: deleting the test headless service @ 05/04/23 07:58:27.354
  STEP: Destroying namespace "dns-89" for this suite. @ 05/04/23 07:58:27.431
• [34.733 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 05/04/23 07:58:27.455
  May  4 07:58:27.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename field-validation @ 05/04/23 07:58:27.456
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:58:27.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:58:27.501
  May  4 07:58:27.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  E0504 07:58:27.746555      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:28.747633      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:29.747862      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0504 07:58:30.114658      20 warnings.go:70] unknown field "alpha"
  W0504 07:58:30.114707      20 warnings.go:70] unknown field "beta"
  W0504 07:58:30.114725      20 warnings.go:70] unknown field "delta"
  W0504 07:58:30.114741      20 warnings.go:70] unknown field "epsilon"
  W0504 07:58:30.114764      20 warnings.go:70] unknown field "gamma"
  May  4 07:58:30.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5546" for this suite. @ 05/04/23 07:58:30.164
• [2.739 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 05/04/23 07:58:30.196
  May  4 07:58:30.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 07:58:30.197
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:58:30.255
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:58:30.259
  STEP: Creating the pod @ 05/04/23 07:58:30.276
  E0504 07:58:30.747940      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:31.748719      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:32.749739      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:33.750778      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:34.751126      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:34.881: INFO: Successfully updated pod "annotationupdatee2ac5a9b-4caf-4d45-a076-1341c5c5ec99"
  E0504 07:58:35.751587      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:36.751943      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:36.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3460" for this suite. @ 05/04/23 07:58:36.929
• [6.777 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 05/04/23 07:58:36.975
  May  4 07:58:36.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename daemonsets @ 05/04/23 07:58:36.976
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:58:37.041
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:58:37.046
  May  4 07:58:37.086: INFO: Create a RollingUpdate DaemonSet
  May  4 07:58:37.108: INFO: Check that daemon pods launch on every node of the cluster
  May  4 07:58:37.112: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:58:37.128: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 07:58:37.128: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  E0504 07:58:37.752161      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:38.140: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:58:38.145: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 07:58:38.145: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  E0504 07:58:38.752539      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:39.134: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:58:39.137: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 07:58:39.137: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  E0504 07:58:39.753101      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:40.134: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:58:40.138: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  4 07:58:40.138: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  May  4 07:58:40.138: INFO: Update the DaemonSet to trigger a rollout
  May  4 07:58:40.164: INFO: Updating DaemonSet daemon-set
  E0504 07:58:40.754219      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:41.754854      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:42.755778      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:43.198: INFO: Roll back the DaemonSet before rollout is complete
  May  4 07:58:43.229: INFO: Updating DaemonSet daemon-set
  May  4 07:58:43.229: INFO: Make sure DaemonSet rollback is complete
  May  4 07:58:43.241: INFO: Wrong image for pod: daemon-set-sm4xc. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  May  4 07:58:43.241: INFO: Pod daemon-set-sm4xc is not available
  May  4 07:58:43.254: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0504 07:58:43.756027      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:44.263: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0504 07:58:44.756359      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:45.263: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0504 07:58:45.756593      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:46.264: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0504 07:58:46.757413      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:47.263: INFO: Pod daemon-set-j528q is not available
  May  4 07:58:47.269: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Deleting DaemonSet "daemon-set" @ 05/04/23 07:58:47.275
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5310, will wait for the garbage collector to delete the pods @ 05/04/23 07:58:47.276
  May  4 07:58:47.357: INFO: Deleting DaemonSet.extensions daemon-set took: 28.014809ms
  May  4 07:58:47.558: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.932347ms
  E0504 07:58:47.758498      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:48.759035      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:49.759340      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:58:50.759610      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:51.076: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 07:58:51.076: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May  4 07:58:51.079: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"86403"},"items":null}

  May  4 07:58:51.081: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"86403"},"items":null}

  May  4 07:58:51.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5310" for this suite. @ 05/04/23 07:58:51.097
• [14.142 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 05/04/23 07:58:51.12
  May  4 07:58:51.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename daemonsets @ 05/04/23 07:58:51.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:58:51.232
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:58:51.237
  STEP: Creating simple DaemonSet "daemon-set" @ 05/04/23 07:58:51.285
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/04/23 07:58:51.307
  May  4 07:58:51.314: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:58:51.364: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 07:58:51.364: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  E0504 07:58:51.759728      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:52.386: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:58:52.391: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 07:58:52.391: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  E0504 07:58:52.760556      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:53.370: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:58:53.375: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 07:58:53.375: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  E0504 07:58:53.760955      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:54.377: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:58:54.381: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  4 07:58:54.381: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 05/04/23 07:58:54.384
  May  4 07:58:54.419: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:58:54.423: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  4 07:58:54.424: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  E0504 07:58:54.761253      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:55.429: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:58:55.432: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  4 07:58:55.432: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  E0504 07:58:55.761930      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:56.435: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:58:56.447: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  4 07:58:56.447: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  E0504 07:58:56.762011      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:57.430: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:58:57.434: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  4 07:58:57.434: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  E0504 07:58:57.762327      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:58.444: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:58:58.447: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  4 07:58:58.447: INFO: Node k8s-node1 is running 0 daemon pod, expected 1
  E0504 07:58:58.762678      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:58:59.430: INFO: DaemonSet pods can't tolerate node k8s-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May  4 07:58:59.434: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  4 07:58:59.434: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/04/23 07:58:59.437
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-805, will wait for the garbage collector to delete the pods @ 05/04/23 07:58:59.437
  May  4 07:58:59.522: INFO: Deleting DaemonSet.extensions daemon-set took: 31.74264ms
  May  4 07:58:59.623: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.655615ms
  E0504 07:58:59.763490      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:00.764360      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:01.764627      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:59:02.043: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  4 07:59:02.043: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May  4 07:59:02.046: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"86531"},"items":null}

  May  4 07:59:02.048: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"86531"},"items":null}

  May  4 07:59:02.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-805" for this suite. @ 05/04/23 07:59:02.063
• [10.958 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 05/04/23 07:59:02.081
  May  4 07:59:02.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 07:59:02.082
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:59:02.132
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:59:02.136
  STEP: Creating configMap with name projected-configmap-test-volume-27866a59-36e4-4a20-9454-15a4f9129445 @ 05/04/23 07:59:02.14
  STEP: Creating a pod to test consume configMaps @ 05/04/23 07:59:02.164
  E0504 07:59:02.765634      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:03.766408      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:04.766903      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:05.767336      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/04/23 07:59:06.193
  May  4 07:59:06.196: INFO: Trying to get logs from node k8s-node2 pod pod-projected-configmaps-e162af33-c843-4371-a530-e7573a9b6e79 container agnhost-container: <nil>
  STEP: delete the pod @ 05/04/23 07:59:06.202
  May  4 07:59:06.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5542" for this suite. @ 05/04/23 07:59:06.272
• [4.206 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 05/04/23 07:59:06.287
  May  4 07:59:06.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename deployment @ 05/04/23 07:59:06.288
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:59:06.318
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:59:06.323
  May  4 07:59:06.377: INFO: Pod name rollover-pod: Found 0 pods out of 1
  E0504 07:59:06.767536      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:07.767919      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:08.768714      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:09.769128      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:10.769540      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:59:11.380: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/04/23 07:59:11.381
  May  4 07:59:11.381: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  E0504 07:59:11.770452      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:12.770821      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:59:13.385: INFO: Creating deployment "test-rollover-deployment"
  May  4 07:59:13.405: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  E0504 07:59:13.771018      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:14.771349      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:59:15.416: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  May  4 07:59:15.422: INFO: Ensure that both replica sets have 1 created replica
  May  4 07:59:15.427: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  May  4 07:59:15.464: INFO: Updating deployment test-rollover-deployment
  May  4 07:59:15.464: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  E0504 07:59:15.771456      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:16.771592      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:59:17.483: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  May  4 07:59:17.488: INFO: Make sure deployment "test-rollover-deployment" is complete
  May  4 07:59:17.495: INFO: all replica sets need to contain the pod-template-hash label
  May  4 07:59:17.495: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 59, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 59, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 59, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 59, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0504 07:59:17.772413      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:18.772629      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:59:19.503: INFO: all replica sets need to contain the pod-template-hash label
  May  4 07:59:19.503: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 59, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 59, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 59, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 59, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0504 07:59:19.773405      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:20.773851      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:59:21.518: INFO: all replica sets need to contain the pod-template-hash label
  May  4 07:59:21.519: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 59, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 59, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 59, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 59, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0504 07:59:21.774736      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:22.775178      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:59:23.503: INFO: all replica sets need to contain the pod-template-hash label
  May  4 07:59:23.503: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 59, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 59, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 59, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 59, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0504 07:59:23.775892      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:24.776412      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:59:25.509: INFO: all replica sets need to contain the pod-template-hash label
  May  4 07:59:25.509: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 59, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 59, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 59, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 59, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0504 07:59:25.776476      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:26.776921      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:59:27.502: INFO: all replica sets need to contain the pod-template-hash label
  May  4 07:59:27.502: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 59, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 59, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 4, 7, 59, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 4, 7, 59, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0504 07:59:27.777875      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:28.778263      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 07:59:29.522: INFO: 
  May  4 07:59:29.522: INFO: Ensure that both old replica sets have no replicas
  May  4 07:59:29.531: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-5648  5f05b009-61c5-4519-bdb7-c5b7a53f48b1 86704 2 2023-05-04 07:59:13 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-04 07:59:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 07:59:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00419ee28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-04 07:59:13 +0000 UTC,LastTransitionTime:2023-05-04 07:59:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-05-04 07:59:27 +0000 UTC,LastTransitionTime:2023-05-04 07:59:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May  4 07:59:29.535: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-5648  4e70114c-ed1b-4aad-b482-a945ba036545 86694 2 2023-05-04 07:59:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 5f05b009-61c5-4519-bdb7-c5b7a53f48b1 0xc00419f2e7 0xc00419f2e8}] [] [{kube-controller-manager Update apps/v1 2023-05-04 07:59:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f05b009-61c5-4519-bdb7-c5b7a53f48b1\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 07:59:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00419f398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May  4 07:59:29.535: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  May  4 07:59:29.535: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5648  adf08e90-64ed-45c2-9cc5-b78fbcd1039e 86703 2 2023-05-04 07:59:06 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 5f05b009-61c5-4519-bdb7-c5b7a53f48b1 0xc00419f1b7 0xc00419f1b8}] [] [{e2e.test Update apps/v1 2023-05-04 07:59:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 07:59:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f05b009-61c5-4519-bdb7-c5b7a53f48b1\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-04 07:59:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00419f278 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May  4 07:59:29.536: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-5648  1f811636-1064-475c-b1b1-dc3603e5e103 86660 2 2023-05-04 07:59:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 5f05b009-61c5-4519-bdb7-c5b7a53f48b1 0xc00419f407 0xc00419f408}] [] [{kube-controller-manager Update apps/v1 2023-05-04 07:59:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f05b009-61c5-4519-bdb7-c5b7a53f48b1\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-04 07:59:15 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00419f4b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May  4 07:59:29.539: INFO: Pod "test-rollover-deployment-57777854c9-ljv9z" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-ljv9z test-rollover-deployment-57777854c9- deployment-5648  5da2c776-4ffc-414a-a295-264c41b8d42b 86676 0 2023-05-04 07:59:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[cni.projectcalico.org/containerID:5ae062e33390a78567f9b37acdf224286804242f664ba95c466aa965191ba798 cni.projectcalico.org/podIP:172.16.169.176/32 cni.projectcalico.org/podIPs:172.16.169.176/32] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 4e70114c-ed1b-4aad-b482-a945ba036545 0xc0039322a7 0xc0039322a8}] [] [{kube-controller-manager Update v1 2023-05-04 07:59:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4e70114c-ed1b-4aad-b482-a945ba036545\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-04 07:59:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-04 07:59:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.169.176\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jwd7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jwd7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:59:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:59:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:59:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-04 07:59:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.0.157,PodIP:172.16.169.176,StartTime:2023-05-04 07:59:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-04 07:59:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:sha256:30e3d1b869f4d327bd612179ed37850611b462c8415691b3de9eb55787f81e71,ContainerID:containerd://fb45f8c2daac5d132184dfd257531a88b31f7007bb00294e4bab64699dc47620,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.169.176,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  4 07:59:29.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5648" for this suite. @ 05/04/23 07:59:29.544
• [23.273 seconds]
------------------------------
S
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 05/04/23 07:59:29.56
  May  4 07:59:29.560: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename cronjob @ 05/04/23 07:59:29.562
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 07:59:29.654
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 07:59:29.658
  STEP: Creating a ReplaceConcurrent cronjob @ 05/04/23 07:59:29.662
  STEP: Ensuring a job is scheduled @ 05/04/23 07:59:29.679
  E0504 07:59:29.778448      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:30.778847      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:31.779548      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:32.779963      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:33.780078      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:34.780250      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:35.780943      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:36.781212      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:37.782174      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:38.782414      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:39.783119      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:40.783508      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:41.783692      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:42.784608      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:43.784772      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:44.785069      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:45.786027      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:46.786433      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:47.787406      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:48.787479      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:49.788108      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:50.788388      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:51.788861      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:52.788959      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:53.789730      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:54.790428      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:55.791085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:56.791435      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:57.792124      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:58.792233      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 07:59:59.793170      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:00.793626      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 05/04/23 08:00:01.682
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 05/04/23 08:00:01.686
  STEP: Ensuring the job is replaced with a new one @ 05/04/23 08:00:01.688
  E0504 08:00:01.794183      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:02.794608      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:03.795601      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:04.796024      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:05.796825      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:06.797243      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:07.797299      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:08.797422      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:09.798230      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:10.798472      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:11.798666      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:12.798731      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:13.799937      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:14.800347      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:15.800796      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:16.801222      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:17.802067      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:18.802606      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:19.803673      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:20.804162      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:21.805080      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:22.805546      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:23.806525      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:24.806942      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:25.807894      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:26.808235      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:27.808633      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:28.808792      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:29.809534      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:30.810498      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:31.810595      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:32.810786      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:33.811646      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:34.812001      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:35.812180      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:36.812607      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:37.813082      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:38.813954      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:39.814548      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:40.814814      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:41.815879      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:42.816235      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:43.816732      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:44.817266      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:45.817875      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:46.818548      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:47.819183      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:48.819375      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:49.819513      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:50.820558      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:51.821214      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:52.821702      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:53.821829      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:54.822058      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:55.823048      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:56.823382      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:57.824044      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:58.824315      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:00:59.824518      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:00.824907      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 05/04/23 08:01:01.692
  May  4 08:01:01.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2011" for this suite. @ 05/04/23 08:01:01.713
• [92.211 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 05/04/23 08:01:01.776
  May  4 08:01:01.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename kubectl @ 05/04/23 08:01:01.777
  E0504 08:01:01.825928      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 08:01:01.896
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 08:01:01.9
  May  4 08:01:01.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-5131 create -f -'
  E0504 08:01:02.826103      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:01:03.580: INFO: stderr: ""
  May  4 08:01:03.580: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  May  4 08:01:03.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-5131 create -f -'
  E0504 08:01:03.826524      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:01:04.198: INFO: stderr: ""
  May  4 08:01:04.198: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/04/23 08:01:04.198
  E0504 08:01:04.827660      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:01:05.204: INFO: Selector matched 1 pods for map[app:agnhost]
  May  4 08:01:05.204: INFO: Found 0 / 1
  E0504 08:01:05.828078      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:01:06.203: INFO: Selector matched 1 pods for map[app:agnhost]
  May  4 08:01:06.203: INFO: Found 1 / 1
  May  4 08:01:06.204: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  May  4 08:01:06.207: INFO: Selector matched 1 pods for map[app:agnhost]
  May  4 08:01:06.207: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May  4 08:01:06.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-5131 describe pod agnhost-primary-6vft7'
  May  4 08:01:06.333: INFO: stderr: ""
  May  4 08:01:06.333: INFO: stdout: "Name:             agnhost-primary-6vft7\nNamespace:        kubectl-5131\nPriority:         0\nService Account:  default\nNode:             k8s-node2/192.168.0.157\nStart Time:       Thu, 04 May 2023 08:01:03 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 35c6d9b6ef6ac46203d081b019ec899f6642a4cf3c6c8b7835fb84e0bfe6d6a0\n                  cni.projectcalico.org/podIP: 172.16.169.141/32\n                  cni.projectcalico.org/podIPs: 172.16.169.141/32\nStatus:           Running\nIP:               172.16.169.141\nIPs:\n  IP:           172.16.169.141\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://4d5921b1386999a3b9a2453f57514f70526dd51f91165c65c3ea0ee0d24d4df0\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       sha256:30e3d1b869f4d327bd612179ed37850611b462c8415691b3de9eb55787f81e71\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 04 May 2023 08:01:05 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zwsgd (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-zwsgd:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-5131/agnhost-primary-6vft7 to k8s-node2\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  May  4 08:01:06.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-5131 describe rc agnhost-primary'
  May  4 08:01:06.467: INFO: stderr: ""
  May  4 08:01:06.467: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5131\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-6vft7\n"
  May  4 08:01:06.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-5131 describe service agnhost-primary'
  May  4 08:01:06.595: INFO: stderr: ""
  May  4 08:01:06.595: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5131\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.97.116.135\nIPs:               10.97.116.135\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.16.169.141:6379\nSession Affinity:  None\nEvents:            <none>\n"
  May  4 08:01:06.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-5131 describe node k8s-master'
  May  4 08:01:06.745: INFO: stderr: ""
  May  4 08:01:06.745: INFO: stdout: "Name:               k8s-master\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-master\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.0.155/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.16.235.192\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 04 May 2023 01:42:02 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8s-master\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 04 May 2023 08:00:57 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 04 May 2023 01:44:50 +0000   Thu, 04 May 2023 01:44:50 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 04 May 2023 07:57:52 +0000   Thu, 04 May 2023 01:41:59 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 04 May 2023 07:57:52 +0000   Thu, 04 May 2023 01:41:59 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 04 May 2023 07:57:52 +0000   Thu, 04 May 2023 01:41:59 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 04 May 2023 07:57:52 +0000   Thu, 04 May 2023 01:44:26 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.0.155\n  Hostname:    k8s-master\nCapacity:\n  cpu:                3\n  ephemeral-storage:  67074052Ki\n  hugepages-2Mi:      0\n  memory:             8167588Ki\n  pods:               110\nAllocatable:\n  cpu:                3\n  ephemeral-storage:  61815446221\n  hugepages-2Mi:      0\n  memory:             8065188Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 ed448fab3a6246dd9a53801bc283c32b\n  System UUID:                9BF6033C-2017-48DB-B9C7-16F4F6178BE7\n  Boot ID:                    615d7142-2bd4-47ef-94a8-6ad85bf98ef3\n  Kernel Version:             3.10.0-1160.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.20\n  Kubelet Version:            v1.27.1\n  Kube-Proxy Version:         v1.27.1\nPodCIDR:                      172.16.0.0/24\nPodCIDRs:                     172.16.0.0/24\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-476fk                                          250m (8%)     0 (0%)      0 (0%)           0 (0%)         6h17m\n  kube-system                 etcd-k8s-master                                            100m (3%)     0 (0%)      100Mi (1%)       0 (0%)         6h19m\n  kube-system                 kube-apiserver-k8s-master                                  250m (8%)     0 (0%)      0 (0%)           0 (0%)         6h18m\n  kube-system                 kube-controller-manager-k8s-master                         200m (6%)     0 (0%)      0 (0%)           0 (0%)         6h18m\n  kube-system                 kube-proxy-4xqp7                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h18m\n  kube-system                 kube-scheduler-k8s-master                                  100m (3%)     0 (0%)      0 (0%)           0 (0%)         6h18m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-1f5c24134e834838-hqg2t    0 (0%)        0 (0%)      0 (0%)           0 (0%)         109m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                900m (30%)  0 (0%)\n  memory             100Mi (1%)  0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
  May  4 08:01:06.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-5131 describe namespace kubectl-5131'
  E0504 08:01:06.829481      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:01:06.941: INFO: stderr: ""
  May  4 08:01:06.941: INFO: stdout: "Name:         kubectl-5131\nLabels:       e2e-framework=kubectl\n              e2e-run=03fb56be-d3a0-4cb6-b1ca-4a4094abfd3d\n              kubernetes.io/metadata.name=kubectl-5131\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  May  4 08:01:06.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5131" for this suite. @ 05/04/23 08:01:06.952
• [5.200 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 05/04/23 08:01:06.978
  May  4 08:01:06.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename kubectl @ 05/04/23 08:01:06.979
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 08:01:07.038
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 08:01:07.041
  STEP: creating all guestbook components @ 05/04/23 08:01:07.045
  May  4 08:01:07.045: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  May  4 08:01:07.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-8698 create -f -'
  May  4 08:01:07.640: INFO: stderr: ""
  May  4 08:01:07.640: INFO: stdout: "service/agnhost-replica created\n"
  May  4 08:01:07.640: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  May  4 08:01:07.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-8698 create -f -'
  E0504 08:01:07.830084      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:01:08.217: INFO: stderr: ""
  May  4 08:01:08.217: INFO: stdout: "service/agnhost-primary created\n"
  May  4 08:01:08.217: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  May  4 08:01:08.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-8698 create -f -'
  E0504 08:01:08.830621      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:01:08.837: INFO: stderr: ""
  May  4 08:01:08.837: INFO: stdout: "service/frontend created\n"
  May  4 08:01:08.837: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  May  4 08:01:08.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-8698 create -f -'
  May  4 08:01:09.297: INFO: stderr: ""
  May  4 08:01:09.297: INFO: stdout: "deployment.apps/frontend created\n"
  May  4 08:01:09.297: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  May  4 08:01:09.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-8698 create -f -'
  E0504 08:01:09.831498      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:01:09.895: INFO: stderr: ""
  May  4 08:01:09.895: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  May  4 08:01:09.896: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  May  4 08:01:09.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-8698 create -f -'
  May  4 08:01:10.559: INFO: stderr: ""
  May  4 08:01:10.559: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 05/04/23 08:01:10.559
  May  4 08:01:10.559: INFO: Waiting for all frontend pods to be Running.
  E0504 08:01:10.837613      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:11.837785      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:12.841841      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:13.842453      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:14.842547      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:01:15.611: INFO: Waiting for frontend to serve content.
  May  4 08:01:15.623: INFO: Trying to add a new entry to the guestbook.
  May  4 08:01:15.637: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 05/04/23 08:01:15.662
  May  4 08:01:15.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-8698 delete --grace-period=0 --force -f -'
  E0504 08:01:15.842657      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:01:15.909: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  4 08:01:15.909: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 05/04/23 08:01:15.909
  May  4 08:01:15.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-8698 delete --grace-period=0 --force -f -'
  May  4 08:01:16.119: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  4 08:01:16.119: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 05/04/23 08:01:16.119
  May  4 08:01:16.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-8698 delete --grace-period=0 --force -f -'
  May  4 08:01:16.336: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  4 08:01:16.336: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 05/04/23 08:01:16.336
  May  4 08:01:16.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-8698 delete --grace-period=0 --force -f -'
  May  4 08:01:16.476: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  4 08:01:16.476: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 05/04/23 08:01:16.477
  May  4 08:01:16.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-8698 delete --grace-period=0 --force -f -'
  May  4 08:01:16.721: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  4 08:01:16.721: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 05/04/23 08:01:16.721
  May  4 08:01:16.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2390127384 --namespace=kubectl-8698 delete --grace-period=0 --force -f -'
  E0504 08:01:16.842943      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:01:16.905: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  4 08:01:16.905: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  May  4 08:01:16.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8698" for this suite. @ 05/04/23 08:01:16.935
• [10.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 05/04/23 08:01:17.036
  May  4 08:01:17.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename conformance-tests @ 05/04/23 08:01:17.038
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 08:01:17.196
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 08:01:17.208
  STEP: Getting node addresses @ 05/04/23 08:01:17.213
  May  4 08:01:17.214: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  May  4 08:01:17.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-4449" for this suite. @ 05/04/23 08:01:17.37
• [0.394 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 05/04/23 08:01:17.446
  May  4 08:01:17.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename init-container @ 05/04/23 08:01:17.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 08:01:17.627
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 08:01:17.634
  STEP: creating the pod @ 05/04/23 08:01:17.641
  May  4 08:01:17.641: INFO: PodSpec: initContainers in spec.initContainers
  E0504 08:01:17.843723      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:18.844334      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:19.844543      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:20.844913      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:21.845652      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:22.845820      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:23.845891      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:01:24.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-3201" for this suite. @ 05/04/23 08:01:24.501
• [7.073 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 05/04/23 08:01:24.521
  May  4 08:01:24.521: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 08:01:24.523
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 08:01:24.597
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 08:01:24.6
  STEP: Creating a pod to test downward API volume plugin @ 05/04/23 08:01:24.604
  E0504 08:01:24.846013      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:25.846646      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:26.847767      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:27.848196      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/04/23 08:01:28.644
  May  4 08:01:28.647: INFO: Trying to get logs from node k8s-node2 pod downwardapi-volume-759bf0e8-e55e-4c45-bfac-9718a056fa99 container client-container: <nil>
  STEP: delete the pod @ 05/04/23 08:01:28.667
  May  4 08:01:28.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1487" for this suite. @ 05/04/23 08:01:28.716
• [4.211 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 05/04/23 08:01:28.733
  May  4 08:01:28.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename sched-preemption @ 05/04/23 08:01:28.735
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 08:01:28.792
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 08:01:28.797
  May  4 08:01:28.837: INFO: Waiting up to 1m0s for all nodes to be ready
  E0504 08:01:28.848907      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:29.849142      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:30.849487      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:31.850176      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:32.850338      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:33.850958      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:34.851142      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:35.851404      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:36.851824      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:37.852058      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:38.852973      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:39.853216      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:40.853576      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:41.854158      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:42.854591      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:43.854563      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:44.855162      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:45.855366      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:46.855870      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:47.856568      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:48.857407      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:49.858563      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:50.859469      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:51.860620      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:52.861163      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:53.861421      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:54.861749      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:55.862013      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:56.862867      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:57.863008      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:58.863506      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:01:59.863842      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:00.864811      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:01.865183      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:02.865936      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:03.867043      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:04.867833      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:05.868028      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:06.869047      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:07.869351      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:08.869712      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:09.870256      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:10.871241      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:11.871983      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:12.872625      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:13.872885      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:14.873561      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:15.873938      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:16.874671      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:17.874942      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:18.875046      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:19.875450      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:20.876141      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:21.876577      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:22.877626      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:23.878419      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:24.879892      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:25.880176      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:26.880151      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:27.880408      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:02:28.875: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 05/04/23 08:02:28.878
  E0504 08:02:28.880514      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:02:28.957: INFO: Created pod: pod0-0-sched-preemption-low-priority
  May  4 08:02:28.978: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  May  4 08:02:29.090: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  May  4 08:02:29.147: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 05/04/23 08:02:29.147
  E0504 08:02:29.880785      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:30.881051      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:31.881119      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:32.881445      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 05/04/23 08:02:33.258
  E0504 08:02:33.882087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:34.882499      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:35.883418      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:36.883786      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:02:37.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-8264" for this suite. @ 05/04/23 08:02:37.393
• [68.675 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 05/04/23 08:02:37.409
  May  4 08:02:37.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename emptydir-wrapper @ 05/04/23 08:02:37.41
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 08:02:37.461
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 08:02:37.465
  STEP: Creating 50 configmaps @ 05/04/23 08:02:37.468
  E0504 08:02:37.884427      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating RC which spawns configmap-volume pods @ 05/04/23 08:02:38.522
  May  4 08:02:38.555: INFO: Pod name wrapped-volume-race-f82cb2a4-d7fd-4e4b-a83c-8daea852f0aa: Found 0 pods out of 5
  E0504 08:02:38.885127      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:39.885664      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:40.886708      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:41.887127      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:42.887509      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:02:43.590: INFO: Pod name wrapped-volume-race-f82cb2a4-d7fd-4e4b-a83c-8daea852f0aa: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/04/23 08:02:43.59
  STEP: Creating RC which spawns configmap-volume pods @ 05/04/23 08:02:43.666
  May  4 08:02:43.723: INFO: Pod name wrapped-volume-race-e6e35e6d-613e-4250-bd73-f8f00de6881d: Found 0 pods out of 5
  E0504 08:02:43.888250      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:44.888518      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:45.889553      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:46.894557      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:47.894807      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:02:48.735: INFO: Pod name wrapped-volume-race-e6e35e6d-613e-4250-bd73-f8f00de6881d: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/04/23 08:02:48.735
  STEP: Creating RC which spawns configmap-volume pods @ 05/04/23 08:02:48.782
  May  4 08:02:48.854: INFO: Pod name wrapped-volume-race-3386fa4a-dd1e-4f2e-b664-89fcda525c8f: Found 0 pods out of 5
  E0504 08:02:48.895445      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:49.895877      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:50.896355      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:51.896802      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:52.897101      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:02:53.865: INFO: Pod name wrapped-volume-race-3386fa4a-dd1e-4f2e-b664-89fcda525c8f: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/04/23 08:02:53.865
  May  4 08:02:53.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-3386fa4a-dd1e-4f2e-b664-89fcda525c8f in namespace emptydir-wrapper-2716, will wait for the garbage collector to delete the pods @ 05/04/23 08:02:53.89
  E0504 08:02:53.897066      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:02:53.960: INFO: Deleting ReplicationController wrapped-volume-race-3386fa4a-dd1e-4f2e-b664-89fcda525c8f took: 15.770624ms
  May  4 08:02:54.064: INFO: Terminating ReplicationController wrapped-volume-race-3386fa4a-dd1e-4f2e-b664-89fcda525c8f pods took: 104.258492ms
  E0504 08:02:54.898102      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:55.898648      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:56.899194      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-e6e35e6d-613e-4250-bd73-f8f00de6881d in namespace emptydir-wrapper-2716, will wait for the garbage collector to delete the pods @ 05/04/23 08:02:56.965
  May  4 08:02:57.046: INFO: Deleting ReplicationController wrapped-volume-race-e6e35e6d-613e-4250-bd73-f8f00de6881d took: 19.159316ms
  May  4 08:02:57.147: INFO: Terminating ReplicationController wrapped-volume-race-e6e35e6d-613e-4250-bd73-f8f00de6881d pods took: 100.949551ms
  E0504 08:02:57.900136      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:58.900988      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:02:59.901396      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-f82cb2a4-d7fd-4e4b-a83c-8daea852f0aa in namespace emptydir-wrapper-2716, will wait for the garbage collector to delete the pods @ 05/04/23 08:03:00.048
  May  4 08:03:00.120: INFO: Deleting ReplicationController wrapped-volume-race-f82cb2a4-d7fd-4e4b-a83c-8daea852f0aa took: 17.80623ms
  May  4 08:03:00.321: INFO: Terminating ReplicationController wrapped-volume-race-f82cb2a4-d7fd-4e4b-a83c-8daea852f0aa pods took: 201.05649ms
  E0504 08:03:00.902412      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:01.903444      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:02.903851      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 05/04/23 08:03:03.222
  E0504 08:03:03.904569      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "emptydir-wrapper-2716" for this suite. @ 05/04/23 08:03:04.226
• [26.837 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 05/04/23 08:03:04.247
  May  4 08:03:04.247: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename projected @ 05/04/23 08:03:04.249
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 08:03:04.314
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 08:03:04.318
  STEP: Creating configMap with name projected-configmap-test-volume-23e2859f-8110-490e-9a9c-8a423bcbf7b6 @ 05/04/23 08:03:04.322
  STEP: Creating a pod to test consume configMaps @ 05/04/23 08:03:04.335
  E0504 08:03:04.905182      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:05.905472      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:06.906240      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:07.906672      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/04/23 08:03:08.374
  May  4 08:03:08.378: INFO: Trying to get logs from node k8s-node2 pod pod-projected-configmaps-c74325f8-2af8-48bf-8f8a-d295e6cd4311 container agnhost-container: <nil>
  STEP: delete the pod @ 05/04/23 08:03:08.401
  May  4 08:03:08.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1166" for this suite. @ 05/04/23 08:03:08.475
• [4.251 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 05/04/23 08:03:08.5
  May  4 08:03:08.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename runtimeclass @ 05/04/23 08:03:08.501
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 08:03:08.546
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 08:03:08.55
  E0504 08:03:08.907385      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:09.908128      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:03:10.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-7629" for this suite. @ 05/04/23 08:03:10.683
• [2.208 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 05/04/23 08:03:10.711
  May  4 08:03:10.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename dns @ 05/04/23 08:03:10.713
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 08:03:10.781
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 08:03:10.786
  STEP: Creating a test headless service @ 05/04/23 08:03:10.789
  E0504 08:03:10.908479      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8582 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8582;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8582 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8582;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8582.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8582.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8582.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8582.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8582.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8582.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8582.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8582.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8582.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8582.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8582.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8582.svc;check="$$(dig +notcp +noall +answer +search 228.111.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.111.228_udp@PTR;check="$$(dig +tcp +noall +answer +search 228.111.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.111.228_tcp@PTR;sleep 1; done
   @ 05/04/23 08:03:10.922
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8582 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8582;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8582 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8582;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8582.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8582.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8582.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8582.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8582.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8582.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8582.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8582.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8582.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8582.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8582.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8582.svc;check="$$(dig +notcp +noall +answer +search 228.111.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.111.228_udp@PTR;check="$$(dig +tcp +noall +answer +search 228.111.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.111.228_tcp@PTR;sleep 1; done
   @ 05/04/23 08:03:10.923
  STEP: creating a pod to probe DNS @ 05/04/23 08:03:10.923
  STEP: submitting the pod to kubernetes @ 05/04/23 08:03:10.923
  E0504 08:03:11.908769      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:12.908911      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:13.910170      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:14.910621      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/04/23 08:03:15.072
  STEP: looking for the results for each expected name from probers @ 05/04/23 08:03:15.074
  May  4 08:03:15.080: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.084: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.088: INFO: Unable to read wheezy_udp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.092: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.096: INFO: Unable to read wheezy_udp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.100: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.103: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.106: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.109: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.113: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.116: INFO: Unable to read 10.99.111.228_udp@PTR from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.120: INFO: Unable to read 10.99.111.228_tcp@PTR from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.123: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.127: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.130: INFO: Unable to read jessie_udp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.134: INFO: Unable to read jessie_tcp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.138: INFO: Unable to read jessie_udp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.141: INFO: Unable to read jessie_tcp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.145: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.149: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.161: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.164: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.180: INFO: Unable to read 10.99.111.228_udp@PTR from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.184: INFO: Unable to read 10.99.111.228_tcp@PTR from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:15.184: INFO: Lookups using dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8582 wheezy_tcp@dns-test-service.dns-8582 wheezy_udp@dns-test-service.dns-8582.svc wheezy_tcp@dns-test-service.dns-8582.svc wheezy_udp@_http._tcp.dns-test-service.dns-8582.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8582.svc wheezy_udp@_http._tcp.test-service-2.dns-8582.svc wheezy_tcp@_http._tcp.test-service-2.dns-8582.svc 10.99.111.228_udp@PTR 10.99.111.228_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8582 jessie_tcp@dns-test-service.dns-8582 jessie_udp@dns-test-service.dns-8582.svc jessie_tcp@dns-test-service.dns-8582.svc jessie_udp@_http._tcp.dns-test-service.dns-8582.svc jessie_tcp@_http._tcp.dns-test-service.dns-8582.svc jessie_udp@_http._tcp.test-service-2.dns-8582.svc jessie_tcp@_http._tcp.test-service-2.dns-8582.svc 10.99.111.228_udp@PTR 10.99.111.228_tcp@PTR]

  E0504 08:03:15.911068      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:16.911471      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:17.911702      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:18.912579      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:19.912795      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:03:20.190: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:20.216: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:20.221: INFO: Unable to read wheezy_udp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:20.226: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:20.230: INFO: Unable to read wheezy_udp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:20.235: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:20.240: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:20.243: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:20.247: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:20.250: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:20.261: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:20.265: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:20.268: INFO: Unable to read jessie_udp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:20.272: INFO: Unable to read jessie_tcp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:20.275: INFO: Unable to read jessie_udp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:20.279: INFO: Unable to read jessie_tcp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:20.301: INFO: Lookups using dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8582 wheezy_tcp@dns-test-service.dns-8582 wheezy_udp@dns-test-service.dns-8582.svc wheezy_tcp@dns-test-service.dns-8582.svc wheezy_udp@_http._tcp.dns-test-service.dns-8582.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8582.svc wheezy_udp@_http._tcp.test-service-2.dns-8582.svc wheezy_tcp@_http._tcp.test-service-2.dns-8582.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8582 jessie_tcp@dns-test-service.dns-8582 jessie_udp@dns-test-service.dns-8582.svc jessie_tcp@dns-test-service.dns-8582.svc]

  E0504 08:03:20.913466      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:21.913785      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:22.914062      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:23.914854      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:24.914993      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:03:25.189: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:25.193: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:25.197: INFO: Unable to read wheezy_udp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:25.200: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:25.203: INFO: Unable to read wheezy_udp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:25.207: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:25.211: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:25.214: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:25.218: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:25.221: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:25.235: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:25.240: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:25.243: INFO: Unable to read jessie_udp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:25.247: INFO: Unable to read jessie_tcp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:25.250: INFO: Unable to read jessie_udp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:25.254: INFO: Unable to read jessie_tcp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:25.274: INFO: Lookups using dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8582 wheezy_tcp@dns-test-service.dns-8582 wheezy_udp@dns-test-service.dns-8582.svc wheezy_tcp@dns-test-service.dns-8582.svc wheezy_udp@_http._tcp.dns-test-service.dns-8582.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8582.svc wheezy_udp@_http._tcp.test-service-2.dns-8582.svc wheezy_tcp@_http._tcp.test-service-2.dns-8582.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8582 jessie_tcp@dns-test-service.dns-8582 jessie_udp@dns-test-service.dns-8582.svc jessie_tcp@dns-test-service.dns-8582.svc]

  E0504 08:03:25.915862      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:26.916610      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:27.916823      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:28.916970      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:29.917212      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:03:30.190: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:30.194: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:30.197: INFO: Unable to read wheezy_udp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:30.202: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:30.206: INFO: Unable to read wheezy_udp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:30.210: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:30.238: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:30.242: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:30.246: INFO: Unable to read jessie_udp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:30.251: INFO: Unable to read jessie_tcp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:30.255: INFO: Unable to read jessie_udp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:30.259: INFO: Unable to read jessie_tcp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:30.283: INFO: Lookups using dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8582 wheezy_tcp@dns-test-service.dns-8582 wheezy_udp@dns-test-service.dns-8582.svc wheezy_tcp@dns-test-service.dns-8582.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8582 jessie_tcp@dns-test-service.dns-8582 jessie_udp@dns-test-service.dns-8582.svc jessie_tcp@dns-test-service.dns-8582.svc]

  E0504 08:03:30.917542      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:31.918433      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:32.918954      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:33.919803      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:34.920748      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:03:35.191: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:35.195: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:35.198: INFO: Unable to read wheezy_udp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:35.202: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:35.205: INFO: Unable to read wheezy_udp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:35.209: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:35.235: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:35.239: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:35.243: INFO: Unable to read jessie_udp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:35.247: INFO: Unable to read jessie_tcp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:35.250: INFO: Unable to read jessie_udp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:35.253: INFO: Unable to read jessie_tcp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:35.273: INFO: Lookups using dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8582 wheezy_tcp@dns-test-service.dns-8582 wheezy_udp@dns-test-service.dns-8582.svc wheezy_tcp@dns-test-service.dns-8582.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8582 jessie_tcp@dns-test-service.dns-8582 jessie_udp@dns-test-service.dns-8582.svc jessie_tcp@dns-test-service.dns-8582.svc]

  E0504 08:03:35.920877      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:36.921057      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:37.921330      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:38.921466      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:39.921770      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:03:40.189: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:40.193: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:40.198: INFO: Unable to read wheezy_udp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:40.201: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:40.205: INFO: Unable to read wheezy_udp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:40.209: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:40.239: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:40.244: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:40.249: INFO: Unable to read jessie_udp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:40.253: INFO: Unable to read jessie_tcp@dns-test-service.dns-8582 from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:40.259: INFO: Unable to read jessie_udp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:40.264: INFO: Unable to read jessie_tcp@dns-test-service.dns-8582.svc from pod dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef: the server could not find the requested resource (get pods dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef)
  May  4 08:03:40.286: INFO: Lookups using dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8582 wheezy_tcp@dns-test-service.dns-8582 wheezy_udp@dns-test-service.dns-8582.svc wheezy_tcp@dns-test-service.dns-8582.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8582 jessie_tcp@dns-test-service.dns-8582 jessie_udp@dns-test-service.dns-8582.svc jessie_tcp@dns-test-service.dns-8582.svc]

  E0504 08:03:40.922753      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:41.923095      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:42.923894      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:43.924616      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:44.925615      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:03:45.292: INFO: DNS probes using dns-8582/dns-test-b0f0ebc9-271c-4107-bd65-04fd55f2d9ef succeeded

  May  4 08:03:45.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/04/23 08:03:45.298
  STEP: deleting the test service @ 05/04/23 08:03:45.496
  STEP: deleting the test headless service @ 05/04/23 08:03:45.753
  STEP: Destroying namespace "dns-8582" for this suite. @ 05/04/23 08:03:45.853
• [35.186 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 05/04/23 08:03:45.899
  May  4 08:03:45.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2390127384
  STEP: Building a namespace api object, basename dns @ 05/04/23 08:03:45.9
  E0504 08:03:45.926204      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/04/23 08:03:45.991
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/04/23 08:03:45.996
  STEP: Creating a test externalName service @ 05/04/23 08:03:46.001
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8809.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local; sleep 1; done
   @ 05/04/23 08:03:46.02
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8809.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local; sleep 1; done
   @ 05/04/23 08:03:46.02
  STEP: creating a pod to probe DNS @ 05/04/23 08:03:46.02
  STEP: submitting the pod to kubernetes @ 05/04/23 08:03:46.02
  E0504 08:03:46.927480      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:47.927615      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:48.927925      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:49.928387      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/04/23 08:03:50.15
  STEP: looking for the results for each expected name from probers @ 05/04/23 08:03:50.154
  May  4 08:03:50.164: INFO: DNS probes using dns-test-df7f61a6-3097-4e06-b62e-8d977ce572c1 succeeded

  STEP: changing the externalName to bar.example.com @ 05/04/23 08:03:50.164
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8809.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local; sleep 1; done
   @ 05/04/23 08:03:50.182
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8809.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local; sleep 1; done
   @ 05/04/23 08:03:50.182
  STEP: creating a second pod to probe DNS @ 05/04/23 08:03:50.182
  STEP: submitting the pod to kubernetes @ 05/04/23 08:03:50.182
  E0504 08:03:50.928457      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:51.928728      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:52.929656      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:53.930657      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/04/23 08:03:54.251
  STEP: looking for the results for each expected name from probers @ 05/04/23 08:03:54.254
  May  4 08:03:54.259: INFO: File wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local from pod  dns-8809/dns-test-6b3dd3cb-bbd3-4e45-95e8-5a2f8ab57adf contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  4 08:03:54.264: INFO: File jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local from pod  dns-8809/dns-test-6b3dd3cb-bbd3-4e45-95e8-5a2f8ab57adf contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  4 08:03:54.264: INFO: Lookups using dns-8809/dns-test-6b3dd3cb-bbd3-4e45-95e8-5a2f8ab57adf failed for: [wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local]

  E0504 08:03:54.932446      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:55.932811      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:56.933072      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:57.933420      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:03:58.933512      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:03:59.269: INFO: File wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local from pod  dns-8809/dns-test-6b3dd3cb-bbd3-4e45-95e8-5a2f8ab57adf contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  4 08:03:59.273: INFO: File jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local from pod  dns-8809/dns-test-6b3dd3cb-bbd3-4e45-95e8-5a2f8ab57adf contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  4 08:03:59.273: INFO: Lookups using dns-8809/dns-test-6b3dd3cb-bbd3-4e45-95e8-5a2f8ab57adf failed for: [wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local]

  E0504 08:03:59.933801      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:04:00.934077      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:04:01.934413      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:04:02.934937      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:04:03.935120      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:04:04.270: INFO: File wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local from pod  dns-8809/dns-test-6b3dd3cb-bbd3-4e45-95e8-5a2f8ab57adf contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  4 08:04:04.275: INFO: File jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local from pod  dns-8809/dns-test-6b3dd3cb-bbd3-4e45-95e8-5a2f8ab57adf contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  4 08:04:04.275: INFO: Lookups using dns-8809/dns-test-6b3dd3cb-bbd3-4e45-95e8-5a2f8ab57adf failed for: [wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local]

  E0504 08:04:04.935888      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:04:05.936260      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:04:06.936737      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:04:07.937759      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:04:08.938592      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:04:09.269: INFO: File wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local from pod  dns-8809/dns-test-6b3dd3cb-bbd3-4e45-95e8-5a2f8ab57adf contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  4 08:04:09.272: INFO: File jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local from pod  dns-8809/dns-test-6b3dd3cb-bbd3-4e45-95e8-5a2f8ab57adf contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  4 08:04:09.272: INFO: Lookups using dns-8809/dns-test-6b3dd3cb-bbd3-4e45-95e8-5a2f8ab57adf failed for: [wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local]

  E0504 08:04:09.938809      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:04:10.939738      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:04:11.940106      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:04:12.940491      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:04:13.941055      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:04:14.269: INFO: File wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local from pod  dns-8809/dns-test-6b3dd3cb-bbd3-4e45-95e8-5a2f8ab57adf contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  4 08:04:14.273: INFO: File jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local from pod  dns-8809/dns-test-6b3dd3cb-bbd3-4e45-95e8-5a2f8ab57adf contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  4 08:04:14.273: INFO: Lookups using dns-8809/dns-test-6b3dd3cb-bbd3-4e45-95e8-5a2f8ab57adf failed for: [wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local]

  E0504 08:04:14.941249      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:04:15.941640      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:04:16.942128      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:04:17.942595      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:04:18.942883      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  4 08:04:19.272: INFO: DNS probes using dns-test-6b3dd3cb-bbd3-4e45-95e8-5a2f8ab57adf succeeded

  STEP: changing the service to type=ClusterIP @ 05/04/23 08:04:19.272
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8809.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local; sleep 1; done
   @ 05/04/23 08:04:19.317
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8809.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local; sleep 1; done
   @ 05/04/23 08:04:19.317
  STEP: creating a third pod to probe DNS @ 05/04/23 08:04:19.317
  STEP: submitting the pod to kubernetes @ 05/04/23 08:04:19.321
  E0504 08:04:19.943476      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:04:20.943654      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:04:21.943889      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0504 08:04:22.944189      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/04/23 08:04:23.429
  STEP: looking for the results for each expected name from probers @ 05/04/23 08:04:23.435
  May  4 08:04:23.445: INFO: DNS probes using dns-test-06c42895-a3d5-44e7-bcf2-fa54ca73f49d succeeded

  May  4 08:04:23.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/04/23 08:04:23.45
  STEP: deleting the pod @ 05/04/23 08:04:23.574
  STEP: deleting the pod @ 05/04/23 08:04:23.766
  STEP: deleting the test externalName service @ 05/04/23 08:04:23.853
  E0504 08:04:23.945176      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "dns-8809" for this suite. @ 05/04/23 08:04:24.088
• [38.239 seconds]
------------------------------
SSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  May  4 08:04:24.140: INFO: Running AfterSuite actions on node 1
  May  4 08:04:24.140: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.111 seconds]
------------------------------

Ran 378 of 7207 Specs in 6755.642 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h52m36.514320632s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

