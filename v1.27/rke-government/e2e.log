  I0428 16:37:22.648561      19 e2e.go:117] Starting e2e run "5e0cf72d-a540-4b84-a10a-53bd9bf0993c" on Ginkgo node 1
  Apr 28 16:37:22.684: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1682699842 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Apr 28 16:37:22.865: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 16:37:22.866: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Apr 28 16:37:22.915: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Apr 28 16:37:22.921: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'rke2-canal' (0 seconds elapsed)
  Apr 28 16:37:22.921: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'rke2-ingress-nginx-controller' (0 seconds elapsed)
  Apr 28 16:37:22.921: INFO: e2e test version: v1.27.1
  Apr 28 16:37:22.922: INFO: kube-apiserver version: v1.27.1+rke2r1
  Apr 28 16:37:22.922: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 16:37:22.927: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.063 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 04/28/23 16:37:23.226
  Apr 28 16:37:23.226: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 16:37:23.228
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:37:23.245
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:37:23.247
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 04/28/23 16:37:23.249
  STEP: Saw pod success @ 04/28/23 16:37:27.268
  Apr 28 16:37:27.271: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-4cb92091-f61e-4ecb-ad4c-7ed6674fa1f7 container test-container: <nil>
  STEP: delete the pod @ 04/28/23 16:37:27.284
  Apr 28 16:37:27.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2271" for this suite. @ 04/28/23 16:37:27.305
• [4.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 04/28/23 16:37:27.322
  Apr 28 16:37:27.322: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename podtemplate @ 04/28/23 16:37:27.322
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:37:27.349
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:37:27.352
  STEP: Create a pod template @ 04/28/23 16:37:27.355
  STEP: Replace a pod template @ 04/28/23 16:37:27.375
  Apr 28 16:37:27.397: INFO: Found updated podtemplate annotation: "true"

  Apr 28 16:37:27.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-6145" for this suite. @ 04/28/23 16:37:27.419
• [0.117 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 04/28/23 16:37:27.439
  Apr 28 16:37:27.439: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename services @ 04/28/23 16:37:27.439
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:37:27.468
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:37:27.47
  STEP: creating service in namespace services-2423 @ 04/28/23 16:37:27.472
  STEP: creating service affinity-clusterip in namespace services-2423 @ 04/28/23 16:37:27.472
  STEP: creating replication controller affinity-clusterip in namespace services-2423 @ 04/28/23 16:37:27.483
  I0428 16:37:27.491296      19 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-2423, replica count: 3
  I0428 16:37:30.542740      19 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 28 16:37:30.548: INFO: Creating new exec pod
  Apr 28 16:37:33.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-2423 exec execpod-affinitypbz2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Apr 28 16:37:33.775: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Apr 28 16:37:33.775: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 16:37:33.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-2423 exec execpod-affinitypbz2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.200.131 80'
  Apr 28 16:37:34.043: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.200.131 80\nConnection to 10.43.200.131 80 port [tcp/http] succeeded!\n"
  Apr 28 16:37:34.043: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 16:37:34.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-2423 exec execpod-affinitypbz2s -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.200.131:80/ ; done'
  Apr 28 16:37:34.432: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.200.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.200.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.200.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.200.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.200.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.200.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.200.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.200.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.200.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.200.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.200.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.200.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.200.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.200.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.200.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.200.131:80/\n"
  Apr 28 16:37:34.432: INFO: stdout: "\naffinity-clusterip-d8q2h\naffinity-clusterip-d8q2h\naffinity-clusterip-d8q2h\naffinity-clusterip-d8q2h\naffinity-clusterip-d8q2h\naffinity-clusterip-d8q2h\naffinity-clusterip-d8q2h\naffinity-clusterip-d8q2h\naffinity-clusterip-d8q2h\naffinity-clusterip-d8q2h\naffinity-clusterip-d8q2h\naffinity-clusterip-d8q2h\naffinity-clusterip-d8q2h\naffinity-clusterip-d8q2h\naffinity-clusterip-d8q2h\naffinity-clusterip-d8q2h"
  Apr 28 16:37:34.432: INFO: Received response from host: affinity-clusterip-d8q2h
  Apr 28 16:37:34.432: INFO: Received response from host: affinity-clusterip-d8q2h
  Apr 28 16:37:34.432: INFO: Received response from host: affinity-clusterip-d8q2h
  Apr 28 16:37:34.432: INFO: Received response from host: affinity-clusterip-d8q2h
  Apr 28 16:37:34.432: INFO: Received response from host: affinity-clusterip-d8q2h
  Apr 28 16:37:34.432: INFO: Received response from host: affinity-clusterip-d8q2h
  Apr 28 16:37:34.432: INFO: Received response from host: affinity-clusterip-d8q2h
  Apr 28 16:37:34.432: INFO: Received response from host: affinity-clusterip-d8q2h
  Apr 28 16:37:34.432: INFO: Received response from host: affinity-clusterip-d8q2h
  Apr 28 16:37:34.432: INFO: Received response from host: affinity-clusterip-d8q2h
  Apr 28 16:37:34.432: INFO: Received response from host: affinity-clusterip-d8q2h
  Apr 28 16:37:34.432: INFO: Received response from host: affinity-clusterip-d8q2h
  Apr 28 16:37:34.432: INFO: Received response from host: affinity-clusterip-d8q2h
  Apr 28 16:37:34.432: INFO: Received response from host: affinity-clusterip-d8q2h
  Apr 28 16:37:34.432: INFO: Received response from host: affinity-clusterip-d8q2h
  Apr 28 16:37:34.432: INFO: Received response from host: affinity-clusterip-d8q2h
  Apr 28 16:37:34.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 16:37:34.436: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-2423, will wait for the garbage collector to delete the pods @ 04/28/23 16:37:34.449
  Apr 28 16:37:34.531: INFO: Deleting ReplicationController affinity-clusterip took: 8.806763ms
  Apr 28 16:37:34.633: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.273812ms
  STEP: Destroying namespace "services-2423" for this suite. @ 04/28/23 16:37:37.059
• [9.627 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 04/28/23 16:37:37.066
  Apr 28 16:37:37.066: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename configmap @ 04/28/23 16:37:37.067
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:37:37.082
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:37:37.085
  STEP: Creating configMap with name configmap-test-upd-1c5af2ec-b968-4f1d-b63d-ba82ae91e8da @ 04/28/23 16:37:37.09
  STEP: Creating the pod @ 04/28/23 16:37:37.094
  STEP: Waiting for pod with text data @ 04/28/23 16:37:39.109
  STEP: Waiting for pod with binary data @ 04/28/23 16:37:39.114
  Apr 28 16:37:39.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4711" for this suite. @ 04/28/23 16:37:39.123
• [2.062 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 04/28/23 16:37:39.129
  Apr 28 16:37:39.129: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename statefulset @ 04/28/23 16:37:39.13
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:37:39.147
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:37:39.15
  STEP: Creating service test in namespace statefulset-3565 @ 04/28/23 16:37:39.152
  Apr 28 16:37:39.190: INFO: Found 0 stateful pods, waiting for 1
  Apr 28 16:37:49.194: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 04/28/23 16:37:49.199
  W0428 16:37:49.208009      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 28 16:37:49.213: INFO: Found 1 stateful pods, waiting for 2
  Apr 28 16:37:59.218: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 16:37:59.218: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 04/28/23 16:37:59.223
  STEP: Delete all of the StatefulSets @ 04/28/23 16:37:59.225
  STEP: Verify that StatefulSets have been deleted @ 04/28/23 16:37:59.231
  Apr 28 16:37:59.233: INFO: Deleting all statefulset in ns statefulset-3565
  Apr 28 16:37:59.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3565" for this suite. @ 04/28/23 16:37:59.243
• [20.126 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 04/28/23 16:37:59.284
  Apr 28 16:37:59.284: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/28/23 16:37:59.285
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:37:59.303
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:37:59.306
  Apr 28 16:37:59.310: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/28/23 16:38:00.864
  Apr 28 16:38:00.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-2811 --namespace=crd-publish-openapi-2811 create -f -'
  Apr 28 16:38:01.739: INFO: stderr: ""
  Apr 28 16:38:01.739: INFO: stdout: "e2e-test-crd-publish-openapi-7824-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Apr 28 16:38:01.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-2811 --namespace=crd-publish-openapi-2811 delete e2e-test-crd-publish-openapi-7824-crds test-cr'
  Apr 28 16:38:01.843: INFO: stderr: ""
  Apr 28 16:38:01.843: INFO: stdout: "e2e-test-crd-publish-openapi-7824-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Apr 28 16:38:01.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-2811 --namespace=crd-publish-openapi-2811 apply -f -'
  Apr 28 16:38:02.992: INFO: stderr: ""
  Apr 28 16:38:02.992: INFO: stdout: "e2e-test-crd-publish-openapi-7824-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Apr 28 16:38:02.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-2811 --namespace=crd-publish-openapi-2811 delete e2e-test-crd-publish-openapi-7824-crds test-cr'
  Apr 28 16:38:03.117: INFO: stderr: ""
  Apr 28 16:38:03.117: INFO: stdout: "e2e-test-crd-publish-openapi-7824-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 04/28/23 16:38:03.117
  Apr 28 16:38:03.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-2811 explain e2e-test-crd-publish-openapi-7824-crds'
  Apr 28 16:38:03.839: INFO: stderr: ""
  Apr 28 16:38:03.839: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-7824-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  Apr 28 16:38:05.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2811" for this suite. @ 04/28/23 16:38:05.468
• [6.189 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 04/28/23 16:38:05.477
  Apr 28 16:38:05.477: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 16:38:05.478
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:38:05.495
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:38:05.498
  STEP: Creating configMap with name configmap-projected-all-test-volume-7a5995d8-721b-4e63-8967-420b99872216 @ 04/28/23 16:38:05.5
  STEP: Creating secret with name secret-projected-all-test-volume-2dc892d3-0f8a-4848-a46f-bb6c819a4ef5 @ 04/28/23 16:38:05.504
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 04/28/23 16:38:05.508
  STEP: Saw pod success @ 04/28/23 16:38:09.532
  Apr 28 16:38:09.534: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod projected-volume-6daa11ff-1852-4bf8-837c-d21cd9e880f5 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 16:38:09.544
  Apr 28 16:38:09.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2210" for this suite. @ 04/28/23 16:38:09.559
• [4.088 seconds]
------------------------------
SS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 04/28/23 16:38:09.565
  Apr 28 16:38:09.565: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename endpointslice @ 04/28/23 16:38:09.566
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:38:09.583
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:38:09.596
  Apr 28 16:38:09.605: INFO: Endpoints addresses: [18.188.155.223 3.138.137.181 52.14.222.189] , ports: [6443]
  Apr 28 16:38:09.605: INFO: EndpointSlices addresses: [18.188.155.223 3.138.137.181 52.14.222.189] , ports: [6443]
  Apr 28 16:38:09.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-9607" for this suite. @ 04/28/23 16:38:09.608
• [0.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 04/28/23 16:38:09.616
  Apr 28 16:38:09.616: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename services @ 04/28/23 16:38:09.616
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:38:09.631
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:38:09.635
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-5601 @ 04/28/23 16:38:09.637
  STEP: changing the ExternalName service to type=NodePort @ 04/28/23 16:38:09.647
  STEP: creating replication controller externalname-service in namespace services-5601 @ 04/28/23 16:38:09.674
  I0428 16:38:09.696497      19 runners.go:194] Created replication controller with name: externalname-service, namespace: services-5601, replica count: 2
  I0428 16:38:12.748601      19 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 28 16:38:12.748: INFO: Creating new exec pod
  Apr 28 16:38:15.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-5601 exec execpodfv2xj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 28 16:38:15.938: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 28 16:38:15.938: INFO: stdout: "externalname-service-dd49c"
  Apr 28 16:38:15.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-5601 exec execpodfv2xj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.237.206 80'
  Apr 28 16:38:16.098: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.237.206 80\nConnection to 10.43.237.206 80 port [tcp/http] succeeded!\n"
  Apr 28 16:38:16.098: INFO: stdout: "externalname-service-mkd8h"
  Apr 28 16:38:16.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-5601 exec execpodfv2xj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.9.127 32716'
  Apr 28 16:38:16.262: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 172.31.9.127 32716\nConnection to 172.31.9.127 32716 port [tcp/*] succeeded!\n"
  Apr 28 16:38:16.262: INFO: stdout: ""
  Apr 28 16:38:17.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-5601 exec execpodfv2xj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.9.127 32716'
  Apr 28 16:38:17.418: INFO: stderr: "+ nc -v -t -w 2 172.31.9.127 32716\nConnection to 172.31.9.127 32716 port [tcp/*] succeeded!\n+ echo hostName\n"
  Apr 28 16:38:17.418: INFO: stdout: ""
  Apr 28 16:38:18.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-5601 exec execpodfv2xj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.9.127 32716'
  Apr 28 16:38:18.456: INFO: stderr: "+ nc -v -t -w 2 172.31.9.127 32716\n+ echo hostName\nConnection to 172.31.9.127 32716 port [tcp/*] succeeded!\n"
  Apr 28 16:38:18.456: INFO: stdout: "externalname-service-mkd8h"
  Apr 28 16:38:18.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-5601 exec execpodfv2xj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.11.161 32716'
  Apr 28 16:38:18.646: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.11.161 32716\nConnection to 172.31.11.161 32716 port [tcp/*] succeeded!\n"
  Apr 28 16:38:18.646: INFO: stdout: ""
  Apr 28 16:38:19.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-5601 exec execpodfv2xj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.11.161 32716'
  Apr 28 16:38:19.832: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 172.31.11.161 32716\nConnection to 172.31.11.161 32716 port [tcp/*] succeeded!\n"
  Apr 28 16:38:19.832: INFO: stdout: "externalname-service-dd49c"
  Apr 28 16:38:19.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 16:38:19.836: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-5601" for this suite. @ 04/28/23 16:38:19.898
• [10.287 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 04/28/23 16:38:19.903
  Apr 28 16:38:19.903: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename pods @ 04/28/23 16:38:19.904
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:38:19.929
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:38:19.932
  STEP: Create a pod @ 04/28/23 16:38:19.934
  STEP: patching /status @ 04/28/23 16:38:21.972
  Apr 28 16:38:21.979: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Apr 28 16:38:21.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-402" for this suite. @ 04/28/23 16:38:21.986
• [2.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 04/28/23 16:38:21.992
  Apr 28 16:38:21.992: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename disruption @ 04/28/23 16:38:21.993
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:38:22.01
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:38:22.013
  STEP: Waiting for the pdb to be processed @ 04/28/23 16:38:22.018
  STEP: Updating PodDisruptionBudget status @ 04/28/23 16:38:24.025
  STEP: Waiting for all pods to be running @ 04/28/23 16:38:24.037
  Apr 28 16:38:24.042: INFO: running pods: 0 < 1
  STEP: locating a running pod @ 04/28/23 16:38:26.045
  STEP: Waiting for the pdb to be processed @ 04/28/23 16:38:26.066
  STEP: Patching PodDisruptionBudget status @ 04/28/23 16:38:26.076
  STEP: Waiting for the pdb to be processed @ 04/28/23 16:38:26.085
  Apr 28 16:38:26.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9217" for this suite. @ 04/28/23 16:38:26.095
• [4.109 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 04/28/23 16:38:26.103
  Apr 28 16:38:26.103: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 16:38:26.104
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:38:26.168
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:38:26.171
  STEP: Creating a pod to test downward api env vars @ 04/28/23 16:38:26.173
  STEP: Saw pod success @ 04/28/23 16:38:30.199
  Apr 28 16:38:30.202: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod downward-api-f702c229-c7b4-4d7c-8482-51482ce18499 container dapi-container: <nil>
  STEP: delete the pod @ 04/28/23 16:38:30.208
  Apr 28 16:38:30.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9910" for this suite. @ 04/28/23 16:38:30.223
• [4.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 04/28/23 16:38:30.229
  Apr 28 16:38:30.229: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename kubelet-test @ 04/28/23 16:38:30.23
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:38:30.245
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:38:30.25
  Apr 28 16:38:32.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-2260" for this suite. @ 04/28/23 16:38:32.281
• [2.057 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 04/28/23 16:38:32.288
  Apr 28 16:38:32.288: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename namespaces @ 04/28/23 16:38:32.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:38:32.3
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:38:32.304
  STEP: creating a Namespace @ 04/28/23 16:38:32.306
  STEP: patching the Namespace @ 04/28/23 16:38:32.317
  STEP: get the Namespace and ensuring it has the label @ 04/28/23 16:38:32.323
  Apr 28 16:38:32.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3670" for this suite. @ 04/28/23 16:38:32.329
  STEP: Destroying namespace "nspatchtest-d16f2146-317e-4011-9f34-37fa697a213f-1432" for this suite. @ 04/28/23 16:38:32.334
• [0.053 seconds]
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 04/28/23 16:38:32.341
  Apr 28 16:38:32.341: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename subpath @ 04/28/23 16:38:32.342
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:38:32.355
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:38:32.358
  STEP: Setting up data @ 04/28/23 16:38:32.363
  STEP: Creating pod pod-subpath-test-secret-ctbd @ 04/28/23 16:38:32.371
  STEP: Creating a pod to test atomic-volume-subpath @ 04/28/23 16:38:32.371
  STEP: Saw pod success @ 04/28/23 16:38:56.429
  Apr 28 16:38:56.431: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-subpath-test-secret-ctbd container test-container-subpath-secret-ctbd: <nil>
  STEP: delete the pod @ 04/28/23 16:38:56.437
  STEP: Deleting pod pod-subpath-test-secret-ctbd @ 04/28/23 16:38:56.447
  Apr 28 16:38:56.447: INFO: Deleting pod "pod-subpath-test-secret-ctbd" in namespace "subpath-5642"
  Apr 28 16:38:56.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5642" for this suite. @ 04/28/23 16:38:56.455
• [24.120 seconds]
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 04/28/23 16:38:56.462
  Apr 28 16:38:56.462: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 16:38:56.463
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:38:56.475
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:38:56.478
  STEP: Creating configMap with name projected-configmap-test-volume-map-b0a87621-aadf-4b99-be89-f837adb9acde @ 04/28/23 16:38:56.48
  STEP: Creating a pod to test consume configMaps @ 04/28/23 16:38:56.484
  STEP: Saw pod success @ 04/28/23 16:39:00.504
  Apr 28 16:39:00.506: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-projected-configmaps-b58cea78-fd6b-438a-b4eb-2aee66f79357 container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 16:39:00.511
  Apr 28 16:39:00.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-241" for this suite. @ 04/28/23 16:39:00.526
• [4.068 seconds]
------------------------------
S
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 04/28/23 16:39:00.531
  Apr 28 16:39:00.531: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 16:39:00.531
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:39:00.553
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:39:00.559
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 16:39:00.562
  STEP: Saw pod success @ 04/28/23 16:39:04.58
  Apr 28 16:39:04.583: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod downwardapi-volume-11150e18-b196-428a-acbc-2ce338f87f4b container client-container: <nil>
  STEP: delete the pod @ 04/28/23 16:39:04.588
  Apr 28 16:39:04.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8589" for this suite. @ 04/28/23 16:39:04.606
• [4.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 04/28/23 16:39:04.614
  Apr 28 16:39:04.614: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename cronjob @ 04/28/23 16:39:04.615
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:39:04.631
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:39:04.633
  STEP: Creating a ReplaceConcurrent cronjob @ 04/28/23 16:39:04.636
  STEP: Ensuring a job is scheduled @ 04/28/23 16:39:04.642
  STEP: Ensuring exactly one is scheduled @ 04/28/23 16:40:00.646
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 04/28/23 16:40:00.648
  STEP: Ensuring the job is replaced with a new one @ 04/28/23 16:40:00.65
  STEP: Removing cronjob @ 04/28/23 16:41:00.654
  Apr 28 16:41:00.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8809" for this suite. @ 04/28/23 16:41:00.663
• [116.060 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 04/28/23 16:41:00.681
  Apr 28 16:41:00.681: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename events @ 04/28/23 16:41:00.682
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:41:00.699
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:41:00.708
  STEP: Create set of events @ 04/28/23 16:41:00.719
  Apr 28 16:41:00.724: INFO: created test-event-1
  Apr 28 16:41:00.728: INFO: created test-event-2
  Apr 28 16:41:00.732: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 04/28/23 16:41:00.732
  STEP: delete collection of events @ 04/28/23 16:41:00.735
  Apr 28 16:41:00.735: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 04/28/23 16:41:00.749
  Apr 28 16:41:00.749: INFO: requesting list of events to confirm quantity
  Apr 28 16:41:00.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-5428" for this suite. @ 04/28/23 16:41:00.758
• [0.081 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 04/28/23 16:41:00.763
  Apr 28 16:41:00.763: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 16:41:00.764
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:41:00.789
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:41:00.794
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 04/28/23 16:41:00.796
  STEP: Saw pod success @ 04/28/23 16:41:04.818
  Apr 28 16:41:04.823: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-49beffb6-5ac4-42c3-9b5b-45892fcab64e container test-container: <nil>
  STEP: delete the pod @ 04/28/23 16:41:04.846
  Apr 28 16:41:04.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2659" for this suite. @ 04/28/23 16:41:04.898
• [4.142 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 04/28/23 16:41:04.905
  Apr 28 16:41:04.905: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename kubelet-test @ 04/28/23 16:41:04.906
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:41:04.922
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:41:04.928
  Apr 28 16:41:08.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1891" for this suite. @ 04/28/23 16:41:08.959
• [4.058 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 04/28/23 16:41:08.964
  Apr 28 16:41:08.964: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename sched-pred @ 04/28/23 16:41:08.965
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:41:08.977
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:41:08.981
  Apr 28 16:41:08.984: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 28 16:41:08.990: INFO: Waiting for terminating namespaces to be deleted...
  Apr 28 16:41:08.992: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-11-161.us-east-2.compute.internal before test
  Apr 28 16:41:09.000: INFO: cloud-controller-manager-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:02 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.000: INFO: 	Container cloud-controller-manager ready: true, restart count 0
  Apr 28 16:41:09.000: INFO: etcd-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:46:40 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.000: INFO: 	Container etcd ready: true, restart count 0
  Apr 28 16:41:09.000: INFO: kube-apiserver-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:46:57 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.000: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 28 16:41:09.000: INFO: kube-controller-manager-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:02 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.000: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 28 16:41:09.000: INFO: kube-proxy-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:05 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.000: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 28 16:41:09.000: INFO: kube-scheduler-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:02 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.000: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 28 16:41:09.000: INFO: rke2-canal-8td74 from kube-system started at 2023-04-28 03:47:01 +0000 UTC (2 container statuses recorded)
  Apr 28 16:41:09.000: INFO: 	Container calico-node ready: true, restart count 0
  Apr 28 16:41:09.000: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 28 16:41:09.000: INFO: rke2-coredns-rke2-coredns-5896cccb79-vcgcz from kube-system started at 2023-04-28 03:47:28 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.000: INFO: 	Container coredns ready: true, restart count 0
  Apr 28 16:41:09.000: INFO: rke2-ingress-nginx-controller-6f4sz from kube-system started at 2023-04-28 03:47:28 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.000: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
  Apr 28 16:41:09.000: INFO: sonobuoy-systemd-logs-daemon-set-19d55eb1839b4b26-n6bzk from sonobuoy started at 2023-04-28 16:37:21 +0000 UTC (2 container statuses recorded)
  Apr 28 16:41:09.000: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 16:41:09.000: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 16:41:09.000: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-14-195.us-east-2.compute.internal before test
  Apr 28 16:41:09.010: INFO: cloud-controller-manager-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:44:16 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.010: INFO: 	Container cloud-controller-manager ready: true, restart count 0
  Apr 28 16:41:09.010: INFO: etcd-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:43:55 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.010: INFO: 	Container etcd ready: true, restart count 0
  Apr 28 16:41:09.010: INFO: helm-install-rke2-canal-8zdl7 from kube-system started at 2023-04-28 03:44:29 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.010: INFO: 	Container helm ready: false, restart count 0
  Apr 28 16:41:09.010: INFO: helm-install-rke2-coredns-8tn5j from kube-system started at 2023-04-28 03:44:29 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.010: INFO: 	Container helm ready: false, restart count 1
  Apr 28 16:41:09.010: INFO: helm-install-rke2-ingress-nginx-v5xks from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.010: INFO: 	Container helm ready: false, restart count 0
  Apr 28 16:41:09.010: INFO: helm-install-rke2-metrics-server-66776 from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.010: INFO: 	Container helm ready: false, restart count 0
  Apr 28 16:41:09.010: INFO: helm-install-rke2-snapshot-controller-crd-xc2xl from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.010: INFO: 	Container helm ready: false, restart count 0
  Apr 28 16:41:09.010: INFO: helm-install-rke2-snapshot-controller-g6gdp from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.010: INFO: 	Container helm ready: false, restart count 0
  Apr 28 16:41:09.010: INFO: helm-install-rke2-snapshot-validation-webhook-6rwkq from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.010: INFO: 	Container helm ready: false, restart count 0
  Apr 28 16:41:09.010: INFO: kube-apiserver-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:44:09 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.010: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 28 16:41:09.010: INFO: kube-controller-manager-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:44:14 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.010: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 28 16:41:09.010: INFO: kube-proxy-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:44:19 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.010: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 28 16:41:09.010: INFO: kube-scheduler-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:44:14 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.010: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 28 16:41:09.010: INFO: rke2-canal-sz474 from kube-system started at 2023-04-28 03:44:35 +0000 UTC (2 container statuses recorded)
  Apr 28 16:41:09.011: INFO: 	Container calico-node ready: true, restart count 0
  Apr 28 16:41:09.011: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 28 16:41:09.011: INFO: rke2-coredns-rke2-coredns-5896cccb79-lb8dr from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.011: INFO: 	Container coredns ready: true, restart count 0
  Apr 28 16:41:09.011: INFO: rke2-coredns-rke2-coredns-autoscaler-f6766cdc9-qrz4d from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.011: INFO: 	Container autoscaler ready: true, restart count 0
  Apr 28 16:41:09.011: INFO: rke2-ingress-nginx-controller-l9r4l from kube-system started at 2023-04-28 03:45:10 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.011: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
  Apr 28 16:41:09.011: INFO: rke2-metrics-server-6d45f6cb4d-587zc from kube-system started at 2023-04-28 03:45:06 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.011: INFO: 	Container metrics-server ready: true, restart count 0
  Apr 28 16:41:09.011: INFO: rke2-snapshot-controller-7bf6d7bf5f-swpkx from kube-system started at 2023-04-28 03:45:23 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.011: INFO: 	Container rke2-snapshot-controller ready: true, restart count 0
  Apr 28 16:41:09.011: INFO: rke2-snapshot-validation-webhook-b65d46c9f-8jsvg from kube-system started at 2023-04-28 03:45:24 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.011: INFO: 	Container rke2-snapshot-validation-webhook ready: true, restart count 0
  Apr 28 16:41:09.011: INFO: sonobuoy-systemd-logs-daemon-set-19d55eb1839b4b26-bxxcq from sonobuoy started at 2023-04-28 16:37:21 +0000 UTC (2 container statuses recorded)
  Apr 28 16:41:09.011: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 16:41:09.011: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 16:41:09.011: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-6-71.us-east-2.compute.internal before test
  Apr 28 16:41:09.020: INFO: cloud-controller-manager-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:18 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.020: INFO: 	Container cloud-controller-manager ready: true, restart count 0
  Apr 28 16:41:09.020: INFO: etcd-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:01 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.020: INFO: 	Container etcd ready: true, restart count 0
  Apr 28 16:41:09.020: INFO: kube-apiserver-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:13 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.020: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 28 16:41:09.020: INFO: kube-controller-manager-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:18 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.020: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 28 16:41:09.020: INFO: kube-proxy-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:20 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.020: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 28 16:41:09.020: INFO: kube-scheduler-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:18 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.020: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 28 16:41:09.020: INFO: rke2-canal-8rz4z from kube-system started at 2023-04-28 03:47:20 +0000 UTC (2 container statuses recorded)
  Apr 28 16:41:09.020: INFO: 	Container calico-node ready: true, restart count 0
  Apr 28 16:41:09.020: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 28 16:41:09.020: INFO: rke2-ingress-nginx-controller-xfwz8 from kube-system started at 2023-04-28 03:47:42 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.021: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
  Apr 28 16:41:09.021: INFO: sonobuoy-systemd-logs-daemon-set-19d55eb1839b4b26-98t5f from sonobuoy started at 2023-04-28 16:37:21 +0000 UTC (2 container statuses recorded)
  Apr 28 16:41:09.021: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 16:41:09.021: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 16:41:09.021: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-9-127.us-east-2.compute.internal before test
  Apr 28 16:41:09.028: INFO: replace-28045000-5bk6s from cronjob-8809 started at 2023-04-28 16:40:00 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.029: INFO: 	Container c ready: true, restart count 0
  Apr 28 16:41:09.029: INFO: replace-28045001-lcfh9 from cronjob-8809 started at 2023-04-28 16:41:00 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.029: INFO: 	Container c ready: true, restart count 0
  Apr 28 16:41:09.029: INFO: kube-proxy-ip-172-31-9-127.us-east-2.compute.internal from kube-system started at 2023-04-28 03:48:58 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.029: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 28 16:41:09.029: INFO: rke2-canal-lc6hn from kube-system started at 2023-04-28 03:48:58 +0000 UTC (2 container statuses recorded)
  Apr 28 16:41:09.029: INFO: 	Container calico-node ready: true, restart count 0
  Apr 28 16:41:09.029: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 28 16:41:09.029: INFO: rke2-ingress-nginx-controller-tgbkj from kube-system started at 2023-04-28 05:38:51 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.029: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
  Apr 28 16:41:09.029: INFO: bin-false96c9130b-338e-4281-89f4-30458c308465 from kubelet-test-1891 started at 2023-04-28 16:41:04 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.029: INFO: 	Container bin-false96c9130b-338e-4281-89f4-30458c308465 ready: false, restart count 0
  Apr 28 16:41:09.029: INFO: sonobuoy from sonobuoy started at 2023-04-28 16:37:20 +0000 UTC (1 container statuses recorded)
  Apr 28 16:41:09.029: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 28 16:41:09.029: INFO: sonobuoy-e2e-job-92a64e7703e04ef2 from sonobuoy started at 2023-04-28 16:37:21 +0000 UTC (2 container statuses recorded)
  Apr 28 16:41:09.029: INFO: 	Container e2e ready: true, restart count 0
  Apr 28 16:41:09.029: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 16:41:09.029: INFO: sonobuoy-systemd-logs-daemon-set-19d55eb1839b4b26-jv86q from sonobuoy started at 2023-04-28 16:37:21 +0000 UTC (2 container statuses recorded)
  Apr 28 16:41:09.029: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 16:41:09.029: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/28/23 16:41:09.029
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/28/23 16:41:11.046
  STEP: Trying to apply a random label on the found node. @ 04/28/23 16:41:11.056
  STEP: verifying the node has the label kubernetes.io/e2e-82211277-fd81-4480-81ef-2deea6633a73 42 @ 04/28/23 16:41:11.084
  STEP: Trying to relaunch the pod, now with labels. @ 04/28/23 16:41:11.09
  STEP: removing the label kubernetes.io/e2e-82211277-fd81-4480-81ef-2deea6633a73 off the node ip-172-31-9-127.us-east-2.compute.internal @ 04/28/23 16:41:13.123
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-82211277-fd81-4480-81ef-2deea6633a73 @ 04/28/23 16:41:13.136
  Apr 28 16:41:13.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-47" for this suite. @ 04/28/23 16:41:13.155
• [4.198 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 04/28/23 16:41:13.167
  Apr 28 16:41:13.167: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 16:41:13.169
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:41:13.183
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:41:13.187
  STEP: creating a replication controller @ 04/28/23 16:41:13.19
  Apr 28 16:41:13.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 create -f -'
  Apr 28 16:41:13.970: INFO: stderr: ""
  Apr 28 16:41:13.970: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/28/23 16:41:13.97
  Apr 28 16:41:13.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 28 16:41:14.048: INFO: stderr: ""
  Apr 28 16:41:14.048: INFO: stdout: "update-demo-nautilus-hjq62 update-demo-nautilus-rbh2p "
  Apr 28 16:41:14.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 get pods update-demo-nautilus-hjq62 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 28 16:41:14.118: INFO: stderr: ""
  Apr 28 16:41:14.118: INFO: stdout: ""
  Apr 28 16:41:14.118: INFO: update-demo-nautilus-hjq62 is created but not running
  Apr 28 16:41:19.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 28 16:41:19.203: INFO: stderr: ""
  Apr 28 16:41:19.203: INFO: stdout: "update-demo-nautilus-hjq62 update-demo-nautilus-rbh2p "
  Apr 28 16:41:19.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 get pods update-demo-nautilus-hjq62 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 28 16:41:19.358: INFO: stderr: ""
  Apr 28 16:41:19.358: INFO: stdout: "true"
  Apr 28 16:41:19.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 get pods update-demo-nautilus-hjq62 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 28 16:41:19.422: INFO: stderr: ""
  Apr 28 16:41:19.422: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 28 16:41:19.422: INFO: validating pod update-demo-nautilus-hjq62
  Apr 28 16:41:19.429: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 28 16:41:19.429: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 28 16:41:19.429: INFO: update-demo-nautilus-hjq62 is verified up and running
  Apr 28 16:41:19.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 get pods update-demo-nautilus-rbh2p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 28 16:41:19.494: INFO: stderr: ""
  Apr 28 16:41:19.494: INFO: stdout: "true"
  Apr 28 16:41:19.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 get pods update-demo-nautilus-rbh2p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 28 16:41:19.552: INFO: stderr: ""
  Apr 28 16:41:19.552: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 28 16:41:19.552: INFO: validating pod update-demo-nautilus-rbh2p
  Apr 28 16:41:19.558: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 28 16:41:19.558: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 28 16:41:19.558: INFO: update-demo-nautilus-rbh2p is verified up and running
  STEP: scaling down the replication controller @ 04/28/23 16:41:19.558
  Apr 28 16:41:19.560: INFO: scanned /root for discovery docs: <nil>
  Apr 28 16:41:19.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  Apr 28 16:41:20.660: INFO: stderr: ""
  Apr 28 16:41:20.660: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/28/23 16:41:20.66
  Apr 28 16:41:20.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 28 16:41:20.724: INFO: stderr: ""
  Apr 28 16:41:20.724: INFO: stdout: "update-demo-nautilus-rbh2p "
  Apr 28 16:41:20.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 get pods update-demo-nautilus-rbh2p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 28 16:41:20.783: INFO: stderr: ""
  Apr 28 16:41:20.783: INFO: stdout: "true"
  Apr 28 16:41:20.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 get pods update-demo-nautilus-rbh2p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 28 16:41:20.900: INFO: stderr: ""
  Apr 28 16:41:20.900: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 28 16:41:20.900: INFO: validating pod update-demo-nautilus-rbh2p
  Apr 28 16:41:20.905: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 28 16:41:20.905: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 28 16:41:20.905: INFO: update-demo-nautilus-rbh2p is verified up and running
  STEP: scaling up the replication controller @ 04/28/23 16:41:20.905
  Apr 28 16:41:20.907: INFO: scanned /root for discovery docs: <nil>
  Apr 28 16:41:20.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  Apr 28 16:41:22.015: INFO: stderr: ""
  Apr 28 16:41:22.015: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/28/23 16:41:22.015
  Apr 28 16:41:22.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 28 16:41:22.141: INFO: stderr: ""
  Apr 28 16:41:22.141: INFO: stdout: "update-demo-nautilus-rbh2p update-demo-nautilus-wbv6w "
  Apr 28 16:41:22.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 get pods update-demo-nautilus-rbh2p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 28 16:41:22.209: INFO: stderr: ""
  Apr 28 16:41:22.209: INFO: stdout: "true"
  Apr 28 16:41:22.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 get pods update-demo-nautilus-rbh2p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 28 16:41:22.286: INFO: stderr: ""
  Apr 28 16:41:22.286: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 28 16:41:22.286: INFO: validating pod update-demo-nautilus-rbh2p
  Apr 28 16:41:22.290: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 28 16:41:22.290: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 28 16:41:22.290: INFO: update-demo-nautilus-rbh2p is verified up and running
  Apr 28 16:41:22.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 get pods update-demo-nautilus-wbv6w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 28 16:41:22.365: INFO: stderr: ""
  Apr 28 16:41:22.365: INFO: stdout: ""
  Apr 28 16:41:22.365: INFO: update-demo-nautilus-wbv6w is created but not running
  Apr 28 16:41:27.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 28 16:41:27.450: INFO: stderr: ""
  Apr 28 16:41:27.450: INFO: stdout: "update-demo-nautilus-rbh2p update-demo-nautilus-wbv6w "
  Apr 28 16:41:27.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 get pods update-demo-nautilus-rbh2p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 28 16:41:27.519: INFO: stderr: ""
  Apr 28 16:41:27.519: INFO: stdout: "true"
  Apr 28 16:41:27.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 get pods update-demo-nautilus-rbh2p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 28 16:41:27.618: INFO: stderr: ""
  Apr 28 16:41:27.618: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 28 16:41:27.618: INFO: validating pod update-demo-nautilus-rbh2p
  Apr 28 16:41:27.621: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 28 16:41:27.622: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 28 16:41:27.622: INFO: update-demo-nautilus-rbh2p is verified up and running
  Apr 28 16:41:27.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 get pods update-demo-nautilus-wbv6w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 28 16:41:27.757: INFO: stderr: ""
  Apr 28 16:41:27.757: INFO: stdout: "true"
  Apr 28 16:41:27.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 get pods update-demo-nautilus-wbv6w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 28 16:41:27.826: INFO: stderr: ""
  Apr 28 16:41:27.826: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 28 16:41:27.826: INFO: validating pod update-demo-nautilus-wbv6w
  Apr 28 16:41:27.832: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 28 16:41:27.832: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 28 16:41:27.833: INFO: update-demo-nautilus-wbv6w is verified up and running
  STEP: using delete to clean up resources @ 04/28/23 16:41:27.833
  Apr 28 16:41:27.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 delete --grace-period=0 --force -f -'
  Apr 28 16:41:27.912: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 28 16:41:27.912: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Apr 28 16:41:27.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 get rc,svc -l name=update-demo --no-headers'
  Apr 28 16:41:28.089: INFO: stderr: "No resources found in kubectl-564 namespace.\n"
  Apr 28 16:41:28.089: INFO: stdout: ""
  Apr 28 16:41:28.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-564 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 28 16:41:28.222: INFO: stderr: ""
  Apr 28 16:41:28.222: INFO: stdout: ""
  Apr 28 16:41:28.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-564" for this suite. @ 04/28/23 16:41:28.23
• [15.068 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 04/28/23 16:41:28.236
  Apr 28 16:41:28.236: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename replication-controller @ 04/28/23 16:41:28.237
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:41:28.248
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:41:28.256
  STEP: Given a ReplicationController is created @ 04/28/23 16:41:28.258
  STEP: When the matched label of one of its pods change @ 04/28/23 16:41:28.262
  Apr 28 16:41:28.265: INFO: Pod name pod-release: Found 0 pods out of 1
  Apr 28 16:41:33.269: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 04/28/23 16:41:33.283
  Apr 28 16:41:33.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9765" for this suite. @ 04/28/23 16:41:33.307
• [5.081 seconds]
------------------------------
SS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 04/28/23 16:41:33.317
  Apr 28 16:41:33.317: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename configmap @ 04/28/23 16:41:33.318
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:41:33.342
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:41:33.345
  STEP: Creating configMap with name cm-test-opt-del-f52a7d07-27dc-46da-a1f0-515823c0cac4 @ 04/28/23 16:41:33.351
  STEP: Creating configMap with name cm-test-opt-upd-88da3b1e-a6d7-4cef-97a4-7871294a8494 @ 04/28/23 16:41:33.357
  STEP: Creating the pod @ 04/28/23 16:41:33.363
  STEP: Deleting configmap cm-test-opt-del-f52a7d07-27dc-46da-a1f0-515823c0cac4 @ 04/28/23 16:41:35.406
  STEP: Updating configmap cm-test-opt-upd-88da3b1e-a6d7-4cef-97a4-7871294a8494 @ 04/28/23 16:41:35.411
  STEP: Creating configMap with name cm-test-opt-create-e3742899-34d3-4535-93b2-90200090a578 @ 04/28/23 16:41:35.416
  STEP: waiting to observe update in volume @ 04/28/23 16:41:35.42
  Apr 28 16:41:37.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3781" for this suite. @ 04/28/23 16:41:37.456
• [4.145 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 04/28/23 16:41:37.464
  Apr 28 16:41:37.464: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename job @ 04/28/23 16:41:37.467
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:41:37.481
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:41:37.485
  STEP: Creating a job @ 04/28/23 16:41:37.489
  STEP: Ensuring active pods == parallelism @ 04/28/23 16:41:37.494
  STEP: delete a job @ 04/28/23 16:41:41.498
  STEP: deleting Job.batch foo in namespace job-2229, will wait for the garbage collector to delete the pods @ 04/28/23 16:41:41.498
  Apr 28 16:41:41.556: INFO: Deleting Job.batch foo took: 5.686546ms
  Apr 28 16:41:41.657: INFO: Terminating Job.batch foo pods took: 100.959583ms
  STEP: Ensuring job was deleted @ 04/28/23 16:42:12.858
  Apr 28 16:42:12.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2229" for this suite. @ 04/28/23 16:42:12.865
• [35.406 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 04/28/23 16:42:12.87
  Apr 28 16:42:12.870: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 16:42:12.871
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:42:12.883
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:42:12.888
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 16:42:12.89
  STEP: Saw pod success @ 04/28/23 16:42:16.916
  Apr 28 16:42:16.919: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod downwardapi-volume-2c4b4023-b0b7-4b9c-b4e8-fb022276d22e container client-container: <nil>
  STEP: delete the pod @ 04/28/23 16:42:16.928
  Apr 28 16:42:16.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2680" for this suite. @ 04/28/23 16:42:16.946
• [4.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 04/28/23 16:42:16.954
  Apr 28 16:42:16.954: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename replication-controller @ 04/28/23 16:42:16.955
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:42:16.975
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:42:16.979
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 04/28/23 16:42:16.981
  STEP: When a replication controller with a matching selector is created @ 04/28/23 16:42:19.002
  STEP: Then the orphan pod is adopted @ 04/28/23 16:42:19.006
  Apr 28 16:42:20.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7969" for this suite. @ 04/28/23 16:42:20.02
• [3.078 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 04/28/23 16:42:20.033
  Apr 28 16:42:20.033: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename deployment @ 04/28/23 16:42:20.034
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:42:20.06
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:42:20.064
  Apr 28 16:42:20.068: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Apr 28 16:42:20.075: INFO: Pod name sample-pod: Found 0 pods out of 1
  Apr 28 16:42:25.079: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/28/23 16:42:25.079
  Apr 28 16:42:25.079: INFO: Creating deployment "test-rolling-update-deployment"
  Apr 28 16:42:25.084: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Apr 28 16:42:25.090: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  Apr 28 16:42:27.098: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Apr 28 16:42:27.105: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Apr 28 16:42:27.118: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3933  070ccb63-b5ce-499a-83c6-2b849bc78049 252872 1 2023-04-28 16:42:25 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-04-28 16:42:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-28 16:42:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000dcc828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-28 16:42:25 +0000 UTC,LastTransitionTime:2023-04-28 16:42:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-04-28 16:42:26 +0000 UTC,LastTransitionTime:2023-04-28 16:42:25 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 28 16:42:27.121: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-3933  4a5d85e7-4275-4efa-9de9-d0d217b7bc40 252862 1 2023-04-28 16:42:25 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 070ccb63-b5ce-499a-83c6-2b849bc78049 0xc004cb99d7 0xc004cb99d8}] [] [{kube-controller-manager Update apps/v1 2023-04-28 16:42:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"070ccb63-b5ce-499a-83c6-2b849bc78049\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-28 16:42:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004cb9a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 16:42:27.121: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Apr 28 16:42:27.121: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3933  b13b290d-5c96-474b-9d64-63d782f79ed1 252871 2 2023-04-28 16:42:20 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 070ccb63-b5ce-499a-83c6-2b849bc78049 0xc004cb9897 0xc004cb9898}] [] [{e2e.test Update apps/v1 2023-04-28 16:42:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-28 16:42:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"070ccb63-b5ce-499a-83c6-2b849bc78049\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-28 16:42:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004cb9968 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 16:42:27.124: INFO: Pod "test-rolling-update-deployment-656d657cd8-746rz" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-746rz test-rolling-update-deployment-656d657cd8- deployment-3933  c3e0a6af-87c9-4f9d-b37e-f33ad56a9681 252861 0 2023-04-28 16:42:25 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[cni.projectcalico.org/containerID:7551be85a790ddb954a0f3a0c63143f213eb440e3bc07f9f1a1582c8c30efd13 cni.projectcalico.org/podIP:10.42.3.205/32 cni.projectcalico.org/podIPs:10.42.3.205/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 4a5d85e7-4275-4efa-9de9-d0d217b7bc40 0xc000dccc47 0xc000dccc48}] [] [{calico Update v1 2023-04-28 16:42:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-28 16:42:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a5d85e7-4275-4efa-9de9-d0d217b7bc40\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-28 16:42:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.205\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q7fzd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q7fzd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-127.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 16:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 16:42:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 16:42:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 16:42:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.127,PodIP:10.42.3.205,StartTime:2023-04-28 16:42:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 16:42:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://73249794b9307e1150729e7657586e4d2aaa1f44009788f9511425e48027cacc,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.205,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 16:42:27.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3933" for this suite. @ 04/28/23 16:42:27.128
• [7.102 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 04/28/23 16:42:27.137
  Apr 28 16:42:27.137: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename endpointslice @ 04/28/23 16:42:27.138
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:42:27.156
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:42:27.161
  STEP: referencing a single matching pod @ 04/28/23 16:42:32.313
  STEP: referencing matching pods with named port @ 04/28/23 16:42:37.32
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 04/28/23 16:42:42.326
  STEP: recreating EndpointSlices after they've been deleted @ 04/28/23 16:42:47.337
  Apr 28 16:42:47.363: INFO: EndpointSlice for Service endpointslice-4546/example-named-port not found
  Apr 28 16:42:57.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-4546" for this suite. @ 04/28/23 16:42:57.386
• [30.260 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 04/28/23 16:42:57.399
  Apr 28 16:42:57.399: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename sched-preemption @ 04/28/23 16:42:57.4
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:42:57.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:42:57.418
  Apr 28 16:42:57.434: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 28 16:43:57.478: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 04/28/23 16:43:57.48
  Apr 28 16:43:57.499: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Apr 28 16:43:57.509: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Apr 28 16:43:57.528: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Apr 28 16:43:57.551: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Apr 28 16:43:57.622: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Apr 28 16:43:57.637: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  Apr 28 16:43:57.670: INFO: Created pod: pod3-0-sched-preemption-medium-priority
  Apr 28 16:43:57.680: INFO: Created pod: pod3-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 04/28/23 16:43:57.68
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 04/28/23 16:44:01.735
  Apr 28 16:44:05.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-4355" for this suite. @ 04/28/23 16:44:05.881
• [68.490 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 04/28/23 16:44:05.89
  Apr 28 16:44:05.890: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 16:44:05.891
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:44:05.906
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:44:05.909
  STEP: Creating projection with secret that has name projected-secret-test-map-4df17dc1-5336-4548-9a36-fec55161473b @ 04/28/23 16:44:05.911
  STEP: Creating a pod to test consume secrets @ 04/28/23 16:44:05.915
  STEP: Saw pod success @ 04/28/23 16:44:09.942
  Apr 28 16:44:09.958: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-projected-secrets-abbe4c3c-c32b-4bf3-9b3c-d6868bc61af9 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 16:44:09.989
  Apr 28 16:44:10.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3929" for this suite. @ 04/28/23 16:44:10.022
• [4.147 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 04/28/23 16:44:10.038
  Apr 28 16:44:10.038: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 16:44:10.039
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:44:10.076
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:44:10.089
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 04/28/23 16:44:10.099
  STEP: Saw pod success @ 04/28/23 16:44:14.127
  Apr 28 16:44:14.130: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-2944552e-8375-4393-896c-8787b825a014 container test-container: <nil>
  STEP: delete the pod @ 04/28/23 16:44:14.136
  Apr 28 16:44:14.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1480" for this suite. @ 04/28/23 16:44:14.154
• [4.132 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 04/28/23 16:44:14.187
  Apr 28 16:44:14.187: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 16:44:14.188
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:44:14.209
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:44:14.212
  STEP: Counting existing ResourceQuota @ 04/28/23 16:44:14.218
  STEP: Creating a ResourceQuota @ 04/28/23 16:44:19.222
  STEP: Ensuring resource quota status is calculated @ 04/28/23 16:44:19.227
  STEP: Creating a ReplicaSet @ 04/28/23 16:44:21.231
  STEP: Ensuring resource quota status captures replicaset creation @ 04/28/23 16:44:21.242
  STEP: Deleting a ReplicaSet @ 04/28/23 16:44:23.246
  STEP: Ensuring resource quota status released usage @ 04/28/23 16:44:23.25
  Apr 28 16:44:25.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-396" for this suite. @ 04/28/23 16:44:25.257
• [11.074 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 04/28/23 16:44:25.262
  Apr 28 16:44:25.262: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename gc @ 04/28/23 16:44:25.263
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:44:25.279
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:44:25.285
  Apr 28 16:44:25.326: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"cf42ed79-0ba4-4934-a132-bab5ddd0f1b0", Controller:(*bool)(0xc004191012), BlockOwnerDeletion:(*bool)(0xc004191013)}}
  Apr 28 16:44:25.336: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"1ca1f2d6-5619-4927-9d0c-be4afb98f0d5", Controller:(*bool)(0xc004191292), BlockOwnerDeletion:(*bool)(0xc004191293)}}
  Apr 28 16:44:25.342: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"3fcb7d0e-58f5-4917-93c9-eef8940fb729", Controller:(*bool)(0xc00419151a), BlockOwnerDeletion:(*bool)(0xc00419151b)}}
  Apr 28 16:44:30.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8520" for this suite. @ 04/28/23 16:44:30.359
• [5.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 04/28/23 16:44:30.367
  Apr 28 16:44:30.367: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename dns @ 04/28/23 16:44:30.367
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:44:30.384
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:44:30.387
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 04/28/23 16:44:30.39
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 04/28/23 16:44:30.39
  STEP: creating a pod to probe DNS @ 04/28/23 16:44:30.39
  STEP: submitting the pod to kubernetes @ 04/28/23 16:44:30.39
  STEP: retrieving the pod @ 04/28/23 16:44:32.408
  STEP: looking for the results for each expected name from probers @ 04/28/23 16:44:32.41
  Apr 28 16:44:32.426: INFO: DNS probes using dns-8431/dns-test-0b4cdb7c-6f73-4055-ad53-6bbe65699b0b succeeded

  Apr 28 16:44:32.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 16:44:32.43
  STEP: Destroying namespace "dns-8431" for this suite. @ 04/28/23 16:44:32.442
• [2.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 04/28/23 16:44:32.456
  Apr 28 16:44:32.456: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 16:44:32.457
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:44:32.475
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:44:32.477
  STEP: Creating a pod to test emptydir volume type on node default medium @ 04/28/23 16:44:32.479
  STEP: Saw pod success @ 04/28/23 16:44:36.497
  Apr 28 16:44:36.506: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-69979473-29bc-445e-845c-84acf9b3b075 container test-container: <nil>
  STEP: delete the pod @ 04/28/23 16:44:36.512
  Apr 28 16:44:36.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3540" for this suite. @ 04/28/23 16:44:36.528
• [4.077 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 04/28/23 16:44:36.534
  Apr 28 16:44:36.534: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename proxy @ 04/28/23 16:44:36.535
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:44:36.56
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:44:36.565
  Apr 28 16:44:36.569: INFO: Creating pod...
  Apr 28 16:44:38.583: INFO: Creating service...
  Apr 28 16:44:38.590: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2888/pods/agnhost/proxy/some/path/with/DELETE
  Apr 28 16:44:38.607: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 28 16:44:38.607: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2888/pods/agnhost/proxy/some/path/with/GET
  Apr 28 16:44:38.611: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Apr 28 16:44:38.611: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2888/pods/agnhost/proxy/some/path/with/HEAD
  Apr 28 16:44:38.615: INFO: http.Client request:HEAD | StatusCode:200
  Apr 28 16:44:38.615: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2888/pods/agnhost/proxy/some/path/with/OPTIONS
  Apr 28 16:44:38.617: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 28 16:44:38.617: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2888/pods/agnhost/proxy/some/path/with/PATCH
  Apr 28 16:44:38.620: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 28 16:44:38.620: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2888/pods/agnhost/proxy/some/path/with/POST
  Apr 28 16:44:38.623: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 28 16:44:38.623: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2888/pods/agnhost/proxy/some/path/with/PUT
  Apr 28 16:44:38.625: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 28 16:44:38.626: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2888/services/test-service/proxy/some/path/with/DELETE
  Apr 28 16:44:38.629: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 28 16:44:38.629: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2888/services/test-service/proxy/some/path/with/GET
  Apr 28 16:44:38.633: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Apr 28 16:44:38.633: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2888/services/test-service/proxy/some/path/with/HEAD
  Apr 28 16:44:38.636: INFO: http.Client request:HEAD | StatusCode:200
  Apr 28 16:44:38.636: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2888/services/test-service/proxy/some/path/with/OPTIONS
  Apr 28 16:44:38.640: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 28 16:44:38.640: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2888/services/test-service/proxy/some/path/with/PATCH
  Apr 28 16:44:38.643: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 28 16:44:38.643: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2888/services/test-service/proxy/some/path/with/POST
  Apr 28 16:44:38.646: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 28 16:44:38.646: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2888/services/test-service/proxy/some/path/with/PUT
  Apr 28 16:44:38.650: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 28 16:44:38.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-2888" for this suite. @ 04/28/23 16:44:38.654
• [2.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 04/28/23 16:44:38.661
  Apr 28 16:44:38.661: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 16:44:38.662
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:44:38.675
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:44:38.678
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 16:44:38.68
  STEP: Saw pod success @ 04/28/23 16:44:42.699
  Apr 28 16:44:42.701: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod downwardapi-volume-a2038392-8fd5-4149-8958-f08958008c47 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 16:44:42.706
  Apr 28 16:44:42.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-446" for this suite. @ 04/28/23 16:44:42.72
• [4.063 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 04/28/23 16:44:42.724
  Apr 28 16:44:42.724: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 16:44:42.725
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:44:42.74
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:44:42.742
  STEP: Starting the proxy @ 04/28/23 16:44:42.744
  Apr 28 16:44:42.745: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-5567 proxy --unix-socket=/tmp/kubectl-proxy-unix3562158207/test'
  STEP: retrieving proxy /api/ output @ 04/28/23 16:44:42.789
  Apr 28 16:44:42.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5567" for this suite. @ 04/28/23 16:44:42.795
• [0.075 seconds]
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 04/28/23 16:44:42.799
  Apr 28 16:44:42.799: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 16:44:42.8
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:44:42.816
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:44:42.82
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 16:44:42.822
  STEP: Saw pod success @ 04/28/23 16:44:46.849
  Apr 28 16:44:46.851: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod downwardapi-volume-93abd52a-ef25-4a86-9a02-7c8d107f3115 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 16:44:46.857
  Apr 28 16:44:46.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9568" for this suite. @ 04/28/23 16:44:46.883
• [4.090 seconds]
------------------------------
SSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 04/28/23 16:44:46.89
  Apr 28 16:44:46.890: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename limitrange @ 04/28/23 16:44:46.891
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:44:46.914
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:44:46.918
  STEP: Creating LimitRange "e2e-limitrange-jz5k4" in namespace "limitrange-8547" @ 04/28/23 16:44:46.92
  STEP: Creating another limitRange in another namespace @ 04/28/23 16:44:46.924
  Apr 28 16:44:46.938: INFO: Namespace "e2e-limitrange-jz5k4-5355" created
  Apr 28 16:44:46.938: INFO: Creating LimitRange "e2e-limitrange-jz5k4" in namespace "e2e-limitrange-jz5k4-5355"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-jz5k4" @ 04/28/23 16:44:46.942
  Apr 28 16:44:46.945: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-jz5k4" in "limitrange-8547" namespace @ 04/28/23 16:44:46.945
  Apr 28 16:44:46.952: INFO: LimitRange "e2e-limitrange-jz5k4" has been patched
  STEP: Delete LimitRange "e2e-limitrange-jz5k4" by Collection with labelSelector: "e2e-limitrange-jz5k4=patched" @ 04/28/23 16:44:46.952
  STEP: Confirm that the limitRange "e2e-limitrange-jz5k4" has been deleted @ 04/28/23 16:44:46.958
  Apr 28 16:44:46.958: INFO: Requesting list of LimitRange to confirm quantity
  Apr 28 16:44:46.960: INFO: Found 0 LimitRange with label "e2e-limitrange-jz5k4=patched"
  Apr 28 16:44:46.960: INFO: LimitRange "e2e-limitrange-jz5k4" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-jz5k4" @ 04/28/23 16:44:46.96
  Apr 28 16:44:46.962: INFO: Found 1 limitRange
  Apr 28 16:44:46.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-8547" for this suite. @ 04/28/23 16:44:46.967
  STEP: Destroying namespace "e2e-limitrange-jz5k4-5355" for this suite. @ 04/28/23 16:44:46.976
• [0.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 04/28/23 16:44:46.99
  Apr 28 16:44:46.990: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename disruption @ 04/28/23 16:44:46.991
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:44:47.008
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:44:47.011
  STEP: Waiting for the pdb to be processed @ 04/28/23 16:44:47.017
  STEP: Waiting for all pods to be running @ 04/28/23 16:44:49.057
  Apr 28 16:44:49.064: INFO: running pods: 0 < 3
  Apr 28 16:44:51.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-8300" for this suite. @ 04/28/23 16:44:51.074
• [4.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 04/28/23 16:44:51.082
  Apr 28 16:44:51.082: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename var-expansion @ 04/28/23 16:44:51.083
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:44:51.095
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:44:51.097
  STEP: creating the pod @ 04/28/23 16:44:51.099
  STEP: waiting for pod running @ 04/28/23 16:44:51.105
  STEP: creating a file in subpath @ 04/28/23 16:44:53.114
  Apr 28 16:44:53.117: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1333 PodName:var-expansion-bc2de6f8-3aa6-4a08-8c91-fb1260a5aa0f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 16:44:53.117: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 16:44:53.117: INFO: ExecWithOptions: Clientset creation
  Apr 28 16:44:53.117: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/var-expansion-1333/pods/var-expansion-bc2de6f8-3aa6-4a08-8c91-fb1260a5aa0f/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 04/28/23 16:44:53.202
  Apr 28 16:44:53.205: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1333 PodName:var-expansion-bc2de6f8-3aa6-4a08-8c91-fb1260a5aa0f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 16:44:53.205: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 16:44:53.205: INFO: ExecWithOptions: Clientset creation
  Apr 28 16:44:53.205: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/var-expansion-1333/pods/var-expansion-bc2de6f8-3aa6-4a08-8c91-fb1260a5aa0f/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 04/28/23 16:44:53.277
  Apr 28 16:44:53.792: INFO: Successfully updated pod "var-expansion-bc2de6f8-3aa6-4a08-8c91-fb1260a5aa0f"
  STEP: waiting for annotated pod running @ 04/28/23 16:44:53.792
  STEP: deleting the pod gracefully @ 04/28/23 16:44:53.797
  Apr 28 16:44:53.797: INFO: Deleting pod "var-expansion-bc2de6f8-3aa6-4a08-8c91-fb1260a5aa0f" in namespace "var-expansion-1333"
  Apr 28 16:44:53.807: INFO: Wait up to 5m0s for pod "var-expansion-bc2de6f8-3aa6-4a08-8c91-fb1260a5aa0f" to be fully deleted
  Apr 28 16:45:27.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1333" for this suite. @ 04/28/23 16:45:27.91
• [36.836 seconds]
------------------------------
S
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 04/28/23 16:45:27.918
  Apr 28 16:45:27.918: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename pods @ 04/28/23 16:45:27.919
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:45:27.943
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:45:27.959
  STEP: creating the pod @ 04/28/23 16:45:27.963
  STEP: submitting the pod to kubernetes @ 04/28/23 16:45:27.963
  STEP: verifying QOS class is set on the pod @ 04/28/23 16:45:27.979
  Apr 28 16:45:27.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7777" for this suite. @ 04/28/23 16:45:27.997
• [0.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 04/28/23 16:45:28.018
  Apr 28 16:45:28.018: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename daemonsets @ 04/28/23 16:45:28.019
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:45:28.045
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:45:28.059
  Apr 28 16:45:28.139: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 04/28/23 16:45:28.152
  Apr 28 16:45:28.160: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:45:28.160: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 04/28/23 16:45:28.16
  Apr 28 16:45:28.190: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:45:28.190: INFO: Node ip-172-31-6-71.us-east-2.compute.internal is running 0 daemon pod, expected 1
  Apr 28 16:45:29.193: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:45:29.194: INFO: Node ip-172-31-6-71.us-east-2.compute.internal is running 0 daemon pod, expected 1
  Apr 28 16:45:30.193: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 28 16:45:30.193: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 04/28/23 16:45:30.197
  Apr 28 16:45:30.216: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 28 16:45:30.216: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  Apr 28 16:45:31.219: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:45:31.219: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 04/28/23 16:45:31.219
  Apr 28 16:45:31.229: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:45:31.229: INFO: Node ip-172-31-6-71.us-east-2.compute.internal is running 0 daemon pod, expected 1
  Apr 28 16:45:32.233: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:45:32.233: INFO: Node ip-172-31-6-71.us-east-2.compute.internal is running 0 daemon pod, expected 1
  Apr 28 16:45:33.232: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:45:33.232: INFO: Node ip-172-31-6-71.us-east-2.compute.internal is running 0 daemon pod, expected 1
  Apr 28 16:45:34.236: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:45:34.236: INFO: Node ip-172-31-6-71.us-east-2.compute.internal is running 0 daemon pod, expected 1
  Apr 28 16:45:35.233: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 28 16:45:35.233: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/28/23 16:45:35.239
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5168, will wait for the garbage collector to delete the pods @ 04/28/23 16:45:35.239
  Apr 28 16:45:35.297: INFO: Deleting DaemonSet.extensions daemon-set took: 5.063561ms
  Apr 28 16:45:35.398: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.391936ms
  Apr 28 16:45:38.002: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:45:38.002: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 28 16:45:38.006: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"254575"},"items":null}

  Apr 28 16:45:38.010: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"254575"},"items":null}

  Apr 28 16:45:38.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5168" for this suite. @ 04/28/23 16:45:38.044
• [10.038 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 04/28/23 16:45:38.056
  Apr 28 16:45:38.056: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename namespaces @ 04/28/23 16:45:38.058
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:45:38.075
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:45:38.084
  STEP: Creating namespace "e2e-ns-mrqcw" @ 04/28/23 16:45:38.087
  Apr 28 16:45:38.101: INFO: Namespace "e2e-ns-mrqcw-4151" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-mrqcw-4151" @ 04/28/23 16:45:38.101
  Apr 28 16:45:38.108: INFO: Namespace "e2e-ns-mrqcw-4151" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-mrqcw-4151" @ 04/28/23 16:45:38.108
  Apr 28 16:45:38.114: INFO: Namespace "e2e-ns-mrqcw-4151" has []v1.FinalizerName{"kubernetes"}
  Apr 28 16:45:38.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4701" for this suite. @ 04/28/23 16:45:38.118
  STEP: Destroying namespace "e2e-ns-mrqcw-4151" for this suite. @ 04/28/23 16:45:38.122
• [0.071 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 04/28/23 16:45:38.128
  Apr 28 16:45:38.128: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename configmap @ 04/28/23 16:45:38.129
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:45:38.157
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:45:38.165
  Apr 28 16:45:38.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1735" for this suite. @ 04/28/23 16:45:38.223
• [0.100 seconds]
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 04/28/23 16:45:38.228
  Apr 28 16:45:38.228: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 16:45:38.232
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:45:38.245
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:45:38.249
  STEP: creating all guestbook components @ 04/28/23 16:45:38.252
  Apr 28 16:45:38.252: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Apr 28 16:45:38.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-9396 create -f -'
  Apr 28 16:45:39.059: INFO: stderr: ""
  Apr 28 16:45:39.059: INFO: stdout: "service/agnhost-replica created\n"
  Apr 28 16:45:39.059: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Apr 28 16:45:39.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-9396 create -f -'
  Apr 28 16:45:39.420: INFO: stderr: ""
  Apr 28 16:45:39.420: INFO: stdout: "service/agnhost-primary created\n"
  Apr 28 16:45:39.421: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Apr 28 16:45:39.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-9396 create -f -'
  Apr 28 16:45:40.352: INFO: stderr: ""
  Apr 28 16:45:40.352: INFO: stdout: "service/frontend created\n"
  Apr 28 16:45:40.352: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Apr 28 16:45:40.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-9396 create -f -'
  Apr 28 16:45:40.593: INFO: stderr: ""
  Apr 28 16:45:40.593: INFO: stdout: "deployment.apps/frontend created\n"
  Apr 28 16:45:40.593: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Apr 28 16:45:40.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-9396 create -f -'
  Apr 28 16:45:40.876: INFO: stderr: ""
  Apr 28 16:45:40.876: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Apr 28 16:45:40.876: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Apr 28 16:45:40.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-9396 create -f -'
  Apr 28 16:45:41.365: INFO: stderr: ""
  Apr 28 16:45:41.365: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 04/28/23 16:45:41.365
  Apr 28 16:45:41.365: INFO: Waiting for all frontend pods to be Running.
  Apr 28 16:45:46.419: INFO: Waiting for frontend to serve content.
  Apr 28 16:45:46.430: INFO: Trying to add a new entry to the guestbook.
  Apr 28 16:45:46.451: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 04/28/23 16:45:46.476
  Apr 28 16:45:46.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-9396 delete --grace-period=0 --force -f -'
  Apr 28 16:45:46.613: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 28 16:45:46.614: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 04/28/23 16:45:46.614
  Apr 28 16:45:46.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-9396 delete --grace-period=0 --force -f -'
  Apr 28 16:45:46.737: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 28 16:45:46.737: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 04/28/23 16:45:46.738
  Apr 28 16:45:46.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-9396 delete --grace-period=0 --force -f -'
  Apr 28 16:45:46.838: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 28 16:45:46.838: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 04/28/23 16:45:46.838
  Apr 28 16:45:46.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-9396 delete --grace-period=0 --force -f -'
  Apr 28 16:45:46.929: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 28 16:45:46.929: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 04/28/23 16:45:46.93
  Apr 28 16:45:46.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-9396 delete --grace-period=0 --force -f -'
  Apr 28 16:45:47.084: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 28 16:45:47.084: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 04/28/23 16:45:47.084
  Apr 28 16:45:47.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-9396 delete --grace-period=0 --force -f -'
  Apr 28 16:45:47.348: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 28 16:45:47.348: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Apr 28 16:45:47.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9396" for this suite. @ 04/28/23 16:45:47.363
• [9.154 seconds]
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 04/28/23 16:45:47.383
  Apr 28 16:45:47.383: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename containers @ 04/28/23 16:45:47.384
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:45:47.419
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:45:47.422
  Apr 28 16:45:49.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-9316" for this suite. @ 04/28/23 16:45:49.454
• [2.076 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 04/28/23 16:45:49.459
  Apr 28 16:45:49.459: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename job @ 04/28/23 16:45:49.461
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:45:49.474
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:45:49.476
  STEP: Creating a job @ 04/28/23 16:45:49.478
  STEP: Ensure pods equal to parallelism count is attached to the job @ 04/28/23 16:45:49.482
  STEP: patching /status @ 04/28/23 16:45:53.486
  STEP: updating /status @ 04/28/23 16:45:53.495
  STEP: get /status @ 04/28/23 16:45:53.501
  Apr 28 16:45:53.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3756" for this suite. @ 04/28/23 16:45:53.507
• [4.052 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 04/28/23 16:45:53.513
  Apr 28 16:45:53.513: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename limitrange @ 04/28/23 16:45:53.513
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:45:53.526
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:45:53.534
  STEP: Creating a LimitRange @ 04/28/23 16:45:53.536
  STEP: Setting up watch @ 04/28/23 16:45:53.536
  STEP: Submitting a LimitRange @ 04/28/23 16:45:53.639
  STEP: Verifying LimitRange creation was observed @ 04/28/23 16:45:53.645
  STEP: Fetching the LimitRange to ensure it has proper values @ 04/28/23 16:45:53.645
  Apr 28 16:45:53.648: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Apr 28 16:45:53.648: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 04/28/23 16:45:53.648
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 04/28/23 16:45:53.653
  Apr 28 16:45:53.660: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Apr 28 16:45:53.660: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 04/28/23 16:45:53.66
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 04/28/23 16:45:53.667
  Apr 28 16:45:53.674: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Apr 28 16:45:53.674: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 04/28/23 16:45:53.674
  STEP: Failing to create a Pod with more than max resources @ 04/28/23 16:45:53.676
  STEP: Updating a LimitRange @ 04/28/23 16:45:53.679
  STEP: Verifying LimitRange updating is effective @ 04/28/23 16:45:53.682
  STEP: Creating a Pod with less than former min resources @ 04/28/23 16:45:55.685
  STEP: Failing to create a Pod with more than max resources @ 04/28/23 16:45:55.69
  STEP: Deleting a LimitRange @ 04/28/23 16:45:55.693
  STEP: Verifying the LimitRange was deleted @ 04/28/23 16:45:55.702
  Apr 28 16:46:00.707: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 04/28/23 16:46:00.707
  Apr 28 16:46:00.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-4887" for this suite. @ 04/28/23 16:46:00.722
• [7.215 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 04/28/23 16:46:00.729
  Apr 28 16:46:00.729: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename sched-pred @ 04/28/23 16:46:00.729
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:46:00.743
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:46:00.746
  Apr 28 16:46:00.748: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 28 16:46:00.753: INFO: Waiting for terminating namespaces to be deleted...
  Apr 28 16:46:00.756: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-11-161.us-east-2.compute.internal before test
  Apr 28 16:46:00.763: INFO: cloud-controller-manager-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:02 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.763: INFO: 	Container cloud-controller-manager ready: true, restart count 0
  Apr 28 16:46:00.763: INFO: etcd-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:46:40 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.763: INFO: 	Container etcd ready: true, restart count 0
  Apr 28 16:46:00.763: INFO: kube-apiserver-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:46:57 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.763: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 28 16:46:00.763: INFO: kube-controller-manager-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:02 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.763: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 28 16:46:00.763: INFO: kube-proxy-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:05 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.763: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 28 16:46:00.763: INFO: kube-scheduler-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:02 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.763: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 28 16:46:00.763: INFO: rke2-canal-8td74 from kube-system started at 2023-04-28 03:47:01 +0000 UTC (2 container statuses recorded)
  Apr 28 16:46:00.763: INFO: 	Container calico-node ready: true, restart count 0
  Apr 28 16:46:00.763: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 28 16:46:00.763: INFO: rke2-coredns-rke2-coredns-5896cccb79-vcgcz from kube-system started at 2023-04-28 03:47:28 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.763: INFO: 	Container coredns ready: true, restart count 0
  Apr 28 16:46:00.763: INFO: rke2-ingress-nginx-controller-6f4sz from kube-system started at 2023-04-28 03:47:28 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.763: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
  Apr 28 16:46:00.763: INFO: sonobuoy-systemd-logs-daemon-set-19d55eb1839b4b26-n6bzk from sonobuoy started at 2023-04-28 16:37:21 +0000 UTC (2 container statuses recorded)
  Apr 28 16:46:00.763: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 16:46:00.763: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 16:46:00.763: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-14-195.us-east-2.compute.internal before test
  Apr 28 16:46:00.776: INFO: cloud-controller-manager-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:44:16 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.776: INFO: 	Container cloud-controller-manager ready: true, restart count 0
  Apr 28 16:46:00.776: INFO: etcd-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:43:55 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.776: INFO: 	Container etcd ready: true, restart count 0
  Apr 28 16:46:00.776: INFO: helm-install-rke2-canal-8zdl7 from kube-system started at 2023-04-28 03:44:29 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.777: INFO: 	Container helm ready: false, restart count 0
  Apr 28 16:46:00.777: INFO: helm-install-rke2-coredns-8tn5j from kube-system started at 2023-04-28 03:44:29 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.777: INFO: 	Container helm ready: false, restart count 1
  Apr 28 16:46:00.777: INFO: helm-install-rke2-ingress-nginx-v5xks from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.777: INFO: 	Container helm ready: false, restart count 0
  Apr 28 16:46:00.777: INFO: helm-install-rke2-metrics-server-66776 from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.777: INFO: 	Container helm ready: false, restart count 0
  Apr 28 16:46:00.777: INFO: helm-install-rke2-snapshot-controller-crd-xc2xl from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.777: INFO: 	Container helm ready: false, restart count 0
  Apr 28 16:46:00.777: INFO: helm-install-rke2-snapshot-controller-g6gdp from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.777: INFO: 	Container helm ready: false, restart count 0
  Apr 28 16:46:00.777: INFO: helm-install-rke2-snapshot-validation-webhook-6rwkq from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.777: INFO: 	Container helm ready: false, restart count 0
  Apr 28 16:46:00.777: INFO: kube-apiserver-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:44:09 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.777: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 28 16:46:00.777: INFO: kube-controller-manager-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:44:14 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.777: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 28 16:46:00.777: INFO: kube-proxy-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:44:19 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.777: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 28 16:46:00.778: INFO: kube-scheduler-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:44:14 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.778: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 28 16:46:00.778: INFO: rke2-canal-sz474 from kube-system started at 2023-04-28 03:44:35 +0000 UTC (2 container statuses recorded)
  Apr 28 16:46:00.778: INFO: 	Container calico-node ready: true, restart count 0
  Apr 28 16:46:00.778: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 28 16:46:00.778: INFO: rke2-coredns-rke2-coredns-5896cccb79-lb8dr from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.778: INFO: 	Container coredns ready: true, restart count 0
  Apr 28 16:46:00.778: INFO: rke2-coredns-rke2-coredns-autoscaler-f6766cdc9-qrz4d from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.778: INFO: 	Container autoscaler ready: true, restart count 0
  Apr 28 16:46:00.778: INFO: rke2-ingress-nginx-controller-l9r4l from kube-system started at 2023-04-28 03:45:10 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.778: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
  Apr 28 16:46:00.778: INFO: rke2-metrics-server-6d45f6cb4d-587zc from kube-system started at 2023-04-28 03:45:06 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.778: INFO: 	Container metrics-server ready: true, restart count 0
  Apr 28 16:46:00.778: INFO: rke2-snapshot-controller-7bf6d7bf5f-swpkx from kube-system started at 2023-04-28 03:45:23 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.778: INFO: 	Container rke2-snapshot-controller ready: true, restart count 0
  Apr 28 16:46:00.778: INFO: rke2-snapshot-validation-webhook-b65d46c9f-8jsvg from kube-system started at 2023-04-28 03:45:24 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.778: INFO: 	Container rke2-snapshot-validation-webhook ready: true, restart count 0
  Apr 28 16:46:00.778: INFO: sonobuoy-systemd-logs-daemon-set-19d55eb1839b4b26-bxxcq from sonobuoy started at 2023-04-28 16:37:21 +0000 UTC (2 container statuses recorded)
  Apr 28 16:46:00.778: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 16:46:00.778: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 16:46:00.778: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-6-71.us-east-2.compute.internal before test
  Apr 28 16:46:00.787: INFO: cloud-controller-manager-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:18 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.787: INFO: 	Container cloud-controller-manager ready: true, restart count 0
  Apr 28 16:46:00.787: INFO: etcd-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:01 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.787: INFO: 	Container etcd ready: true, restart count 0
  Apr 28 16:46:00.787: INFO: kube-apiserver-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:13 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.787: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 28 16:46:00.787: INFO: kube-controller-manager-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:18 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.787: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 28 16:46:00.787: INFO: kube-proxy-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:20 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.787: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 28 16:46:00.787: INFO: kube-scheduler-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:18 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.787: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 28 16:46:00.787: INFO: rke2-canal-8rz4z from kube-system started at 2023-04-28 03:47:20 +0000 UTC (2 container statuses recorded)
  Apr 28 16:46:00.787: INFO: 	Container calico-node ready: true, restart count 0
  Apr 28 16:46:00.787: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 28 16:46:00.787: INFO: rke2-ingress-nginx-controller-xfwz8 from kube-system started at 2023-04-28 03:47:42 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.787: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
  Apr 28 16:46:00.787: INFO: sonobuoy-systemd-logs-daemon-set-19d55eb1839b4b26-98t5f from sonobuoy started at 2023-04-28 16:37:21 +0000 UTC (2 container statuses recorded)
  Apr 28 16:46:00.787: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 16:46:00.787: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 16:46:00.787: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-9-127.us-east-2.compute.internal before test
  Apr 28 16:46:00.794: INFO: suspend-false-to-true-6nj9p from job-3756 started at 2023-04-28 16:45:49 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.794: INFO: 	Container c ready: true, restart count 0
  Apr 28 16:46:00.794: INFO: suspend-false-to-true-bdkxl from job-3756 started at 2023-04-28 16:45:49 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.794: INFO: 	Container c ready: true, restart count 0
  Apr 28 16:46:00.794: INFO: kube-proxy-ip-172-31-9-127.us-east-2.compute.internal from kube-system started at 2023-04-28 03:48:58 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.794: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 28 16:46:00.794: INFO: rke2-canal-lc6hn from kube-system started at 2023-04-28 03:48:58 +0000 UTC (2 container statuses recorded)
  Apr 28 16:46:00.794: INFO: 	Container calico-node ready: true, restart count 0
  Apr 28 16:46:00.794: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 28 16:46:00.794: INFO: rke2-ingress-nginx-controller-tgbkj from kube-system started at 2023-04-28 05:38:51 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.794: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
  Apr 28 16:46:00.794: INFO: pod-qos-class-d7554bb5-7ee5-40a4-96f6-b046a3557080 from pods-7777 started at 2023-04-28 16:45:27 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.794: INFO: 	Container agnhost ready: false, restart count 0
  Apr 28 16:46:00.794: INFO: sonobuoy from sonobuoy started at 2023-04-28 16:37:20 +0000 UTC (1 container statuses recorded)
  Apr 28 16:46:00.794: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 28 16:46:00.794: INFO: sonobuoy-e2e-job-92a64e7703e04ef2 from sonobuoy started at 2023-04-28 16:37:21 +0000 UTC (2 container statuses recorded)
  Apr 28 16:46:00.794: INFO: 	Container e2e ready: true, restart count 0
  Apr 28 16:46:00.794: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 16:46:00.794: INFO: sonobuoy-systemd-logs-daemon-set-19d55eb1839b4b26-jv86q from sonobuoy started at 2023-04-28 16:37:21 +0000 UTC (2 container statuses recorded)
  Apr 28 16:46:00.794: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 16:46:00.794: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node ip-172-31-11-161.us-east-2.compute.internal @ 04/28/23 16:46:06.984
  STEP: verifying the node has the label node ip-172-31-14-195.us-east-2.compute.internal @ 04/28/23 16:46:07.044
  STEP: verifying the node has the label node ip-172-31-6-71.us-east-2.compute.internal @ 04/28/23 16:46:07.076
  STEP: verifying the node has the label node ip-172-31-9-127.us-east-2.compute.internal @ 04/28/23 16:46:07.093
  Apr 28 16:46:07.124: INFO: Pod suspend-false-to-true-6nj9p requesting resource cpu=0m on Node ip-172-31-9-127.us-east-2.compute.internal
  Apr 28 16:46:07.125: INFO: Pod suspend-false-to-true-bdkxl requesting resource cpu=0m on Node ip-172-31-9-127.us-east-2.compute.internal
  Apr 28 16:46:07.125: INFO: Pod cloud-controller-manager-ip-172-31-11-161.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-31-11-161.us-east-2.compute.internal
  Apr 28 16:46:07.126: INFO: Pod cloud-controller-manager-ip-172-31-14-195.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-31-14-195.us-east-2.compute.internal
  Apr 28 16:46:07.129: INFO: Pod cloud-controller-manager-ip-172-31-6-71.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-31-6-71.us-east-2.compute.internal
  Apr 28 16:46:07.129: INFO: Pod etcd-ip-172-31-11-161.us-east-2.compute.internal requesting resource cpu=200m on Node ip-172-31-11-161.us-east-2.compute.internal
  Apr 28 16:46:07.129: INFO: Pod etcd-ip-172-31-14-195.us-east-2.compute.internal requesting resource cpu=200m on Node ip-172-31-14-195.us-east-2.compute.internal
  Apr 28 16:46:07.130: INFO: Pod etcd-ip-172-31-6-71.us-east-2.compute.internal requesting resource cpu=200m on Node ip-172-31-6-71.us-east-2.compute.internal
  Apr 28 16:46:07.130: INFO: Pod kube-apiserver-ip-172-31-11-161.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-11-161.us-east-2.compute.internal
  Apr 28 16:46:07.130: INFO: Pod kube-apiserver-ip-172-31-14-195.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-14-195.us-east-2.compute.internal
  Apr 28 16:46:07.132: INFO: Pod kube-apiserver-ip-172-31-6-71.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-6-71.us-east-2.compute.internal
  Apr 28 16:46:07.135: INFO: Pod kube-controller-manager-ip-172-31-11-161.us-east-2.compute.internal requesting resource cpu=200m on Node ip-172-31-11-161.us-east-2.compute.internal
  Apr 28 16:46:07.138: INFO: Pod kube-controller-manager-ip-172-31-14-195.us-east-2.compute.internal requesting resource cpu=200m on Node ip-172-31-14-195.us-east-2.compute.internal
  Apr 28 16:46:07.138: INFO: Pod kube-controller-manager-ip-172-31-6-71.us-east-2.compute.internal requesting resource cpu=200m on Node ip-172-31-6-71.us-east-2.compute.internal
  Apr 28 16:46:07.139: INFO: Pod kube-proxy-ip-172-31-11-161.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-11-161.us-east-2.compute.internal
  Apr 28 16:46:07.139: INFO: Pod kube-proxy-ip-172-31-14-195.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-14-195.us-east-2.compute.internal
  Apr 28 16:46:07.139: INFO: Pod kube-proxy-ip-172-31-6-71.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-6-71.us-east-2.compute.internal
  Apr 28 16:46:07.140: INFO: Pod kube-proxy-ip-172-31-9-127.us-east-2.compute.internal requesting resource cpu=250m on Node ip-172-31-9-127.us-east-2.compute.internal
  Apr 28 16:46:07.140: INFO: Pod kube-scheduler-ip-172-31-11-161.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-31-11-161.us-east-2.compute.internal
  Apr 28 16:46:07.140: INFO: Pod kube-scheduler-ip-172-31-14-195.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-31-14-195.us-east-2.compute.internal
  Apr 28 16:46:07.140: INFO: Pod kube-scheduler-ip-172-31-6-71.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-31-6-71.us-east-2.compute.internal
  Apr 28 16:46:07.141: INFO: Pod rke2-canal-8rz4z requesting resource cpu=250m on Node ip-172-31-6-71.us-east-2.compute.internal
  Apr 28 16:46:07.141: INFO: Pod rke2-canal-8td74 requesting resource cpu=250m on Node ip-172-31-11-161.us-east-2.compute.internal
  Apr 28 16:46:07.141: INFO: Pod rke2-canal-lc6hn requesting resource cpu=250m on Node ip-172-31-9-127.us-east-2.compute.internal
  Apr 28 16:46:07.141: INFO: Pod rke2-canal-sz474 requesting resource cpu=250m on Node ip-172-31-14-195.us-east-2.compute.internal
  Apr 28 16:46:07.142: INFO: Pod rke2-coredns-rke2-coredns-5896cccb79-lb8dr requesting resource cpu=100m on Node ip-172-31-14-195.us-east-2.compute.internal
  Apr 28 16:46:07.142: INFO: Pod rke2-coredns-rke2-coredns-5896cccb79-vcgcz requesting resource cpu=100m on Node ip-172-31-11-161.us-east-2.compute.internal
  Apr 28 16:46:07.142: INFO: Pod rke2-coredns-rke2-coredns-autoscaler-f6766cdc9-qrz4d requesting resource cpu=25m on Node ip-172-31-14-195.us-east-2.compute.internal
  Apr 28 16:46:07.142: INFO: Pod rke2-ingress-nginx-controller-6f4sz requesting resource cpu=100m on Node ip-172-31-11-161.us-east-2.compute.internal
  Apr 28 16:46:07.142: INFO: Pod rke2-ingress-nginx-controller-l9r4l requesting resource cpu=100m on Node ip-172-31-14-195.us-east-2.compute.internal
  Apr 28 16:46:07.143: INFO: Pod rke2-ingress-nginx-controller-tgbkj requesting resource cpu=100m on Node ip-172-31-9-127.us-east-2.compute.internal
  Apr 28 16:46:07.143: INFO: Pod rke2-ingress-nginx-controller-xfwz8 requesting resource cpu=100m on Node ip-172-31-6-71.us-east-2.compute.internal
  Apr 28 16:46:07.143: INFO: Pod rke2-metrics-server-6d45f6cb4d-587zc requesting resource cpu=0m on Node ip-172-31-14-195.us-east-2.compute.internal
  Apr 28 16:46:07.143: INFO: Pod rke2-snapshot-controller-7bf6d7bf5f-swpkx requesting resource cpu=0m on Node ip-172-31-14-195.us-east-2.compute.internal
  Apr 28 16:46:07.143: INFO: Pod rke2-snapshot-validation-webhook-b65d46c9f-8jsvg requesting resource cpu=0m on Node ip-172-31-14-195.us-east-2.compute.internal
  Apr 28 16:46:07.144: INFO: Pod pod-qos-class-d7554bb5-7ee5-40a4-96f6-b046a3557080 requesting resource cpu=100m on Node ip-172-31-9-127.us-east-2.compute.internal
  Apr 28 16:46:07.144: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-9-127.us-east-2.compute.internal
  Apr 28 16:46:07.144: INFO: Pod sonobuoy-e2e-job-92a64e7703e04ef2 requesting resource cpu=0m on Node ip-172-31-9-127.us-east-2.compute.internal
  Apr 28 16:46:07.144: INFO: Pod sonobuoy-systemd-logs-daemon-set-19d55eb1839b4b26-98t5f requesting resource cpu=0m on Node ip-172-31-6-71.us-east-2.compute.internal
  Apr 28 16:46:07.144: INFO: Pod sonobuoy-systemd-logs-daemon-set-19d55eb1839b4b26-bxxcq requesting resource cpu=0m on Node ip-172-31-14-195.us-east-2.compute.internal
  Apr 28 16:46:07.145: INFO: Pod sonobuoy-systemd-logs-daemon-set-19d55eb1839b4b26-jv86q requesting resource cpu=0m on Node ip-172-31-9-127.us-east-2.compute.internal
  Apr 28 16:46:07.145: INFO: Pod sonobuoy-systemd-logs-daemon-set-19d55eb1839b4b26-n6bzk requesting resource cpu=0m on Node ip-172-31-11-161.us-east-2.compute.internal
  STEP: Starting Pods to consume most of the cluster CPU. @ 04/28/23 16:46:07.145
  Apr 28 16:46:07.146: INFO: Creating a pod which consumes cpu=315m on Node ip-172-31-11-161.us-east-2.compute.internal
  Apr 28 16:46:07.154: INFO: Creating a pod which consumes cpu=297m on Node ip-172-31-14-195.us-east-2.compute.internal
  Apr 28 16:46:07.161: INFO: Creating a pod which consumes cpu=385m on Node ip-172-31-6-71.us-east-2.compute.internal
  Apr 28 16:46:07.168: INFO: Creating a pod which consumes cpu=910m on Node ip-172-31-9-127.us-east-2.compute.internal
  STEP: Creating another pod that requires unavailable amount of CPU. @ 04/28/23 16:46:11.205
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-65c97b16-f7b6-4e1a-8f5f-c5f63b6ceb49.175a27119276a7d0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4038/filler-pod-65c97b16-f7b6-4e1a-8f5f-c5f63b6ceb49 to ip-172-31-9-127.us-east-2.compute.internal] @ 04/28/23 16:46:11.209
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-65c97b16-f7b6-4e1a-8f5f-c5f63b6ceb49.175a2711b7f30085], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/28/23 16:46:11.209
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-65c97b16-f7b6-4e1a-8f5f-c5f63b6ceb49.175a2711b90e64ba], Reason = [Created], Message = [Created container filler-pod-65c97b16-f7b6-4e1a-8f5f-c5f63b6ceb49] @ 04/28/23 16:46:11.209
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-65c97b16-f7b6-4e1a-8f5f-c5f63b6ceb49.175a2711c1429a47], Reason = [Started], Message = [Started container filler-pod-65c97b16-f7b6-4e1a-8f5f-c5f63b6ceb49] @ 04/28/23 16:46:11.209
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6895192b-7340-49c3-9322-175e915ac0df.175a2711914cab49], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4038/filler-pod-6895192b-7340-49c3-9322-175e915ac0df to ip-172-31-6-71.us-east-2.compute.internal] @ 04/28/23 16:46:11.209
  STEP: Considering event: 
  Type = [Warning], Name = [filler-pod-6895192b-7340-49c3-9322-175e915ac0df.175a2711d645cf9a], Reason = [FailedMount], Message = [MountVolume.SetUp failed for volume "kube-api-access-v8vpd" : failed to sync configmap cache: timed out waiting for the condition] @ 04/28/23 16:46:11.209
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6895192b-7340-49c3-9322-175e915ac0df.175a271210ce7b87], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/28/23 16:46:11.209
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6895192b-7340-49c3-9322-175e915ac0df.175a27121290a07f], Reason = [Created], Message = [Created container filler-pod-6895192b-7340-49c3-9322-175e915ac0df] @ 04/28/23 16:46:11.209
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6895192b-7340-49c3-9322-175e915ac0df.175a27121f265362], Reason = [Started], Message = [Started container filler-pod-6895192b-7340-49c3-9322-175e915ac0df] @ 04/28/23 16:46:11.209
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6e8687a9-f335-41d1-b2d1-904545e9d1db.175a271190e0ae68], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4038/filler-pod-6e8687a9-f335-41d1-b2d1-904545e9d1db to ip-172-31-14-195.us-east-2.compute.internal] @ 04/28/23 16:46:11.209
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6e8687a9-f335-41d1-b2d1-904545e9d1db.175a2711bf24f3a5], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/28/23 16:46:11.209
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6e8687a9-f335-41d1-b2d1-904545e9d1db.175a2711c0181f9b], Reason = [Created], Message = [Created container filler-pod-6e8687a9-f335-41d1-b2d1-904545e9d1db] @ 04/28/23 16:46:11.209
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6e8687a9-f335-41d1-b2d1-904545e9d1db.175a2711cb4b87a5], Reason = [Started], Message = [Started container filler-pod-6e8687a9-f335-41d1-b2d1-904545e9d1db] @ 04/28/23 16:46:11.209
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-b1cd6d56-6ce7-41f4-928d-d16676c1de77.175a27118ff980e9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4038/filler-pod-b1cd6d56-6ce7-41f4-928d-d16676c1de77 to ip-172-31-11-161.us-east-2.compute.internal] @ 04/28/23 16:46:11.209
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-b1cd6d56-6ce7-41f4-928d-d16676c1de77.175a2711c629a8dd], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/28/23 16:46:11.209
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-b1cd6d56-6ce7-41f4-928d-d16676c1de77.175a2711c7247415], Reason = [Created], Message = [Created container filler-pod-b1cd6d56-6ce7-41f4-928d-d16676c1de77] @ 04/28/23 16:46:11.21
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-b1cd6d56-6ce7-41f4-928d-d16676c1de77.175a2711d20c1c91], Reason = [Started], Message = [Started container filler-pod-b1cd6d56-6ce7-41f4-928d-d16676c1de77] @ 04/28/23 16:46:11.21
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.175a271281aac307], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu. preemption: 0/4 nodes are available: 4 No preemption victims found for incoming pod..] @ 04/28/23 16:46:11.222
  STEP: removing the label node off the node ip-172-31-11-161.us-east-2.compute.internal @ 04/28/23 16:46:12.218
  STEP: verifying the node doesn't have the label node @ 04/28/23 16:46:12.232
  STEP: removing the label node off the node ip-172-31-14-195.us-east-2.compute.internal @ 04/28/23 16:46:12.237
  STEP: verifying the node doesn't have the label node @ 04/28/23 16:46:12.258
  STEP: removing the label node off the node ip-172-31-6-71.us-east-2.compute.internal @ 04/28/23 16:46:12.266
  STEP: verifying the node doesn't have the label node @ 04/28/23 16:46:12.29
  STEP: removing the label node off the node ip-172-31-9-127.us-east-2.compute.internal @ 04/28/23 16:46:12.297
  STEP: verifying the node doesn't have the label node @ 04/28/23 16:46:12.32
  Apr 28 16:46:12.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-4038" for this suite. @ 04/28/23 16:46:12.34
• [11.640 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 04/28/23 16:46:12.377
  Apr 28 16:46:12.377: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 16:46:12.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:46:12.392
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:46:12.395
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 16:46:12.397
  STEP: Saw pod success @ 04/28/23 16:46:16.421
  Apr 28 16:46:16.424: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod downwardapi-volume-12c888b7-262a-43d2-a0f2-e8343cdfd773 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 16:46:16.429
  Apr 28 16:46:16.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7268" for this suite. @ 04/28/23 16:46:16.448
• [4.079 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 04/28/23 16:46:16.469
  Apr 28 16:46:16.469: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 16:46:16.47
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:46:16.487
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:46:16.495
  STEP: Creating projection with secret that has name projected-secret-test-195e1db7-b820-44d8-b8e7-74a3279650f2 @ 04/28/23 16:46:16.5
  STEP: Creating a pod to test consume secrets @ 04/28/23 16:46:16.504
  STEP: Saw pod success @ 04/28/23 16:46:20.523
  Apr 28 16:46:20.525: INFO: Trying to get logs from node ip-172-31-6-71.us-east-2.compute.internal pod pod-projected-secrets-c29d6baf-30b5-4fba-b3ec-4b894bbbad35 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 16:46:20.539
  Apr 28 16:46:20.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6623" for this suite. @ 04/28/23 16:46:20.554
• [4.089 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 04/28/23 16:46:20.559
  Apr 28 16:46:20.559: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 16:46:20.56
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:46:20.574
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:46:20.576
  STEP: Discovering how many secrets are in namespace by default @ 04/28/23 16:46:20.578
  STEP: Counting existing ResourceQuota @ 04/28/23 16:46:25.581
  STEP: Creating a ResourceQuota @ 04/28/23 16:46:30.584
  STEP: Ensuring resource quota status is calculated @ 04/28/23 16:46:30.589
  STEP: Creating a Secret @ 04/28/23 16:46:32.592
  STEP: Ensuring resource quota status captures secret creation @ 04/28/23 16:46:32.603
  STEP: Deleting a secret @ 04/28/23 16:46:34.607
  STEP: Ensuring resource quota status released usage @ 04/28/23 16:46:34.623
  Apr 28 16:46:36.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5104" for this suite. @ 04/28/23 16:46:36.63
• [16.076 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 04/28/23 16:46:36.637
  Apr 28 16:46:36.637: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename gc @ 04/28/23 16:46:36.638
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:46:36.652
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:46:36.656
  STEP: create the rc @ 04/28/23 16:46:36.661
  W0428 16:46:36.667001      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 04/28/23 16:46:40.676
  STEP: wait for the rc to be deleted @ 04/28/23 16:46:40.682
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 04/28/23 16:46:45.697
  STEP: Gathering metrics @ 04/28/23 16:47:15.709
  Apr 28 16:47:15.835: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 28 16:47:15.835: INFO: Deleting pod "simpletest.rc-2nhd6" in namespace "gc-3342"
  Apr 28 16:47:15.848: INFO: Deleting pod "simpletest.rc-44cgc" in namespace "gc-3342"
  Apr 28 16:47:15.867: INFO: Deleting pod "simpletest.rc-46fdl" in namespace "gc-3342"
  Apr 28 16:47:15.926: INFO: Deleting pod "simpletest.rc-476rw" in namespace "gc-3342"
  Apr 28 16:47:15.945: INFO: Deleting pod "simpletest.rc-4cgn2" in namespace "gc-3342"
  Apr 28 16:47:15.967: INFO: Deleting pod "simpletest.rc-4d8xr" in namespace "gc-3342"
  Apr 28 16:47:15.992: INFO: Deleting pod "simpletest.rc-4lzbv" in namespace "gc-3342"
  Apr 28 16:47:16.028: INFO: Deleting pod "simpletest.rc-4s45b" in namespace "gc-3342"
  Apr 28 16:47:16.057: INFO: Deleting pod "simpletest.rc-4vz8p" in namespace "gc-3342"
  Apr 28 16:47:16.083: INFO: Deleting pod "simpletest.rc-5zk8s" in namespace "gc-3342"
  Apr 28 16:47:16.101: INFO: Deleting pod "simpletest.rc-6rvcf" in namespace "gc-3342"
  Apr 28 16:47:16.116: INFO: Deleting pod "simpletest.rc-7jcwm" in namespace "gc-3342"
  Apr 28 16:47:16.140: INFO: Deleting pod "simpletest.rc-7s9mj" in namespace "gc-3342"
  Apr 28 16:47:16.173: INFO: Deleting pod "simpletest.rc-8gzzj" in namespace "gc-3342"
  Apr 28 16:47:16.201: INFO: Deleting pod "simpletest.rc-8jrdr" in namespace "gc-3342"
  Apr 28 16:47:16.221: INFO: Deleting pod "simpletest.rc-8swdr" in namespace "gc-3342"
  Apr 28 16:47:16.244: INFO: Deleting pod "simpletest.rc-98qk7" in namespace "gc-3342"
  Apr 28 16:47:16.256: INFO: Deleting pod "simpletest.rc-999wx" in namespace "gc-3342"
  Apr 28 16:47:16.312: INFO: Deleting pod "simpletest.rc-9z67h" in namespace "gc-3342"
  Apr 28 16:47:16.325: INFO: Deleting pod "simpletest.rc-b4s7v" in namespace "gc-3342"
  Apr 28 16:47:16.363: INFO: Deleting pod "simpletest.rc-b6gv5" in namespace "gc-3342"
  Apr 28 16:47:16.378: INFO: Deleting pod "simpletest.rc-bhqdg" in namespace "gc-3342"
  Apr 28 16:47:16.396: INFO: Deleting pod "simpletest.rc-c2clh" in namespace "gc-3342"
  Apr 28 16:47:16.431: INFO: Deleting pod "simpletest.rc-cfrvk" in namespace "gc-3342"
  Apr 28 16:47:16.470: INFO: Deleting pod "simpletest.rc-cm6qb" in namespace "gc-3342"
  Apr 28 16:47:16.490: INFO: Deleting pod "simpletest.rc-cmrgn" in namespace "gc-3342"
  Apr 28 16:47:16.518: INFO: Deleting pod "simpletest.rc-cpds4" in namespace "gc-3342"
  Apr 28 16:47:16.565: INFO: Deleting pod "simpletest.rc-dc2h9" in namespace "gc-3342"
  Apr 28 16:47:16.594: INFO: Deleting pod "simpletest.rc-dhwsv" in namespace "gc-3342"
  Apr 28 16:47:16.609: INFO: Deleting pod "simpletest.rc-djgvq" in namespace "gc-3342"
  Apr 28 16:47:16.640: INFO: Deleting pod "simpletest.rc-f4v6t" in namespace "gc-3342"
  Apr 28 16:47:16.659: INFO: Deleting pod "simpletest.rc-fhbsr" in namespace "gc-3342"
  Apr 28 16:47:16.681: INFO: Deleting pod "simpletest.rc-ft6t5" in namespace "gc-3342"
  Apr 28 16:47:16.709: INFO: Deleting pod "simpletest.rc-g299p" in namespace "gc-3342"
  Apr 28 16:47:16.750: INFO: Deleting pod "simpletest.rc-g7ggj" in namespace "gc-3342"
  Apr 28 16:47:16.772: INFO: Deleting pod "simpletest.rc-gpbvw" in namespace "gc-3342"
  Apr 28 16:47:16.781: INFO: Deleting pod "simpletest.rc-gr477" in namespace "gc-3342"
  Apr 28 16:47:16.801: INFO: Deleting pod "simpletest.rc-gr4gj" in namespace "gc-3342"
  Apr 28 16:47:16.822: INFO: Deleting pod "simpletest.rc-gskf2" in namespace "gc-3342"
  Apr 28 16:47:16.847: INFO: Deleting pod "simpletest.rc-gtlh9" in namespace "gc-3342"
  Apr 28 16:47:16.881: INFO: Deleting pod "simpletest.rc-gxrjl" in namespace "gc-3342"
  Apr 28 16:47:16.923: INFO: Deleting pod "simpletest.rc-h7wnj" in namespace "gc-3342"
  Apr 28 16:47:16.957: INFO: Deleting pod "simpletest.rc-h8jjg" in namespace "gc-3342"
  Apr 28 16:47:16.977: INFO: Deleting pod "simpletest.rc-hk97g" in namespace "gc-3342"
  Apr 28 16:47:16.993: INFO: Deleting pod "simpletest.rc-hm5cq" in namespace "gc-3342"
  Apr 28 16:47:17.009: INFO: Deleting pod "simpletest.rc-hzv4m" in namespace "gc-3342"
  Apr 28 16:47:17.036: INFO: Deleting pod "simpletest.rc-j2ppp" in namespace "gc-3342"
  Apr 28 16:47:17.052: INFO: Deleting pod "simpletest.rc-jd7fh" in namespace "gc-3342"
  Apr 28 16:47:17.078: INFO: Deleting pod "simpletest.rc-jvfbh" in namespace "gc-3342"
  Apr 28 16:47:17.102: INFO: Deleting pod "simpletest.rc-jwqf9" in namespace "gc-3342"
  Apr 28 16:47:17.119: INFO: Deleting pod "simpletest.rc-kb62r" in namespace "gc-3342"
  Apr 28 16:47:17.144: INFO: Deleting pod "simpletest.rc-kdfj4" in namespace "gc-3342"
  Apr 28 16:47:17.178: INFO: Deleting pod "simpletest.rc-kvmmm" in namespace "gc-3342"
  Apr 28 16:47:17.201: INFO: Deleting pod "simpletest.rc-l5dnp" in namespace "gc-3342"
  Apr 28 16:47:17.221: INFO: Deleting pod "simpletest.rc-lcq96" in namespace "gc-3342"
  Apr 28 16:47:17.239: INFO: Deleting pod "simpletest.rc-lsctf" in namespace "gc-3342"
  Apr 28 16:47:17.278: INFO: Deleting pod "simpletest.rc-mfwt5" in namespace "gc-3342"
  Apr 28 16:47:17.357: INFO: Deleting pod "simpletest.rc-mgmm9" in namespace "gc-3342"
  Apr 28 16:47:17.385: INFO: Deleting pod "simpletest.rc-mhzxz" in namespace "gc-3342"
  Apr 28 16:47:17.401: INFO: Deleting pod "simpletest.rc-mk7rg" in namespace "gc-3342"
  Apr 28 16:47:17.414: INFO: Deleting pod "simpletest.rc-nbqpb" in namespace "gc-3342"
  Apr 28 16:47:17.442: INFO: Deleting pod "simpletest.rc-np4s8" in namespace "gc-3342"
  Apr 28 16:47:17.470: INFO: Deleting pod "simpletest.rc-nqbx9" in namespace "gc-3342"
  Apr 28 16:47:17.505: INFO: Deleting pod "simpletest.rc-nrjpf" in namespace "gc-3342"
  Apr 28 16:47:17.557: INFO: Deleting pod "simpletest.rc-ns57h" in namespace "gc-3342"
  Apr 28 16:47:17.571: INFO: Deleting pod "simpletest.rc-pdmw8" in namespace "gc-3342"
  Apr 28 16:47:17.593: INFO: Deleting pod "simpletest.rc-plzbh" in namespace "gc-3342"
  Apr 28 16:47:17.630: INFO: Deleting pod "simpletest.rc-prkkx" in namespace "gc-3342"
  Apr 28 16:47:17.646: INFO: Deleting pod "simpletest.rc-q7swn" in namespace "gc-3342"
  Apr 28 16:47:17.659: INFO: Deleting pod "simpletest.rc-q85zx" in namespace "gc-3342"
  Apr 28 16:47:17.692: INFO: Deleting pod "simpletest.rc-qdc7w" in namespace "gc-3342"
  Apr 28 16:47:17.712: INFO: Deleting pod "simpletest.rc-qdqk9" in namespace "gc-3342"
  Apr 28 16:47:17.731: INFO: Deleting pod "simpletest.rc-qhwpg" in namespace "gc-3342"
  Apr 28 16:47:17.747: INFO: Deleting pod "simpletest.rc-qhxwt" in namespace "gc-3342"
  Apr 28 16:47:17.768: INFO: Deleting pod "simpletest.rc-qmt5b" in namespace "gc-3342"
  Apr 28 16:47:17.788: INFO: Deleting pod "simpletest.rc-qq824" in namespace "gc-3342"
  Apr 28 16:47:17.816: INFO: Deleting pod "simpletest.rc-qsr4z" in namespace "gc-3342"
  Apr 28 16:47:17.835: INFO: Deleting pod "simpletest.rc-rp24b" in namespace "gc-3342"
  Apr 28 16:47:17.871: INFO: Deleting pod "simpletest.rc-s4bp2" in namespace "gc-3342"
  Apr 28 16:47:17.915: INFO: Deleting pod "simpletest.rc-svprm" in namespace "gc-3342"
  Apr 28 16:47:17.935: INFO: Deleting pod "simpletest.rc-sxphb" in namespace "gc-3342"
  Apr 28 16:47:17.964: INFO: Deleting pod "simpletest.rc-szvz7" in namespace "gc-3342"
  Apr 28 16:47:17.991: INFO: Deleting pod "simpletest.rc-t7jwd" in namespace "gc-3342"
  Apr 28 16:47:18.019: INFO: Deleting pod "simpletest.rc-tm5s2" in namespace "gc-3342"
  Apr 28 16:47:18.041: INFO: Deleting pod "simpletest.rc-twbkh" in namespace "gc-3342"
  Apr 28 16:47:18.061: INFO: Deleting pod "simpletest.rc-tzchp" in namespace "gc-3342"
  Apr 28 16:47:18.076: INFO: Deleting pod "simpletest.rc-tzcwp" in namespace "gc-3342"
  Apr 28 16:47:18.095: INFO: Deleting pod "simpletest.rc-tzfgv" in namespace "gc-3342"
  Apr 28 16:47:18.107: INFO: Deleting pod "simpletest.rc-vcswv" in namespace "gc-3342"
  Apr 28 16:47:18.134: INFO: Deleting pod "simpletest.rc-vhntb" in namespace "gc-3342"
  Apr 28 16:47:18.149: INFO: Deleting pod "simpletest.rc-w9t8j" in namespace "gc-3342"
  Apr 28 16:47:18.164: INFO: Deleting pod "simpletest.rc-wgbl8" in namespace "gc-3342"
  Apr 28 16:47:18.196: INFO: Deleting pod "simpletest.rc-wqjg5" in namespace "gc-3342"
  Apr 28 16:47:18.206: INFO: Deleting pod "simpletest.rc-wsz64" in namespace "gc-3342"
  Apr 28 16:47:18.235: INFO: Deleting pod "simpletest.rc-xl76f" in namespace "gc-3342"
  Apr 28 16:47:18.248: INFO: Deleting pod "simpletest.rc-xnjqx" in namespace "gc-3342"
  Apr 28 16:47:18.262: INFO: Deleting pod "simpletest.rc-xxpjs" in namespace "gc-3342"
  Apr 28 16:47:18.272: INFO: Deleting pod "simpletest.rc-zg5np" in namespace "gc-3342"
  Apr 28 16:47:18.310: INFO: Deleting pod "simpletest.rc-zgxdb" in namespace "gc-3342"
  Apr 28 16:47:18.365: INFO: Deleting pod "simpletest.rc-zms2f" in namespace "gc-3342"
  Apr 28 16:47:18.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3342" for this suite. @ 04/28/23 16:47:18.453
• [41.868 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 04/28/23 16:47:18.505
  Apr 28 16:47:18.505: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename services @ 04/28/23 16:47:18.506
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:47:18.533
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:47:18.539
  STEP: creating service in namespace services-4938 @ 04/28/23 16:47:18.545
  STEP: creating service affinity-nodeport-transition in namespace services-4938 @ 04/28/23 16:47:18.545
  STEP: creating replication controller affinity-nodeport-transition in namespace services-4938 @ 04/28/23 16:47:18.557
  I0428 16:47:18.572061      19 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-4938, replica count: 3
  I0428 16:47:21.629035      19 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0428 16:47:24.630085      19 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0428 16:47:27.630789      19 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 28 16:47:27.638: INFO: Creating new exec pod
  Apr 28 16:47:30.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-4938 exec execpod-affinityjsr4p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Apr 28 16:47:30.835: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Apr 28 16:47:30.835: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 16:47:30.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-4938 exec execpod-affinityjsr4p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.18.230 80'
  Apr 28 16:47:31.094: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.18.230 80\nConnection to 10.43.18.230 80 port [tcp/http] succeeded!\n"
  Apr 28 16:47:31.094: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 16:47:31.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-4938 exec execpod-affinityjsr4p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.11.161 30053'
  Apr 28 16:47:31.254: INFO: stderr: "+ + nc -v -t -w 2 172.31.11.161 30053\necho hostName\nConnection to 172.31.11.161 30053 port [tcp/*] succeeded!\n"
  Apr 28 16:47:31.254: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 16:47:31.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-4938 exec execpod-affinityjsr4p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.6.71 30053'
  Apr 28 16:47:31.427: INFO: stderr: "+ nc -v -t -w 2 172.31.6.71 30053\n+ echo hostName\nConnection to 172.31.6.71 30053 port [tcp/*] succeeded!\n"
  Apr 28 16:47:31.427: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 16:47:31.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-4938 exec execpod-affinityjsr4p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.161:30053/ ; done'
  Apr 28 16:47:31.657: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n"
  Apr 28 16:47:31.657: INFO: stdout: "\naffinity-nodeport-transition-kqxqm\naffinity-nodeport-transition-xf6g5\naffinity-nodeport-transition-kqxqm\naffinity-nodeport-transition-kqxqm\naffinity-nodeport-transition-jznp4\naffinity-nodeport-transition-jznp4\naffinity-nodeport-transition-xf6g5\naffinity-nodeport-transition-jznp4\naffinity-nodeport-transition-kqxqm\naffinity-nodeport-transition-xf6g5\naffinity-nodeport-transition-kqxqm\naffinity-nodeport-transition-jznp4\naffinity-nodeport-transition-kqxqm\naffinity-nodeport-transition-jznp4\naffinity-nodeport-transition-kqxqm\naffinity-nodeport-transition-xf6g5"
  Apr 28 16:47:31.657: INFO: Received response from host: affinity-nodeport-transition-kqxqm
  Apr 28 16:47:31.657: INFO: Received response from host: affinity-nodeport-transition-xf6g5
  Apr 28 16:47:31.657: INFO: Received response from host: affinity-nodeport-transition-kqxqm
  Apr 28 16:47:31.657: INFO: Received response from host: affinity-nodeport-transition-kqxqm
  Apr 28 16:47:31.657: INFO: Received response from host: affinity-nodeport-transition-jznp4
  Apr 28 16:47:31.657: INFO: Received response from host: affinity-nodeport-transition-jznp4
  Apr 28 16:47:31.657: INFO: Received response from host: affinity-nodeport-transition-xf6g5
  Apr 28 16:47:31.657: INFO: Received response from host: affinity-nodeport-transition-jznp4
  Apr 28 16:47:31.657: INFO: Received response from host: affinity-nodeport-transition-kqxqm
  Apr 28 16:47:31.657: INFO: Received response from host: affinity-nodeport-transition-xf6g5
  Apr 28 16:47:31.657: INFO: Received response from host: affinity-nodeport-transition-kqxqm
  Apr 28 16:47:31.657: INFO: Received response from host: affinity-nodeport-transition-jznp4
  Apr 28 16:47:31.657: INFO: Received response from host: affinity-nodeport-transition-kqxqm
  Apr 28 16:47:31.657: INFO: Received response from host: affinity-nodeport-transition-jznp4
  Apr 28 16:47:31.657: INFO: Received response from host: affinity-nodeport-transition-kqxqm
  Apr 28 16:47:31.657: INFO: Received response from host: affinity-nodeport-transition-xf6g5
  Apr 28 16:47:31.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-4938 exec execpod-affinityjsr4p -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.161:30053/ ; done'
  Apr 28 16:47:31.882: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:30053/\n"
  Apr 28 16:47:31.882: INFO: stdout: "\naffinity-nodeport-transition-xf6g5\naffinity-nodeport-transition-xf6g5\naffinity-nodeport-transition-xf6g5\naffinity-nodeport-transition-xf6g5\naffinity-nodeport-transition-xf6g5\naffinity-nodeport-transition-xf6g5\naffinity-nodeport-transition-xf6g5\naffinity-nodeport-transition-xf6g5\naffinity-nodeport-transition-xf6g5\naffinity-nodeport-transition-xf6g5\naffinity-nodeport-transition-xf6g5\naffinity-nodeport-transition-xf6g5\naffinity-nodeport-transition-xf6g5\naffinity-nodeport-transition-xf6g5\naffinity-nodeport-transition-xf6g5\naffinity-nodeport-transition-xf6g5"
  Apr 28 16:47:31.882: INFO: Received response from host: affinity-nodeport-transition-xf6g5
  Apr 28 16:47:31.882: INFO: Received response from host: affinity-nodeport-transition-xf6g5
  Apr 28 16:47:31.882: INFO: Received response from host: affinity-nodeport-transition-xf6g5
  Apr 28 16:47:31.882: INFO: Received response from host: affinity-nodeport-transition-xf6g5
  Apr 28 16:47:31.882: INFO: Received response from host: affinity-nodeport-transition-xf6g5
  Apr 28 16:47:31.882: INFO: Received response from host: affinity-nodeport-transition-xf6g5
  Apr 28 16:47:31.882: INFO: Received response from host: affinity-nodeport-transition-xf6g5
  Apr 28 16:47:31.882: INFO: Received response from host: affinity-nodeport-transition-xf6g5
  Apr 28 16:47:31.882: INFO: Received response from host: affinity-nodeport-transition-xf6g5
  Apr 28 16:47:31.882: INFO: Received response from host: affinity-nodeport-transition-xf6g5
  Apr 28 16:47:31.882: INFO: Received response from host: affinity-nodeport-transition-xf6g5
  Apr 28 16:47:31.882: INFO: Received response from host: affinity-nodeport-transition-xf6g5
  Apr 28 16:47:31.882: INFO: Received response from host: affinity-nodeport-transition-xf6g5
  Apr 28 16:47:31.882: INFO: Received response from host: affinity-nodeport-transition-xf6g5
  Apr 28 16:47:31.882: INFO: Received response from host: affinity-nodeport-transition-xf6g5
  Apr 28 16:47:31.882: INFO: Received response from host: affinity-nodeport-transition-xf6g5
  Apr 28 16:47:31.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 16:47:31.886: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4938, will wait for the garbage collector to delete the pods @ 04/28/23 16:47:31.897
  Apr 28 16:47:31.967: INFO: Deleting ReplicationController affinity-nodeport-transition took: 4.876708ms
  Apr 28 16:47:32.068: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.955444ms
  STEP: Destroying namespace "services-4938" for this suite. @ 04/28/23 16:47:34.389
• [15.888 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 04/28/23 16:47:34.393
  Apr 28 16:47:34.393: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 16:47:34.394
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:47:34.41
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:47:34.413
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 16:47:34.415
  STEP: Saw pod success @ 04/28/23 16:47:38.435
  Apr 28 16:47:38.437: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod downwardapi-volume-7564dc0e-f245-4c05-96ab-942e4ac0521a container client-container: <nil>
  STEP: delete the pod @ 04/28/23 16:47:38.443
  Apr 28 16:47:38.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7020" for this suite. @ 04/28/23 16:47:38.458
• [4.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 04/28/23 16:47:38.465
  Apr 28 16:47:38.465: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename events @ 04/28/23 16:47:38.465
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:47:38.477
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:47:38.481
  STEP: creating a test event @ 04/28/23 16:47:38.483
  STEP: listing events in all namespaces @ 04/28/23 16:47:38.489
  STEP: listing events in test namespace @ 04/28/23 16:47:38.496
  STEP: listing events with field selection filtering on source @ 04/28/23 16:47:38.499
  STEP: listing events with field selection filtering on reportingController @ 04/28/23 16:47:38.502
  STEP: getting the test event @ 04/28/23 16:47:38.506
  STEP: patching the test event @ 04/28/23 16:47:38.508
  STEP: getting the test event @ 04/28/23 16:47:38.514
  STEP: updating the test event @ 04/28/23 16:47:38.516
  STEP: getting the test event @ 04/28/23 16:47:38.521
  STEP: deleting the test event @ 04/28/23 16:47:38.523
  STEP: listing events in all namespaces @ 04/28/23 16:47:38.528
  STEP: listing events in test namespace @ 04/28/23 16:47:38.532
  Apr 28 16:47:38.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-7281" for this suite. @ 04/28/23 16:47:38.538
• [0.078 seconds]
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 04/28/23 16:47:38.543
  Apr 28 16:47:38.543: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename configmap @ 04/28/23 16:47:38.544
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:47:38.576
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:47:38.58
  STEP: Creating configMap with name configmap-test-volume-fd00a3bd-8a98-4f69-a26c-fa9ee64f3d45 @ 04/28/23 16:47:38.582
  STEP: Creating a pod to test consume configMaps @ 04/28/23 16:47:38.587
  STEP: Saw pod success @ 04/28/23 16:47:42.603
  Apr 28 16:47:42.605: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-configmaps-6534a3a0-fe7f-4f3b-a14c-e49d8f99a59b container configmap-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 16:47:42.61
  Apr 28 16:47:42.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3846" for this suite. @ 04/28/23 16:47:42.624
• [4.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 04/28/23 16:47:42.631
  Apr 28 16:47:42.631: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename webhook @ 04/28/23 16:47:42.632
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:47:42.645
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:47:42.647
  STEP: Setting up server cert @ 04/28/23 16:47:42.669
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 16:47:43.414
  STEP: Deploying the webhook pod @ 04/28/23 16:47:43.421
  STEP: Wait for the deployment to be ready @ 04/28/23 16:47:43.432
  Apr 28 16:47:43.442: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/28/23 16:47:45.449
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 16:47:45.458
  Apr 28 16:47:46.460: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 04/28/23 16:47:46.462
  STEP: create a pod @ 04/28/23 16:47:46.477
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 04/28/23 16:47:48.488
  Apr 28 16:47:48.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=webhook-5897 attach --namespace=webhook-5897 to-be-attached-pod -i -c=container1'
  Apr 28 16:47:48.605: INFO: rc: 1
  Apr 28 16:47:48.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5897" for this suite. @ 04/28/23 16:47:48.659
  STEP: Destroying namespace "webhook-markers-8202" for this suite. @ 04/28/23 16:47:48.666
• [6.043 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 04/28/23 16:47:48.675
  Apr 28 16:47:48.675: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename secrets @ 04/28/23 16:47:48.676
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:47:48.687
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:47:48.689
  STEP: Creating secret with name secret-test-8f2a2dc3-ef68-4bbe-8bf4-9dfb702b0fe5 @ 04/28/23 16:47:48.706
  STEP: Creating a pod to test consume secrets @ 04/28/23 16:47:48.71
  STEP: Saw pod success @ 04/28/23 16:47:52.729
  Apr 28 16:47:52.737: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-secrets-976a8edf-4cda-4beb-9795-0a3e8ca60c9a container secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 16:47:52.753
  Apr 28 16:47:52.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1760" for this suite. @ 04/28/23 16:47:52.799
  STEP: Destroying namespace "secret-namespace-8681" for this suite. @ 04/28/23 16:47:52.807
• [4.137 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 04/28/23 16:47:52.817
  Apr 28 16:47:52.817: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename field-validation @ 04/28/23 16:47:52.818
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:47:52.832
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:47:52.835
  Apr 28 16:47:52.837: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  W0428 16:47:55.366388      19 warnings.go:70] unknown field "alpha"
  W0428 16:47:55.366412      19 warnings.go:70] unknown field "beta"
  W0428 16:47:55.366419      19 warnings.go:70] unknown field "delta"
  W0428 16:47:55.366425      19 warnings.go:70] unknown field "epsilon"
  W0428 16:47:55.366430      19 warnings.go:70] unknown field "gamma"
  Apr 28 16:47:55.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4624" for this suite. @ 04/28/23 16:47:55.389
• [2.576 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 04/28/23 16:47:55.394
  Apr 28 16:47:55.394: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename daemonsets @ 04/28/23 16:47:55.395
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:47:55.411
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:47:55.414
  STEP: Creating simple DaemonSet "daemon-set" @ 04/28/23 16:47:55.434
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/28/23 16:47:55.437
  Apr 28 16:47:55.443: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:47:55.443: INFO: Node ip-172-31-11-161.us-east-2.compute.internal is running 0 daemon pod, expected 1
  Apr 28 16:47:56.449: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 28 16:47:56.449: INFO: Node ip-172-31-11-161.us-east-2.compute.internal is running 0 daemon pod, expected 1
  Apr 28 16:47:57.451: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
  Apr 28 16:47:57.451: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 04/28/23 16:47:57.453
  Apr 28 16:47:57.469: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 28 16:47:57.469: INFO: Node ip-172-31-9-127.us-east-2.compute.internal is running 0 daemon pod, expected 1
  Apr 28 16:47:58.479: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 28 16:47:58.479: INFO: Node ip-172-31-9-127.us-east-2.compute.internal is running 0 daemon pod, expected 1
  Apr 28 16:47:59.476: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 28 16:47:59.476: INFO: Node ip-172-31-9-127.us-east-2.compute.internal is running 0 daemon pod, expected 1
  Apr 28 16:48:00.478: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
  Apr 28 16:48:00.478: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/28/23 16:48:00.481
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4810, will wait for the garbage collector to delete the pods @ 04/28/23 16:48:00.481
  Apr 28 16:48:00.542: INFO: Deleting DaemonSet.extensions daemon-set took: 7.911898ms
  Apr 28 16:48:00.643: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.002774ms
  Apr 28 16:48:03.452: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:48:03.452: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 28 16:48:03.455: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"258125"},"items":null}

  Apr 28 16:48:03.461: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"258125"},"items":null}

  Apr 28 16:48:03.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-4810" for this suite. @ 04/28/23 16:48:03.498
• [8.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 04/28/23 16:48:03.51
  Apr 28 16:48:03.510: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename configmap @ 04/28/23 16:48:03.511
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:48:03.566
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:48:03.572
  STEP: Creating configMap with name configmap-test-volume-58a91cc0-c4ae-47b6-b6f9-a398833ff225 @ 04/28/23 16:48:03.575
  STEP: Creating a pod to test consume configMaps @ 04/28/23 16:48:03.582
  STEP: Saw pod success @ 04/28/23 16:48:07.632
  Apr 28 16:48:07.634: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-configmaps-da41493e-57e1-4fd7-9690-426d48df3380 container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 16:48:07.64
  Apr 28 16:48:07.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8395" for this suite. @ 04/28/23 16:48:07.671
• [4.172 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 04/28/23 16:48:07.683
  Apr 28 16:48:07.683: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename container-probe @ 04/28/23 16:48:07.684
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:48:07.712
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:48:07.72
  STEP: Creating pod test-webserver-616a0fae-6959-4c43-9ed2-217969db985c in namespace container-probe-1684 @ 04/28/23 16:48:07.724
  Apr 28 16:48:09.746: INFO: Started pod test-webserver-616a0fae-6959-4c43-9ed2-217969db985c in namespace container-probe-1684
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/28/23 16:48:09.746
  Apr 28 16:48:09.749: INFO: Initial restart count of pod test-webserver-616a0fae-6959-4c43-9ed2-217969db985c is 0
  Apr 28 16:52:10.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 16:52:10.29
  STEP: Destroying namespace "container-probe-1684" for this suite. @ 04/28/23 16:52:10.301
• [242.638 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 04/28/23 16:52:10.324
  Apr 28 16:52:10.324: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 04/28/23 16:52:10.342
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:52:10.354
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:52:10.358
  STEP: creating a target pod @ 04/28/23 16:52:10.361
  STEP: adding an ephemeral container @ 04/28/23 16:52:12.379
  STEP: checking pod container endpoints @ 04/28/23 16:52:16.405
  Apr 28 16:52:16.405: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5525 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 16:52:16.405: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 16:52:16.406: INFO: ExecWithOptions: Clientset creation
  Apr 28 16:52:16.406: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/ephemeral-containers-test-5525/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Apr 28 16:52:16.488: INFO: Exec stderr: ""
  Apr 28 16:52:16.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-5525" for this suite. @ 04/28/23 16:52:16.505
• [6.187 seconds]
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 04/28/23 16:52:16.511
  Apr 28 16:52:16.511: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename container-probe @ 04/28/23 16:52:16.512
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:52:16.529
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:52:16.532
  STEP: Creating pod busybox-dc90d7da-8a68-4c2c-87dc-917e6e9744d9 in namespace container-probe-4621 @ 04/28/23 16:52:16.534
  Apr 28 16:52:18.549: INFO: Started pod busybox-dc90d7da-8a68-4c2c-87dc-917e6e9744d9 in namespace container-probe-4621
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/28/23 16:52:18.549
  Apr 28 16:52:18.551: INFO: Initial restart count of pod busybox-dc90d7da-8a68-4c2c-87dc-917e6e9744d9 is 0
  Apr 28 16:56:19.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 16:56:19.118
  STEP: Destroying namespace "container-probe-4621" for this suite. @ 04/28/23 16:56:19.132
• [242.631 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 04/28/23 16:56:19.162
  Apr 28 16:56:19.162: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/28/23 16:56:19.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:56:19.184
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:56:19.188
  STEP: create the container to handle the HTTPGet hook request. @ 04/28/23 16:56:19.194
  STEP: create the pod with lifecycle hook @ 04/28/23 16:56:21.216
  STEP: delete the pod with lifecycle hook @ 04/28/23 16:56:23.233
  STEP: check prestop hook @ 04/28/23 16:56:27.257
  Apr 28 16:56:27.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-4535" for this suite. @ 04/28/23 16:56:27.28
• [8.139 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 04/28/23 16:56:27.288
  Apr 28 16:56:27.288: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename container-probe @ 04/28/23 16:56:27.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:56:27.304
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:56:27.307
  STEP: Creating pod test-grpc-520633f7-2a01-4e64-b7c2-73a59c3471cd in namespace container-probe-7077 @ 04/28/23 16:56:27.31
  Apr 28 16:56:29.325: INFO: Started pod test-grpc-520633f7-2a01-4e64-b7c2-73a59c3471cd in namespace container-probe-7077
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/28/23 16:56:29.325
  Apr 28 16:56:29.328: INFO: Initial restart count of pod test-grpc-520633f7-2a01-4e64-b7c2-73a59c3471cd is 0
  Apr 28 16:57:45.493: INFO: Restart count of pod container-probe-7077/test-grpc-520633f7-2a01-4e64-b7c2-73a59c3471cd is now 1 (1m16.165301556s elapsed)
  Apr 28 16:57:45.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 16:57:45.496
  STEP: Destroying namespace "container-probe-7077" for this suite. @ 04/28/23 16:57:45.506
• [78.246 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 04/28/23 16:57:45.536
  Apr 28 16:57:45.536: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename daemonsets @ 04/28/23 16:57:45.54
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:57:45.56
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:57:45.563
  Apr 28 16:57:45.582: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/28/23 16:57:45.587
  Apr 28 16:57:45.599: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:57:45.600: INFO: Node ip-172-31-11-161.us-east-2.compute.internal is running 0 daemon pod, expected 1
  Apr 28 16:57:46.606: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:57:46.606: INFO: Node ip-172-31-11-161.us-east-2.compute.internal is running 0 daemon pod, expected 1
  Apr 28 16:57:47.607: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
  Apr 28 16:57:47.607: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: Update daemon pods image. @ 04/28/23 16:57:47.615
  STEP: Check that daemon pods images are updated. @ 04/28/23 16:57:47.623
  Apr 28 16:57:47.626: INFO: Wrong image for pod: daemon-set-7k544. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 16:57:47.626: INFO: Wrong image for pod: daemon-set-d7b4z. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 16:57:47.626: INFO: Wrong image for pod: daemon-set-f4tnn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 16:57:47.626: INFO: Wrong image for pod: daemon-set-gbx2d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 16:57:48.633: INFO: Wrong image for pod: daemon-set-7k544. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 16:57:48.633: INFO: Wrong image for pod: daemon-set-f4tnn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 16:57:48.633: INFO: Wrong image for pod: daemon-set-gbx2d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 16:57:49.633: INFO: Wrong image for pod: daemon-set-7k544. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 16:57:49.633: INFO: Wrong image for pod: daemon-set-f4tnn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 16:57:49.633: INFO: Wrong image for pod: daemon-set-gbx2d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 16:57:50.634: INFO: Pod daemon-set-2d6g9 is not available
  Apr 28 16:57:50.634: INFO: Wrong image for pod: daemon-set-7k544. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 16:57:50.634: INFO: Wrong image for pod: daemon-set-f4tnn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 16:57:50.634: INFO: Wrong image for pod: daemon-set-gbx2d. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 16:57:51.633: INFO: Wrong image for pod: daemon-set-7k544. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 16:57:51.633: INFO: Wrong image for pod: daemon-set-f4tnn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 16:57:52.633: INFO: Wrong image for pod: daemon-set-7k544. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 16:57:52.633: INFO: Pod daemon-set-cdtq4 is not available
  Apr 28 16:57:52.633: INFO: Wrong image for pod: daemon-set-f4tnn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 16:57:53.639: INFO: Wrong image for pod: daemon-set-7k544. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 16:57:54.634: INFO: Wrong image for pod: daemon-set-7k544. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 28 16:57:54.634: INFO: Pod daemon-set-pbjct is not available
  Apr 28 16:57:56.632: INFO: Pod daemon-set-wfm8l is not available
  STEP: Check that daemon pods are still running on every node of the cluster. @ 04/28/23 16:57:56.636
  Apr 28 16:57:56.643: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 28 16:57:56.643: INFO: Node ip-172-31-9-127.us-east-2.compute.internal is running 0 daemon pod, expected 1
  Apr 28 16:57:57.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 28 16:57:57.652: INFO: Node ip-172-31-9-127.us-east-2.compute.internal is running 0 daemon pod, expected 1
  Apr 28 16:57:58.650: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
  Apr 28 16:57:58.650: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/28/23 16:57:58.662
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5133, will wait for the garbage collector to delete the pods @ 04/28/23 16:57:58.662
  Apr 28 16:57:58.723: INFO: Deleting DaemonSet.extensions daemon-set took: 7.96599ms
  Apr 28 16:57:58.823: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.154086ms
  Apr 28 16:58:00.827: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 16:58:00.827: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 28 16:58:00.829: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"261399"},"items":null}

  Apr 28 16:58:00.831: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"261399"},"items":null}

  Apr 28 16:58:00.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5133" for this suite. @ 04/28/23 16:58:00.847
• [15.317 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 04/28/23 16:58:00.853
  Apr 28 16:58:00.853: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 16:58:00.854
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:58:00.891
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:58:00.894
  STEP: Creating configMap with name projected-configmap-test-volume-map-b4a9e59b-3735-4feb-965d-aada0664c510 @ 04/28/23 16:58:00.897
  STEP: Creating a pod to test consume configMaps @ 04/28/23 16:58:00.901
  STEP: Saw pod success @ 04/28/23 16:58:04.952
  Apr 28 16:58:04.963: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-projected-configmaps-d7e26b10-c157-45f7-a8d9-21c2f26b5d35 container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 16:58:04.994
  Apr 28 16:58:05.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8081" for this suite. @ 04/28/23 16:58:05.017
• [4.177 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 04/28/23 16:58:05.035
  Apr 28 16:58:05.035: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename secrets @ 04/28/23 16:58:05.037
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:58:05.058
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:58:05.062
  STEP: Creating secret with name secret-test-8d5afd4c-000a-4130-9963-707665f1c504 @ 04/28/23 16:58:05.064
  STEP: Creating a pod to test consume secrets @ 04/28/23 16:58:05.068
  STEP: Saw pod success @ 04/28/23 16:58:09.085
  Apr 28 16:58:09.088: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-secrets-8c597af9-7b35-47fe-bc6a-097e926c679f container secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 16:58:09.094
  Apr 28 16:58:09.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1319" for this suite. @ 04/28/23 16:58:09.113
• [4.082 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 04/28/23 16:58:09.12
  Apr 28 16:58:09.120: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename disruption @ 04/28/23 16:58:09.121
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:58:09.134
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:58:09.136
  STEP: creating the pdb @ 04/28/23 16:58:09.139
  STEP: Waiting for the pdb to be processed @ 04/28/23 16:58:09.143
  STEP: updating the pdb @ 04/28/23 16:58:09.15
  STEP: Waiting for the pdb to be processed @ 04/28/23 16:58:09.158
  STEP: patching the pdb @ 04/28/23 16:58:11.166
  STEP: Waiting for the pdb to be processed @ 04/28/23 16:58:11.173
  STEP: Waiting for the pdb to be deleted @ 04/28/23 16:58:13.187
  Apr 28 16:58:13.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-4276" for this suite. @ 04/28/23 16:58:13.193
• [4.079 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 04/28/23 16:58:13.202
  Apr 28 16:58:13.202: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 16:58:13.203
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:58:13.223
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:58:13.226
  STEP: Creating the pod @ 04/28/23 16:58:13.228
  Apr 28 16:58:15.771: INFO: Successfully updated pod "annotationupdate07e76e0f-265b-4f32-81d9-04bcc785dd19"
  Apr 28 16:58:17.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-553" for this suite. @ 04/28/23 16:58:17.79
• [4.597 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 04/28/23 16:58:17.802
  Apr 28 16:58:17.802: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename statefulset @ 04/28/23 16:58:17.804
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:58:17.818
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:58:17.821
  STEP: Creating service test in namespace statefulset-6690 @ 04/28/23 16:58:17.823
  STEP: Creating stateful set ss in namespace statefulset-6690 @ 04/28/23 16:58:17.829
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6690 @ 04/28/23 16:58:17.838
  Apr 28 16:58:17.843: INFO: Found 0 stateful pods, waiting for 1
  Apr 28 16:58:27.847: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 04/28/23 16:58:27.847
  Apr 28 16:58:27.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=statefulset-6690 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 28 16:58:28.094: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 28 16:58:28.094: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 28 16:58:28.094: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 28 16:58:28.098: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Apr 28 16:58:38.102: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 28 16:58:38.102: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 16:58:38.125: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
  Apr 28 16:58:38.126: INFO: ss-0  ip-172-31-9-127.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:58:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:58:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:58:17 +0000 UTC  }]
  Apr 28 16:58:38.126: INFO: ss-1                                              Pending         []
  Apr 28 16:58:38.126: INFO: 
  Apr 28 16:58:38.126: INFO: StatefulSet ss has not reached scale 3, at 2
  Apr 28 16:58:39.129: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991656398s
  Apr 28 16:58:40.133: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988081011s
  Apr 28 16:58:41.136: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98482277s
  Apr 28 16:58:42.139: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.981530391s
  Apr 28 16:58:43.144: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978326363s
  Apr 28 16:58:44.148: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973672252s
  Apr 28 16:58:45.152: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.969615571s
  Apr 28 16:58:46.157: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.96506839s
  Apr 28 16:58:47.160: INFO: Verifying statefulset ss doesn't scale past 3 for another 960.671786ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6690 @ 04/28/23 16:58:48.161
  Apr 28 16:58:48.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=statefulset-6690 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 28 16:58:48.309: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 28 16:58:48.309: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 28 16:58:48.309: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 28 16:58:48.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=statefulset-6690 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 28 16:58:48.503: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Apr 28 16:58:48.503: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 28 16:58:48.503: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 28 16:58:48.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=statefulset-6690 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 28 16:58:48.654: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Apr 28 16:58:48.654: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 28 16:58:48.654: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 28 16:58:48.657: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
  Apr 28 16:58:58.662: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 16:58:58.662: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 16:58:58.662: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 04/28/23 16:58:58.662
  Apr 28 16:58:58.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=statefulset-6690 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 28 16:58:58.862: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 28 16:58:58.862: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 28 16:58:58.862: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 28 16:58:58.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=statefulset-6690 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 28 16:58:59.018: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 28 16:58:59.018: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 28 16:58:59.018: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 28 16:58:59.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=statefulset-6690 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 28 16:58:59.174: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 28 16:58:59.174: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 28 16:58:59.174: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 28 16:58:59.174: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 16:58:59.177: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  Apr 28 16:59:09.184: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 28 16:59:09.184: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Apr 28 16:59:09.184: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Apr 28 16:59:09.194: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
  Apr 28 16:59:09.194: INFO: ss-0  ip-172-31-9-127.us-east-2.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:58:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:58:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:58:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:58:17 +0000 UTC  }]
  Apr 28 16:59:09.194: INFO: ss-1  ip-172-31-6-71.us-east-2.compute.internal    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:58:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:58:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:58:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:58:38 +0000 UTC  }]
  Apr 28 16:59:09.194: INFO: ss-2  ip-172-31-11-161.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:58:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:58:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:58:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:58:38 +0000 UTC  }]
  Apr 28 16:59:09.194: INFO: 
  Apr 28 16:59:09.194: INFO: StatefulSet ss has not reached scale 0, at 3
  Apr 28 16:59:10.198: INFO: POD   NODE                                         PHASE      GRACE  CONDITIONS
  Apr 28 16:59:10.198: INFO: ss-2  ip-172-31-11-161.us-east-2.compute.internal  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:58:38 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:58:59 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:58:59 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 16:58:38 +0000 UTC  }]
  Apr 28 16:59:10.198: INFO: 
  Apr 28 16:59:10.198: INFO: StatefulSet ss has not reached scale 0, at 1
  Apr 28 16:59:11.202: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.992048551s
  Apr 28 16:59:12.205: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.988576598s
  Apr 28 16:59:13.208: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.985356831s
  Apr 28 16:59:14.211: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.982270481s
  Apr 28 16:59:15.214: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.979228284s
  Apr 28 16:59:16.217: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.976261504s
  Apr 28 16:59:17.221: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.972737909s
  Apr 28 16:59:18.224: INFO: Verifying statefulset ss doesn't scale past 0 for another 969.053792ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6690 @ 04/28/23 16:59:19.225
  Apr 28 16:59:19.228: INFO: Scaling statefulset ss to 0
  Apr 28 16:59:19.236: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 16:59:19.238: INFO: Deleting all statefulset in ns statefulset-6690
  Apr 28 16:59:19.241: INFO: Scaling statefulset ss to 0
  Apr 28 16:59:19.249: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 16:59:19.251: INFO: Deleting statefulset ss
  Apr 28 16:59:19.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6690" for this suite. @ 04/28/23 16:59:19.268
• [61.471 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 04/28/23 16:59:19.277
  Apr 28 16:59:19.277: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename pod-network-test @ 04/28/23 16:59:19.278
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:59:19.292
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:59:19.297
  STEP: Performing setup for networking test in namespace pod-network-test-966 @ 04/28/23 16:59:19.3
  STEP: creating a selector @ 04/28/23 16:59:19.3
  STEP: Creating the service pods in kubernetes @ 04/28/23 16:59:19.3
  Apr 28 16:59:19.300: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 04/28/23 16:59:41.441
  Apr 28 16:59:43.468: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
  Apr 28 16:59:43.468: INFO: Going to poll 10.42.1.164 on port 8081 at least 0 times, with a maximum of 46 tries before failing
  Apr 28 16:59:43.470: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.1.164 8081 | grep -v '^\s*$'] Namespace:pod-network-test-966 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 16:59:43.470: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 16:59:43.470: INFO: ExecWithOptions: Clientset creation
  Apr 28 16:59:43.470: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-966/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.42.1.164+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 28 16:59:44.548: INFO: Found all 1 expected endpoints: [netserver-0]
  Apr 28 16:59:44.548: INFO: Going to poll 10.42.0.149 on port 8081 at least 0 times, with a maximum of 46 tries before failing
  Apr 28 16:59:44.551: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.0.149 8081 | grep -v '^\s*$'] Namespace:pod-network-test-966 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 16:59:44.551: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 16:59:44.551: INFO: ExecWithOptions: Clientset creation
  Apr 28 16:59:44.552: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-966/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.42.0.149+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 28 16:59:45.634: INFO: Found all 1 expected endpoints: [netserver-1]
  Apr 28 16:59:45.634: INFO: Going to poll 10.42.2.188 on port 8081 at least 0 times, with a maximum of 46 tries before failing
  Apr 28 16:59:45.637: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.2.188 8081 | grep -v '^\s*$'] Namespace:pod-network-test-966 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 16:59:45.637: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 16:59:45.638: INFO: ExecWithOptions: Clientset creation
  Apr 28 16:59:45.638: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-966/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.42.2.188+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 28 16:59:46.716: INFO: Found all 1 expected endpoints: [netserver-2]
  Apr 28 16:59:46.716: INFO: Going to poll 10.42.3.21 on port 8081 at least 0 times, with a maximum of 46 tries before failing
  Apr 28 16:59:46.719: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.3.21 8081 | grep -v '^\s*$'] Namespace:pod-network-test-966 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 16:59:46.719: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 16:59:46.720: INFO: ExecWithOptions: Clientset creation
  Apr 28 16:59:46.720: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-966/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.42.3.21+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 28 16:59:47.820: INFO: Found all 1 expected endpoints: [netserver-3]
  Apr 28 16:59:47.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-966" for this suite. @ 04/28/23 16:59:47.826
• [28.555 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 04/28/23 16:59:47.834
  Apr 28 16:59:47.834: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename emptydir-wrapper @ 04/28/23 16:59:47.834
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 16:59:47.852
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 16:59:47.855
  STEP: Creating 50 configmaps @ 04/28/23 16:59:47.857
  STEP: Creating RC which spawns configmap-volume pods @ 04/28/23 16:59:48.089
  Apr 28 16:59:48.220: INFO: Pod name wrapped-volume-race-67ba807d-9b57-4709-a8c9-0c08e811d543: Found 3 pods out of 5
  Apr 28 16:59:53.230: INFO: Pod name wrapped-volume-race-67ba807d-9b57-4709-a8c9-0c08e811d543: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/28/23 16:59:53.23
  STEP: Creating RC which spawns configmap-volume pods @ 04/28/23 16:59:53.327
  Apr 28 16:59:53.369: INFO: Pod name wrapped-volume-race-f6e66a36-a82b-4cfc-ba80-2246e932b4e9: Found 0 pods out of 5
  Apr 28 16:59:58.375: INFO: Pod name wrapped-volume-race-f6e66a36-a82b-4cfc-ba80-2246e932b4e9: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/28/23 16:59:58.375
  STEP: Creating RC which spawns configmap-volume pods @ 04/28/23 16:59:58.392
  Apr 28 16:59:58.408: INFO: Pod name wrapped-volume-race-57f6767d-90a0-4127-b4bb-9e4b798acdef: Found 0 pods out of 5
  Apr 28 17:00:03.452: INFO: Pod name wrapped-volume-race-57f6767d-90a0-4127-b4bb-9e4b798acdef: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/28/23 17:00:03.452
  Apr 28 17:00:03.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-57f6767d-90a0-4127-b4bb-9e4b798acdef in namespace emptydir-wrapper-9291, will wait for the garbage collector to delete the pods @ 04/28/23 17:00:03.476
  Apr 28 17:00:03.545: INFO: Deleting ReplicationController wrapped-volume-race-57f6767d-90a0-4127-b4bb-9e4b798acdef took: 12.184358ms
  Apr 28 17:00:03.846: INFO: Terminating ReplicationController wrapped-volume-race-57f6767d-90a0-4127-b4bb-9e4b798acdef pods took: 300.861149ms
  STEP: deleting ReplicationController wrapped-volume-race-f6e66a36-a82b-4cfc-ba80-2246e932b4e9 in namespace emptydir-wrapper-9291, will wait for the garbage collector to delete the pods @ 04/28/23 17:00:06.347
  Apr 28 17:00:06.409: INFO: Deleting ReplicationController wrapped-volume-race-f6e66a36-a82b-4cfc-ba80-2246e932b4e9 took: 6.763016ms
  Apr 28 17:00:06.510: INFO: Terminating ReplicationController wrapped-volume-race-f6e66a36-a82b-4cfc-ba80-2246e932b4e9 pods took: 100.478208ms
  STEP: deleting ReplicationController wrapped-volume-race-67ba807d-9b57-4709-a8c9-0c08e811d543 in namespace emptydir-wrapper-9291, will wait for the garbage collector to delete the pods @ 04/28/23 17:00:09.31
  Apr 28 17:00:09.371: INFO: Deleting ReplicationController wrapped-volume-race-67ba807d-9b57-4709-a8c9-0c08e811d543 took: 6.771189ms
  Apr 28 17:00:09.474: INFO: Terminating ReplicationController wrapped-volume-race-67ba807d-9b57-4709-a8c9-0c08e811d543 pods took: 103.414629ms
  STEP: Cleaning up the configMaps @ 04/28/23 17:00:12.387
  STEP: Destroying namespace "emptydir-wrapper-9291" for this suite. @ 04/28/23 17:00:12.643
• [24.814 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 04/28/23 17:00:12.651
  Apr 28 17:00:12.651: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:00:12.652
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:00:12.667
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:00:12.67
  STEP: Creating configMap with name cm-test-opt-del-43db6ef6-aaa3-4c91-ab6f-36d0c00fb08a @ 04/28/23 17:00:12.675
  STEP: Creating configMap with name cm-test-opt-upd-de21319f-14c4-43bf-a7fa-9e7bc6b9061b @ 04/28/23 17:00:12.68
  STEP: Creating the pod @ 04/28/23 17:00:12.684
  STEP: Deleting configmap cm-test-opt-del-43db6ef6-aaa3-4c91-ab6f-36d0c00fb08a @ 04/28/23 17:00:14.726
  STEP: Updating configmap cm-test-opt-upd-de21319f-14c4-43bf-a7fa-9e7bc6b9061b @ 04/28/23 17:00:14.732
  STEP: Creating configMap with name cm-test-opt-create-b4c5023c-ebdd-4d9e-a495-7f82cd4aec81 @ 04/28/23 17:00:14.736
  STEP: waiting to observe update in volume @ 04/28/23 17:00:14.74
  Apr 28 17:00:16.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7047" for this suite. @ 04/28/23 17:00:16.774
• [4.129 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 04/28/23 17:00:16.781
  Apr 28 17:00:16.781: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:00:16.782
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:00:16.805
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:00:16.812
  STEP: Creating configMap with name projected-configmap-test-volume-ec4218be-c0c9-41e9-8b89-07877eb1ac69 @ 04/28/23 17:00:16.82
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:00:16.824
  STEP: Saw pod success @ 04/28/23 17:00:20.845
  Apr 28 17:00:20.854: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-projected-configmaps-490ddf38-cda2-4434-aba1-5de9105400e8 container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 17:00:20.864
  Apr 28 17:00:20.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7721" for this suite. @ 04/28/23 17:00:20.892
• [4.119 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 04/28/23 17:00:20.901
  Apr 28 17:00:20.901: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename csiinlinevolumes @ 04/28/23 17:00:20.903
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:00:20.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:00:20.934
  STEP: creating @ 04/28/23 17:00:20.939
  STEP: getting @ 04/28/23 17:00:20.957
  STEP: listing in namespace @ 04/28/23 17:00:20.962
  STEP: patching @ 04/28/23 17:00:20.975
  STEP: deleting @ 04/28/23 17:00:20.983
  Apr 28 17:00:20.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-1438" for this suite. @ 04/28/23 17:00:20.995
• [0.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 04/28/23 17:00:21.011
  Apr 28 17:00:21.011: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename job @ 04/28/23 17:00:21.013
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:00:21.027
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:00:21.03
  STEP: Creating a suspended job @ 04/28/23 17:00:21.034
  STEP: Patching the Job @ 04/28/23 17:00:21.039
  STEP: Watching for Job to be patched @ 04/28/23 17:00:21.063
  Apr 28 17:00:21.065: INFO: Event ADDED observed for Job e2e-b26sk in namespace job-7952 with labels: map[e2e-job-label:e2e-b26sk] and annotations: map[batch.kubernetes.io/job-tracking:]
  Apr 28 17:00:21.065: INFO: Event MODIFIED observed for Job e2e-b26sk in namespace job-7952 with labels: map[e2e-job-label:e2e-b26sk] and annotations: map[batch.kubernetes.io/job-tracking:]
  Apr 28 17:00:21.065: INFO: Event MODIFIED found for Job e2e-b26sk in namespace job-7952 with labels: map[e2e-b26sk:patched e2e-job-label:e2e-b26sk] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 04/28/23 17:00:21.065
  STEP: Watching for Job to be updated @ 04/28/23 17:00:21.072
  Apr 28 17:00:21.074: INFO: Event MODIFIED found for Job e2e-b26sk in namespace job-7952 with labels: map[e2e-b26sk:patched e2e-job-label:e2e-b26sk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 28 17:00:21.074: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 04/28/23 17:00:21.074
  Apr 28 17:00:21.078: INFO: Job: e2e-b26sk as labels: map[e2e-b26sk:patched e2e-job-label:e2e-b26sk]
  STEP: Waiting for job to complete @ 04/28/23 17:00:21.078
  STEP: Delete a job collection with a labelselector @ 04/28/23 17:00:31.082
  STEP: Watching for Job to be deleted @ 04/28/23 17:00:31.089
  Apr 28 17:00:31.091: INFO: Event MODIFIED observed for Job e2e-b26sk in namespace job-7952 with labels: map[e2e-b26sk:patched e2e-job-label:e2e-b26sk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 28 17:00:31.091: INFO: Event MODIFIED observed for Job e2e-b26sk in namespace job-7952 with labels: map[e2e-b26sk:patched e2e-job-label:e2e-b26sk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 28 17:00:31.091: INFO: Event MODIFIED observed for Job e2e-b26sk in namespace job-7952 with labels: map[e2e-b26sk:patched e2e-job-label:e2e-b26sk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 28 17:00:31.091: INFO: Event MODIFIED observed for Job e2e-b26sk in namespace job-7952 with labels: map[e2e-b26sk:patched e2e-job-label:e2e-b26sk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 28 17:00:31.091: INFO: Event MODIFIED observed for Job e2e-b26sk in namespace job-7952 with labels: map[e2e-b26sk:patched e2e-job-label:e2e-b26sk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 28 17:00:31.091: INFO: Event DELETED found for Job e2e-b26sk in namespace job-7952 with labels: map[e2e-b26sk:patched e2e-job-label:e2e-b26sk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 04/28/23 17:00:31.091
  Apr 28 17:00:31.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7952" for this suite. @ 04/28/23 17:00:31.101
• [10.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 04/28/23 17:00:31.122
  Apr 28 17:00:31.122: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename configmap @ 04/28/23 17:00:31.123
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:00:31.137
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:00:31.14
  STEP: Creating configMap with name configmap-test-volume-map-e555d2fc-2204-4c8d-8808-a32a49d4b514 @ 04/28/23 17:00:31.143
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:00:31.148
  STEP: Saw pod success @ 04/28/23 17:00:35.171
  Apr 28 17:00:35.173: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-configmaps-cdc379d9-2f2d-4b87-8d88-f88a4d2aa9ef container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 17:00:35.179
  Apr 28 17:00:35.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7611" for this suite. @ 04/28/23 17:00:35.197
• [4.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 04/28/23 17:00:35.206
  Apr 28 17:00:35.206: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename sched-preemption @ 04/28/23 17:00:35.207
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:00:35.223
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:00:35.244
  Apr 28 17:00:35.269: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 28 17:01:35.351: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 04/28/23 17:01:35.354
  Apr 28 17:01:35.377: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Apr 28 17:01:35.387: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Apr 28 17:01:35.416: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Apr 28 17:01:35.428: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Apr 28 17:01:35.464: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Apr 28 17:01:35.480: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  Apr 28 17:01:35.505: INFO: Created pod: pod3-0-sched-preemption-medium-priority
  Apr 28 17:01:35.514: INFO: Created pod: pod3-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 04/28/23 17:01:35.514
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 04/28/23 17:01:37.553
  Apr 28 17:01:41.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-2300" for this suite. @ 04/28/23 17:01:41.641
• [66.440 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 04/28/23 17:01:41.646
  Apr 28 17:01:41.646: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:01:41.647
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:01:41.661
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:01:41.665
  STEP: Creating projection with secret that has name projected-secret-test-map-56c02434-ce3a-4381-82ee-4c19a3a74ac9 @ 04/28/23 17:01:41.666
  STEP: Creating a pod to test consume secrets @ 04/28/23 17:01:41.67
  STEP: Saw pod success @ 04/28/23 17:01:45.693
  Apr 28 17:01:45.696: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-projected-secrets-16fc0ab7-21ab-4224-a25b-8242377748b1 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 17:01:45.701
  Apr 28 17:01:45.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8487" for this suite. @ 04/28/23 17:01:45.718
• [4.078 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 04/28/23 17:01:45.726
  Apr 28 17:01:45.726: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename replication-controller @ 04/28/23 17:01:45.727
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:01:45.743
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:01:45.746
  STEP: creating a ReplicationController @ 04/28/23 17:01:45.751
  STEP: waiting for RC to be added @ 04/28/23 17:01:45.761
  STEP: waiting for available Replicas @ 04/28/23 17:01:45.763
  STEP: patching ReplicationController @ 04/28/23 17:01:47.568
  STEP: waiting for RC to be modified @ 04/28/23 17:01:47.578
  STEP: patching ReplicationController status @ 04/28/23 17:01:47.578
  STEP: waiting for RC to be modified @ 04/28/23 17:01:47.591
  STEP: waiting for available Replicas @ 04/28/23 17:01:47.601
  STEP: fetching ReplicationController status @ 04/28/23 17:01:47.602
  STEP: patching ReplicationController scale @ 04/28/23 17:01:47.605
  STEP: waiting for RC to be modified @ 04/28/23 17:01:47.62
  STEP: waiting for ReplicationController's scale to be the max amount @ 04/28/23 17:01:47.62
  STEP: fetching ReplicationController; ensuring that it's patched @ 04/28/23 17:01:49.608
  STEP: updating ReplicationController status @ 04/28/23 17:01:49.61
  STEP: waiting for RC to be modified @ 04/28/23 17:01:49.615
  STEP: listing all ReplicationControllers @ 04/28/23 17:01:49.616
  STEP: checking that ReplicationController has expected values @ 04/28/23 17:01:49.62
  STEP: deleting ReplicationControllers by collection @ 04/28/23 17:01:49.621
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 04/28/23 17:01:49.626
  Apr 28 17:01:49.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0428 17:01:49.679989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-7040" for this suite. @ 04/28/23 17:01:49.682
• [3.963 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 04/28/23 17:01:49.691
  Apr 28 17:01:49.691: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename statefulset @ 04/28/23 17:01:49.692
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:01:49.704
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:01:49.707
  STEP: Creating service test in namespace statefulset-4068 @ 04/28/23 17:01:49.709
  STEP: Creating a new StatefulSet @ 04/28/23 17:01:49.715
  Apr 28 17:01:49.728: INFO: Found 0 stateful pods, waiting for 3
  E0428 17:01:50.680201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:51.680308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:52.680632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:53.681737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:54.681809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:55.681916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:56.682786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:57.683224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:58.683338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:01:59.683453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:01:59.733: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 17:01:59.733: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 17:01:59.733: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 04/28/23 17:01:59.74
  Apr 28 17:01:59.758: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 04/28/23 17:01:59.758
  E0428 17:02:00.683571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:01.683767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:02.684707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:03.684955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:04.685215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:05.685329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:06.685352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:07.686382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:08.686512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:09.686639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not applying an update when the partition is greater than the number of replicas @ 04/28/23 17:02:09.775
  STEP: Performing a canary update @ 04/28/23 17:02:09.775
  Apr 28 17:02:09.795: INFO: Updating stateful set ss2
  Apr 28 17:02:09.805: INFO: Waiting for Pod statefulset-4068/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0428 17:02:10.686759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:11.689329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:12.689910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:13.690504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:14.693904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:15.693985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:16.694284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:17.694351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:18.694557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:19.694740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Restoring Pods to the correct revision when they are deleted @ 04/28/23 17:02:19.812
  Apr 28 17:02:19.933: INFO: Found 2 stateful pods, waiting for 3
  E0428 17:02:20.695705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:21.701208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:22.696837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:23.697609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:24.698387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:25.698911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:26.699736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:27.699921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:28.700031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:29.700222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:02:29.941: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 17:02:29.941: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 17:02:29.941: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 04/28/23 17:02:29.952
  Apr 28 17:02:29.974: INFO: Updating stateful set ss2
  Apr 28 17:02:29.992: INFO: Waiting for Pod statefulset-4068/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0428 17:02:30.700351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:31.700508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:32.700849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:33.700970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:34.701049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:35.701196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:36.701404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:37.701857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:38.702887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:39.703091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:02:40.027: INFO: Updating stateful set ss2
  Apr 28 17:02:40.037: INFO: Waiting for StatefulSet statefulset-4068/ss2 to complete update
  Apr 28 17:02:40.037: INFO: Waiting for Pod statefulset-4068/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0428 17:02:40.703999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:41.704086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:42.704834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:43.705054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:44.705292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:45.705373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:46.706344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:47.706805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:48.706930      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:49.707058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:02:50.099: INFO: Deleting all statefulset in ns statefulset-4068
  Apr 28 17:02:50.106: INFO: Scaling statefulset ss2 to 0
  E0428 17:02:50.708089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:51.708323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:52.709091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:53.709299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:54.709377      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:55.709455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:56.709952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:57.710836      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:58.711049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:02:59.711137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:00.146: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 17:03:00.156: INFO: Deleting statefulset ss2
  Apr 28 17:03:00.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4068" for this suite. @ 04/28/23 17:03:00.191
• [70.508 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 04/28/23 17:03:00.205
  Apr 28 17:03:00.205: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename webhook @ 04/28/23 17:03:00.206
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:03:00.218
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:03:00.224
  STEP: Setting up server cert @ 04/28/23 17:03:00.245
  E0428 17:03:00.711403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 17:03:00.953
  STEP: Deploying the webhook pod @ 04/28/23 17:03:00.959
  STEP: Wait for the deployment to be ready @ 04/28/23 17:03:00.971
  Apr 28 17:03:00.977: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0428 17:03:01.712207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:02.713146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 17:03:02.988
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 17:03:03.009
  E0428 17:03:03.713256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:04.009: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 28 17:03:04.013: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 04/28/23 17:03:04.523
  STEP: Creating a custom resource that should be denied by the webhook @ 04/28/23 17:03:04.539
  E0428 17:03:04.713627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:05.714052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 04/28/23 17:03:06.555
  STEP: Updating the custom resource with disallowed data should be denied @ 04/28/23 17:03:06.561
  STEP: Deleting the custom resource should be denied @ 04/28/23 17:03:06.568
  STEP: Remove the offending key and value from the custom resource data @ 04/28/23 17:03:06.581
  STEP: Deleting the updated custom resource should be successful @ 04/28/23 17:03:06.592
  Apr 28 17:03:06.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0428 17:03:06.725293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-5911" for this suite. @ 04/28/23 17:03:07.232
  STEP: Destroying namespace "webhook-markers-302" for this suite. @ 04/28/23 17:03:07.241
• [7.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 04/28/23 17:03:07.258
  Apr 28 17:03:07.258: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename configmap @ 04/28/23 17:03:07.259
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:03:07.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:03:07.278
  STEP: creating a ConfigMap @ 04/28/23 17:03:07.28
  STEP: fetching the ConfigMap @ 04/28/23 17:03:07.286
  STEP: patching the ConfigMap @ 04/28/23 17:03:07.289
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 04/28/23 17:03:07.3
  STEP: deleting the ConfigMap by collection with a label selector @ 04/28/23 17:03:07.309
  STEP: listing all ConfigMaps in test namespace @ 04/28/23 17:03:07.319
  Apr 28 17:03:07.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7026" for this suite. @ 04/28/23 17:03:07.335
• [0.087 seconds]
------------------------------
SSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 04/28/23 17:03:07.349
  Apr 28 17:03:07.349: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename events @ 04/28/23 17:03:07.352
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:03:07.377
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:03:07.382
  STEP: creating a test event @ 04/28/23 17:03:07.384
  STEP: listing all events in all namespaces @ 04/28/23 17:03:07.389
  STEP: patching the test event @ 04/28/23 17:03:07.393
  STEP: fetching the test event @ 04/28/23 17:03:07.399
  STEP: updating the test event @ 04/28/23 17:03:07.405
  STEP: getting the test event @ 04/28/23 17:03:07.413
  STEP: deleting the test event @ 04/28/23 17:03:07.418
  STEP: listing all events in all namespaces @ 04/28/23 17:03:07.424
  Apr 28 17:03:07.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-4321" for this suite. @ 04/28/23 17:03:07.431
• [0.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 04/28/23 17:03:07.439
  Apr 28 17:03:07.439: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename replicaset @ 04/28/23 17:03:07.44
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:03:07.453
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:03:07.456
  Apr 28 17:03:07.458: INFO: Creating ReplicaSet my-hostname-basic-256c765f-a893-4a35-a9b2-9683a17e7b06
  Apr 28 17:03:07.471: INFO: Pod name my-hostname-basic-256c765f-a893-4a35-a9b2-9683a17e7b06: Found 0 pods out of 1
  E0428 17:03:07.726042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:08.726279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:09.726381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:10.726533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:11.726654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:12.479: INFO: Pod name my-hostname-basic-256c765f-a893-4a35-a9b2-9683a17e7b06: Found 1 pods out of 1
  Apr 28 17:03:12.479: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-256c765f-a893-4a35-a9b2-9683a17e7b06" is running
  Apr 28 17:03:12.484: INFO: Pod "my-hostname-basic-256c765f-a893-4a35-a9b2-9683a17e7b06-wszjd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-28 17:03:07 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-28 17:03:08 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-28 17:03:08 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-28 17:03:07 +0000 UTC Reason: Message:}])
  Apr 28 17:03:12.484: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 04/28/23 17:03:12.484
  Apr 28 17:03:12.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3253" for this suite. @ 04/28/23 17:03:12.51
• [5.087 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 04/28/23 17:03:12.526
  Apr 28 17:03:12.526: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 17:03:12.527
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:03:12.559
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:03:12.566
  STEP: Counting existing ResourceQuota @ 04/28/23 17:03:12.568
  E0428 17:03:12.727201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:13.727373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:14.728363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:15.729145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:16.730146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/28/23 17:03:17.572
  STEP: Ensuring resource quota status is calculated @ 04/28/23 17:03:17.576
  E0428 17:03:17.730483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:18.730761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 04/28/23 17:03:19.58
  STEP: Ensuring resource quota status captures replication controller creation @ 04/28/23 17:03:19.591
  E0428 17:03:19.731473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:20.731578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 04/28/23 17:03:21.594
  STEP: Ensuring resource quota status released usage @ 04/28/23 17:03:21.599
  E0428 17:03:21.732211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:22.732868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:23.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7198" for this suite. @ 04/28/23 17:03:23.607
• [11.086 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 04/28/23 17:03:23.613
  Apr 28 17:03:23.613: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename webhook @ 04/28/23 17:03:23.613
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:03:23.626
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:03:23.629
  STEP: Setting up server cert @ 04/28/23 17:03:23.664
  E0428 17:03:23.733380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 17:03:24.192
  STEP: Deploying the webhook pod @ 04/28/23 17:03:24.197
  STEP: Wait for the deployment to be ready @ 04/28/23 17:03:24.208
  Apr 28 17:03:24.223: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0428 17:03:24.734339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:25.734423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 17:03:26.235
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 17:03:26.247
  E0428 17:03:26.734566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:27.248: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 04/28/23 17:03:27.251
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/28/23 17:03:27.251
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 04/28/23 17:03:27.268
  E0428 17:03:27.734891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 04/28/23 17:03:28.277
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/28/23 17:03:28.277
  E0428 17:03:28.737708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is longer than webhook latency @ 04/28/23 17:03:29.299
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/28/23 17:03:29.299
  E0428 17:03:29.738702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:30.739280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:31.739475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:32.740200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:33.740257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 04/28/23 17:03:34.33
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/28/23 17:03:34.33
  E0428 17:03:34.740898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:35.741207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:36.741285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:37.742160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:38.742278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:39.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1453" for this suite. @ 04/28/23 17:03:39.467
  STEP: Destroying namespace "webhook-markers-4027" for this suite. @ 04/28/23 17:03:39.475
• [15.869 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 04/28/23 17:03:39.482
  Apr 28 17:03:39.482: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename conformance-tests @ 04/28/23 17:03:39.483
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:03:39.503
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:03:39.505
  STEP: Getting node addresses @ 04/28/23 17:03:39.508
  Apr 28 17:03:39.508: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Apr 28 17:03:39.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-8943" for this suite. @ 04/28/23 17:03:39.519
• [0.042 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 04/28/23 17:03:39.526
  Apr 28 17:03:39.526: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename svcaccounts @ 04/28/23 17:03:39.527
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:03:39.543
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:03:39.548
  STEP: creating a ServiceAccount @ 04/28/23 17:03:39.551
  STEP: watching for the ServiceAccount to be added @ 04/28/23 17:03:39.56
  STEP: patching the ServiceAccount @ 04/28/23 17:03:39.562
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 04/28/23 17:03:39.566
  STEP: deleting the ServiceAccount @ 04/28/23 17:03:39.569
  Apr 28 17:03:39.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-991" for this suite. @ 04/28/23 17:03:39.591
• [0.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 04/28/23 17:03:39.6
  Apr 28 17:03:39.600: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename container-runtime @ 04/28/23 17:03:39.602
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:03:39.616
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:03:39.62
  STEP: create the container @ 04/28/23 17:03:39.622
  W0428 17:03:39.629345      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/28/23 17:03:39.629
  E0428 17:03:39.742846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:40.742906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:41.742961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 04/28/23 17:03:42.646
  STEP: the container should be terminated @ 04/28/23 17:03:42.654
  STEP: the termination message should be set @ 04/28/23 17:03:42.654
  Apr 28 17:03:42.654: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 04/28/23 17:03:42.654
  Apr 28 17:03:42.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-2990" for this suite. @ 04/28/23 17:03:42.689
• [3.097 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 04/28/23 17:03:42.698
  Apr 28 17:03:42.698: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename var-expansion @ 04/28/23 17:03:42.699
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:03:42.735
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:03:42.738
  E0428 17:03:42.743083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:43.743257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:44.743549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:44.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 17:03:44.775: INFO: Deleting pod "var-expansion-54d26710-83e2-4b3d-9e22-5ee4ffa40389" in namespace "var-expansion-9781"
  Apr 28 17:03:44.785: INFO: Wait up to 5m0s for pod "var-expansion-54d26710-83e2-4b3d-9e22-5ee4ffa40389" to be fully deleted
  E0428 17:03:45.743672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:46.743861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:47.744912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:48.745216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-9781" for this suite. @ 04/28/23 17:03:48.802
• [6.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 04/28/23 17:03:48.807
  Apr 28 17:03:48.807: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/28/23 17:03:48.808
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:03:48.819
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:03:48.823
  Apr 28 17:03:48.825: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 17:03:49.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4084" for this suite. @ 04/28/23 17:03:49.368
• [0.567 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 04/28/23 17:03:49.376
  Apr 28 17:03:49.376: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename sysctl @ 04/28/23 17:03:49.377
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:03:49.388
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:03:49.391
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 04/28/23 17:03:49.394
  STEP: Watching for error events or started pod @ 04/28/23 17:03:49.401
  E0428 17:03:49.745354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:50.745701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 04/28/23 17:03:51.404
  E0428 17:03:51.746509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:52.747086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 04/28/23 17:03:53.412
  STEP: Getting logs from the pod @ 04/28/23 17:03:53.412
  STEP: Checking that the sysctl is actually updated @ 04/28/23 17:03:53.424
  Apr 28 17:03:53.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-7047" for this suite. @ 04/28/23 17:03:53.427
• [4.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 04/28/23 17:03:53.434
  Apr 28 17:03:53.434: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename replication-controller @ 04/28/23 17:03:53.434
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:03:53.45
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:03:53.454
  STEP: Creating ReplicationController "e2e-rc-gbbrv" @ 04/28/23 17:03:53.456
  Apr 28 17:03:53.460: INFO: Get Replication Controller "e2e-rc-gbbrv" to confirm replicas
  E0428 17:03:53.748422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:54.463: INFO: Get Replication Controller "e2e-rc-gbbrv" to confirm replicas
  Apr 28 17:03:54.466: INFO: Found 1 replicas for "e2e-rc-gbbrv" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-gbbrv" @ 04/28/23 17:03:54.466
  STEP: Updating a scale subresource @ 04/28/23 17:03:54.469
  STEP: Verifying replicas where modified for replication controller "e2e-rc-gbbrv" @ 04/28/23 17:03:54.474
  Apr 28 17:03:54.474: INFO: Get Replication Controller "e2e-rc-gbbrv" to confirm replicas
  E0428 17:03:54.748570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:55.478: INFO: Get Replication Controller "e2e-rc-gbbrv" to confirm replicas
  Apr 28 17:03:55.481: INFO: Found 2 replicas for "e2e-rc-gbbrv" replication controller
  Apr 28 17:03:55.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7706" for this suite. @ 04/28/23 17:03:55.484
• [2.055 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 04/28/23 17:03:55.49
  Apr 28 17:03:55.490: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/28/23 17:03:55.491
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:03:55.502
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:03:55.505
  Apr 28 17:03:55.508: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  E0428 17:03:55.749092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:56.749850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:03:57.757087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 04/28/23 17:03:58.076
  Apr 28 17:03:58.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-9804 --namespace=crd-publish-openapi-9804 create -f -'
  E0428 17:03:58.757391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:58.877: INFO: stderr: ""
  Apr 28 17:03:58.877: INFO: stdout: "e2e-test-crd-publish-openapi-7137-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Apr 28 17:03:58.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-9804 --namespace=crd-publish-openapi-9804 delete e2e-test-crd-publish-openapi-7137-crds test-foo'
  Apr 28 17:03:59.032: INFO: stderr: ""
  Apr 28 17:03:59.032: INFO: stdout: "e2e-test-crd-publish-openapi-7137-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Apr 28 17:03:59.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-9804 --namespace=crd-publish-openapi-9804 apply -f -'
  E0428 17:03:59.757521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:03:59.916: INFO: stderr: ""
  Apr 28 17:03:59.916: INFO: stdout: "e2e-test-crd-publish-openapi-7137-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Apr 28 17:03:59.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-9804 --namespace=crd-publish-openapi-9804 delete e2e-test-crd-publish-openapi-7137-crds test-foo'
  Apr 28 17:04:00.039: INFO: stderr: ""
  Apr 28 17:04:00.040: INFO: stdout: "e2e-test-crd-publish-openapi-7137-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 04/28/23 17:04:00.04
  Apr 28 17:04:00.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-9804 --namespace=crd-publish-openapi-9804 create -f -'
  E0428 17:04:00.757632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:01.119: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 04/28/23 17:04:01.119
  Apr 28 17:04:01.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-9804 --namespace=crd-publish-openapi-9804 create -f -'
  Apr 28 17:04:01.339: INFO: rc: 1
  Apr 28 17:04:01.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-9804 --namespace=crd-publish-openapi-9804 apply -f -'
  Apr 28 17:04:01.559: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 04/28/23 17:04:01.559
  Apr 28 17:04:01.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-9804 --namespace=crd-publish-openapi-9804 create -f -'
  E0428 17:04:01.758279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:01.789: INFO: rc: 1
  Apr 28 17:04:01.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-9804 --namespace=crd-publish-openapi-9804 apply -f -'
  Apr 28 17:04:02.016: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 04/28/23 17:04:02.016
  Apr 28 17:04:02.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-9804 explain e2e-test-crd-publish-openapi-7137-crds'
  Apr 28 17:04:02.223: INFO: stderr: ""
  Apr 28 17:04:02.223: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-7137-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 04/28/23 17:04:02.223
  Apr 28 17:04:02.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-9804 explain e2e-test-crd-publish-openapi-7137-crds.metadata'
  Apr 28 17:04:02.437: INFO: stderr: ""
  Apr 28 17:04:02.437: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-7137-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Apr 28 17:04:02.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-9804 explain e2e-test-crd-publish-openapi-7137-crds.spec'
  Apr 28 17:04:02.676: INFO: stderr: ""
  Apr 28 17:04:02.676: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-7137-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Apr 28 17:04:02.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-9804 explain e2e-test-crd-publish-openapi-7137-crds.spec.bars'
  E0428 17:04:02.759179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:02.958: INFO: stderr: ""
  Apr 28 17:04:02.958: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-7137-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 04/28/23 17:04:02.959
  Apr 28 17:04:02.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-9804 explain e2e-test-crd-publish-openapi-7137-crds.spec.bars2'
  Apr 28 17:04:03.240: INFO: rc: 1
  E0428 17:04:03.759256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:04.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9804" for this suite. @ 04/28/23 17:04:04.612
• [9.128 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 04/28/23 17:04:04.62
  Apr 28 17:04:04.620: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename statefulset @ 04/28/23 17:04:04.62
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:04:04.643
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:04:04.646
  STEP: Creating service test in namespace statefulset-3981 @ 04/28/23 17:04:04.648
  STEP: Creating statefulset ss in namespace statefulset-3981 @ 04/28/23 17:04:04.663
  Apr 28 17:04:04.674: INFO: Found 0 stateful pods, waiting for 1
  E0428 17:04:04.759342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:05.759411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:06.759569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:07.759939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:08.761008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:09.761862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:10.762280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:11.762393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:12.762862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:13.763134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:14.681: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 04/28/23 17:04:14.692
  STEP: Getting /status @ 04/28/23 17:04:14.702
  Apr 28 17:04:14.706: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 04/28/23 17:04:14.706
  Apr 28 17:04:14.714: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 04/28/23 17:04:14.714
  Apr 28 17:04:14.717: INFO: Observed &StatefulSet event: ADDED
  Apr 28 17:04:14.717: INFO: Found Statefulset ss in namespace statefulset-3981 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 28 17:04:14.717: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 04/28/23 17:04:14.717
  Apr 28 17:04:14.717: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 28 17:04:14.724: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 04/28/23 17:04:14.724
  Apr 28 17:04:14.726: INFO: Observed &StatefulSet event: ADDED
  Apr 28 17:04:14.726: INFO: Observed Statefulset ss in namespace statefulset-3981 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 28 17:04:14.726: INFO: Observed &StatefulSet event: MODIFIED
  Apr 28 17:04:14.726: INFO: Deleting all statefulset in ns statefulset-3981
  Apr 28 17:04:14.729: INFO: Scaling statefulset ss to 0
  E0428 17:04:14.794074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:15.793273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:16.793928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:17.794266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:18.794504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:19.794700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:20.794816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:21.795014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:22.795901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:23.796025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:24.744: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 17:04:24.746: INFO: Deleting statefulset ss
  Apr 28 17:04:24.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3981" for this suite. @ 04/28/23 17:04:24.763
• [20.148 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 04/28/23 17:04:24.77
  Apr 28 17:04:24.770: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename deployment @ 04/28/23 17:04:24.771
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:04:24.795
  E0428 17:04:24.796671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:04:24.8
  Apr 28 17:04:24.806: INFO: Creating deployment "webserver-deployment"
  Apr 28 17:04:24.815: INFO: Waiting for observed generation 1
  E0428 17:04:25.798369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:26.798507      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:26.821: INFO: Waiting for all required pods to come up
  Apr 28 17:04:26.824: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 04/28/23 17:04:26.824
  E0428 17:04:27.798950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:28.799071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:28.836: INFO: Waiting for deployment "webserver-deployment" to complete
  Apr 28 17:04:28.840: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Apr 28 17:04:28.848: INFO: Updating deployment webserver-deployment
  Apr 28 17:04:28.848: INFO: Waiting for observed generation 2
  E0428 17:04:29.801238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:30.801548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:30.855: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Apr 28 17:04:30.857: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Apr 28 17:04:30.861: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Apr 28 17:04:30.875: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Apr 28 17:04:30.875: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Apr 28 17:04:30.880: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Apr 28 17:04:30.887: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Apr 28 17:04:30.888: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Apr 28 17:04:30.896: INFO: Updating deployment webserver-deployment
  Apr 28 17:04:30.896: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Apr 28 17:04:30.908: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Apr 28 17:04:30.911: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Apr 28 17:04:30.949: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-9044  84e2ebff-7e89-42ce-bda6-2b1739d98148 265782 3 2023-04-28 17:04:24 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-28 17:04:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-28 17:04:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034f9758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-04-28 17:04:29 +0000 UTC,LastTransitionTime:2023-04-28 17:04:24 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-28 17:04:30 +0000 UTC,LastTransitionTime:2023-04-28 17:04:30 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Apr 28 17:04:30.971: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-9044  8fd4c438-3f97-497e-b412-ca041e65556e 265781 3 2023-04-28 17:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 84e2ebff-7e89-42ce-bda6-2b1739d98148 0xc0034f9c67 0xc0034f9c68}] [] [{kube-controller-manager Update apps/v1 2023-04-28 17:04:28 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-28 17:04:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84e2ebff-7e89-42ce-bda6-2b1739d98148\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034f9d18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 17:04:30.971: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Apr 28 17:04:30.971: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-9044  457280b2-b926-457f-a984-e283662af4d7 265778 3 2023-04-28 17:04:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 84e2ebff-7e89-42ce-bda6-2b1739d98148 0xc0034f9b77 0xc0034f9b78}] [] [{kube-controller-manager Update apps/v1 2023-04-28 17:04:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-28 17:04:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84e2ebff-7e89-42ce-bda6-2b1739d98148\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034f9c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 17:04:30.999: INFO: Pod "webserver-deployment-67bd4bf6dc-5ndq5" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-5ndq5 webserver-deployment-67bd4bf6dc- deployment-9044  5b2e22a1-6c76-4626-813d-709e95d850bb 265804 0 2023-04-28 17:04:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 457280b2-b926-457f-a984-e283662af4d7 0xc004ab4217 0xc004ab4218}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"457280b2-b926-457f-a984-e283662af4d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-glxbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-glxbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-14-195.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:30.999: INFO: Pod "webserver-deployment-67bd4bf6dc-6jcgx" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6jcgx webserver-deployment-67bd4bf6dc- deployment-9044  ff08fb3c-1c37-4fab-8a79-c8a3c0bc5603 265615 0 2023-04-28 17:04:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:90d30eb688b2a66edd8a4c3fa616594425a87a109f7ab9937fe576c7b765f8fd cni.projectcalico.org/podIP:10.42.1.172/32 cni.projectcalico.org/podIPs:10.42.1.172/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 457280b2-b926-457f-a984-e283662af4d7 0xc004ab4390 0xc004ab4391}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"457280b2-b926-457f-a984-e283662af4d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-28 17:04:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-28 17:04:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.172\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qgnbs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qgnbs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-161.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.161,PodIP:10.42.1.172,StartTime:2023-04-28 17:04:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:04:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a2e1969491f21c0c2cd10698dc72a375f121ffa34ae1e80055ce42f4f130bddf,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.172,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:30.999: INFO: Pod "webserver-deployment-67bd4bf6dc-9v7nr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-9v7nr webserver-deployment-67bd4bf6dc- deployment-9044  309b4437-60ae-4504-9687-5f364b1105e2 265808 0 2023-04-28 17:04:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 457280b2-b926-457f-a984-e283662af4d7 0xc004ab4587 0xc004ab4588}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"457280b2-b926-457f-a984-e283662af4d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c9q52,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c9q52,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:30.999: INFO: Pod "webserver-deployment-67bd4bf6dc-bb5qd" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-bb5qd webserver-deployment-67bd4bf6dc- deployment-9044  0495ef27-2c83-4fc9-b39c-e4683edb1bca 265617 0 2023-04-28 17:04:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:258fc1108a3fbebd1db8b337aaf1d5f184db5d24a77b32dfaa7d1b7372166682 cni.projectcalico.org/podIP:10.42.1.171/32 cni.projectcalico.org/podIPs:10.42.1.171/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 457280b2-b926-457f-a984-e283662af4d7 0xc004ab46f0 0xc004ab46f1}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"457280b2-b926-457f-a984-e283662af4d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-28 17:04:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-28 17:04:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r89cf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r89cf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-161.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.161,PodIP:10.42.1.171,StartTime:2023-04-28 17:04:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:04:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f6751e129f2a50cf2d8f93422c958eb82319ab5185b2cf51b900d3a0a0bffaed,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.171,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:30.999: INFO: Pod "webserver-deployment-67bd4bf6dc-frq8d" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-frq8d webserver-deployment-67bd4bf6dc- deployment-9044  089ba21d-98b7-4cdc-906b-3d4102b2c9ab 265631 0 2023-04-28 17:04:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:ec75ba6ddf9036040b79f2db1685a232aaeaeda0e9eade69d60b9c66105b0579 cni.projectcalico.org/podIP:10.42.0.163/32 cni.projectcalico.org/podIPs:10.42.0.163/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 457280b2-b926-457f-a984-e283662af4d7 0xc004ab4907 0xc004ab4908}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"457280b2-b926-457f-a984-e283662af4d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-28 17:04:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-28 17:04:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.163\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vm4kl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vm4kl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-14-195.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.14.195,PodIP:10.42.0.163,StartTime:2023-04-28 17:04:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:04:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2ad72519e63b949825695d26a7dbf98eac33bd21ebe8d98490cddb79c7f98067,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.163,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:31.000: INFO: Pod "webserver-deployment-67bd4bf6dc-g9qxn" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-g9qxn webserver-deployment-67bd4bf6dc- deployment-9044  664de55a-f5ae-431f-97ac-060ba345a2a4 265789 0 2023-04-28 17:04:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 457280b2-b926-457f-a984-e283662af4d7 0xc004ab4b17 0xc004ab4b18}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"457280b2-b926-457f-a984-e283662af4d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-28 17:04:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wlvq4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wlvq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-127.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.127,PodIP:,StartTime:2023-04-28 17:04:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:31.000: INFO: Pod "webserver-deployment-67bd4bf6dc-gn2h4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-gn2h4 webserver-deployment-67bd4bf6dc- deployment-9044  6c556dff-f0e6-46a4-ad33-e31d189e18bc 265793 0 2023-04-28 17:04:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 457280b2-b926-457f-a984-e283662af4d7 0xc004ab4cd7 0xc004ab4cd8}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"457280b2-b926-457f-a984-e283662af4d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ft99t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ft99t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-161.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:31.000: INFO: Pod "webserver-deployment-67bd4bf6dc-m46lx" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-m46lx webserver-deployment-67bd4bf6dc- deployment-9044  2507d54f-0d7d-4739-a782-ec46654a0386 265797 0 2023-04-28 17:04:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 457280b2-b926-457f-a984-e283662af4d7 0xc004ab4e30 0xc004ab4e31}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"457280b2-b926-457f-a984-e283662af4d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r28dm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r28dm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:31.000: INFO: Pod "webserver-deployment-67bd4bf6dc-mz7t8" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-mz7t8 webserver-deployment-67bd4bf6dc- deployment-9044  4a5f055f-f190-48f4-a5f4-5b132f7887b0 265645 0 2023-04-28 17:04:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:3913293917fece3e732d7dd467662f7f9b08f11f52c5fbf2bdb1647cef9e7513 cni.projectcalico.org/podIP:10.42.2.195/32 cni.projectcalico.org/podIPs:10.42.2.195/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 457280b2-b926-457f-a984-e283662af4d7 0xc004ab4f90 0xc004ab4f91}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"457280b2-b926-457f-a984-e283662af4d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-28 17:04:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-28 17:04:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.195\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kvm9p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kvm9p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-71.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.71,PodIP:10.42.2.195,StartTime:2023-04-28 17:04:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:04:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://18ad77d8c61a668d27d3a52ef01a571e1d884fa98d892f618f930c649e615698,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.195,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:31.000: INFO: Pod "webserver-deployment-67bd4bf6dc-p2qgg" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-p2qgg webserver-deployment-67bd4bf6dc- deployment-9044  0bec4fdf-86ce-445a-88f2-35f65c1fedba 265806 0 2023-04-28 17:04:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 457280b2-b926-457f-a984-e283662af4d7 0xc004ab5187 0xc004ab5188}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"457280b2-b926-457f-a984-e283662af4d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kpwk7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kpwk7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-127.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:31.000: INFO: Pod "webserver-deployment-67bd4bf6dc-rbwtd" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rbwtd webserver-deployment-67bd4bf6dc- deployment-9044  f6d9127a-cdd8-4907-bfa2-9ce874734dae 265633 0 2023-04-28 17:04:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:88444940491ef8b572ee3d83b6828aaf71c7303b5e7d58b0d2dc41d0a5092a5d cni.projectcalico.org/podIP:10.42.0.162/32 cni.projectcalico.org/podIPs:10.42.0.162/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 457280b2-b926-457f-a984-e283662af4d7 0xc004ab5300 0xc004ab5301}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"457280b2-b926-457f-a984-e283662af4d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-28 17:04:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-28 17:04:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zrw7m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zrw7m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-14-195.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.14.195,PodIP:10.42.0.162,StartTime:2023-04-28 17:04:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:04:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8ce275df574d9092585bd40a6a0424d59cf59aba913f47bed9dfa70429ed4baa,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.162,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:31.000: INFO: Pod "webserver-deployment-67bd4bf6dc-rl2bw" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rl2bw webserver-deployment-67bd4bf6dc- deployment-9044  c4f17bd5-b615-4a4a-b255-f110ea1ce2ab 265641 0 2023-04-28 17:04:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:f1eb7c3aa7e58abc9d7c07bc48a61a367470c082883a1b065f70dfc81c6049b9 cni.projectcalico.org/podIP:10.42.2.197/32 cni.projectcalico.org/podIPs:10.42.2.197/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 457280b2-b926-457f-a984-e283662af4d7 0xc004ab5517 0xc004ab5518}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"457280b2-b926-457f-a984-e283662af4d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-28 17:04:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-28 17:04:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.197\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x7tzm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x7tzm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-71.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.71,PodIP:10.42.2.197,StartTime:2023-04-28 17:04:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:04:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a3b10f1d1b9bc3823a93abfc7993d8bf0518f881145585b099baf639f407a0ad,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.197,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:31.001: INFO: Pod "webserver-deployment-67bd4bf6dc-swz62" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-swz62 webserver-deployment-67bd4bf6dc- deployment-9044  5a9be1a9-d9e2-48ee-894b-e0f6a6015053 265657 0 2023-04-28 17:04:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:4c102faf76ba567b251afb16446870dbe6636a3fd0804361c7da8fb230e4ea0c cni.projectcalico.org/podIP:10.42.3.50/32 cni.projectcalico.org/podIPs:10.42.3.50/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 457280b2-b926-457f-a984-e283662af4d7 0xc004ab5737 0xc004ab5738}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"457280b2-b926-457f-a984-e283662af4d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-28 17:04:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-28 17:04:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hgtn2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hgtn2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-127.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.127,PodIP:10.42.3.50,StartTime:2023-04-28 17:04:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:04:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://56b739a458b23c48e056dda7b66999fa576809dccae84c4d7b2cf291bb5b793a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.50,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:31.001: INFO: Pod "webserver-deployment-67bd4bf6dc-tmp9n" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-tmp9n webserver-deployment-67bd4bf6dc- deployment-9044  2a4afba5-3848-477a-8b54-5aa7addc8226 265799 0 2023-04-28 17:04:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 457280b2-b926-457f-a984-e283662af4d7 0xc004ab5950 0xc004ab5951}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"457280b2-b926-457f-a984-e283662af4d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dkxgm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dkxgm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:31.001: INFO: Pod "webserver-deployment-67bd4bf6dc-xd5hf" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xd5hf webserver-deployment-67bd4bf6dc- deployment-9044  03f664b6-b360-42a8-92f2-f4bbb713646e 265648 0 2023-04-28 17:04:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:bdae36088c2f9085bbca579611d2847ff80e4f91c50bce95d260e6cda4bccfcc cni.projectcalico.org/podIP:10.42.2.196/32 cni.projectcalico.org/podIPs:10.42.2.196/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 457280b2-b926-457f-a984-e283662af4d7 0xc004ab5ac0 0xc004ab5ac1}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"457280b2-b926-457f-a984-e283662af4d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-28 17:04:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-28 17:04:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.196\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fg662,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fg662,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-71.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.71,PodIP:10.42.2.196,StartTime:2023-04-28 17:04:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:04:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f183538f5e9202a044f2f39a705569dcc042711b5c2e6a0bcbe12c79efc6b8c4,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.196,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:31.002: INFO: Pod "webserver-deployment-67bd4bf6dc-zwxxd" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-zwxxd webserver-deployment-67bd4bf6dc- deployment-9044  e6e40468-dbc7-4c27-931e-4175a21a48b8 265805 0 2023-04-28 17:04:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 457280b2-b926-457f-a984-e283662af4d7 0xc004ab5eb7 0xc004ab5eb8}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"457280b2-b926-457f-a984-e283662af4d7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-28 17:04:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8kh9h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8kh9h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-127.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.127,PodIP:,StartTime:2023-04-28 17:04:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:31.003: INFO: Pod "webserver-deployment-7b75d79cf5-2pft4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-2pft4 webserver-deployment-7b75d79cf5- deployment-9044  6a999aa9-7e5a-43a0-b5d4-954d2baf57e9 265775 0 2023-04-28 17:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:1e2a086f986b1fb583f479991928c5119a02f3748ceb78da52e75d076273e138 cni.projectcalico.org/podIP:10.42.0.164/32 cni.projectcalico.org/podIPs:10.42.0.164/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8fd4c438-3f97-497e-b412-ca041e65556e 0xc002f04187 0xc002f04188}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8fd4c438-3f97-497e-b412-ca041e65556e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-28 17:04:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-28 17:04:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.164\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x25zh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x25zh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-14-195.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.14.195,PodIP:10.42.0.164,StartTime:2023-04-28 17:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.0.164,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:31.004: INFO: Pod "webserver-deployment-7b75d79cf5-5d5kc" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-5d5kc webserver-deployment-7b75d79cf5- deployment-9044  0c16b644-fd04-49e6-a83a-95083c21129a 265800 0 2023-04-28 17:04:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8fd4c438-3f97-497e-b412-ca041e65556e 0xc002f04540 0xc002f04541}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8fd4c438-3f97-497e-b412-ca041e65556e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2xp2p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2xp2p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:31.004: INFO: Pod "webserver-deployment-7b75d79cf5-6d6vg" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-6d6vg webserver-deployment-7b75d79cf5- deployment-9044  e3709f10-702f-4e80-b133-c3e4588f1142 265798 0 2023-04-28 17:04:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8fd4c438-3f97-497e-b412-ca041e65556e 0xc002f04690 0xc002f04691}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8fd4c438-3f97-497e-b412-ca041e65556e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bjtmk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bjtmk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:31.005: INFO: Pod "webserver-deployment-7b75d79cf5-6pc6w" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-6pc6w webserver-deployment-7b75d79cf5- deployment-9044  902b984e-9b56-446c-ab5e-c53bc97be520 265728 0 2023-04-28 17:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:bc78f1919ef3d6d4d159844fec0c69304024c10d0484407d75668b034ae4a048 cni.projectcalico.org/podIP:10.42.3.53/32 cni.projectcalico.org/podIPs:10.42.3.53/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8fd4c438-3f97-497e-b412-ca041e65556e 0xc002f04800 0xc002f04801}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8fd4c438-3f97-497e-b412-ca041e65556e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-28 17:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-28 17:04:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k2ddh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k2ddh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-127.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.127,PodIP:,StartTime:2023-04-28 17:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:31.006: INFO: Pod "webserver-deployment-7b75d79cf5-fvbq6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-fvbq6 webserver-deployment-7b75d79cf5- deployment-9044  284bb43a-587c-45a9-86ff-cd223805eaba 265765 0 2023-04-28 17:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:0e7a24a3b97c8cbbb9ccbd32f19210dbc7f66527ea5283dec9edbadac9c14fee cni.projectcalico.org/podIP:10.42.1.173/32 cni.projectcalico.org/podIPs:10.42.1.173/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8fd4c438-3f97-497e-b412-ca041e65556e 0xc002f04a17 0xc002f04a18}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8fd4c438-3f97-497e-b412-ca041e65556e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-28 17:04:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-28 17:04:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bpxn7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bpxn7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-161.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.161,PodIP:10.42.1.173,StartTime:2023-04-28 17:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.173,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:31.007: INFO: Pod "webserver-deployment-7b75d79cf5-kbfdr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-kbfdr webserver-deployment-7b75d79cf5- deployment-9044  dbb88a41-cb75-4290-912f-f18627cc404f 265796 0 2023-04-28 17:04:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8fd4c438-3f97-497e-b412-ca041e65556e 0xc002f04c40 0xc002f04c41}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8fd4c438-3f97-497e-b412-ca041e65556e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mk5kb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mk5kb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-71.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:31.007: INFO: Pod "webserver-deployment-7b75d79cf5-rvhp6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-rvhp6 webserver-deployment-7b75d79cf5- deployment-9044  66500bd2-109f-40a9-95d6-7c6f8ca2c2ca 265759 0 2023-04-28 17:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:42171174f25e8e3987c7a9c129283bfb3dffe335a7b390ce2f27592bff5d9261 cni.projectcalico.org/podIP:10.42.2.198/32 cni.projectcalico.org/podIPs:10.42.2.198/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8fd4c438-3f97-497e-b412-ca041e65556e 0xc002f04dc0 0xc002f04dc1}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8fd4c438-3f97-497e-b412-ca041e65556e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-28 17:04:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-28 17:04:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-547x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-547x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-71.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.71,PodIP:10.42.2.198,StartTime:2023-04-28 17:04:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.198,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:31.008: INFO: Pod "webserver-deployment-7b75d79cf5-s7v92" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-s7v92 webserver-deployment-7b75d79cf5- deployment-9044  296cf270-6acc-4a68-9d95-6ac10edeceec 265731 0 2023-04-28 17:04:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:448bda8f611f923afadcd90306097640068348bbb4459c054055d451e8b31d7c cni.projectcalico.org/podIP:10.42.3.54/32 cni.projectcalico.org/podIPs:10.42.3.54/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8fd4c438-3f97-497e-b412-ca041e65556e 0xc002f05000 0xc002f05001}] [] [{kube-controller-manager Update v1 2023-04-28 17:04:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8fd4c438-3f97-497e-b412-ca041e65556e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-28 17:04:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-28 17:04:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9f8m2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9f8m2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-127.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:04:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.127,PodIP:,StartTime:2023-04-28 17:04:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:04:31.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9044" for this suite. @ 04/28/23 17:04:31.046
• [6.311 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 04/28/23 17:04:31.081
  Apr 28 17:04:31.081: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 17:04:31.082
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:04:31.122
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:04:31.128
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 04/28/23 17:04:31.131
  E0428 17:04:31.801808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:32.802236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:33.802379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:34.802505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:04:35.159
  Apr 28 17:04:35.162: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-5b7bcc60-0661-4346-b4bb-7967e9c61b46 container test-container: <nil>
  STEP: delete the pod @ 04/28/23 17:04:35.167
  Apr 28 17:04:35.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7649" for this suite. @ 04/28/23 17:04:35.184
• [4.107 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 04/28/23 17:04:35.188
  Apr 28 17:04:35.188: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:04:35.189
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:04:35.202
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:04:35.204
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 17:04:35.207
  E0428 17:04:35.805877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:36.806891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:37.807731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:38.808066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:04:39.227
  Apr 28 17:04:39.229: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod downwardapi-volume-ffc1f46c-8089-4a45-bf29-ef211551f239 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 17:04:39.235
  Apr 28 17:04:39.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9196" for this suite. @ 04/28/23 17:04:39.252
• [4.076 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 04/28/23 17:04:39.265
  Apr 28 17:04:39.265: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename replicaset @ 04/28/23 17:04:39.266
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:04:39.281
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:04:39.284
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 04/28/23 17:04:39.287
  E0428 17:04:39.809199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:40.809274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replicaset with a matching selector is created @ 04/28/23 17:04:41.304
  STEP: Then the orphan pod is adopted @ 04/28/23 17:04:41.308
  E0428 17:04:41.810175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When the matched label of one of its pods change @ 04/28/23 17:04:42.315
  Apr 28 17:04:42.318: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 04/28/23 17:04:42.333
  E0428 17:04:42.810892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:04:43.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-676" for this suite. @ 04/28/23 17:04:43.348
• [4.090 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 04/28/23 17:04:43.361
  Apr 28 17:04:43.361: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 17:04:43.362
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:04:43.395
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:04:43.397
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 17:04:43.404
  E0428 17:04:43.811023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:44.811362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:45.812320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:46.812390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:04:47.426
  Apr 28 17:04:47.428: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod downwardapi-volume-10f836a5-6b9c-4e42-a5aa-794a7c9b0454 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 17:04:47.433
  Apr 28 17:04:47.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3756" for this suite. @ 04/28/23 17:04:47.448
• [4.091 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 04/28/23 17:04:47.453
  Apr 28 17:04:47.453: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename taint-multiple-pods @ 04/28/23 17:04:47.454
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:04:47.466
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:04:47.47
  Apr 28 17:04:47.474: INFO: Waiting up to 1m0s for all nodes to be ready
  E0428 17:04:47.812715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:48.813783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:49.814247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:50.814335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:51.814915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:52.815854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:53.816319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:54.816558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:55.816673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:56.817165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:57.817771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:58.818819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:04:59.818938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:00.819208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:01.820244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:02.820759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:03.820849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:04.821903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:05.821956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:06.822248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:07.822786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:08.823083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:09.823587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:10.824409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:11.825263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:12.826216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:13.826932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:14.827168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:15.827518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:16.827600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:17.828114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:18.828239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:19.829153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:20.829212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:21.829794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:22.830101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:23.830669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:24.830706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:25.830773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:26.830873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:27.831816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:28.831985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:29.832811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:30.832960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:31.833756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:32.834589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:33.834633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:34.834873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:35.835034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:36.835225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:37.836350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:38.836487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:39.837006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:40.837189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:41.837295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:42.838144      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:43.838894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:44.839144      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:45.839608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:46.839754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:05:47.513: INFO: Waiting for terminating namespaces to be deleted...
  Apr 28 17:05:47.516: INFO: Starting informer...
  STEP: Starting pods... @ 04/28/23 17:05:47.516
  Apr 28 17:05:47.734: INFO: Pod1 is running on ip-172-31-9-127.us-east-2.compute.internal. Tainting Node
  E0428 17:05:47.840482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:48.840547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:49.841265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:05:49.969: INFO: Pod2 is running on ip-172-31-9-127.us-east-2.compute.internal. Tainting Node
  STEP: Trying to apply a taint on the Node @ 04/28/23 17:05:49.969
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/28/23 17:05:49.994
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 04/28/23 17:05:50.018
  E0428 17:05:50.842475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:51.842593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:52.842892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:53.843294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:54.843484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:55.844493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:05:56.390: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  E0428 17:05:56.845386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:57.846014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:58.846174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:05:59.846198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:00.846363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:01.846630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:02.847040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:03.847180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:04.847372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:05.847571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:06.847716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:07.847840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:08.848054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:09.848170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:10.853231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:11.853340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:12.853735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:13.853872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:14.854334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:15.854461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:06:16.446: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Apr 28 17:06:16.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/28/23 17:06:16.468
  STEP: Destroying namespace "taint-multiple-pods-3639" for this suite. @ 04/28/23 17:06:16.476
• [89.044 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 04/28/23 17:06:16.503
  Apr 28 17:06:16.503: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename services @ 04/28/23 17:06:16.505
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:06:16.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:06:16.528
  STEP: creating a Service @ 04/28/23 17:06:16.535
  STEP: watching for the Service to be added @ 04/28/23 17:06:16.543
  Apr 28 17:06:16.545: INFO: Found Service test-service-sx5dp in namespace services-7619 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Apr 28 17:06:16.545: INFO: Service test-service-sx5dp created
  STEP: Getting /status @ 04/28/23 17:06:16.546
  Apr 28 17:06:16.550: INFO: Service test-service-sx5dp has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 04/28/23 17:06:16.55
  STEP: watching for the Service to be patched @ 04/28/23 17:06:16.555
  Apr 28 17:06:16.559: INFO: observed Service test-service-sx5dp in namespace services-7619 with annotations: map[] & LoadBalancer: {[]}
  Apr 28 17:06:16.560: INFO: Found Service test-service-sx5dp in namespace services-7619 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Apr 28 17:06:16.560: INFO: Service test-service-sx5dp has service status patched
  STEP: updating the ServiceStatus @ 04/28/23 17:06:16.56
  Apr 28 17:06:16.570: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 04/28/23 17:06:16.571
  Apr 28 17:06:16.573: INFO: Observed Service test-service-sx5dp in namespace services-7619 with annotations: map[] & Conditions: {[]}
  Apr 28 17:06:16.573: INFO: Observed event: &Service{ObjectMeta:{test-service-sx5dp  services-7619  b1ea61c7-705e-4147-8f1a-2324e17ec334 266759 0 2023-04-28 17:06:16 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-04-28 17:06:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-04-28 17:06:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.43.61.188,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.43.61.188],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Apr 28 17:06:16.573: INFO: Found Service test-service-sx5dp in namespace services-7619 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 28 17:06:16.574: INFO: Service test-service-sx5dp has service status updated
  STEP: patching the service @ 04/28/23 17:06:16.574
  STEP: watching for the Service to be patched @ 04/28/23 17:06:16.58
  Apr 28 17:06:16.582: INFO: observed Service test-service-sx5dp in namespace services-7619 with labels: map[test-service-static:true]
  Apr 28 17:06:16.583: INFO: observed Service test-service-sx5dp in namespace services-7619 with labels: map[test-service-static:true]
  Apr 28 17:06:16.583: INFO: observed Service test-service-sx5dp in namespace services-7619 with labels: map[test-service-static:true]
  Apr 28 17:06:16.583: INFO: Found Service test-service-sx5dp in namespace services-7619 with labels: map[test-service:patched test-service-static:true]
  Apr 28 17:06:16.583: INFO: Service test-service-sx5dp patched
  STEP: deleting the service @ 04/28/23 17:06:16.583
  STEP: watching for the Service to be deleted @ 04/28/23 17:06:16.595
  Apr 28 17:06:16.597: INFO: Observed event: ADDED
  Apr 28 17:06:16.597: INFO: Observed event: MODIFIED
  Apr 28 17:06:16.597: INFO: Observed event: MODIFIED
  Apr 28 17:06:16.598: INFO: Observed event: MODIFIED
  Apr 28 17:06:16.598: INFO: Found Service test-service-sx5dp in namespace services-7619 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Apr 28 17:06:16.598: INFO: Service test-service-sx5dp deleted
  Apr 28 17:06:16.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7619" for this suite. @ 04/28/23 17:06:16.602
• [0.103 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 04/28/23 17:06:16.61
  Apr 28 17:06:16.610: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename sched-pred @ 04/28/23 17:06:16.611
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:06:16.635
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:06:16.638
  Apr 28 17:06:16.641: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 28 17:06:16.647: INFO: Waiting for terminating namespaces to be deleted...
  Apr 28 17:06:16.649: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-11-161.us-east-2.compute.internal before test
  Apr 28 17:06:16.656: INFO: cloud-controller-manager-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:02 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.657: INFO: 	Container cloud-controller-manager ready: true, restart count 0
  Apr 28 17:06:16.657: INFO: etcd-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:46:40 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.657: INFO: 	Container etcd ready: true, restart count 0
  Apr 28 17:06:16.657: INFO: kube-apiserver-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:46:57 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.657: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 28 17:06:16.657: INFO: kube-controller-manager-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:02 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.657: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 28 17:06:16.657: INFO: kube-proxy-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:05 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.657: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 28 17:06:16.657: INFO: kube-scheduler-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:02 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.657: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 28 17:06:16.657: INFO: rke2-canal-8td74 from kube-system started at 2023-04-28 03:47:01 +0000 UTC (2 container statuses recorded)
  Apr 28 17:06:16.657: INFO: 	Container calico-node ready: true, restart count 0
  Apr 28 17:06:16.657: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 28 17:06:16.657: INFO: rke2-coredns-rke2-coredns-5896cccb79-vcgcz from kube-system started at 2023-04-28 03:47:28 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.657: INFO: 	Container coredns ready: true, restart count 0
  Apr 28 17:06:16.657: INFO: rke2-ingress-nginx-controller-6f4sz from kube-system started at 2023-04-28 03:47:28 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.657: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
  Apr 28 17:06:16.657: INFO: sonobuoy-systemd-logs-daemon-set-19d55eb1839b4b26-n6bzk from sonobuoy started at 2023-04-28 16:37:21 +0000 UTC (2 container statuses recorded)
  Apr 28 17:06:16.657: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 17:06:16.657: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 17:06:16.657: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-14-195.us-east-2.compute.internal before test
  Apr 28 17:06:16.666: INFO: cloud-controller-manager-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:44:16 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.666: INFO: 	Container cloud-controller-manager ready: true, restart count 0
  Apr 28 17:06:16.666: INFO: etcd-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:43:55 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.666: INFO: 	Container etcd ready: true, restart count 0
  Apr 28 17:06:16.666: INFO: helm-install-rke2-canal-8zdl7 from kube-system started at 2023-04-28 03:44:29 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.666: INFO: 	Container helm ready: false, restart count 0
  Apr 28 17:06:16.666: INFO: helm-install-rke2-coredns-8tn5j from kube-system started at 2023-04-28 03:44:29 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.666: INFO: 	Container helm ready: false, restart count 1
  Apr 28 17:06:16.666: INFO: helm-install-rke2-ingress-nginx-v5xks from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.666: INFO: 	Container helm ready: false, restart count 0
  Apr 28 17:06:16.666: INFO: helm-install-rke2-metrics-server-66776 from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.666: INFO: 	Container helm ready: false, restart count 0
  Apr 28 17:06:16.666: INFO: helm-install-rke2-snapshot-controller-crd-xc2xl from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.666: INFO: 	Container helm ready: false, restart count 0
  Apr 28 17:06:16.666: INFO: helm-install-rke2-snapshot-controller-g6gdp from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.666: INFO: 	Container helm ready: false, restart count 0
  Apr 28 17:06:16.666: INFO: helm-install-rke2-snapshot-validation-webhook-6rwkq from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.666: INFO: 	Container helm ready: false, restart count 0
  Apr 28 17:06:16.666: INFO: kube-apiserver-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:44:09 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.666: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 28 17:06:16.666: INFO: kube-controller-manager-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:44:14 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.666: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 28 17:06:16.666: INFO: kube-proxy-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:44:19 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.666: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 28 17:06:16.666: INFO: kube-scheduler-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:44:14 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.666: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 28 17:06:16.666: INFO: rke2-canal-sz474 from kube-system started at 2023-04-28 03:44:35 +0000 UTC (2 container statuses recorded)
  Apr 28 17:06:16.666: INFO: 	Container calico-node ready: true, restart count 0
  Apr 28 17:06:16.666: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 28 17:06:16.666: INFO: rke2-coredns-rke2-coredns-5896cccb79-lb8dr from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.666: INFO: 	Container coredns ready: true, restart count 0
  Apr 28 17:06:16.666: INFO: rke2-coredns-rke2-coredns-autoscaler-f6766cdc9-qrz4d from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.666: INFO: 	Container autoscaler ready: true, restart count 0
  Apr 28 17:06:16.666: INFO: rke2-ingress-nginx-controller-l9r4l from kube-system started at 2023-04-28 03:45:10 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.666: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
  Apr 28 17:06:16.666: INFO: rke2-metrics-server-6d45f6cb4d-587zc from kube-system started at 2023-04-28 03:45:06 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.666: INFO: 	Container metrics-server ready: true, restart count 0
  Apr 28 17:06:16.666: INFO: rke2-snapshot-controller-7bf6d7bf5f-swpkx from kube-system started at 2023-04-28 03:45:23 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.666: INFO: 	Container rke2-snapshot-controller ready: true, restart count 0
  Apr 28 17:06:16.666: INFO: rke2-snapshot-validation-webhook-b65d46c9f-8jsvg from kube-system started at 2023-04-28 03:45:24 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.666: INFO: 	Container rke2-snapshot-validation-webhook ready: true, restart count 0
  Apr 28 17:06:16.666: INFO: sonobuoy-systemd-logs-daemon-set-19d55eb1839b4b26-bxxcq from sonobuoy started at 2023-04-28 16:37:21 +0000 UTC (2 container statuses recorded)
  Apr 28 17:06:16.666: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 17:06:16.666: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 17:06:16.666: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-6-71.us-east-2.compute.internal before test
  Apr 28 17:06:16.675: INFO: cloud-controller-manager-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:18 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.675: INFO: 	Container cloud-controller-manager ready: true, restart count 0
  Apr 28 17:06:16.675: INFO: etcd-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:01 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.675: INFO: 	Container etcd ready: true, restart count 0
  Apr 28 17:06:16.675: INFO: kube-apiserver-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:13 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.675: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 28 17:06:16.675: INFO: kube-controller-manager-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:18 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.675: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 28 17:06:16.675: INFO: kube-proxy-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:20 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.675: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 28 17:06:16.676: INFO: kube-scheduler-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:18 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.676: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 28 17:06:16.676: INFO: rke2-canal-8rz4z from kube-system started at 2023-04-28 03:47:20 +0000 UTC (2 container statuses recorded)
  Apr 28 17:06:16.676: INFO: 	Container calico-node ready: true, restart count 0
  Apr 28 17:06:16.676: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 28 17:06:16.676: INFO: rke2-ingress-nginx-controller-xfwz8 from kube-system started at 2023-04-28 03:47:42 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.676: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
  Apr 28 17:06:16.676: INFO: sonobuoy-systemd-logs-daemon-set-19d55eb1839b4b26-98t5f from sonobuoy started at 2023-04-28 16:37:21 +0000 UTC (2 container statuses recorded)
  Apr 28 17:06:16.676: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 17:06:16.676: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 17:06:16.676: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-9-127.us-east-2.compute.internal before test
  Apr 28 17:06:16.685: INFO: kube-proxy-ip-172-31-9-127.us-east-2.compute.internal from kube-system started at 2023-04-28 03:48:58 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.685: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 28 17:06:16.685: INFO: rke2-canal-lc6hn from kube-system started at 2023-04-28 03:48:58 +0000 UTC (2 container statuses recorded)
  Apr 28 17:06:16.685: INFO: 	Container calico-node ready: true, restart count 0
  Apr 28 17:06:16.685: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 28 17:06:16.686: INFO: rke2-ingress-nginx-controller-qjl8l from kube-system started at 2023-04-28 17:06:16 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.686: INFO: 	Container rke2-ingress-nginx-controller ready: false, restart count 0
  Apr 28 17:06:16.686: INFO: sonobuoy from sonobuoy started at 2023-04-28 16:37:20 +0000 UTC (1 container statuses recorded)
  Apr 28 17:06:16.686: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 28 17:06:16.686: INFO: sonobuoy-e2e-job-92a64e7703e04ef2 from sonobuoy started at 2023-04-28 16:37:21 +0000 UTC (2 container statuses recorded)
  Apr 28 17:06:16.686: INFO: 	Container e2e ready: true, restart count 0
  Apr 28 17:06:16.686: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 17:06:16.686: INFO: sonobuoy-systemd-logs-daemon-set-19d55eb1839b4b26-jv86q from sonobuoy started at 2023-04-28 16:37:21 +0000 UTC (2 container statuses recorded)
  Apr 28 17:06:16.686: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 17:06:16.686: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 04/28/23 17:06:16.687
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.175a282b2f1ed27e], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match Pod's node affinity/selector. preemption: 0/4 nodes are available: 4 Preemption is not helpful for scheduling..] @ 04/28/23 17:06:16.725
  E0428 17:06:16.855932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:06:17.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-2501" for this suite. @ 04/28/23 17:06:17.726
• [1.123 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 04/28/23 17:06:17.734
  Apr 28 17:06:17.734: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 04/28/23 17:06:17.735
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:06:17.751
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:06:17.754
  STEP: Setting up the test @ 04/28/23 17:06:17.757
  STEP: Creating hostNetwork=false pod @ 04/28/23 17:06:17.757
  E0428 17:06:17.856244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:18.869296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 04/28/23 17:06:19.777
  E0428 17:06:19.869629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:20.874684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running the test @ 04/28/23 17:06:21.794
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 04/28/23 17:06:21.794
  Apr 28 17:06:21.794: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1369 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:06:21.794: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 17:06:21.794: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:06:21.794: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1369/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  E0428 17:06:21.875616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:06:21.890: INFO: Exec stderr: ""
  Apr 28 17:06:21.890: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1369 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:06:21.890: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 17:06:21.891: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:06:21.891: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1369/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 28 17:06:21.978: INFO: Exec stderr: ""
  Apr 28 17:06:21.978: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1369 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:06:21.978: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 17:06:21.978: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:06:21.978: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1369/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 28 17:06:22.049: INFO: Exec stderr: ""
  Apr 28 17:06:22.049: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1369 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:06:22.049: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 17:06:22.050: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:06:22.050: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1369/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 28 17:06:22.519: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 04/28/23 17:06:22.519
  Apr 28 17:06:22.519: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1369 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:06:22.519: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 17:06:22.520: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:06:22.520: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1369/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Apr 28 17:06:22.592: INFO: Exec stderr: ""
  Apr 28 17:06:22.592: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1369 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:06:22.592: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 17:06:22.592: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:06:22.592: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1369/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Apr 28 17:06:22.701: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 04/28/23 17:06:22.704
  Apr 28 17:06:22.704: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1369 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:06:22.704: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 17:06:22.705: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:06:22.705: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1369/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 28 17:06:22.800: INFO: Exec stderr: ""
  Apr 28 17:06:22.800: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1369 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:06:22.800: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 17:06:22.800: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:06:22.801: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1369/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  E0428 17:06:22.876418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:06:22.884: INFO: Exec stderr: ""
  Apr 28 17:06:22.884: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1369 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:06:22.884: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 17:06:22.884: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:06:22.885: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1369/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 28 17:06:22.971: INFO: Exec stderr: ""
  Apr 28 17:06:22.971: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1369 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:06:22.971: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 17:06:22.971: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:06:22.971: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1369/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 28 17:06:23.080: INFO: Exec stderr: ""
  Apr 28 17:06:23.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-1369" for this suite. @ 04/28/23 17:06:23.084
• [5.356 seconds]
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 04/28/23 17:06:23.09
  Apr 28 17:06:23.090: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename subpath @ 04/28/23 17:06:23.091
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:06:23.106
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:06:23.108
  STEP: Setting up data @ 04/28/23 17:06:23.11
  STEP: Creating pod pod-subpath-test-downwardapi-sm9v @ 04/28/23 17:06:23.117
  STEP: Creating a pod to test atomic-volume-subpath @ 04/28/23 17:06:23.117
  E0428 17:06:23.877381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:24.877534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:25.877652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:26.878272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:27.878938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:28.879003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:29.879491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:30.881270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:31.881367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:32.881962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:33.883017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:34.883474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:35.883976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:36.884179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:37.884762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:38.884968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:39.885986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:40.893221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:41.893750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:42.894308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:43.894419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:44.899188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:45.899291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:46.899348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:06:47.175
  Apr 28 17:06:47.177: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-subpath-test-downwardapi-sm9v container test-container-subpath-downwardapi-sm9v: <nil>
  STEP: delete the pod @ 04/28/23 17:06:47.192
  STEP: Deleting pod pod-subpath-test-downwardapi-sm9v @ 04/28/23 17:06:47.217
  Apr 28 17:06:47.217: INFO: Deleting pod "pod-subpath-test-downwardapi-sm9v" in namespace "subpath-7089"
  Apr 28 17:06:47.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-7089" for this suite. @ 04/28/23 17:06:47.222
• [24.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 04/28/23 17:06:47.232
  Apr 28 17:06:47.232: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename pod-network-test @ 04/28/23 17:06:47.233
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:06:47.249
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:06:47.254
  STEP: Performing setup for networking test in namespace pod-network-test-3117 @ 04/28/23 17:06:47.256
  STEP: creating a selector @ 04/28/23 17:06:47.256
  STEP: Creating the service pods in kubernetes @ 04/28/23 17:06:47.256
  Apr 28 17:06:47.256: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0428 17:06:47.901095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:48.901405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:49.901502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:50.902516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:51.902752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:52.903844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:53.904578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:54.904688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:55.905218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:56.905421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:57.905660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:58.905857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:06:59.906908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:00.908785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:01.909581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:02.910075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:03.910303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:04.910407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:05.911282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:06.911278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:07.911531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:08.911543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 04/28/23 17:07:09.425
  E0428 17:07:09.911734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:10.911867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:07:11.460: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
  Apr 28 17:07:11.460: INFO: Going to poll 10.42.1.174 on port 8083 at least 0 times, with a maximum of 46 tries before failing
  Apr 28 17:07:11.462: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.1.174:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3117 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:07:11.462: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 17:07:11.463: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:07:11.463: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-3117/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.42.1.174%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 28 17:07:11.534: INFO: Found all 1 expected endpoints: [netserver-0]
  Apr 28 17:07:11.534: INFO: Going to poll 10.42.0.165 on port 8083 at least 0 times, with a maximum of 46 tries before failing
  Apr 28 17:07:11.537: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.0.165:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3117 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:07:11.538: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 17:07:11.538: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:07:11.538: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-3117/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.42.0.165%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 28 17:07:11.611: INFO: Found all 1 expected endpoints: [netserver-1]
  Apr 28 17:07:11.611: INFO: Going to poll 10.42.2.199 on port 8083 at least 0 times, with a maximum of 46 tries before failing
  Apr 28 17:07:11.614: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.2.199:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3117 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:07:11.614: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 17:07:11.614: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:07:11.614: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-3117/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.42.2.199%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 28 17:07:11.704: INFO: Found all 1 expected endpoints: [netserver-2]
  Apr 28 17:07:11.704: INFO: Going to poll 10.42.3.65 on port 8083 at least 0 times, with a maximum of 46 tries before failing
  Apr 28 17:07:11.707: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.3.65:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3117 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:07:11.707: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 17:07:11.707: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:07:11.707: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-3117/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.42.3.65%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 28 17:07:11.794: INFO: Found all 1 expected endpoints: [netserver-3]
  Apr 28 17:07:11.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-3117" for this suite. @ 04/28/23 17:07:11.798
• [24.572 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 04/28/23 17:07:11.805
  Apr 28 17:07:11.805: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename subpath @ 04/28/23 17:07:11.81
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:07:11.826
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:07:11.829
  STEP: Setting up data @ 04/28/23 17:07:11.831
  STEP: Creating pod pod-subpath-test-configmap-mj7l @ 04/28/23 17:07:11.84
  STEP: Creating a pod to test atomic-volume-subpath @ 04/28/23 17:07:11.84
  E0428 17:07:11.912345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:12.912530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:13.912798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:14.913010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:15.913767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:16.913876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:17.916914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:18.917182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:19.917512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:20.918170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:21.919031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:22.920032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:23.920065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:24.920992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:25.921936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:26.922282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:27.922961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:28.923307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:29.923575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:30.923781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:31.924539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:32.926327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:33.926334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:34.926636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:07:35.925
  E0428 17:07:35.927626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:07:35.927: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-subpath-test-configmap-mj7l container test-container-subpath-configmap-mj7l: <nil>
  STEP: delete the pod @ 04/28/23 17:07:35.934
  STEP: Deleting pod pod-subpath-test-configmap-mj7l @ 04/28/23 17:07:35.948
  Apr 28 17:07:35.948: INFO: Deleting pod "pod-subpath-test-configmap-mj7l" in namespace "subpath-1693"
  Apr 28 17:07:35.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-1693" for this suite. @ 04/28/23 17:07:35.953
• [24.152 seconds]
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 04/28/23 17:07:35.958
  Apr 28 17:07:35.958: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename configmap @ 04/28/23 17:07:35.96
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:07:35.97
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:07:35.973
  STEP: Creating configMap that has name configmap-test-emptyKey-ff52608a-5f9b-4df0-8deb-7c1acd01d897 @ 04/28/23 17:07:35.975
  Apr 28 17:07:35.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5481" for this suite. @ 04/28/23 17:07:35.979
• [0.026 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 04/28/23 17:07:35.984
  Apr 28 17:07:35.984: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:07:35.985
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:07:36.003
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:07:36.006
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-cda70abb-c7ec-4e1b-bff4-b50091bf5df7 @ 04/28/23 17:07:36.011
  STEP: Creating the pod @ 04/28/23 17:07:36.016
  E0428 17:07:36.928701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:37.928994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap projected-configmap-test-upd-cda70abb-c7ec-4e1b-bff4-b50091bf5df7 @ 04/28/23 17:07:38.038
  STEP: waiting to observe update in volume @ 04/28/23 17:07:38.042
  E0428 17:07:38.929363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:39.930233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:40.930335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:41.931063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:07:42.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6052" for this suite. @ 04/28/23 17:07:42.066
• [6.086 seconds]
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 04/28/23 17:07:42.071
  Apr 28 17:07:42.071: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename replicaset @ 04/28/23 17:07:42.072
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:07:42.084
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:07:42.086
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 04/28/23 17:07:42.088
  Apr 28 17:07:42.096: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0428 17:07:42.932317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:43.932403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:44.932527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:45.932643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:46.932758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:07:47.099: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/28/23 17:07:47.099
  STEP: getting scale subresource @ 04/28/23 17:07:47.099
  STEP: updating a scale subresource @ 04/28/23 17:07:47.103
  STEP: verifying the replicaset Spec.Replicas was modified @ 04/28/23 17:07:47.111
  STEP: Patch a scale subresource @ 04/28/23 17:07:47.118
  Apr 28 17:07:47.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-6151" for this suite. @ 04/28/23 17:07:47.157
• [5.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 04/28/23 17:07:47.174
  Apr 28 17:07:47.174: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 17:07:47.175
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:07:47.206
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:07:47.208
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 17:07:47.212
  E0428 17:07:47.939684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:48.939751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:49.939871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:50.939883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:07:51.233
  Apr 28 17:07:51.235: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod downwardapi-volume-a163658d-aee3-4f6a-9f2a-c7c6995eae15 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 17:07:51.24
  Apr 28 17:07:51.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3240" for this suite. @ 04/28/23 17:07:51.258
• [4.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 04/28/23 17:07:51.264
  Apr 28 17:07:51.264: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename webhook @ 04/28/23 17:07:51.265
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:07:51.278
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:07:51.28
  STEP: Setting up server cert @ 04/28/23 17:07:51.298
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 17:07:51.624
  STEP: Deploying the webhook pod @ 04/28/23 17:07:51.644
  STEP: Wait for the deployment to be ready @ 04/28/23 17:07:51.662
  Apr 28 17:07:51.675: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0428 17:07:51.940613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:52.941419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 17:07:53.686
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 17:07:53.7
  E0428 17:07:53.941527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:07:54.700: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 04/28/23 17:07:54.703
  STEP: create a namespace for the webhook @ 04/28/23 17:07:54.719
  STEP: create a configmap should be unconditionally rejected by the webhook @ 04/28/23 17:07:54.747
  Apr 28 17:07:54.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2532" for this suite. @ 04/28/23 17:07:54.851
  STEP: Destroying namespace "webhook-markers-4716" for this suite. @ 04/28/23 17:07:54.873
  STEP: Destroying namespace "fail-closed-namespace-5084" for this suite. @ 04/28/23 17:07:54.885
• [3.633 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 04/28/23 17:07:54.898
  Apr 28 17:07:54.898: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename namespaces @ 04/28/23 17:07:54.898
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:07:54.923
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:07:54.931
  STEP: Creating a test namespace @ 04/28/23 17:07:54.933
  E0428 17:07:54.942531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:07:54.956
  STEP: Creating a service in the namespace @ 04/28/23 17:07:54.961
  STEP: Deleting the namespace @ 04/28/23 17:07:54.987
  STEP: Waiting for the namespace to be removed. @ 04/28/23 17:07:55
  E0428 17:07:55.943415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:56.943530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:57.943545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:58.943785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:07:59.944390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:00.944576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 04/28/23 17:08:01.003
  STEP: Verifying there is no service in the namespace @ 04/28/23 17:08:01.015
  Apr 28 17:08:01.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4265" for this suite. @ 04/28/23 17:08:01.022
  STEP: Destroying namespace "nsdeletetest-7334" for this suite. @ 04/28/23 17:08:01.027
  Apr 28 17:08:01.029: INFO: Namespace nsdeletetest-7334 was already deleted
  STEP: Destroying namespace "nsdeletetest-9517" for this suite. @ 04/28/23 17:08:01.03
• [6.137 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 04/28/23 17:08:01.036
  Apr 28 17:08:01.036: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename certificates @ 04/28/23 17:08:01.036
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:08:01.053
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:08:01.057
  STEP: getting /apis @ 04/28/23 17:08:01.77
  STEP: getting /apis/certificates.k8s.io @ 04/28/23 17:08:01.776
  STEP: getting /apis/certificates.k8s.io/v1 @ 04/28/23 17:08:01.779
  STEP: creating @ 04/28/23 17:08:01.78
  STEP: getting @ 04/28/23 17:08:01.814
  STEP: listing @ 04/28/23 17:08:01.821
  STEP: watching @ 04/28/23 17:08:01.831
  Apr 28 17:08:01.831: INFO: starting watch
  STEP: patching @ 04/28/23 17:08:01.833
  STEP: updating @ 04/28/23 17:08:01.846
  Apr 28 17:08:01.858: INFO: waiting for watch events with expected annotations
  Apr 28 17:08:01.858: INFO: saw patched and updated annotations
  STEP: getting /approval @ 04/28/23 17:08:01.858
  STEP: patching /approval @ 04/28/23 17:08:01.865
  STEP: updating /approval @ 04/28/23 17:08:01.878
  STEP: getting /status @ 04/28/23 17:08:01.885
  STEP: patching /status @ 04/28/23 17:08:01.89
  STEP: updating /status @ 04/28/23 17:08:01.896
  STEP: deleting @ 04/28/23 17:08:01.902
  STEP: deleting a collection @ 04/28/23 17:08:01.912
  Apr 28 17:08:01.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-7646" for this suite. @ 04/28/23 17:08:01.926
• [0.895 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 04/28/23 17:08:01.934
  Apr 28 17:08:01.934: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename events @ 04/28/23 17:08:01.934
  E0428 17:08:01.944670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:08:01.948
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:08:01.95
  STEP: Create set of events @ 04/28/23 17:08:01.952
  STEP: get a list of Events with a label in the current namespace @ 04/28/23 17:08:01.964
  STEP: delete a list of events @ 04/28/23 17:08:01.967
  Apr 28 17:08:01.967: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 04/28/23 17:08:01.983
  Apr 28 17:08:01.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-3462" for this suite. @ 04/28/23 17:08:01.99
• [0.061 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 04/28/23 17:08:01.998
  Apr 28 17:08:01.998: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 17:08:01.999
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:08:02.015
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:08:02.018
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/28/23 17:08:02.02
  Apr 28 17:08:02.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-2937 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Apr 28 17:08:02.097: INFO: stderr: ""
  Apr 28 17:08:02.097: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 04/28/23 17:08:02.097
  Apr 28 17:08:02.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-2937 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Apr 28 17:08:02.176: INFO: stderr: ""
  Apr 28 17:08:02.176: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/28/23 17:08:02.176
  Apr 28 17:08:02.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-2937 delete pods e2e-test-httpd-pod'
  E0428 17:08:02.944711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:03.945033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:08:04.880: INFO: stderr: ""
  Apr 28 17:08:04.880: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 28 17:08:04.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2937" for this suite. @ 04/28/23 17:08:04.884
• [2.893 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 04/28/23 17:08:04.892
  Apr 28 17:08:04.892: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename csiinlinevolumes @ 04/28/23 17:08:04.893
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:08:04.922
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:08:04.926
  STEP: creating @ 04/28/23 17:08:04.929
  E0428 17:08:04.944968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting @ 04/28/23 17:08:04.95
  STEP: listing @ 04/28/23 17:08:04.962
  STEP: deleting @ 04/28/23 17:08:04.968
  Apr 28 17:08:04.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-5765" for this suite. @ 04/28/23 17:08:05.011
• [0.127 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 04/28/23 17:08:05.019
  Apr 28 17:08:05.019: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename containers @ 04/28/23 17:08:05.02
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:08:05.052
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:08:05.057
  STEP: Creating a pod to test override command @ 04/28/23 17:08:05.059
  E0428 17:08:05.945996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:06.946106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:07.947123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:08.947307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:08:09.078
  Apr 28 17:08:09.080: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod client-containers-a568bfd4-d63e-4c45-91ab-100335d076c7 container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 17:08:09.085
  Apr 28 17:08:09.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-3270" for this suite. @ 04/28/23 17:08:09.099
• [4.086 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 04/28/23 17:08:09.105
  Apr 28 17:08:09.105: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 17:08:09.106
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:08:09.118
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:08:09.121
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 04/28/23 17:08:09.123
  E0428 17:08:09.947341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:10.947420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:11.947834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:12.950764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:08:13.139
  Apr 28 17:08:13.141: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-22785b50-c908-43af-86b4-ce310bac0856 container test-container: <nil>
  STEP: delete the pod @ 04/28/23 17:08:13.147
  Apr 28 17:08:13.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3400" for this suite. @ 04/28/23 17:08:13.165
• [4.064 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 04/28/23 17:08:13.171
  Apr 28 17:08:13.171: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename security-context-test @ 04/28/23 17:08:13.172
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:08:13.187
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:08:13.19
  E0428 17:08:13.951394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:14.951476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:15.951761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:16.952611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:08:17.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-5069" for this suite. @ 04/28/23 17:08:17.222
• [4.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 04/28/23 17:08:17.228
  Apr 28 17:08:17.228: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename runtimeclass @ 04/28/23 17:08:17.229
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:08:17.243
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:08:17.246
  STEP: Deleting RuntimeClass runtimeclass-2586-delete-me @ 04/28/23 17:08:17.253
  STEP: Waiting for the RuntimeClass to disappear @ 04/28/23 17:08:17.257
  Apr 28 17:08:17.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-2586" for this suite. @ 04/28/23 17:08:17.271
• [0.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 04/28/23 17:08:17.28
  Apr 28 17:08:17.280: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename deployment @ 04/28/23 17:08:17.281
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:08:17.293
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:08:17.296
  Apr 28 17:08:17.309: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0428 17:08:17.954088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:18.954139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:19.954395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:20.956052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:21.956405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:08:22.315: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/28/23 17:08:22.315
  Apr 28 17:08:22.315: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 04/28/23 17:08:22.331
  Apr 28 17:08:22.355: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4853  aa09b3d0-8834-47b0-9644-e8b9e3250a3a 268057 1 2023-04-28 17:08:22 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-04-28 17:08:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004cb82a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Apr 28 17:08:22.359: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
  Apr 28 17:08:22.359: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Apr 28 17:08:22.360: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4853  005231c1-3944-4a54-b5bd-4b5b35689475 268059 1 2023-04-28 17:08:17 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment aa09b3d0-8834-47b0-9644-e8b9e3250a3a 0xc0026460b7 0xc0026460b8}] [] [{e2e.test Update apps/v1 2023-04-28 17:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-28 17:08:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-28 17:08:22 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"aa09b3d0-8834-47b0-9644-e8b9e3250a3a\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002646398 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 17:08:22.370: INFO: Pod "test-cleanup-controller-tnczk" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-tnczk test-cleanup-controller- deployment-4853  f2c07239-04e6-44b2-95f4-c06cea5e3a95 268032 0 2023-04-28 17:08:17 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:cb33f0e729853555b69f47f89ab5acf57daf21273bc3b71d88478056914e8f2f cni.projectcalico.org/podIP:10.42.3.76/32 cni.projectcalico.org/podIPs:10.42.3.76/32] [{apps/v1 ReplicaSet test-cleanup-controller 005231c1-3944-4a54-b5bd-4b5b35689475 0xc002646797 0xc002646798}] [] [{calico Update v1 2023-04-28 17:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-28 17:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"005231c1-3944-4a54-b5bd-4b5b35689475\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-28 17:08:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2rg2n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2rg2n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-127.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:08:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.127,PodIP:10.42.3.76,StartTime:2023-04-28 17:08:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:08:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7cf92e806ef254e15843d1ad073b8a87a0d1d85cc8840cbe94022bec39281763,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.76,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:08:22.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4853" for this suite. @ 04/28/23 17:08:22.38
• [5.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 04/28/23 17:08:22.399
  Apr 28 17:08:22.399: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 17:08:22.4
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:08:22.419
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:08:22.424
  STEP: starting the proxy server @ 04/28/23 17:08:22.431
  Apr 28 17:08:22.431: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-9591 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 04/28/23 17:08:22.481
  Apr 28 17:08:22.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9591" for this suite. @ 04/28/23 17:08:22.493
• [0.102 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 04/28/23 17:08:22.502
  Apr 28 17:08:22.502: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/28/23 17:08:22.503
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:08:22.516
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:08:22.518
  STEP: create the container to handle the HTTPGet hook request. @ 04/28/23 17:08:22.525
  E0428 17:08:22.957195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:23.957308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:24.957346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:25.957430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 04/28/23 17:08:26.55
  E0428 17:08:26.958254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:27.959116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 04/28/23 17:08:28.562
  STEP: delete the pod with lifecycle hook @ 04/28/23 17:08:28.58
  E0428 17:08:28.959223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:29.959498      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:30.959535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:31.959929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:08:32.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-1842" for this suite. @ 04/28/23 17:08:32.599
• [10.103 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 04/28/23 17:08:32.605
  Apr 28 17:08:32.605: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 17:08:32.606
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:08:32.617
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:08:32.621
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 04/28/23 17:08:32.623
  E0428 17:08:32.960817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:33.960997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:34.961527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:35.961585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:08:36.644
  Apr 28 17:08:36.647: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-d643bce4-8f0f-4578-b56a-f49284927d34 container test-container: <nil>
  STEP: delete the pod @ 04/28/23 17:08:36.654
  Apr 28 17:08:36.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-595" for this suite. @ 04/28/23 17:08:36.672
• [4.073 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 04/28/23 17:08:36.688
  Apr 28 17:08:36.688: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:08:36.694
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:08:36.707
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:08:36.711
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 17:08:36.713
  E0428 17:08:36.973153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:37.962149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:38.962822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:39.963751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:08:40.73
  Apr 28 17:08:40.732: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod downwardapi-volume-07dba941-5e9e-46c8-a830-b6641d4844c8 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 17:08:40.738
  Apr 28 17:08:40.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3254" for this suite. @ 04/28/23 17:08:40.754
• [4.073 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 04/28/23 17:08:40.762
  Apr 28 17:08:40.762: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename disruption @ 04/28/23 17:08:40.766
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:08:40.779
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:08:40.782
  STEP: Creating a pdb that targets all three pods in a test replica set @ 04/28/23 17:08:40.784
  STEP: Waiting for the pdb to be processed @ 04/28/23 17:08:40.787
  E0428 17:08:40.964279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:41.964381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: First trying to evict a pod which shouldn't be evictable @ 04/28/23 17:08:42.803
  STEP: Waiting for all pods to be running @ 04/28/23 17:08:42.803
  Apr 28 17:08:42.806: INFO: pods: 0 < 3
  E0428 17:08:42.964781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:43.964883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 04/28/23 17:08:44.81
  STEP: Updating the pdb to allow a pod to be evicted @ 04/28/23 17:08:44.818
  STEP: Waiting for the pdb to be processed @ 04/28/23 17:08:44.83
  E0428 17:08:44.965415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:45.966317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 04/28/23 17:08:46.843
  STEP: Waiting for all pods to be running @ 04/28/23 17:08:46.843
  STEP: Waiting for the pdb to observed all healthy pods @ 04/28/23 17:08:46.845
  STEP: Patching the pdb to disallow a pod to be evicted @ 04/28/23 17:08:46.868
  STEP: Waiting for the pdb to be processed @ 04/28/23 17:08:46.896
  E0428 17:08:46.966540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:47.971741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 04/28/23 17:08:48.906
  STEP: locating a running pod @ 04/28/23 17:08:48.909
  STEP: Deleting the pdb to allow a pod to be evicted @ 04/28/23 17:08:48.915
  STEP: Waiting for the pdb to be deleted @ 04/28/23 17:08:48.92
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 04/28/23 17:08:48.922
  STEP: Waiting for all pods to be running @ 04/28/23 17:08:48.922
  Apr 28 17:08:48.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-7759" for this suite. @ 04/28/23 17:08:48.945
• [8.200 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 04/28/23 17:08:48.962
  Apr 28 17:08:48.962: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:08:48.963
  E0428 17:08:48.972577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:08:48.983
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:08:48.987
  STEP: Creating projection with secret that has name projected-secret-test-95ef2401-6fa7-464a-b43e-f9baf95865c9 @ 04/28/23 17:08:48.989
  STEP: Creating a pod to test consume secrets @ 04/28/23 17:08:48.994
  E0428 17:08:49.974445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:50.974911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:51.976595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:52.977189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:08:53.011
  Apr 28 17:08:53.013: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-projected-secrets-7769dd46-c35a-426d-b3ec-c2e302093132 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 17:08:53.018
  Apr 28 17:08:53.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4575" for this suite. @ 04/28/23 17:08:53.034
• [4.077 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 04/28/23 17:08:53.04
  Apr 28 17:08:53.040: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 17:08:53.042
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:08:53.061
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:08:53.064
  STEP: Creating a pod to test downward api env vars @ 04/28/23 17:08:53.069
  E0428 17:08:53.977308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:54.978819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:55.982060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:56.982352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:08:57.089
  Apr 28 17:08:57.092: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod downward-api-566e6826-1faa-44bc-b996-b9a97cc8d81c container dapi-container: <nil>
  STEP: delete the pod @ 04/28/23 17:08:57.099
  Apr 28 17:08:57.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2575" for this suite. @ 04/28/23 17:08:57.116
• [4.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 04/28/23 17:08:57.122
  Apr 28 17:08:57.122: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename pod-network-test @ 04/28/23 17:08:57.123
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:08:57.134
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:08:57.137
  STEP: Performing setup for networking test in namespace pod-network-test-1583 @ 04/28/23 17:08:57.139
  STEP: creating a selector @ 04/28/23 17:08:57.139
  STEP: Creating the service pods in kubernetes @ 04/28/23 17:08:57.139
  Apr 28 17:08:57.139: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0428 17:08:57.983160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:58.983431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:08:59.983564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:00.983946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:01.984058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:02.984097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:03.984208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:04.984367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:05.984638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:06.984743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:07.985679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:08.985798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:09.986280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:10.986952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:11.987070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:12.987548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:13.987997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:14.988310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:15.988786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:16.988963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:17.989002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:18.989249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 04/28/23 17:09:19.259
  E0428 17:09:19.990095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:20.990196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:09:21.286: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
  Apr 28 17:09:21.286: INFO: Breadth first check of 10.42.1.176 on host 172.31.11.161...
  Apr 28 17:09:21.288: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.85:9080/dial?request=hostname&protocol=udp&host=10.42.1.176&port=8081&tries=1'] Namespace:pod-network-test-1583 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:09:21.288: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 17:09:21.289: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:09:21.289: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-1583/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.85%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.42.1.176%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 28 17:09:21.368: INFO: Waiting for responses: map[]
  Apr 28 17:09:21.368: INFO: reached 10.42.1.176 after 0/1 tries
  Apr 28 17:09:21.368: INFO: Breadth first check of 10.42.0.166 on host 172.31.14.195...
  Apr 28 17:09:21.371: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.85:9080/dial?request=hostname&protocol=udp&host=10.42.0.166&port=8081&tries=1'] Namespace:pod-network-test-1583 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:09:21.371: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 17:09:21.371: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:09:21.371: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-1583/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.85%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.42.0.166%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 28 17:09:21.473: INFO: Waiting for responses: map[]
  Apr 28 17:09:21.473: INFO: reached 10.42.0.166 after 0/1 tries
  Apr 28 17:09:21.473: INFO: Breadth first check of 10.42.2.202 on host 172.31.6.71...
  Apr 28 17:09:21.476: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.85:9080/dial?request=hostname&protocol=udp&host=10.42.2.202&port=8081&tries=1'] Namespace:pod-network-test-1583 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:09:21.476: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 17:09:21.477: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:09:21.478: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-1583/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.85%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.42.2.202%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 28 17:09:21.582: INFO: Waiting for responses: map[]
  Apr 28 17:09:21.582: INFO: reached 10.42.2.202 after 0/1 tries
  Apr 28 17:09:21.582: INFO: Breadth first check of 10.42.3.84 on host 172.31.9.127...
  Apr 28 17:09:21.585: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.85:9080/dial?request=hostname&protocol=udp&host=10.42.3.84&port=8081&tries=1'] Namespace:pod-network-test-1583 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:09:21.585: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 17:09:21.585: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:09:21.585: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-1583/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.85%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.42.3.84%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 28 17:09:21.682: INFO: Waiting for responses: map[]
  Apr 28 17:09:21.682: INFO: reached 10.42.3.84 after 0/1 tries
  Apr 28 17:09:21.682: INFO: Going to retry 0 out of 4 pods....
  Apr 28 17:09:21.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-1583" for this suite. @ 04/28/23 17:09:21.686
• [24.569 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 04/28/23 17:09:21.701
  Apr 28 17:09:21.701: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename configmap @ 04/28/23 17:09:21.702
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:09:21.719
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:09:21.722
  STEP: Creating configMap with name configmap-test-volume-map-ec0e1154-4825-4196-9a25-31b7cf3ec9eb @ 04/28/23 17:09:21.725
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:09:21.73
  E0428 17:09:21.992141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:22.993151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:23.993201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:24.994283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:09:25.75
  Apr 28 17:09:25.752: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-configmaps-b46ee67b-ff22-473d-a9e3-793ceb134ee2 container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 17:09:25.757
  Apr 28 17:09:25.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7296" for this suite. @ 04/28/23 17:09:25.773
• [4.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 04/28/23 17:09:25.779
  Apr 28 17:09:25.779: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename services @ 04/28/23 17:09:25.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:09:25.795
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:09:25.799
  STEP: creating service endpoint-test2 in namespace services-4684 @ 04/28/23 17:09:25.801
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4684 to expose endpoints map[] @ 04/28/23 17:09:25.81
  Apr 28 17:09:25.819: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  E0428 17:09:25.994746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:09:26.831: INFO: successfully validated that service endpoint-test2 in namespace services-4684 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-4684 @ 04/28/23 17:09:26.831
  E0428 17:09:26.995600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:27.996659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4684 to expose endpoints map[pod1:[80]] @ 04/28/23 17:09:28.865
  Apr 28 17:09:28.871: INFO: successfully validated that service endpoint-test2 in namespace services-4684 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 04/28/23 17:09:28.871
  Apr 28 17:09:28.871: INFO: Creating new exec pod
  E0428 17:09:28.996998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:29.997141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:30.997213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:09:31.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-4684 exec execpod5k4x8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E0428 17:09:31.999560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:09:32.269: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 28 17:09:32.269: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:09:32.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-4684 exec execpod5k4x8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.175.172 80'
  Apr 28 17:09:32.494: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.175.172 80\nConnection to 10.43.175.172 80 port [tcp/http] succeeded!\n"
  Apr 28 17:09:32.494: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-4684 @ 04/28/23 17:09:32.494
  E0428 17:09:32.998042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:33.998161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4684 to expose endpoints map[pod1:[80] pod2:[80]] @ 04/28/23 17:09:34.534
  Apr 28 17:09:34.567: INFO: successfully validated that service endpoint-test2 in namespace services-4684 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 04/28/23 17:09:34.567
  E0428 17:09:34.999099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:09:35.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-4684 exec execpod5k4x8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 28 17:09:35.879: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 28 17:09:35.879: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:09:35.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-4684 exec execpod5k4x8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.175.172 80'
  E0428 17:09:36.000209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:09:36.377: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.175.172 80\nConnection to 10.43.175.172 80 port [tcp/http] succeeded!\n"
  Apr 28 17:09:36.377: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-4684 @ 04/28/23 17:09:36.377
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4684 to expose endpoints map[pod2:[80]] @ 04/28/23 17:09:36.404
  Apr 28 17:09:36.448: INFO: successfully validated that service endpoint-test2 in namespace services-4684 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 04/28/23 17:09:36.448
  E0428 17:09:37.000271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:09:37.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-4684 exec execpod5k4x8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 28 17:09:37.599: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 28 17:09:37.599: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:09:37.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-4684 exec execpod5k4x8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.175.172 80'
  Apr 28 17:09:37.781: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.175.172 80\nConnection to 10.43.175.172 80 port [tcp/http] succeeded!\n"
  Apr 28 17:09:37.781: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-4684 @ 04/28/23 17:09:37.781
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4684 to expose endpoints map[] @ 04/28/23 17:09:37.796
  Apr 28 17:09:37.807: INFO: successfully validated that service endpoint-test2 in namespace services-4684 exposes endpoints map[]
  Apr 28 17:09:37.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4684" for this suite. @ 04/28/23 17:09:37.844
• [12.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 04/28/23 17:09:37.851
  Apr 28 17:09:37.851: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename secrets @ 04/28/23 17:09:37.852
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:09:37.865
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:09:37.872
  STEP: Creating secret with name secret-test-6feca66b-106b-4c86-ae59-11372d9c1b64 @ 04/28/23 17:09:37.875
  STEP: Creating a pod to test consume secrets @ 04/28/23 17:09:37.881
  E0428 17:09:38.000393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:39.000485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:40.001174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:41.001345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:09:41.902
  Apr 28 17:09:41.908: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-secrets-97f32ad8-b129-4936-aea9-489e0aa44ffe container secret-env-test: <nil>
  STEP: delete the pod @ 04/28/23 17:09:41.925
  Apr 28 17:09:41.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6394" for this suite. @ 04/28/23 17:09:41.948
• [4.102 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 04/28/23 17:09:41.953
  Apr 28 17:09:41.953: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename watch @ 04/28/23 17:09:41.954
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:09:41.97
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:09:41.974
  STEP: getting a starting resourceVersion @ 04/28/23 17:09:41.976
  STEP: starting a background goroutine to produce watch events @ 04/28/23 17:09:41.979
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 04/28/23 17:09:41.979
  E0428 17:09:42.003332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:43.007204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:44.007737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:09:44.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2999" for this suite. @ 04/28/23 17:09:44.81
• [2.909 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 04/28/23 17:09:44.867
  Apr 28 17:09:44.867: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename job @ 04/28/23 17:09:44.868
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:09:44.896
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:09:44.9
  STEP: Creating a job @ 04/28/23 17:09:44.902
  STEP: Ensuring active pods == parallelism @ 04/28/23 17:09:44.91
  E0428 17:09:45.008388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:46.008434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Orphaning one of the Job's Pods @ 04/28/23 17:09:46.913
  E0428 17:09:47.009410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:09:47.426: INFO: Successfully updated pod "adopt-release-c4mph"
  STEP: Checking that the Job readopts the Pod @ 04/28/23 17:09:47.426
  E0428 17:09:48.009939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:49.010713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing the labels from the Job's Pod @ 04/28/23 17:09:49.437
  Apr 28 17:09:50.004: INFO: Successfully updated pod "adopt-release-c4mph"
  STEP: Checking that the Job releases the Pod @ 04/28/23 17:09:50.004
  E0428 17:09:50.011962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:51.012525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:52.012333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:09:52.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1576" for this suite. @ 04/28/23 17:09:52.019
• [7.160 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 04/28/23 17:09:52.029
  Apr 28 17:09:52.029: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 17:09:52.03
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:09:52.065
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:09:52.072
  STEP: Counting existing ResourceQuota @ 04/28/23 17:09:52.081
  E0428 17:09:53.012904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:54.013745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:55.014628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:56.014727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:57.014769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/28/23 17:09:57.085
  STEP: Ensuring resource quota status is calculated @ 04/28/23 17:09:57.089
  E0428 17:09:58.014992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:09:59.015185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod that fits quota @ 04/28/23 17:09:59.093
  STEP: Ensuring ResourceQuota status captures the pod usage @ 04/28/23 17:09:59.109
  E0428 17:10:00.015364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:01.016429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 04/28/23 17:10:01.114
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 04/28/23 17:10:01.116
  STEP: Ensuring a pod cannot update its resource requirements @ 04/28/23 17:10:01.118
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 04/28/23 17:10:01.122
  E0428 17:10:02.017110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:03.017958      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 04/28/23 17:10:03.126
  STEP: Ensuring resource quota status released the pod usage @ 04/28/23 17:10:03.153
  E0428 17:10:04.018031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:05.018157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:05.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6914" for this suite. @ 04/28/23 17:10:05.161
• [13.138 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 04/28/23 17:10:05.169
  Apr 28 17:10:05.169: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename services @ 04/28/23 17:10:05.17
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:10:05.187
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:10:05.19
  Apr 28 17:10:05.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4653" for this suite. @ 04/28/23 17:10:05.198
• [0.035 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 04/28/23 17:10:05.205
  Apr 28 17:10:05.205: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename kubelet-test @ 04/28/23 17:10:05.208
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:10:05.225
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:10:05.228
  STEP: Waiting for pod completion @ 04/28/23 17:10:05.237
  E0428 17:10:06.018618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:07.018720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:08.019802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:09.020427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:09.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-961" for this suite. @ 04/28/23 17:10:09.256
• [4.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 04/28/23 17:10:09.273
  Apr 28 17:10:09.273: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename services @ 04/28/23 17:10:09.274
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:10:09.302
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:10:09.307
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4551 @ 04/28/23 17:10:09.31
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 04/28/23 17:10:09.327
  STEP: creating service externalsvc in namespace services-4551 @ 04/28/23 17:10:09.327
  STEP: creating replication controller externalsvc in namespace services-4551 @ 04/28/23 17:10:09.347
  I0428 17:10:09.364399      19 runners.go:194] Created replication controller with name: externalsvc, namespace: services-4551, replica count: 2
  E0428 17:10:10.020850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:11.021204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:12.021300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 17:10:12.415824      19 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 04/28/23 17:10:12.42
  Apr 28 17:10:12.432: INFO: Creating new exec pod
  E0428 17:10:13.022289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:14.022387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:14.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-4551 exec execpodf7cxx -- /bin/sh -x -c nslookup clusterip-service.services-4551.svc.cluster.local'
  Apr 28 17:10:14.653: INFO: stderr: "+ nslookup clusterip-service.services-4551.svc.cluster.local\n"
  Apr 28 17:10:14.653: INFO: stdout: "Server:\t\t10.43.0.10\nAddress:\t10.43.0.10#53\n\nclusterip-service.services-4551.svc.cluster.local\tcanonical name = externalsvc.services-4551.svc.cluster.local.\nName:\texternalsvc.services-4551.svc.cluster.local\nAddress: 10.43.17.223\n\n"
  Apr 28 17:10:14.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-4551, will wait for the garbage collector to delete the pods @ 04/28/23 17:10:14.657
  Apr 28 17:10:14.718: INFO: Deleting ReplicationController externalsvc took: 8.133856ms
  Apr 28 17:10:14.819: INFO: Terminating ReplicationController externalsvc pods took: 100.908886ms
  E0428 17:10:15.022905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:16.023581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:17.024267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:17.044: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-4551" for this suite. @ 04/28/23 17:10:17.062
• [7.796 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 04/28/23 17:10:17.07
  Apr 28 17:10:17.070: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 17:10:17.071
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:10:17.085
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:10:17.087
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 04/28/23 17:10:17.09
  E0428 17:10:18.025307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:19.025431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:20.025555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:21.025661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:10:21.107
  Apr 28 17:10:21.110: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-afc60583-3863-4467-83e6-bda19c7c9d5f container test-container: <nil>
  STEP: delete the pod @ 04/28/23 17:10:21.115
  Apr 28 17:10:21.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5478" for this suite. @ 04/28/23 17:10:21.131
• [4.067 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 04/28/23 17:10:21.138
  Apr 28 17:10:21.138: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename services @ 04/28/23 17:10:21.139
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:10:21.159
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:10:21.163
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-4741 @ 04/28/23 17:10:21.167
  STEP: changing the ExternalName service to type=ClusterIP @ 04/28/23 17:10:21.174
  STEP: creating replication controller externalname-service in namespace services-4741 @ 04/28/23 17:10:21.185
  I0428 17:10:21.197572      19 runners.go:194] Created replication controller with name: externalname-service, namespace: services-4741, replica count: 2
  E0428 17:10:22.026246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:23.026331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:24.026584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 17:10:24.249097      19 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 28 17:10:24.249: INFO: Creating new exec pod
  E0428 17:10:25.029213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:26.030270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:27.030394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:27.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-4741 exec execpod8ljbg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 28 17:10:27.695: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 28 17:10:27.695: INFO: stdout: ""
  E0428 17:10:28.031104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:28.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-4741 exec execpod8ljbg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 28 17:10:28.837: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 28 17:10:28.837: INFO: stdout: ""
  E0428 17:10:29.031570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:29.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-4741 exec execpod8ljbg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 28 17:10:29.838: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 28 17:10:29.838: INFO: stdout: "externalname-service-xjgfl"
  Apr 28 17:10:29.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-4741 exec execpod8ljbg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.112.178 80'
  E0428 17:10:30.031914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:30.047: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.112.178 80\nConnection to 10.43.112.178 80 port [tcp/http] succeeded!\n"
  Apr 28 17:10:30.047: INFO: stdout: ""
  E0428 17:10:31.032729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:31.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-4741 exec execpod8ljbg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.112.178 80'
  Apr 28 17:10:31.204: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.112.178 80\nConnection to 10.43.112.178 80 port [tcp/http] succeeded!\n"
  Apr 28 17:10:31.204: INFO: stdout: ""
  E0428 17:10:32.032824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:32.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-4741 exec execpod8ljbg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.112.178 80'
  Apr 28 17:10:32.225: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.112.178 80\nConnection to 10.43.112.178 80 port [tcp/http] succeeded!\n"
  Apr 28 17:10:32.225: INFO: stdout: "externalname-service-xjgfl"
  Apr 28 17:10:32.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 17:10:32.229: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-4741" for this suite. @ 04/28/23 17:10:32.253
• [11.132 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 04/28/23 17:10:32.272
  Apr 28 17:10:32.272: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename aggregator @ 04/28/23 17:10:32.273
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:10:32.295
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:10:32.299
  Apr 28 17:10:32.305: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Registering the sample API server. @ 04/28/23 17:10:32.307
  Apr 28 17:10:32.773: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Apr 28 17:10:32.827: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  E0428 17:10:33.033598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:34.033747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:34.910: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:10:35.034503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:36.035561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:36.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:10:37.036531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:38.036694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:38.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:10:39.037249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:40.037357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:40.915: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:10:41.037943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:42.038303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:42.915: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:10:43.038674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:44.039164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:44.917: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:10:45.039647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:46.039712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:46.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:10:47.040514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:48.040977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:48.914: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:10:49.041429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:50.041893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:50.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:10:51.041995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:52.042117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:52.914: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:10:53.042232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:54.042336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:54.930: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:10:55.042482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:56.042808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:56.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:10:57.042853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:10:58.043247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:10:58.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:10:59.044136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:00.044361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:11:00.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:11:01.044909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:02.045295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:11:02.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:11:03.046065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:04.046173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:11:04.915: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:11:05.046321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:06.046496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:11:06.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:11:07.047227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:08.048189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:11:08.918: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 10, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:11:09.048750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:10.049265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:11.061492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:11:11.146: INFO: Waited 227.298477ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 04/28/23 17:11:11.23
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 04/28/23 17:11:11.234
  STEP: List APIServices @ 04/28/23 17:11:11.24
  Apr 28 17:11:11.252: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 04/28/23 17:11:11.252
  Apr 28 17:11:11.265: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 04/28/23 17:11:11.265
  Apr 28 17:11:11.280: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.April, 28, 17, 11, 10, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 04/28/23 17:11:11.28
  Apr 28 17:11:11.285: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-04-28 17:11:10 +0000 UTC Passed all checks passed}
  Apr 28 17:11:11.291: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 28 17:11:11.291: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 04/28/23 17:11:11.291
  Apr 28 17:11:11.305: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-81710796" @ 04/28/23 17:11:11.305
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 04/28/23 17:11:11.363
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 04/28/23 17:11:11.372
  STEP: Patch APIService Status @ 04/28/23 17:11:11.376
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 04/28/23 17:11:11.388
  Apr 28 17:11:11.401: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-04-28 17:11:10 +0000 UTC Passed all checks passed}
  Apr 28 17:11:11.401: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 28 17:11:11.401: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Apr 28 17:11:11.401: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 04/28/23 17:11:11.402
  STEP: Confirm that the generated APIService has been deleted @ 04/28/23 17:11:11.408
  Apr 28 17:11:11.408: INFO: Requesting list of APIServices to confirm quantity
  Apr 28 17:11:11.420: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Apr 28 17:11:11.420: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Apr 28 17:11:11.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-1211" for this suite. @ 04/28/23 17:11:11.588
• [39.330 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 04/28/23 17:11:11.602
  Apr 28 17:11:11.602: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 17:11:11.603
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:11:11.628
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:11:11.634
  E0428 17:11:12.062186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:13.062256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:14.062357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:15.062448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:16.062550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:17.063448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:18.063777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:19.064463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:20.065495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:21.065598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:22.066276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:23.073387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:24.073493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:25.074331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:26.074412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:27.074537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:28.074948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 04/28/23 17:11:28.641
  E0428 17:11:29.075632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:30.076230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:31.077094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:32.078075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:33.078195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/28/23 17:11:33.644
  STEP: Ensuring resource quota status is calculated @ 04/28/23 17:11:33.657
  E0428 17:11:34.078626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:35.078737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ConfigMap @ 04/28/23 17:11:35.661
  STEP: Ensuring resource quota status captures configMap creation @ 04/28/23 17:11:35.673
  E0428 17:11:36.079365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:37.079783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ConfigMap @ 04/28/23 17:11:37.681
  STEP: Ensuring resource quota status released usage @ 04/28/23 17:11:37.695
  E0428 17:11:38.080360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:39.080470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:11:39.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7160" for this suite. @ 04/28/23 17:11:39.703
• [28.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 04/28/23 17:11:39.715
  Apr 28 17:11:39.715: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename gc @ 04/28/23 17:11:39.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:11:39.74
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:11:39.749
  STEP: create the deployment @ 04/28/23 17:11:39.769
  W0428 17:11:39.787964      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 04/28/23 17:11:39.788
  E0428 17:11:40.081621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the deployment @ 04/28/23 17:11:40.307
  STEP: wait for all rs to be garbage collected @ 04/28/23 17:11:40.316
  STEP: expected 0 rs, got 1 rs @ 04/28/23 17:11:40.321
  STEP: expected 0 pods, got 2 pods @ 04/28/23 17:11:40.323
  STEP: Gathering metrics @ 04/28/23 17:11:40.831
  Apr 28 17:11:40.903: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 28 17:11:40.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-9040" for this suite. @ 04/28/23 17:11:40.916
• [1.206 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 04/28/23 17:11:40.923
  Apr 28 17:11:40.923: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename secrets @ 04/28/23 17:11:40.924
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:11:40.935
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:11:40.939
  Apr 28 17:11:40.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2099" for this suite. @ 04/28/23 17:11:40.975
• [0.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 04/28/23 17:11:40.981
  Apr 28 17:11:40.981: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename cronjob @ 04/28/23 17:11:40.981
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:11:40.996
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:11:40.998
  STEP: Creating a cronjob @ 04/28/23 17:11:41.001
  STEP: creating @ 04/28/23 17:11:41.001
  STEP: getting @ 04/28/23 17:11:41.005
  STEP: listing @ 04/28/23 17:11:41.007
  STEP: watching @ 04/28/23 17:11:41.009
  Apr 28 17:11:41.009: INFO: starting watch
  STEP: cluster-wide listing @ 04/28/23 17:11:41.01
  STEP: cluster-wide watching @ 04/28/23 17:11:41.012
  Apr 28 17:11:41.012: INFO: starting watch
  STEP: patching @ 04/28/23 17:11:41.013
  STEP: updating @ 04/28/23 17:11:41.019
  Apr 28 17:11:41.026: INFO: waiting for watch events with expected annotations
  Apr 28 17:11:41.026: INFO: saw patched and updated annotations
  STEP: patching /status @ 04/28/23 17:11:41.026
  STEP: updating /status @ 04/28/23 17:11:41.03
  STEP: get /status @ 04/28/23 17:11:41.036
  STEP: deleting @ 04/28/23 17:11:41.038
  STEP: deleting a collection @ 04/28/23 17:11:41.051
  Apr 28 17:11:41.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-4744" for this suite. @ 04/28/23 17:11:41.061
• [0.085 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 04/28/23 17:11:41.066
  Apr 28 17:11:41.066: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename crd-webhook @ 04/28/23 17:11:41.067
  E0428 17:11:41.084962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:11:41.085
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:11:41.087
  STEP: Setting up server cert @ 04/28/23 17:11:41.089
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 04/28/23 17:11:41.331
  STEP: Deploying the custom resource conversion webhook pod @ 04/28/23 17:11:41.337
  STEP: Wait for the deployment to be ready @ 04/28/23 17:11:41.349
  Apr 28 17:11:41.360: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0428 17:11:42.085415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:43.086032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 17:11:43.368
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 17:11:43.377
  E0428 17:11:44.086588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:11:44.377: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Apr 28 17:11:44.380: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  E0428 17:11:45.086630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:46.086708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 04/28/23 17:11:46.926
  STEP: v2 custom resource should be converted @ 04/28/23 17:11:46.93
  Apr 28 17:11:46.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0428 17:11:47.086778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-1596" for this suite. @ 04/28/23 17:11:47.537
• [6.495 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 04/28/23 17:11:47.562
  Apr 28 17:11:47.562: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename webhook @ 04/28/23 17:11:47.57
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:11:47.616
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:11:47.623
  STEP: Setting up server cert @ 04/28/23 17:11:47.665
  E0428 17:11:48.091769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 17:11:48.513
  STEP: Deploying the webhook pod @ 04/28/23 17:11:48.517
  STEP: Wait for the deployment to be ready @ 04/28/23 17:11:48.527
  Apr 28 17:11:48.533: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0428 17:11:49.091857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:50.092029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 17:11:50.541
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 17:11:50.55
  E0428 17:11:51.092972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:11:51.550: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 04/28/23 17:11:51.599
  STEP: Creating a configMap that should be mutated @ 04/28/23 17:11:51.612
  STEP: Deleting the collection of validation webhooks @ 04/28/23 17:11:51.639
  STEP: Creating a configMap that should not be mutated @ 04/28/23 17:11:51.676
  Apr 28 17:11:51.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9989" for this suite. @ 04/28/23 17:11:51.803
  STEP: Destroying namespace "webhook-markers-6125" for this suite. @ 04/28/23 17:11:51.811
• [4.263 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 04/28/23 17:11:51.835
  Apr 28 17:11:51.837: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename container-probe @ 04/28/23 17:11:51.845
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:11:51.861
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:11:51.863
  E0428 17:11:52.093897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:53.094179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:54.095111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:55.095735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:56.096789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:57.096755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:58.096842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:11:59.097883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:00.098724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:01.099427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:02.100286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:03.100864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:04.101381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:05.102282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:06.102585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:07.103543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:08.104433      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:09.104505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:10.105542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:11.106319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:12.106475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:13.107328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:14.107925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:15.108879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:16.109250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:17.110168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:18.110663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:19.111521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:20.111865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:21.112416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:22.112877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:23.113865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:24.114195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:25.114633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:26.115367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:27.115690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:28.116560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:29.117056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:30.117501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:31.118559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:32.119057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:33.119170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:34.119459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:35.119519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:36.120177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:37.121198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:38.121468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:39.121542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:40.122292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:41.123118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:42.124060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:43.124152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:44.124685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:45.124919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:46.125019      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:47.125460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:48.126076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:49.126822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:50.127772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:51.128577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:12:51.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-6635" for this suite. @ 04/28/23 17:12:51.886
• [60.061 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 04/28/23 17:12:51.899
  Apr 28 17:12:51.901: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename container-probe @ 04/28/23 17:12:51.903
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:12:51.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:12:51.932
  STEP: Creating pod busybox-6d014be9-64fb-4920-8b54-0b89cc823622 in namespace container-probe-2818 @ 04/28/23 17:12:51.938
  E0428 17:12:52.129173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:53.129219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:12:53.951: INFO: Started pod busybox-6d014be9-64fb-4920-8b54-0b89cc823622 in namespace container-probe-2818
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/28/23 17:12:53.951
  Apr 28 17:12:53.953: INFO: Initial restart count of pod busybox-6d014be9-64fb-4920-8b54-0b89cc823622 is 0
  E0428 17:12:54.130126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:55.130259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:56.131327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:57.131976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:58.132080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:12:59.132194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:00.132921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:01.133595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:02.134055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:03.134139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:04.134874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:05.135294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:06.135376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:07.135847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:08.136503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:09.136886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:10.137913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:11.138254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:12.138686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:13.138792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:14.139596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:15.139681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:16.140725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:17.140855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:18.141216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:19.141333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:20.141577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:21.141818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:22.142688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:23.142783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:24.143312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:25.143490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:26.143930      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:27.143999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:28.144688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:29.144812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:30.145708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:31.146323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:32.146640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:33.147257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:34.147404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:35.147652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:36.147847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:37.147971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:38.148962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:39.149319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:40.149867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:41.150348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:42.155730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:43.155736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:13:44.058: INFO: Restart count of pod container-probe-2818/busybox-6d014be9-64fb-4920-8b54-0b89cc823622 is now 1 (50.105321373s elapsed)
  Apr 28 17:13:44.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 17:13:44.062
  STEP: Destroying namespace "container-probe-2818" for this suite. @ 04/28/23 17:13:44.074
• [52.182 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 04/28/23 17:13:44.089
  Apr 28 17:13:44.089: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 17:13:44.09
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:13:44.11
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:13:44.113
  STEP: creating Agnhost RC @ 04/28/23 17:13:44.116
  Apr 28 17:13:44.116: INFO: namespace kubectl-6749
  Apr 28 17:13:44.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-6749 create -f -'
  E0428 17:13:44.162612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:45.162644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:13:45.310: INFO: stderr: ""
  Apr 28 17:13:45.310: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/28/23 17:13:45.31
  E0428 17:13:46.162737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:13:46.314: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 28 17:13:46.314: INFO: Found 0 / 1
  E0428 17:13:47.163206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:13:47.318: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 28 17:13:47.318: INFO: Found 1 / 1
  Apr 28 17:13:47.318: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Apr 28 17:13:47.320: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 28 17:13:47.320: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 28 17:13:47.320: INFO: wait on agnhost-primary startup in kubectl-6749 
  Apr 28 17:13:47.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-6749 logs agnhost-primary-ct89t agnhost-primary'
  Apr 28 17:13:47.420: INFO: stderr: ""
  Apr 28 17:13:47.420: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 04/28/23 17:13:47.42
  Apr 28 17:13:47.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-6749 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Apr 28 17:13:47.510: INFO: stderr: ""
  Apr 28 17:13:47.510: INFO: stdout: "service/rm2 exposed\n"
  Apr 28 17:13:47.516: INFO: Service rm2 in namespace kubectl-6749 found.
  E0428 17:13:48.163523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:49.163716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: exposing service @ 04/28/23 17:13:49.523
  Apr 28 17:13:49.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-6749 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Apr 28 17:13:49.623: INFO: stderr: ""
  Apr 28 17:13:49.623: INFO: stdout: "service/rm3 exposed\n"
  Apr 28 17:13:49.628: INFO: Service rm3 in namespace kubectl-6749 found.
  E0428 17:13:50.164457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:51.164650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:13:51.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6749" for this suite. @ 04/28/23 17:13:51.637
• [7.553 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 04/28/23 17:13:51.648
  Apr 28 17:13:51.648: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename emptydir-wrapper @ 04/28/23 17:13:51.649
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:13:51.661
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:13:51.665
  E0428 17:13:52.164749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:53.165510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:13:53.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 04/28/23 17:13:53.696
  STEP: Cleaning up the configmap @ 04/28/23 17:13:53.7
  STEP: Cleaning up the pod @ 04/28/23 17:13:53.704
  STEP: Destroying namespace "emptydir-wrapper-3477" for this suite. @ 04/28/23 17:13:53.714
• [2.077 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 04/28/23 17:13:53.727
  Apr 28 17:13:53.727: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename prestop @ 04/28/23 17:13:53.728
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:13:53.748
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:13:53.752
  STEP: Creating server pod server in namespace prestop-352 @ 04/28/23 17:13:53.755
  STEP: Waiting for pods to come up. @ 04/28/23 17:13:53.761
  E0428 17:13:54.169187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:55.169256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating tester pod tester in namespace prestop-352 @ 04/28/23 17:13:55.769
  E0428 17:13:56.169363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:57.169404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pre-stop pod @ 04/28/23 17:13:57.783
  E0428 17:13:58.171767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:13:59.172132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:00.172249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:01.172383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:02.172492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:14:02.799: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Apr 28 17:14:02.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 04/28/23 17:14:02.804
  STEP: Destroying namespace "prestop-352" for this suite. @ 04/28/23 17:14:02.828
• [9.134 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 04/28/23 17:14:02.87
  Apr 28 17:14:02.870: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename deployment @ 04/28/23 17:14:02.872
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:14:02.905
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:14:02.913
  Apr 28 17:14:02.936: INFO: Pod name rollover-pod: Found 0 pods out of 1
  E0428 17:14:03.174765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:04.174881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:05.174979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:06.175187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:07.175282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:14:07.940: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/28/23 17:14:07.94
  Apr 28 17:14:07.940: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  E0428 17:14:08.175801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:09.176117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:14:09.947: INFO: Creating deployment "test-rollover-deployment"
  Apr 28 17:14:09.960: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  E0428 17:14:10.177051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:11.177293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:14:11.973: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Apr 28 17:14:11.978: INFO: Ensure that both replica sets have 1 created replica
  Apr 28 17:14:11.982: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Apr 28 17:14:12.004: INFO: Updating deployment test-rollover-deployment
  Apr 28 17:14:12.004: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  E0428 17:14:12.178063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:13.178262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:14:14.031: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Apr 28 17:14:14.037: INFO: Make sure deployment "test-rollover-deployment" is complete
  Apr 28 17:14:14.042: INFO: all replica sets need to contain the pod-template-hash label
  Apr 28 17:14:14.042: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 14, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 14, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 14, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 14, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:14:14.179052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:15.179341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:14:16.052: INFO: all replica sets need to contain the pod-template-hash label
  Apr 28 17:14:16.052: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 14, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 14, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 14, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 14, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:14:16.180007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:17.180236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:14:18.049: INFO: all replica sets need to contain the pod-template-hash label
  Apr 28 17:14:18.049: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 14, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 14, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 14, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 14, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:14:18.181193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:19.181241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:14:20.060: INFO: all replica sets need to contain the pod-template-hash label
  Apr 28 17:14:20.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 14, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 14, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 14, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 14, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:14:20.182166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:21.182514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:14:22.049: INFO: all replica sets need to contain the pod-template-hash label
  Apr 28 17:14:22.049: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 14, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 14, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 14, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 14, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0428 17:14:22.183264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:23.184042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:14:24.049: INFO: 
  Apr 28 17:14:24.049: INFO: Ensure that both old replica sets have no replicas
  Apr 28 17:14:24.061: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-4885  11ad7f4c-7f86-4c02-96e7-956d58bd3584 271452 2 2023-04-28 17:14:09 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-28 17:14:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-28 17:14:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00288c578 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-28 17:14:10 +0000 UTC,LastTransitionTime:2023-04-28 17:14:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-04-28 17:14:23 +0000 UTC,LastTransitionTime:2023-04-28 17:14:09 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 28 17:14:24.064: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-4885  d0fab9ba-1000-49fd-934f-2987cb58297b 271442 2 2023-04-28 17:14:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 11ad7f4c-7f86-4c02-96e7-956d58bd3584 0xc00288ca47 0xc00288ca48}] [] [{kube-controller-manager Update apps/v1 2023-04-28 17:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11ad7f4c-7f86-4c02-96e7-956d58bd3584\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-28 17:14:23 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00288caf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 17:14:24.065: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Apr 28 17:14:24.065: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4885  bb8c30be-be01-440f-9d3e-a29141995886 271451 2 2023-04-28 17:14:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 11ad7f4c-7f86-4c02-96e7-956d58bd3584 0xc00288c917 0xc00288c918}] [] [{e2e.test Update apps/v1 2023-04-28 17:14:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-28 17:14:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11ad7f4c-7f86-4c02-96e7-956d58bd3584\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-28 17:14:23 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00288c9d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 17:14:24.065: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-4885  9e4dc9b0-bb88-4445-b191-642fac6fcbf7 271371 2 2023-04-28 17:14:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 11ad7f4c-7f86-4c02-96e7-956d58bd3584 0xc00288cb67 0xc00288cb68}] [] [{kube-controller-manager Update apps/v1 2023-04-28 17:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11ad7f4c-7f86-4c02-96e7-956d58bd3584\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-28 17:14:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00288cc18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 17:14:24.069: INFO: Pod "test-rollover-deployment-57777854c9-zhzk9" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-zhzk9 test-rollover-deployment-57777854c9- deployment-4885  226d9214-965e-41b4-a1b3-00bce5d34043 271391 0 2023-04-28 17:14:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[cni.projectcalico.org/containerID:5377f836ff52661f65fcd5971c30228a640276bf944420a8f675f77ee736d057 cni.projectcalico.org/podIP:10.42.3.111/32 cni.projectcalico.org/podIPs:10.42.3.111/32] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 d0fab9ba-1000-49fd-934f-2987cb58297b 0xc003cdc217 0xc003cdc218}] [] [{calico Update v1 2023-04-28 17:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-28 17:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d0fab9ba-1000-49fd-934f-2987cb58297b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-28 17:14:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x8zzw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x8zzw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-127.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:14:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:14:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:14:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:14:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.127,PodIP:10.42.3.111,StartTime:2023-04-28 17:14:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:14:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://5ef7ed83584f5e43671ea6494a327b4a3d9f54ca76153b9308b3cddb67dbb197,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.111,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:14:24.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4885" for this suite. @ 04/28/23 17:14:24.073
• [21.210 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 04/28/23 17:14:24.081
  Apr 28 17:14:24.081: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename replication-controller @ 04/28/23 17:14:24.082
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:14:24.145
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:14:24.148
  STEP: Creating replication controller my-hostname-basic-dcd45356-cf7c-409c-a247-223c6c57efc8 @ 04/28/23 17:14:24.158
  Apr 28 17:14:24.165: INFO: Pod name my-hostname-basic-dcd45356-cf7c-409c-a247-223c6c57efc8: Found 0 pods out of 1
  E0428 17:14:24.185232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:25.185324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:26.185549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:27.185708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:28.185816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:14:29.170: INFO: Pod name my-hostname-basic-dcd45356-cf7c-409c-a247-223c6c57efc8: Found 1 pods out of 1
  Apr 28 17:14:29.170: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-dcd45356-cf7c-409c-a247-223c6c57efc8" are running
  Apr 28 17:14:29.173: INFO: Pod "my-hostname-basic-dcd45356-cf7c-409c-a247-223c6c57efc8-59qst" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-28 17:14:24 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-28 17:14:25 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-28 17:14:25 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-28 17:14:24 +0000 UTC Reason: Message:}])
  Apr 28 17:14:29.173: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 04/28/23 17:14:29.173
  Apr 28 17:14:29.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0428 17:14:29.185868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-65" for this suite. @ 04/28/23 17:14:29.19
• [5.121 seconds]
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 04/28/23 17:14:29.202
  Apr 28 17:14:29.202: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename var-expansion @ 04/28/23 17:14:29.203
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:14:29.219
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:14:29.225
  STEP: Creating a pod to test env composition @ 04/28/23 17:14:29.228
  E0428 17:14:30.186324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:31.186418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:32.186590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:33.186714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:14:33.288
  Apr 28 17:14:33.291: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod var-expansion-61ed2579-7689-48cd-b9e4-5f527f61bc3a container dapi-container: <nil>
  STEP: delete the pod @ 04/28/23 17:14:33.299
  Apr 28 17:14:33.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2178" for this suite. @ 04/28/23 17:14:33.313
• [4.116 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 04/28/23 17:14:33.319
  Apr 28 17:14:33.319: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename field-validation @ 04/28/23 17:14:33.319
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:14:33.33
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:14:33.333
  Apr 28 17:14:33.336: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  E0428 17:14:34.187467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:35.187758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0428 17:14:35.868507      19 warnings.go:70] unknown field "alpha"
  W0428 17:14:35.868530      19 warnings.go:70] unknown field "beta"
  W0428 17:14:35.868537      19 warnings.go:70] unknown field "delta"
  W0428 17:14:35.868543      19 warnings.go:70] unknown field "epsilon"
  W0428 17:14:35.868548      19 warnings.go:70] unknown field "gamma"
  Apr 28 17:14:35.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-12" for this suite. @ 04/28/23 17:14:35.896
• [2.583 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 04/28/23 17:14:35.904
  Apr 28 17:14:35.904: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/28/23 17:14:35.905
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:14:35.922
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:14:35.925
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 04/28/23 17:14:35.927
  Apr 28 17:14:35.928: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  E0428 17:14:36.188366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:37.194186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:38.194725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:39.194793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:40.195698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:41.196494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 04/28/23 17:14:41.983
  Apr 28 17:14:41.983: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  E0428 17:14:42.197449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:43.197708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:14:43.438: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  E0428 17:14:44.197707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:45.198678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:46.199161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:47.199365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:48.199905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:49.200108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:14:49.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3489" for this suite. @ 04/28/23 17:14:49.576
• [13.677 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 04/28/23 17:14:49.582
  Apr 28 17:14:49.582: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/28/23 17:14:49.583
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:14:49.604
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:14:49.607
  STEP: set up a multi version CRD @ 04/28/23 17:14:49.611
  Apr 28 17:14:49.612: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  E0428 17:14:50.200437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:51.201272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:52.202285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:53.202850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mark a version not serverd @ 04/28/23 17:14:53.374
  STEP: check the unserved version gets removed @ 04/28/23 17:14:53.391
  E0428 17:14:54.210267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 04/28/23 17:14:54.409
  E0428 17:14:55.211194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:56.211568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:57.215440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:14:57.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1780" for this suite. @ 04/28/23 17:14:57.404
• [7.827 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 04/28/23 17:14:57.411
  Apr 28 17:14:57.412: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename gc @ 04/28/23 17:14:57.414
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:14:57.428
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:14:57.431
  STEP: create the rc1 @ 04/28/23 17:14:57.436
  STEP: create the rc2 @ 04/28/23 17:14:57.441
  E0428 17:14:58.216071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:14:59.216211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:00.216315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:01.217668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:02.218989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:03.231037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 04/28/23 17:15:03.466
  E0428 17:15:04.233237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc simpletest-rc-to-be-deleted @ 04/28/23 17:15:04.713
  STEP: wait for the rc to be deleted @ 04/28/23 17:15:04.749
  E0428 17:15:05.271797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:06.240811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:07.241693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:08.241847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:09.242853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:15:09.767: INFO: 65 pods remaining
  Apr 28 17:15:09.767: INFO: 65 pods has nil DeletionTimestamp
  Apr 28 17:15:09.767: INFO: 
  E0428 17:15:10.243392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:11.243838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:12.251467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:13.252143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:14.252260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 04/28/23 17:15:14.763
  Apr 28 17:15:14.915: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 28 17:15:14.915: INFO: Deleting pod "simpletest-rc-to-be-deleted-22rl7" in namespace "gc-1808"
  Apr 28 17:15:14.929: INFO: Deleting pod "simpletest-rc-to-be-deleted-2c69r" in namespace "gc-1808"
  Apr 28 17:15:14.948: INFO: Deleting pod "simpletest-rc-to-be-deleted-2fsp4" in namespace "gc-1808"
  Apr 28 17:15:14.968: INFO: Deleting pod "simpletest-rc-to-be-deleted-2j6l9" in namespace "gc-1808"
  Apr 28 17:15:14.982: INFO: Deleting pod "simpletest-rc-to-be-deleted-2nh7b" in namespace "gc-1808"
  Apr 28 17:15:14.996: INFO: Deleting pod "simpletest-rc-to-be-deleted-2tlpg" in namespace "gc-1808"
  Apr 28 17:15:15.014: INFO: Deleting pod "simpletest-rc-to-be-deleted-4892f" in namespace "gc-1808"
  Apr 28 17:15:15.027: INFO: Deleting pod "simpletest-rc-to-be-deleted-4st6x" in namespace "gc-1808"
  Apr 28 17:15:15.044: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vkt4" in namespace "gc-1808"
  Apr 28 17:15:15.070: INFO: Deleting pod "simpletest-rc-to-be-deleted-5lb8h" in namespace "gc-1808"
  Apr 28 17:15:15.096: INFO: Deleting pod "simpletest-rc-to-be-deleted-5w5c2" in namespace "gc-1808"
  Apr 28 17:15:15.126: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wsvz" in namespace "gc-1808"
  Apr 28 17:15:15.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-65rqz" in namespace "gc-1808"
  Apr 28 17:15:15.187: INFO: Deleting pod "simpletest-rc-to-be-deleted-68kvg" in namespace "gc-1808"
  Apr 28 17:15:15.249: INFO: Deleting pod "simpletest-rc-to-be-deleted-6b9d7" in namespace "gc-1808"
  E0428 17:15:15.253152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:15:15.260: INFO: Deleting pod "simpletest-rc-to-be-deleted-6nkg9" in namespace "gc-1808"
  Apr 28 17:15:15.289: INFO: Deleting pod "simpletest-rc-to-be-deleted-7sw79" in namespace "gc-1808"
  Apr 28 17:15:15.323: INFO: Deleting pod "simpletest-rc-to-be-deleted-82ff9" in namespace "gc-1808"
  Apr 28 17:15:15.337: INFO: Deleting pod "simpletest-rc-to-be-deleted-86dct" in namespace "gc-1808"
  Apr 28 17:15:15.357: INFO: Deleting pod "simpletest-rc-to-be-deleted-8b5ds" in namespace "gc-1808"
  Apr 28 17:15:15.381: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bspl" in namespace "gc-1808"
  Apr 28 17:15:15.403: INFO: Deleting pod "simpletest-rc-to-be-deleted-8cvpm" in namespace "gc-1808"
  Apr 28 17:15:15.417: INFO: Deleting pod "simpletest-rc-to-be-deleted-8j6f4" in namespace "gc-1808"
  Apr 28 17:15:15.439: INFO: Deleting pod "simpletest-rc-to-be-deleted-96ktd" in namespace "gc-1808"
  Apr 28 17:15:15.462: INFO: Deleting pod "simpletest-rc-to-be-deleted-9p9kv" in namespace "gc-1808"
  Apr 28 17:15:15.497: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vkmf" in namespace "gc-1808"
  Apr 28 17:15:15.521: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5b9s" in namespace "gc-1808"
  Apr 28 17:15:15.542: INFO: Deleting pod "simpletest-rc-to-be-deleted-bhg5f" in namespace "gc-1808"
  Apr 28 17:15:15.584: INFO: Deleting pod "simpletest-rc-to-be-deleted-c4lv7" in namespace "gc-1808"
  Apr 28 17:15:15.595: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7jts" in namespace "gc-1808"
  Apr 28 17:15:15.613: INFO: Deleting pod "simpletest-rc-to-be-deleted-cjtwz" in namespace "gc-1808"
  Apr 28 17:15:15.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnbrd" in namespace "gc-1808"
  Apr 28 17:15:15.658: INFO: Deleting pod "simpletest-rc-to-be-deleted-dxxqz" in namespace "gc-1808"
  Apr 28 17:15:15.690: INFO: Deleting pod "simpletest-rc-to-be-deleted-dzwn7" in namespace "gc-1808"
  Apr 28 17:15:15.717: INFO: Deleting pod "simpletest-rc-to-be-deleted-fc2sz" in namespace "gc-1808"
  Apr 28 17:15:15.747: INFO: Deleting pod "simpletest-rc-to-be-deleted-fl788" in namespace "gc-1808"
  Apr 28 17:15:15.778: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzpcv" in namespace "gc-1808"
  Apr 28 17:15:15.815: INFO: Deleting pod "simpletest-rc-to-be-deleted-gbfd4" in namespace "gc-1808"
  Apr 28 17:15:15.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-gql4b" in namespace "gc-1808"
  Apr 28 17:15:15.883: INFO: Deleting pod "simpletest-rc-to-be-deleted-hglpw" in namespace "gc-1808"
  Apr 28 17:15:15.902: INFO: Deleting pod "simpletest-rc-to-be-deleted-hnj49" in namespace "gc-1808"
  Apr 28 17:15:15.915: INFO: Deleting pod "simpletest-rc-to-be-deleted-hz7lv" in namespace "gc-1808"
  Apr 28 17:15:15.941: INFO: Deleting pod "simpletest-rc-to-be-deleted-j722x" in namespace "gc-1808"
  Apr 28 17:15:15.973: INFO: Deleting pod "simpletest-rc-to-be-deleted-jwcmz" in namespace "gc-1808"
  Apr 28 17:15:15.996: INFO: Deleting pod "simpletest-rc-to-be-deleted-jxzhv" in namespace "gc-1808"
  Apr 28 17:15:16.011: INFO: Deleting pod "simpletest-rc-to-be-deleted-kdzst" in namespace "gc-1808"
  Apr 28 17:15:16.038: INFO: Deleting pod "simpletest-rc-to-be-deleted-kphmx" in namespace "gc-1808"
  Apr 28 17:15:16.052: INFO: Deleting pod "simpletest-rc-to-be-deleted-kzf24" in namespace "gc-1808"
  Apr 28 17:15:16.066: INFO: Deleting pod "simpletest-rc-to-be-deleted-ld84f" in namespace "gc-1808"
  Apr 28 17:15:16.095: INFO: Deleting pod "simpletest-rc-to-be-deleted-lp8vg" in namespace "gc-1808"
  Apr 28 17:15:16.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1808" for this suite. @ 04/28/23 17:15:16.136
• [18.736 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 04/28/23 17:15:16.147
  Apr 28 17:15:16.147: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 17:15:16.148
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:15:16.166
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:15:16.173
  STEP: Creating Pod @ 04/28/23 17:15:16.19
  E0428 17:15:16.257243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:17.257512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:18.258388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:19.258490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 04/28/23 17:15:20.236
  Apr 28 17:15:20.236: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7006 PodName:pod-sharedvolume-23841984-3d6f-490c-804c-f7b3282b4ceb ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:15:20.236: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 17:15:20.237: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:15:20.237: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/emptydir-7006/pods/pod-sharedvolume-23841984-3d6f-490c-804c-f7b3282b4ceb/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  E0428 17:15:20.259101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:15:20.369: INFO: Exec stderr: ""
  Apr 28 17:15:20.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7006" for this suite. @ 04/28/23 17:15:20.385
• [4.251 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 04/28/23 17:15:20.398
  Apr 28 17:15:20.398: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename configmap @ 04/28/23 17:15:20.399
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:15:20.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:15:20.423
  STEP: Creating configMap with name configmap-test-volume-4d03727f-cf8b-40b2-a69b-f698e8220123 @ 04/28/23 17:15:20.428
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:15:20.439
  E0428 17:15:21.259723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:22.259763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:23.259866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:24.260237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:15:24.47
  Apr 28 17:15:24.475: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-configmaps-67debcb9-e2c0-40e1-8263-8d0d0baeeafc container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 17:15:24.48
  Apr 28 17:15:24.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7902" for this suite. @ 04/28/23 17:15:24.496
• [4.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 04/28/23 17:15:24.503
  Apr 28 17:15:24.503: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 17:15:24.504
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:15:24.517
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:15:24.52
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 17:15:24.522
  E0428 17:15:25.261262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:26.262275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:27.270561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:28.270895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:15:28.541
  Apr 28 17:15:28.550: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod downwardapi-volume-b231068c-ac7a-4759-a014-00646454aaa8 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 17:15:28.558
  Apr 28 17:15:28.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6478" for this suite. @ 04/28/23 17:15:28.577
• [4.079 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 04/28/23 17:15:28.583
  Apr 28 17:15:28.583: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 17:15:28.583
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:15:28.598
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:15:28.602
  STEP: Creating a ResourceQuota @ 04/28/23 17:15:28.606
  STEP: Getting a ResourceQuota @ 04/28/23 17:15:28.61
  STEP: Listing all ResourceQuotas with LabelSelector @ 04/28/23 17:15:28.614
  STEP: Patching the ResourceQuota @ 04/28/23 17:15:28.616
  STEP: Deleting a Collection of ResourceQuotas @ 04/28/23 17:15:28.621
  STEP: Verifying the deleted ResourceQuota @ 04/28/23 17:15:28.633
  Apr 28 17:15:28.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5952" for this suite. @ 04/28/23 17:15:28.639
• [0.062 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 04/28/23 17:15:28.645
  Apr 28 17:15:28.645: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename dns @ 04/28/23 17:15:28.646
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:15:28.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:15:28.662
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2934.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2934.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 04/28/23 17:15:28.664
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2934.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2934.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 04/28/23 17:15:28.664
  STEP: creating a pod to probe /etc/hosts @ 04/28/23 17:15:28.664
  STEP: submitting the pod to kubernetes @ 04/28/23 17:15:28.664
  E0428 17:15:29.271013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:30.271796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/28/23 17:15:30.681
  STEP: looking for the results for each expected name from probers @ 04/28/23 17:15:30.683
  Apr 28 17:15:30.697: INFO: DNS probes using dns-2934/dns-test-e287a036-15b6-4e2d-918d-2db5d61f388a succeeded

  Apr 28 17:15:30.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 17:15:30.7
  STEP: Destroying namespace "dns-2934" for this suite. @ 04/28/23 17:15:30.712
• [2.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 04/28/23 17:15:30.722
  Apr 28 17:15:30.722: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename csistoragecapacity @ 04/28/23 17:15:30.723
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:15:30.745
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:15:30.747
  STEP: getting /apis @ 04/28/23 17:15:30.75
  STEP: getting /apis/storage.k8s.io @ 04/28/23 17:15:30.753
  STEP: getting /apis/storage.k8s.io/v1 @ 04/28/23 17:15:30.754
  STEP: creating @ 04/28/23 17:15:30.755
  STEP: watching @ 04/28/23 17:15:30.767
  Apr 28 17:15:30.767: INFO: starting watch
  STEP: getting @ 04/28/23 17:15:30.772
  STEP: listing in namespace @ 04/28/23 17:15:30.774
  STEP: listing across namespaces @ 04/28/23 17:15:30.776
  STEP: patching @ 04/28/23 17:15:30.778
  STEP: updating @ 04/28/23 17:15:30.784
  Apr 28 17:15:30.790: INFO: waiting for watch events with expected annotations in namespace
  Apr 28 17:15:30.790: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 04/28/23 17:15:30.79
  STEP: deleting a collection @ 04/28/23 17:15:30.807
  Apr 28 17:15:30.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-1902" for this suite. @ 04/28/23 17:15:30.836
• [0.119 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 04/28/23 17:15:30.845
  Apr 28 17:15:30.845: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 17:15:30.846
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:15:30.862
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:15:30.865
  STEP: creating the pod @ 04/28/23 17:15:30.868
  Apr 28 17:15:30.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-5469 create -f -'
  E0428 17:15:31.271893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:15:31.917: INFO: stderr: ""
  Apr 28 17:15:31.917: INFO: stdout: "pod/pause created\n"
  E0428 17:15:32.272022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:33.272368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 04/28/23 17:15:33.926
  Apr 28 17:15:33.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-5469 label pods pause testing-label=testing-label-value'
  Apr 28 17:15:34.103: INFO: stderr: ""
  Apr 28 17:15:34.103: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 04/28/23 17:15:34.103
  Apr 28 17:15:34.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-5469 get pod pause -L testing-label'
  Apr 28 17:15:34.251: INFO: stderr: ""
  Apr 28 17:15:34.251: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 04/28/23 17:15:34.251
  Apr 28 17:15:34.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-5469 label pods pause testing-label-'
  E0428 17:15:34.272479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:15:34.368: INFO: stderr: ""
  Apr 28 17:15:34.368: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 04/28/23 17:15:34.368
  Apr 28 17:15:34.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-5469 get pod pause -L testing-label'
  Apr 28 17:15:34.472: INFO: stderr: ""
  Apr 28 17:15:34.472: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
  STEP: using delete to clean up resources @ 04/28/23 17:15:34.472
  Apr 28 17:15:34.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-5469 delete --grace-period=0 --force -f -'
  Apr 28 17:15:34.581: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 28 17:15:34.581: INFO: stdout: "pod \"pause\" force deleted\n"
  Apr 28 17:15:34.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-5469 get rc,svc -l name=pause --no-headers'
  Apr 28 17:15:34.655: INFO: stderr: "No resources found in kubectl-5469 namespace.\n"
  Apr 28 17:15:34.655: INFO: stdout: ""
  Apr 28 17:15:34.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-5469 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 28 17:15:34.716: INFO: stderr: ""
  Apr 28 17:15:34.716: INFO: stdout: ""
  Apr 28 17:15:34.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5469" for this suite. @ 04/28/23 17:15:34.721
• [3.882 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 04/28/23 17:15:34.726
  Apr 28 17:15:34.726: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename ingressclass @ 04/28/23 17:15:34.727
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:15:34.742
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:15:34.744
  STEP: getting /apis @ 04/28/23 17:15:34.746
  STEP: getting /apis/networking.k8s.io @ 04/28/23 17:15:34.75
  STEP: getting /apis/networking.k8s.iov1 @ 04/28/23 17:15:34.751
  STEP: creating @ 04/28/23 17:15:34.752
  STEP: getting @ 04/28/23 17:15:34.766
  STEP: listing @ 04/28/23 17:15:34.768
  STEP: watching @ 04/28/23 17:15:34.77
  Apr 28 17:15:34.770: INFO: starting watch
  STEP: patching @ 04/28/23 17:15:34.771
  STEP: updating @ 04/28/23 17:15:34.775
  Apr 28 17:15:34.780: INFO: waiting for watch events with expected annotations
  Apr 28 17:15:34.780: INFO: saw patched and updated annotations
  STEP: deleting @ 04/28/23 17:15:34.78
  STEP: deleting a collection @ 04/28/23 17:15:34.789
  Apr 28 17:15:34.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-3814" for this suite. @ 04/28/23 17:15:34.806
• [0.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 04/28/23 17:15:34.822
  Apr 28 17:15:34.822: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 17:15:34.823
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:15:34.848
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:15:34.861
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 17:15:34.864
  E0428 17:15:35.272570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:36.272717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:37.273712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:38.275629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:15:38.892
  Apr 28 17:15:38.896: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod downwardapi-volume-a3b64dd4-94c1-48d0-9da9-e7640acc8480 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 17:15:38.901
  Apr 28 17:15:38.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5065" for this suite. @ 04/28/23 17:15:38.918
• [4.102 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 04/28/23 17:15:38.926
  Apr 28 17:15:38.926: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename field-validation @ 04/28/23 17:15:38.927
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:15:38.939
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:15:38.943
  Apr 28 17:15:38.948: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  E0428 17:15:39.275794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:40.275916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:41.276090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:15:41.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9087" for this suite. @ 04/28/23 17:15:41.5
• [2.578 seconds]
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 04/28/23 17:15:41.505
  Apr 28 17:15:41.505: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename kubectl-logs @ 04/28/23 17:15:41.506
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:15:41.521
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:15:41.523
  STEP: creating an pod @ 04/28/23 17:15:41.525
  Apr 28 17:15:41.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-logs-790 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Apr 28 17:15:41.597: INFO: stderr: ""
  Apr 28 17:15:41.597: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 04/28/23 17:15:41.597
  Apr 28 17:15:41.597: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E0428 17:15:42.277572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:43.278063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:15:43.604: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 04/28/23 17:15:43.604
  Apr 28 17:15:43.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-logs-790 logs logs-generator logs-generator'
  Apr 28 17:15:43.687: INFO: stderr: ""
  Apr 28 17:15:43.687: INFO: stdout: "I0428 17:15:42.373905       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/qnr 344\nI0428 17:15:42.573997       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/nhb 242\nI0428 17:15:42.774650       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/r597 369\nI0428 17:15:42.974945       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/xl7 396\nI0428 17:15:43.174326       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/8cbt 346\nI0428 17:15:43.374617       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/4h5 284\nI0428 17:15:43.574916       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/8nbx 460\n"
  STEP: limiting log lines @ 04/28/23 17:15:43.687
  Apr 28 17:15:43.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-logs-790 logs logs-generator logs-generator --tail=1'
  Apr 28 17:15:43.784: INFO: stderr: ""
  Apr 28 17:15:43.784: INFO: stdout: "I0428 17:15:43.774211       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/r8jm 273\n"
  Apr 28 17:15:43.784: INFO: got output "I0428 17:15:43.774211       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/r8jm 273\n"
  STEP: limiting log bytes @ 04/28/23 17:15:43.784
  Apr 28 17:15:43.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-logs-790 logs logs-generator logs-generator --limit-bytes=1'
  Apr 28 17:15:43.895: INFO: stderr: ""
  Apr 28 17:15:43.895: INFO: stdout: "I"
  Apr 28 17:15:43.895: INFO: got output "I"
  STEP: exposing timestamps @ 04/28/23 17:15:43.895
  Apr 28 17:15:43.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-logs-790 logs logs-generator logs-generator --tail=1 --timestamps'
  Apr 28 17:15:44.144: INFO: stderr: ""
  Apr 28 17:15:44.144: INFO: stdout: "2023-04-28T17:15:43.977485581Z I0428 17:15:43.977382       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/jwrz 548\n"
  Apr 28 17:15:44.144: INFO: got output "2023-04-28T17:15:43.977485581Z I0428 17:15:43.977382       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/jwrz 548\n"
  STEP: restricting to a time range @ 04/28/23 17:15:44.144
  E0428 17:15:44.278730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:45.279139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:46.279247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:15:46.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-logs-790 logs logs-generator logs-generator --since=1s'
  Apr 28 17:15:46.743: INFO: stderr: ""
  Apr 28 17:15:46.743: INFO: stdout: "I0428 17:15:45.774826       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/tz8 478\nI0428 17:15:45.974011       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/65b 428\nI0428 17:15:46.174202       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/4sbd 503\nI0428 17:15:46.374497       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/c72 338\nI0428 17:15:46.574909       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/g6d 474\n"
  Apr 28 17:15:46.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-logs-790 logs logs-generator logs-generator --since=24h'
  Apr 28 17:15:46.829: INFO: stderr: ""
  Apr 28 17:15:46.829: INFO: stdout: "I0428 17:15:42.373905       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/qnr 344\nI0428 17:15:42.573997       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/nhb 242\nI0428 17:15:42.774650       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/r597 369\nI0428 17:15:42.974945       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/xl7 396\nI0428 17:15:43.174326       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/8cbt 346\nI0428 17:15:43.374617       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/4h5 284\nI0428 17:15:43.574916       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/8nbx 460\nI0428 17:15:43.774211       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/r8jm 273\nI0428 17:15:43.977382       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/jwrz 548\nI0428 17:15:44.175634       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/b6p 225\nI0428 17:15:44.373955       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/7tr 341\nI0428 17:15:44.574182       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/96jc 489\nI0428 17:15:44.774449       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/jhmv 252\nI0428 17:15:44.974755       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/ss8 565\nI0428 17:15:45.174006       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/28h 536\nI0428 17:15:45.374188       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/lp7c 531\nI0428 17:15:45.574504       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/xt57 347\nI0428 17:15:45.774826       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/tz8 478\nI0428 17:15:45.974011       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/65b 428\nI0428 17:15:46.174202       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/4sbd 503\nI0428 17:15:46.374497       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/c72 338\nI0428 17:15:46.574909       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/g6d 474\nI0428 17:15:46.774003       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/f75 553\n"
  Apr 28 17:15:46.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-logs-790 delete pod logs-generator'
  E0428 17:15:47.279806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:48.280479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:15:48.590: INFO: stderr: ""
  Apr 28 17:15:48.590: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Apr 28 17:15:48.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-790" for this suite. @ 04/28/23 17:15:48.594
• [7.093 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 04/28/23 17:15:48.601
  Apr 28 17:15:48.601: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 17:15:48.602
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:15:48.613
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:15:48.617
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 04/28/23 17:15:48.619
  E0428 17:15:49.280577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:50.281211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:51.282088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:52.282195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:15:52.639
  Apr 28 17:15:52.641: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-05770d20-d90c-4092-83f2-0066aed2e352 container test-container: <nil>
  STEP: delete the pod @ 04/28/23 17:15:52.647
  Apr 28 17:15:52.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3462" for this suite. @ 04/28/23 17:15:52.665
• [4.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 04/28/23 17:15:52.675
  Apr 28 17:15:52.675: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename secrets @ 04/28/23 17:15:52.676
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:15:52.692
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:15:52.694
  STEP: Creating secret with name secret-test-map-4ca2f857-6d48-4ff1-a614-0bf403d2c496 @ 04/28/23 17:15:52.696
  STEP: Creating a pod to test consume secrets @ 04/28/23 17:15:52.701
  E0428 17:15:53.282294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:54.282394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:55.283294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:56.283482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:15:56.72
  Apr 28 17:15:56.723: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-secrets-5f57fa39-2472-4f3c-9665-3fcbf741d776 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 17:15:56.728
  Apr 28 17:15:56.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7542" for this suite. @ 04/28/23 17:15:56.744
• [4.076 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 04/28/23 17:15:56.751
  Apr 28 17:15:56.751: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename runtimeclass @ 04/28/23 17:15:56.751
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:15:56.77
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:15:56.772
  E0428 17:15:57.284381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:15:58.285277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:15:58.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-4748" for this suite. @ 04/28/23 17:15:58.798
• [2.052 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 04/28/23 17:15:58.81
  Apr 28 17:15:58.810: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename statefulset @ 04/28/23 17:15:58.81
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:15:58.823
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:15:58.828
  STEP: Creating service test in namespace statefulset-8698 @ 04/28/23 17:15:58.83
  STEP: Creating a new StatefulSet @ 04/28/23 17:15:58.835
  Apr 28 17:15:58.859: INFO: Found 1 stateful pods, waiting for 3
  E0428 17:15:59.285385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:00.285468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:01.285812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:02.286353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:03.287259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:04.287408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:05.287725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:06.287966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:07.288132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:08.290754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:16:08.862: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 17:16:08.862: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 17:16:08.862: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 17:16:08.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=statefulset-8698 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 28 17:16:09.034: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 28 17:16:09.034: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 28 17:16:09.034: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0428 17:16:09.291043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:10.291552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:11.291664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:12.291855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:13.292040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:14.292155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:15.292778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:16.292898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:17.292993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:18.293225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 04/28/23 17:16:19.051
  Apr 28 17:16:19.069: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 04/28/23 17:16:19.069
  E0428 17:16:19.294283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:20.294414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:21.294520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:22.294654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:23.295351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:24.295443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:25.296390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:26.296613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:27.296846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:28.297216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating Pods in reverse ordinal order @ 04/28/23 17:16:29.087
  Apr 28 17:16:29.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=statefulset-8698 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 28 17:16:29.228: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 28 17:16:29.228: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 28 17:16:29.228: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0428 17:16:29.298105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:30.298887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:31.299064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:32.299150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:33.299775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:34.299895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:35.300984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:36.301961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:37.301318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:38.301420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back to a previous revision @ 04/28/23 17:16:39.249
  Apr 28 17:16:39.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=statefulset-8698 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0428 17:16:39.302222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:16:39.416: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 28 17:16:39.416: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 28 17:16:39.416: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0428 17:16:40.303045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:41.303170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:42.303392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:43.304248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:44.304577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:45.305044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:46.305207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:47.305329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:48.305622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:49.306276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:16:49.446: INFO: Updating stateful set ss2
  E0428 17:16:50.306368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:51.307373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:52.307474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:53.308074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:54.308507      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:55.308612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:56.309449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:57.309555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:58.309778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:16:59.310492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back update in reverse ordinal order @ 04/28/23 17:16:59.457
  Apr 28 17:16:59.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=statefulset-8698 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 28 17:16:59.621: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 28 17:16:59.621: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 28 17:16:59.621: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0428 17:17:00.311363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:01.311667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:02.312015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:03.312122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:04.312326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:05.312600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:06.314103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:07.314368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:08.315253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:09.315397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:17:09.642: INFO: Deleting all statefulset in ns statefulset-8698
  Apr 28 17:17:09.645: INFO: Scaling statefulset ss2 to 0
  E0428 17:17:10.315553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:11.315783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:12.315905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:13.316286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:14.316447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:15.316605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:16.316928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:17.317238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:18.317352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:19.317574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:17:19.658: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 17:17:19.660: INFO: Deleting statefulset ss2
  Apr 28 17:17:19.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8698" for this suite. @ 04/28/23 17:17:19.676
• [80.873 seconds]
------------------------------
SS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 04/28/23 17:17:19.684
  Apr 28 17:17:19.684: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename deployment @ 04/28/23 17:17:19.686
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:17:19.707
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:17:19.711
  Apr 28 17:17:19.714: INFO: Creating deployment "test-recreate-deployment"
  Apr 28 17:17:19.719: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Apr 28 17:17:19.736: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  E0428 17:17:20.332754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:21.332790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:17:21.741: INFO: Waiting deployment "test-recreate-deployment" to complete
  Apr 28 17:17:21.744: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Apr 28 17:17:21.753: INFO: Updating deployment test-recreate-deployment
  Apr 28 17:17:21.753: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Apr 28 17:17:21.825: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-1273  98611107-d4ae-426e-8287-87b57d438713 275132 2 2023-04-28 17:17:19 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-28 17:17:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-28 17:17:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00288ce88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-28 17:17:21 +0000 UTC,LastTransitionTime:2023-04-28 17:17:21 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-04-28 17:17:21 +0000 UTC,LastTransitionTime:2023-04-28 17:17:19 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Apr 28 17:17:21.833: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-1273  39082fff-00a7-404c-8ce7-93cc23ca6ff8 275129 1 2023-04-28 17:17:21 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 98611107-d4ae-426e-8287-87b57d438713 0xc00288d247 0xc00288d248}] [] [{kube-controller-manager Update apps/v1 2023-04-28 17:17:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"98611107-d4ae-426e-8287-87b57d438713\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-28 17:17:21 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00288d2e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 17:17:21.833: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Apr 28 17:17:21.834: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-1273  ddcedfc3-23ea-458b-9236-a1e9f474495b 275120 2 2023-04-28 17:17:19 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 98611107-d4ae-426e-8287-87b57d438713 0xc00288d357 0xc00288d358}] [] [{kube-controller-manager Update apps/v1 2023-04-28 17:17:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"98611107-d4ae-426e-8287-87b57d438713\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-28 17:17:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00288d528 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 17:17:21.849: INFO: Pod "test-recreate-deployment-54757ffd6c-jsfkt" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-jsfkt test-recreate-deployment-54757ffd6c- deployment-1273  9b1ee4a9-9f05-4bfc-9cb1-fc2ede4c4379 275133 0 2023-04-28 17:17:21 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 39082fff-00a7-404c-8ce7-93cc23ca6ff8 0xc00426e177 0xc00426e178}] [] [{kube-controller-manager Update v1 2023-04-28 17:17:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39082fff-00a7-404c-8ce7-93cc23ca6ff8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-28 17:17:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lfxc4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lfxc4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-127.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:17:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:17:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:17:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:17:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.127,PodIP:,StartTime:2023-04-28 17:17:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:17:21.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1273" for this suite. @ 04/28/23 17:17:21.853
• [2.175 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 04/28/23 17:17:21.86
  Apr 28 17:17:21.860: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 17:17:21.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:17:21.872
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:17:21.876
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/28/23 17:17:21.878
  Apr 28 17:17:21.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-7595 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Apr 28 17:17:21.947: INFO: stderr: ""
  Apr 28 17:17:21.947: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 04/28/23 17:17:21.947
  Apr 28 17:17:21.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-7595 delete pods e2e-test-httpd-pod'
  E0428 17:17:22.334929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:23.335044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:17:23.890: INFO: stderr: ""
  Apr 28 17:17:23.890: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 28 17:17:23.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7595" for this suite. @ 04/28/23 17:17:23.894
• [2.039 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 04/28/23 17:17:23.899
  Apr 28 17:17:23.899: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename runtimeclass @ 04/28/23 17:17:23.9
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:17:23.913
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:17:23.916
  E0428 17:17:24.335592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:25.335726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:17:25.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-8012" for this suite. @ 04/28/23 17:17:25.942
• [2.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 04/28/23 17:17:25.948
  Apr 28 17:17:25.948: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename services @ 04/28/23 17:17:25.953
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:17:25.965
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:17:25.969
  STEP: creating service in namespace services-1636 @ 04/28/23 17:17:25.971
  STEP: creating service affinity-nodeport in namespace services-1636 @ 04/28/23 17:17:25.971
  STEP: creating replication controller affinity-nodeport in namespace services-1636 @ 04/28/23 17:17:25.982
  I0428 17:17:25.997386      19 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-1636, replica count: 3
  E0428 17:17:26.336745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:27.337640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:28.338078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 17:17:29.049117      19 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 28 17:17:29.058: INFO: Creating new exec pod
  E0428 17:17:29.338715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:30.339785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:31.340072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:17:32.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-1636 exec execpod-affinitydc6kh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Apr 28 17:17:32.318: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Apr 28 17:17:32.318: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:17:32.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-1636 exec execpod-affinitydc6kh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.22.10 80'
  E0428 17:17:32.341909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:17:32.490: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.22.10 80\nConnection to 10.43.22.10 80 port [tcp/http] succeeded!\n"
  Apr 28 17:17:32.490: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:17:32.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-1636 exec execpod-affinitydc6kh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.14.195 32661'
  Apr 28 17:17:32.650: INFO: stderr: "+ nc -v -t -w 2 172.31.14.195 32661\n+ echo hostName\nConnection to 172.31.14.195 32661 port [tcp/*] succeeded!\n"
  Apr 28 17:17:32.650: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:17:32.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-1636 exec execpod-affinitydc6kh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.9.127 32661'
  Apr 28 17:17:33.011: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.9.127 32661\nConnection to 172.31.9.127 32661 port [tcp/*] succeeded!\n"
  Apr 28 17:17:33.011: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:17:33.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-1636 exec execpod-affinitydc6kh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.161:32661/ ; done'
  Apr 28 17:17:33.307: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:32661/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:32661/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:32661/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:32661/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:32661/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:32661/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:32661/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:32661/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:32661/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:32661/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:32661/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:32661/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:32661/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:32661/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:32661/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.161:32661/\n"
  Apr 28 17:17:33.307: INFO: stdout: "\naffinity-nodeport-58ddb\naffinity-nodeport-58ddb\naffinity-nodeport-58ddb\naffinity-nodeport-58ddb\naffinity-nodeport-58ddb\naffinity-nodeport-58ddb\naffinity-nodeport-58ddb\naffinity-nodeport-58ddb\naffinity-nodeport-58ddb\naffinity-nodeport-58ddb\naffinity-nodeport-58ddb\naffinity-nodeport-58ddb\naffinity-nodeport-58ddb\naffinity-nodeport-58ddb\naffinity-nodeport-58ddb\naffinity-nodeport-58ddb"
  Apr 28 17:17:33.307: INFO: Received response from host: affinity-nodeport-58ddb
  Apr 28 17:17:33.307: INFO: Received response from host: affinity-nodeport-58ddb
  Apr 28 17:17:33.307: INFO: Received response from host: affinity-nodeport-58ddb
  Apr 28 17:17:33.307: INFO: Received response from host: affinity-nodeport-58ddb
  Apr 28 17:17:33.307: INFO: Received response from host: affinity-nodeport-58ddb
  Apr 28 17:17:33.307: INFO: Received response from host: affinity-nodeport-58ddb
  Apr 28 17:17:33.307: INFO: Received response from host: affinity-nodeport-58ddb
  Apr 28 17:17:33.307: INFO: Received response from host: affinity-nodeport-58ddb
  Apr 28 17:17:33.307: INFO: Received response from host: affinity-nodeport-58ddb
  Apr 28 17:17:33.307: INFO: Received response from host: affinity-nodeport-58ddb
  Apr 28 17:17:33.307: INFO: Received response from host: affinity-nodeport-58ddb
  Apr 28 17:17:33.307: INFO: Received response from host: affinity-nodeport-58ddb
  Apr 28 17:17:33.307: INFO: Received response from host: affinity-nodeport-58ddb
  Apr 28 17:17:33.307: INFO: Received response from host: affinity-nodeport-58ddb
  Apr 28 17:17:33.307: INFO: Received response from host: affinity-nodeport-58ddb
  Apr 28 17:17:33.307: INFO: Received response from host: affinity-nodeport-58ddb
  Apr 28 17:17:33.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 17:17:33.311: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-1636, will wait for the garbage collector to delete the pods @ 04/28/23 17:17:33.329
  E0428 17:17:33.342957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:17:33.392: INFO: Deleting ReplicationController affinity-nodeport took: 5.228279ms
  Apr 28 17:17:33.492: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.284193ms
  E0428 17:17:34.343387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:35.344432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-1636" for this suite. @ 04/28/23 17:17:35.716
• [9.772 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 04/28/23 17:17:35.723
  Apr 28 17:17:35.723: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename webhook @ 04/28/23 17:17:35.724
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:17:35.74
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:17:35.743
  STEP: Setting up server cert @ 04/28/23 17:17:35.76
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 17:17:36.056
  STEP: Deploying the webhook pod @ 04/28/23 17:17:36.062
  STEP: Wait for the deployment to be ready @ 04/28/23 17:17:36.073
  Apr 28 17:17:36.087: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0428 17:17:36.345366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:37.345493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 17:17:38.097
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 17:17:38.106
  E0428 17:17:38.346734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:17:39.106: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 04/28/23 17:17:39.109
  STEP: create a pod that should be denied by the webhook @ 04/28/23 17:17:39.128
  STEP: create a pod that causes the webhook to hang @ 04/28/23 17:17:39.144
  E0428 17:17:39.347203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:40.347744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:41.347870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:42.348077      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:43.348352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:44.348668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:45.348859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:46.349046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:47.349200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:48.349347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 04/28/23 17:17:49.15
  STEP: create a configmap that should be admitted by the webhook @ 04/28/23 17:17:49.165
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 04/28/23 17:17:49.176
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 04/28/23 17:17:49.182
  STEP: create a namespace that bypass the webhook @ 04/28/23 17:17:49.186
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 04/28/23 17:17:49.205
  Apr 28 17:17:49.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0428 17:17:49.349584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-6388" for this suite. @ 04/28/23 17:17:49.365
  STEP: Destroying namespace "webhook-markers-6895" for this suite. @ 04/28/23 17:17:49.379
  STEP: Destroying namespace "exempted-namespace-4262" for this suite. @ 04/28/23 17:17:49.416
• [13.701 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 04/28/23 17:17:49.424
  Apr 28 17:17:49.424: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename gc @ 04/28/23 17:17:49.425
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:17:49.449
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:17:49.451
  STEP: create the deployment @ 04/28/23 17:17:49.453
  W0428 17:17:49.458445      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 04/28/23 17:17:49.458
  STEP: delete the deployment @ 04/28/23 17:17:49.979
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 04/28/23 17:17:50.007
  E0428 17:17:50.349767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 04/28/23 17:17:50.538
  Apr 28 17:17:50.605: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 28 17:17:50.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4975" for this suite. @ 04/28/23 17:17:50.609
• [1.189 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 04/28/23 17:17:50.615
  Apr 28 17:17:50.615: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename container-probe @ 04/28/23 17:17:50.616
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:17:50.627
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:17:50.63
  STEP: Creating pod liveness-aa00772d-4cc3-49c0-99b7-4bc29021c76f in namespace container-probe-6194 @ 04/28/23 17:17:50.632
  E0428 17:17:51.350797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:52.351074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:17:52.646: INFO: Started pod liveness-aa00772d-4cc3-49c0-99b7-4bc29021c76f in namespace container-probe-6194
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/28/23 17:17:52.646
  Apr 28 17:17:52.648: INFO: Initial restart count of pod liveness-aa00772d-4cc3-49c0-99b7-4bc29021c76f is 0
  E0428 17:17:53.351400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:54.351379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:55.351476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:56.351582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:57.352563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:58.352691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:17:59.353255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:00.354313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:01.354379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:02.354682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:03.355561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:04.355783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:05.355821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:06.355928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:07.356154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:08.356240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:09.357193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:10.357412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:11.357501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:12.357719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:13.357837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:14.358348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:15.358431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:16.358706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:17.358850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:18.358960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:19.359389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:20.360285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:21.361280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:22.362282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:23.362634      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:24.362755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:25.362870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:26.362951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:27.363218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:28.363499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:29.364101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:30.364360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:31.364463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:32.364724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:33.365569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:34.366263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:35.366464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:36.367173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:37.367240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:38.367519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:39.368238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:40.368423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:41.368570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:42.368672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:43.369357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:44.370313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:45.370493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:46.370686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:47.371715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:48.372026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:49.373108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:50.373279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:51.374295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:52.374749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:53.375570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:54.375682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:55.376276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:56.376681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:57.376744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:58.377145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:18:59.377914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:00.378346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:01.378372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:02.378550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:03.379476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:04.381296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:05.381401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:06.381513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:07.382131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:08.382253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:09.382468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:10.382620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:11.382691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:12.382792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:13.383870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:14.384297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:15.384196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:16.384320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:17.385268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:18.385392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:19.385498      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:20.385618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:21.385719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:22.386273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:23.386706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:24.386905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:25.387013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:26.387205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:27.387418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:28.387708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:29.388162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:30.388357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:31.388480      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:32.388584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:33.389374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:34.390240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:35.390344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:36.390452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:37.390554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:38.390656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:39.391583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:40.391814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:41.391885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:42.391998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:43.392111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:44.392357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:45.392513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:46.392740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:47.393438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:48.394282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:49.394410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:50.394650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:51.395291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:52.395430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:53.396251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:54.396555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:55.397445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:56.397929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:57.397886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:58.397987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:19:59.398322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:00.398437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:01.398639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:02.398849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:03.399310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:04.399438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:05.400399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:06.400517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:07.401413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:08.402341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:09.402441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:10.402521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:11.403550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:12.403755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:13.404692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:14.404946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:15.406015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:16.407066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:17.407183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:18.407486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:19.407557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:20.407712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:21.407777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:22.407872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:23.407950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:24.408075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:25.408311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:26.408422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:27.408508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:28.408609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:29.409637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:30.409748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:31.409841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:32.409984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:33.410269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:34.410492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:35.410957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:36.411126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:37.411956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:38.412302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:39.413312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:40.413409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:41.414310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:42.414659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:43.414768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:44.420272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:45.421327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:46.421468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:47.421515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:48.421621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:49.422335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:50.422589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:51.422646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:52.422864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:53.423243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:54.423514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:55.423905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:56.424312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:57.423827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:58.423895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:20:59.424625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:00.424734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:01.424843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:02.424979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:03.425028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:04.425285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:05.426293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:06.426573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:07.427369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:08.427628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:09.427720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:10.427873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:11.428006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:12.428246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:13.428359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:14.428670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:15.428822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:16.429783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:17.430799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:18.430906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:19.431897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:20.432019      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:21.432878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:22.433462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:23.433919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:24.434076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:25.434984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:26.435092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:27.435604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:28.436028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:29.437000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:30.437195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:31.437796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:32.437897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:33.438048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:34.438241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:35.438342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:36.438900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:37.439674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:38.439915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:39.440893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:40.441223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:41.441214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:42.442338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:43.442909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:44.446235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:45.446305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:46.446441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:47.447468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:48.447582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:49.448594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:50.448751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:51.449768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:52.449938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:21:53.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 17:21:53.227
  STEP: Destroying namespace "container-probe-6194" for this suite. @ 04/28/23 17:21:53.271
• [242.665 seconds]
------------------------------
SS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 04/28/23 17:21:53.286
  Apr 28 17:21:53.286: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename endpointslicemirroring @ 04/28/23 17:21:53.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:21:53.307
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:21:53.311
  STEP: mirroring a new custom Endpoint @ 04/28/23 17:21:53.332
  Apr 28 17:21:53.348: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  E0428 17:21:53.450572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:54.450968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring an update to a custom Endpoint @ 04/28/23 17:21:55.351
  Apr 28 17:21:55.358: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  E0428 17:21:55.451423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:56.451629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring deletion of a custom Endpoint @ 04/28/23 17:21:57.362
  Apr 28 17:21:57.372: INFO: Waiting for 0 EndpointSlices to exist, got 1
  E0428 17:21:57.452401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:21:58.452970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:21:59.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-3708" for this suite. @ 04/28/23 17:21:59.377
• [6.095 seconds]
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 04/28/23 17:21:59.382
  Apr 28 17:21:59.382: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename security-context @ 04/28/23 17:21:59.383
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:21:59.401
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:21:59.406
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 04/28/23 17:21:59.408
  E0428 17:21:59.455400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:00.455701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:01.456124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:02.456250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:22:03.427
  Apr 28 17:22:03.430: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod security-context-18a96fa6-f467-4a54-bb74-3ae7fe2c7d69 container test-container: <nil>
  STEP: delete the pod @ 04/28/23 17:22:03.443
  E0428 17:22:03.457742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:22:03.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-3243" for this suite. @ 04/28/23 17:22:03.464
• [4.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 04/28/23 17:22:03.473
  Apr 28 17:22:03.473: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename discovery @ 04/28/23 17:22:03.474
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:22:03.492
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:22:03.494
  STEP: Setting up server cert @ 04/28/23 17:22:03.498
  Apr 28 17:22:03.797: INFO: Checking APIGroup: apiregistration.k8s.io
  Apr 28 17:22:03.798: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Apr 28 17:22:03.798: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Apr 28 17:22:03.798: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Apr 28 17:22:03.798: INFO: Checking APIGroup: apps
  Apr 28 17:22:03.799: INFO: PreferredVersion.GroupVersion: apps/v1
  Apr 28 17:22:03.799: INFO: Versions found [{apps/v1 v1}]
  Apr 28 17:22:03.799: INFO: apps/v1 matches apps/v1
  Apr 28 17:22:03.799: INFO: Checking APIGroup: events.k8s.io
  Apr 28 17:22:03.801: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Apr 28 17:22:03.802: INFO: Versions found [{events.k8s.io/v1 v1}]
  Apr 28 17:22:03.802: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Apr 28 17:22:03.802: INFO: Checking APIGroup: authentication.k8s.io
  Apr 28 17:22:03.803: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Apr 28 17:22:03.804: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Apr 28 17:22:03.804: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Apr 28 17:22:03.804: INFO: Checking APIGroup: authorization.k8s.io
  Apr 28 17:22:03.814: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Apr 28 17:22:03.814: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Apr 28 17:22:03.814: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Apr 28 17:22:03.814: INFO: Checking APIGroup: autoscaling
  Apr 28 17:22:03.815: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Apr 28 17:22:03.815: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Apr 28 17:22:03.815: INFO: autoscaling/v2 matches autoscaling/v2
  Apr 28 17:22:03.815: INFO: Checking APIGroup: batch
  Apr 28 17:22:03.817: INFO: PreferredVersion.GroupVersion: batch/v1
  Apr 28 17:22:03.817: INFO: Versions found [{batch/v1 v1}]
  Apr 28 17:22:03.817: INFO: batch/v1 matches batch/v1
  Apr 28 17:22:03.817: INFO: Checking APIGroup: certificates.k8s.io
  Apr 28 17:22:03.818: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Apr 28 17:22:03.818: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Apr 28 17:22:03.818: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Apr 28 17:22:03.818: INFO: Checking APIGroup: networking.k8s.io
  Apr 28 17:22:03.819: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Apr 28 17:22:03.819: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Apr 28 17:22:03.819: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Apr 28 17:22:03.819: INFO: Checking APIGroup: policy
  Apr 28 17:22:03.820: INFO: PreferredVersion.GroupVersion: policy/v1
  Apr 28 17:22:03.820: INFO: Versions found [{policy/v1 v1}]
  Apr 28 17:22:03.820: INFO: policy/v1 matches policy/v1
  Apr 28 17:22:03.820: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Apr 28 17:22:03.821: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Apr 28 17:22:03.821: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Apr 28 17:22:03.821: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Apr 28 17:22:03.821: INFO: Checking APIGroup: storage.k8s.io
  Apr 28 17:22:03.822: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Apr 28 17:22:03.822: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Apr 28 17:22:03.822: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Apr 28 17:22:03.822: INFO: Checking APIGroup: admissionregistration.k8s.io
  Apr 28 17:22:03.823: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Apr 28 17:22:03.823: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Apr 28 17:22:03.823: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Apr 28 17:22:03.823: INFO: Checking APIGroup: apiextensions.k8s.io
  Apr 28 17:22:03.823: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Apr 28 17:22:03.823: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Apr 28 17:22:03.823: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Apr 28 17:22:03.823: INFO: Checking APIGroup: scheduling.k8s.io
  Apr 28 17:22:03.824: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Apr 28 17:22:03.824: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Apr 28 17:22:03.824: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Apr 28 17:22:03.824: INFO: Checking APIGroup: coordination.k8s.io
  Apr 28 17:22:03.825: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Apr 28 17:22:03.825: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Apr 28 17:22:03.825: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Apr 28 17:22:03.825: INFO: Checking APIGroup: node.k8s.io
  Apr 28 17:22:03.826: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Apr 28 17:22:03.826: INFO: Versions found [{node.k8s.io/v1 v1}]
  Apr 28 17:22:03.826: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Apr 28 17:22:03.826: INFO: Checking APIGroup: discovery.k8s.io
  Apr 28 17:22:03.827: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Apr 28 17:22:03.827: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Apr 28 17:22:03.827: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Apr 28 17:22:03.827: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Apr 28 17:22:03.828: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Apr 28 17:22:03.828: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Apr 28 17:22:03.828: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Apr 28 17:22:03.828: INFO: Checking APIGroup: crd.projectcalico.org
  Apr 28 17:22:03.829: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
  Apr 28 17:22:03.829: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
  Apr 28 17:22:03.829: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
  Apr 28 17:22:03.829: INFO: Checking APIGroup: helm.cattle.io
  Apr 28 17:22:03.834: INFO: PreferredVersion.GroupVersion: helm.cattle.io/v1
  Apr 28 17:22:03.834: INFO: Versions found [{helm.cattle.io/v1 v1}]
  Apr 28 17:22:03.834: INFO: helm.cattle.io/v1 matches helm.cattle.io/v1
  Apr 28 17:22:03.834: INFO: Checking APIGroup: k3s.cattle.io
  Apr 28 17:22:03.836: INFO: PreferredVersion.GroupVersion: k3s.cattle.io/v1
  Apr 28 17:22:03.836: INFO: Versions found [{k3s.cattle.io/v1 v1}]
  Apr 28 17:22:03.836: INFO: k3s.cattle.io/v1 matches k3s.cattle.io/v1
  Apr 28 17:22:03.836: INFO: Checking APIGroup: snapshot.storage.k8s.io
  Apr 28 17:22:03.837: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
  Apr 28 17:22:03.837: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
  Apr 28 17:22:03.837: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
  Apr 28 17:22:03.837: INFO: Checking APIGroup: metrics.k8s.io
  Apr 28 17:22:03.838: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
  Apr 28 17:22:03.838: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
  Apr 28 17:22:03.838: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
  Apr 28 17:22:03.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-3885" for this suite. @ 04/28/23 17:22:03.842
• [0.375 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 04/28/23 17:22:03.855
  Apr 28 17:22:03.856: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename watch @ 04/28/23 17:22:03.859
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:22:03.874
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:22:03.881
  STEP: creating a watch on configmaps @ 04/28/23 17:22:03.884
  STEP: creating a new configmap @ 04/28/23 17:22:03.885
  STEP: modifying the configmap once @ 04/28/23 17:22:03.888
  STEP: closing the watch once it receives two notifications @ 04/28/23 17:22:03.896
  Apr 28 17:22:03.896: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7748  df9098af-927b-4c02-9721-aa6e58ef47d5 276945 0 2023-04-28 17:22:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-28 17:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 17:22:03.897: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7748  df9098af-927b-4c02-9721-aa6e58ef47d5 276946 0 2023-04-28 17:22:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-28 17:22:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 04/28/23 17:22:03.897
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 04/28/23 17:22:03.903
  STEP: deleting the configmap @ 04/28/23 17:22:03.905
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 04/28/23 17:22:03.912
  Apr 28 17:22:03.912: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7748  df9098af-927b-4c02-9721-aa6e58ef47d5 276947 0 2023-04-28 17:22:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-28 17:22:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 17:22:03.912: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7748  df9098af-927b-4c02-9721-aa6e58ef47d5 276948 0 2023-04-28 17:22:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-28 17:22:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 17:22:03.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7748" for this suite. @ 04/28/23 17:22:03.916
• [0.067 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 04/28/23 17:22:03.922
  Apr 28 17:22:03.922: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename deployment @ 04/28/23 17:22:03.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:22:03.937
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:22:03.942
  Apr 28 17:22:03.944: INFO: Creating simple deployment test-new-deployment
  Apr 28 17:22:03.970: INFO: deployment "test-new-deployment" doesn't have the required revision set
  E0428 17:22:04.457820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:05.457994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 04/28/23 17:22:06.002
  STEP: updating a scale subresource @ 04/28/23 17:22:06.004
  STEP: verifying the deployment Spec.Replicas was modified @ 04/28/23 17:22:06.012
  STEP: Patch a scale subresource @ 04/28/23 17:22:06.026
  Apr 28 17:22:06.075: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-1615  d5fe61ac-f570-49ee-88ee-cac96fee37ff 276995 3 2023-04-28 17:22:03 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-04-28 17:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-28 17:22:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0021c3608 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-28 17:22:05 +0000 UTC,LastTransitionTime:2023-04-28 17:22:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-04-28 17:22:05 +0000 UTC,LastTransitionTime:2023-04-28 17:22:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 28 17:22:06.094: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-1615  28ced22b-f6ac-4c41-aa8b-2e8e9a07a83f 277001 3 2023-04-28 17:22:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment d5fe61ac-f570-49ee-88ee-cac96fee37ff 0xc0041b7ac7 0xc0041b7ac8}] [] [{kube-controller-manager Update apps/v1 2023-04-28 17:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5fe61ac-f570-49ee-88ee-cac96fee37ff\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-28 17:22:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041b7b58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 17:22:06.104: INFO: Pod "test-new-deployment-67bd4bf6dc-2wjcm" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-2wjcm test-new-deployment-67bd4bf6dc- deployment-1615  dee6e8df-b173-49ba-8966-77f901ce59ff 276987 0 2023-04-28 17:22:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:e57991de7c304fb7f67600492102329688df5d51986f60ed23e93f0ae40ffab4 cni.projectcalico.org/podIP:10.42.3.159/32 cni.projectcalico.org/podIPs:10.42.3.159/32] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 28ced22b-f6ac-4c41-aa8b-2e8e9a07a83f 0xc0021c3aa7 0xc0021c3aa8}] [] [{kube-controller-manager Update v1 2023-04-28 17:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28ced22b-f6ac-4c41-aa8b-2e8e9a07a83f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-28 17:22:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-28 17:22:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rskhw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rskhw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-127.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:22:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:22:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:22:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:22:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.127,PodIP:10.42.3.159,StartTime:2023-04-28 17:22:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:22:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://631fd64547145eb1cc500f68fc51af114e010179e1f638e03d85c602e65e70c8,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.159,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:22:06.104: INFO: Pod "test-new-deployment-67bd4bf6dc-jscmg" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-jscmg test-new-deployment-67bd4bf6dc- deployment-1615  05f7d6fa-6eef-40fc-ab1c-95bba8e16182 276999 0 2023-04-28 17:22:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 28ced22b-f6ac-4c41-aa8b-2e8e9a07a83f 0xc0021c3ca7 0xc0021c3ca8}] [] [{kube-controller-manager Update v1 2023-04-28 17:22:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28ced22b-f6ac-4c41-aa8b-2e8e9a07a83f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8m8lv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8m8lv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-71.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:22:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:22:06.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1615" for this suite. @ 04/28/23 17:22:06.114
• [2.207 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 04/28/23 17:22:06.13
  Apr 28 17:22:06.130: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename init-container @ 04/28/23 17:22:06.131
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:22:06.156
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:22:06.16
  STEP: creating the pod @ 04/28/23 17:22:06.164
  Apr 28 17:22:06.164: INFO: PodSpec: initContainers in spec.initContainers
  E0428 17:22:06.458822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:07.459574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:08.460018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:09.460805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:10.460956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:11.461206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:12.461394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:13.461510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:14.481528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:15.482328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:16.482415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:17.482512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:18.482937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:19.483165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:20.483362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:21.483553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:22.483827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:23.485716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:24.485802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:25.486656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:26.486774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:27.487315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:28.487570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:29.487742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:30.487863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:31.488054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:32.488264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:33.488452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:34.488655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:35.488760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:36.488958      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:37.489192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:38.489924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:39.490379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:40.490563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:41.490760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:42.491131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:43.491188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:44.491421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:45.491531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:46.491644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:47.491949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:48.492313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:49.492504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:50.493201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:22:50.666: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-ad7df1ee-cbec-4c87-94dc-96b50130db00", GenerateName:"", Namespace:"init-container-646", SelfLink:"", UID:"223e5ba2-c713-4f0f-99ad-c31ec207078b", ResourceVersion:"277306", Generation:0, CreationTimestamp:time.Date(2023, time.April, 28, 17, 22, 6, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"164459653"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"9be68bf09b9372e44af998c2eeef811c4f56f9c905515ac58973f3046575a52c", "cni.projectcalico.org/podIP":"10.42.3.160/32", "cni.projectcalico.org/podIPs":"10.42.3.160/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 28, 17, 22, 6, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004679848), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 28, 17, 22, 6, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004679878), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 28, 17, 22, 50, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0046798a8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-t2g2t", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00475ca00), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-t2g2t", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-t2g2t", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-t2g2t", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00443d3f0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-9-127.us-east-2.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0005abdc0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00443d480)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00443d4a0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00443d4a8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00443d4ac), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000ed7270), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 28, 17, 22, 6, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 28, 17, 22, 6, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 28, 17, 22, 6, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 28, 17, 22, 6, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.9.127", PodIP:"10.42.3.160", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.42.3.160"}}, StartTime:time.Date(2023, time.April, 28, 17, 22, 6, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0005abea0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0005abf10)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://f010fcfbe8c7b1caa300a962743fe429cb9e6a3fdee5119eca872308be7c1c60", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00475ca80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00475ca60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00443d52f), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Apr 28 17:22:50.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-646" for this suite. @ 04/28/23 17:22:50.676
• [44.552 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 04/28/23 17:22:50.683
  Apr 28 17:22:50.683: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:22:50.684
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:22:50.703
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:22:50.705
  STEP: Creating the pod @ 04/28/23 17:22:50.707
  E0428 17:22:51.493350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:52.493540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:22:53.243: INFO: Successfully updated pod "labelsupdate5d476d22-8cfe-4c2a-91d9-739a99f55846"
  E0428 17:22:53.494074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:54.494177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:55.494215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:56.494533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:22:57.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3810" for this suite. @ 04/28/23 17:22:57.277
• [6.598 seconds]
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 04/28/23 17:22:57.282
  Apr 28 17:22:57.282: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename secrets @ 04/28/23 17:22:57.283
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:22:57.294
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:22:57.298
  STEP: creating secret secrets-5195/secret-test-9e63a60e-0601-4e2c-9e28-5c248ff907da @ 04/28/23 17:22:57.306
  STEP: Creating a pod to test consume secrets @ 04/28/23 17:22:57.311
  E0428 17:22:57.494926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:58.495316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:22:59.495724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:00.496033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:23:01.331
  Apr 28 17:23:01.333: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-configmaps-b94ec660-4ddb-4f4d-a7d8-11cf60ced36f container env-test: <nil>
  STEP: delete the pod @ 04/28/23 17:23:01.339
  Apr 28 17:23:01.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5195" for this suite. @ 04/28/23 17:23:01.353
• [4.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 04/28/23 17:23:01.359
  Apr 28 17:23:01.359: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename pods @ 04/28/23 17:23:01.36
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:23:01.374
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:23:01.377
  E0428 17:23:01.496667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:02.496922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:03.497528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:04.498351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:05.498900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:06.499518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:23:07.43
  Apr 28 17:23:07.432: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod client-envvars-f7ac9198-e673-49d8-8b70-9bfc56ea9bdf container env3cont: <nil>
  STEP: delete the pod @ 04/28/23 17:23:07.464
  Apr 28 17:23:07.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-102" for this suite. @ 04/28/23 17:23:07.499
  E0428 17:23:07.500581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
• [6.146 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 04/28/23 17:23:07.505
  Apr 28 17:23:07.505: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename crd-watch @ 04/28/23 17:23:07.506
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:23:07.521
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:23:07.524
  Apr 28 17:23:07.526: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  E0428 17:23:08.501584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:09.501676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 04/28/23 17:23:10.056
  Apr 28 17:23:10.119: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-28T17:23:10Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-28T17:23:10Z]] name:name1 resourceVersion:277517 uid:3786d4db-3cd1-44a8-9bbc-4cb23849700c] num:map[num1:9223372036854775807 num2:1000000]]}
  E0428 17:23:10.501776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:11.501884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:12.502285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:13.502387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:14.502619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:15.502926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:16.503112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:17.503317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:18.503620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:19.504208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 04/28/23 17:23:20.119
  Apr 28 17:23:20.126: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-28T17:23:20Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-28T17:23:20Z]] name:name2 resourceVersion:277591 uid:1f72857b-c735-4689-9297-220c75f1dc14] num:map[num1:9223372036854775807 num2:1000000]]}
  E0428 17:23:20.505197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:21.505537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:22.505646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:23.506206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:24.506444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:25.506660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:26.506851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:27.507048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:28.508125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:29.508208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 04/28/23 17:23:30.126
  Apr 28 17:23:30.132: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-28T17:23:10Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-28T17:23:30Z]] name:name1 resourceVersion:277637 uid:3786d4db-3cd1-44a8-9bbc-4cb23849700c] num:map[num1:9223372036854775807 num2:1000000]]}
  E0428 17:23:30.508254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:31.508505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:32.508739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:33.509273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:34.510149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:35.510257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:36.510686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:37.510806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:38.511368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:39.512074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 04/28/23 17:23:40.133
  Apr 28 17:23:40.140: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-28T17:23:20Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-28T17:23:40Z]] name:name2 resourceVersion:277687 uid:1f72857b-c735-4689-9297-220c75f1dc14] num:map[num1:9223372036854775807 num2:1000000]]}
  E0428 17:23:40.512699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:41.512890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:42.513109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:43.513257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:44.513372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:45.514304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:46.514478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:47.514675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:48.515137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:49.515396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 04/28/23 17:23:50.141
  Apr 28 17:23:50.147: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-28T17:23:10Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-28T17:23:30Z]] name:name1 resourceVersion:277737 uid:3786d4db-3cd1-44a8-9bbc-4cb23849700c] num:map[num1:9223372036854775807 num2:1000000]]}
  E0428 17:23:50.515997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:51.516204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:52.516396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:53.516525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:54.516789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:55.516975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:56.517192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:57.517444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:58.517721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:23:59.517910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 04/28/23 17:24:00.147
  Apr 28 17:24:00.155: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-28T17:23:20Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-28T17:23:40Z]] name:name2 resourceVersion:277787 uid:1f72857b-c735-4689-9297-220c75f1dc14] num:map[num1:9223372036854775807 num2:1000000]]}
  E0428 17:24:00.518774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:01.519010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:02.519911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:03.520606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:04.520722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:05.520918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:06.521119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:07.521217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:08.522290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:09.522504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:10.522964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:24:10.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-1166" for this suite. @ 04/28/23 17:24:10.673
• [63.173 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 04/28/23 17:24:10.678
  Apr 28 17:24:10.678: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename deployment @ 04/28/23 17:24:10.679
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:24:10.69
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:24:10.693
  STEP: creating a Deployment @ 04/28/23 17:24:10.698
  Apr 28 17:24:10.698: INFO: Creating simple deployment test-deployment-jp4qp
  Apr 28 17:24:10.710: INFO: deployment "test-deployment-jp4qp" doesn't have the required revision set
  E0428 17:24:11.525423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:12.525444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Getting /status @ 04/28/23 17:24:13.004
  Apr 28 17:24:13.009: INFO: Deployment test-deployment-jp4qp has Conditions: [{Available True 2023-04-28 17:24:11 +0000 UTC 2023-04-28 17:24:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-04-28 17:24:11 +0000 UTC 2023-04-28 17:24:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jp4qp-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 04/28/23 17:24:13.009
  Apr 28 17:24:13.020: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 28, 17, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 28, 17, 24, 10, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-jp4qp-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 04/28/23 17:24:13.02
  Apr 28 17:24:13.022: INFO: Observed &Deployment event: ADDED
  Apr 28 17:24:13.022: INFO: Observed Deployment test-deployment-jp4qp in namespace deployment-7190 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-28 17:24:10 +0000 UTC 2023-04-28 17:24:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jp4qp-5994cf9475"}
  Apr 28 17:24:13.022: INFO: Observed &Deployment event: MODIFIED
  Apr 28 17:24:13.022: INFO: Observed Deployment test-deployment-jp4qp in namespace deployment-7190 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-28 17:24:10 +0000 UTC 2023-04-28 17:24:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jp4qp-5994cf9475"}
  Apr 28 17:24:13.022: INFO: Observed Deployment test-deployment-jp4qp in namespace deployment-7190 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-28 17:24:10 +0000 UTC 2023-04-28 17:24:10 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 28 17:24:13.023: INFO: Observed &Deployment event: MODIFIED
  Apr 28 17:24:13.023: INFO: Observed Deployment test-deployment-jp4qp in namespace deployment-7190 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-28 17:24:10 +0000 UTC 2023-04-28 17:24:10 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 28 17:24:13.023: INFO: Observed Deployment test-deployment-jp4qp in namespace deployment-7190 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-28 17:24:10 +0000 UTC 2023-04-28 17:24:10 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jp4qp-5994cf9475" is progressing.}
  Apr 28 17:24:13.023: INFO: Observed &Deployment event: MODIFIED
  Apr 28 17:24:13.023: INFO: Observed Deployment test-deployment-jp4qp in namespace deployment-7190 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-28 17:24:11 +0000 UTC 2023-04-28 17:24:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 28 17:24:13.023: INFO: Observed Deployment test-deployment-jp4qp in namespace deployment-7190 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-28 17:24:11 +0000 UTC 2023-04-28 17:24:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jp4qp-5994cf9475" has successfully progressed.}
  Apr 28 17:24:13.023: INFO: Observed &Deployment event: MODIFIED
  Apr 28 17:24:13.023: INFO: Observed Deployment test-deployment-jp4qp in namespace deployment-7190 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-28 17:24:11 +0000 UTC 2023-04-28 17:24:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 28 17:24:13.023: INFO: Observed Deployment test-deployment-jp4qp in namespace deployment-7190 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-28 17:24:11 +0000 UTC 2023-04-28 17:24:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jp4qp-5994cf9475" has successfully progressed.}
  Apr 28 17:24:13.023: INFO: Found Deployment test-deployment-jp4qp in namespace deployment-7190 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 28 17:24:13.024: INFO: Deployment test-deployment-jp4qp has an updated status
  STEP: patching the Statefulset Status @ 04/28/23 17:24:13.024
  Apr 28 17:24:13.024: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 28 17:24:13.030: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 04/28/23 17:24:13.031
  Apr 28 17:24:13.035: INFO: Observed &Deployment event: ADDED
  Apr 28 17:24:13.035: INFO: Observed deployment test-deployment-jp4qp in namespace deployment-7190 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-28 17:24:10 +0000 UTC 2023-04-28 17:24:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jp4qp-5994cf9475"}
  Apr 28 17:24:13.035: INFO: Observed &Deployment event: MODIFIED
  Apr 28 17:24:13.035: INFO: Observed deployment test-deployment-jp4qp in namespace deployment-7190 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-28 17:24:10 +0000 UTC 2023-04-28 17:24:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jp4qp-5994cf9475"}
  Apr 28 17:24:13.035: INFO: Observed deployment test-deployment-jp4qp in namespace deployment-7190 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-28 17:24:10 +0000 UTC 2023-04-28 17:24:10 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 28 17:24:13.036: INFO: Observed &Deployment event: MODIFIED
  Apr 28 17:24:13.036: INFO: Observed deployment test-deployment-jp4qp in namespace deployment-7190 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-28 17:24:10 +0000 UTC 2023-04-28 17:24:10 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 28 17:24:13.036: INFO: Observed deployment test-deployment-jp4qp in namespace deployment-7190 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-28 17:24:10 +0000 UTC 2023-04-28 17:24:10 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jp4qp-5994cf9475" is progressing.}
  Apr 28 17:24:13.036: INFO: Observed &Deployment event: MODIFIED
  Apr 28 17:24:13.036: INFO: Observed deployment test-deployment-jp4qp in namespace deployment-7190 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-28 17:24:11 +0000 UTC 2023-04-28 17:24:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 28 17:24:13.036: INFO: Observed deployment test-deployment-jp4qp in namespace deployment-7190 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-28 17:24:11 +0000 UTC 2023-04-28 17:24:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jp4qp-5994cf9475" has successfully progressed.}
  Apr 28 17:24:13.036: INFO: Observed &Deployment event: MODIFIED
  Apr 28 17:24:13.036: INFO: Observed deployment test-deployment-jp4qp in namespace deployment-7190 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-28 17:24:11 +0000 UTC 2023-04-28 17:24:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 28 17:24:13.037: INFO: Observed deployment test-deployment-jp4qp in namespace deployment-7190 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-28 17:24:11 +0000 UTC 2023-04-28 17:24:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jp4qp-5994cf9475" has successfully progressed.}
  Apr 28 17:24:13.037: INFO: Observed deployment test-deployment-jp4qp in namespace deployment-7190 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 28 17:24:13.037: INFO: Observed &Deployment event: MODIFIED
  Apr 28 17:24:13.037: INFO: Found deployment test-deployment-jp4qp in namespace deployment-7190 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Apr 28 17:24:13.037: INFO: Deployment test-deployment-jp4qp has a patched status
  Apr 28 17:24:13.040: INFO: Deployment "test-deployment-jp4qp":
  &Deployment{ObjectMeta:{test-deployment-jp4qp  deployment-7190  718ee233-3991-4c10-abfe-c6f3589a1333 277881 1 2023-04-28 17:24:10 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-04-28 17:24:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-04-28 17:24:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-04-28 17:24:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b33bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-jp4qp-5994cf9475",LastUpdateTime:2023-04-28 17:24:13 +0000 UTC,LastTransitionTime:2023-04-28 17:24:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 28 17:24:13.045: INFO: New ReplicaSet "test-deployment-jp4qp-5994cf9475" of Deployment "test-deployment-jp4qp":
  &ReplicaSet{ObjectMeta:{test-deployment-jp4qp-5994cf9475  deployment-7190  d8b6c87b-d934-4c30-a4ea-9991432d898d 277870 1 2023-04-28 17:24:10 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-jp4qp 718ee233-3991-4c10-abfe-c6f3589a1333 0xc002e2c450 0xc002e2c451}] [] [{kube-controller-manager Update apps/v1 2023-04-28 17:24:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"718ee233-3991-4c10-abfe-c6f3589a1333\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-28 17:24:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e2c4f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 28 17:24:13.053: INFO: Pod "test-deployment-jp4qp-5994cf9475-kjtnr" is available:
  &Pod{ObjectMeta:{test-deployment-jp4qp-5994cf9475-kjtnr test-deployment-jp4qp-5994cf9475- deployment-7190  e46ee747-05cb-4a6a-a857-c9fb929f569c 277869 0 2023-04-28 17:24:10 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[cni.projectcalico.org/containerID:e5647ed89372da5e030bf9c0a3ccb08964b3055440b6c708b2678f6bd87361f7 cni.projectcalico.org/podIP:10.42.3.165/32 cni.projectcalico.org/podIPs:10.42.3.165/32] [{apps/v1 ReplicaSet test-deployment-jp4qp-5994cf9475 d8b6c87b-d934-4c30-a4ea-9991432d898d 0xc005708000 0xc005708001}] [] [{kube-controller-manager Update v1 2023-04-28 17:24:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8b6c87b-d934-4c30-a4ea-9991432d898d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-28 17:24:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-28 17:24:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.165\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j42m5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j42m5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-127.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:24:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:24:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:24:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 17:24:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.127,PodIP:10.42.3.165,StartTime:2023-04-28 17:24:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 17:24:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://04f3308a40ad858aa5f8cac94e58041f730aa63dafa6c5c31e6c52241a138fcb,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.165,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 28 17:24:13.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7190" for this suite. @ 04/28/23 17:24:13.057
• [2.384 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 04/28/23 17:24:13.064
  Apr 28 17:24:13.064: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename subjectreview @ 04/28/23 17:24:13.078
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:24:13.091
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:24:13.096
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-1875" @ 04/28/23 17:24:13.098
  Apr 28 17:24:13.102: INFO: saUsername: "system:serviceaccount:subjectreview-1875:e2e"
  Apr 28 17:24:13.102: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-1875"}
  Apr 28 17:24:13.102: INFO: saUID: "faa514e4-b9d6-4747-ae20-fc728eb995c4"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-1875:e2e" @ 04/28/23 17:24:13.102
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-1875:e2e" @ 04/28/23 17:24:13.103
  Apr 28 17:24:13.104: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-1875:e2e" api 'list' configmaps in "subjectreview-1875" namespace @ 04/28/23 17:24:13.104
  Apr 28 17:24:13.105: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-1875:e2e" @ 04/28/23 17:24:13.105
  Apr 28 17:24:13.107: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Apr 28 17:24:13.107: INFO: LocalSubjectAccessReview has been verified
  Apr 28 17:24:13.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-1875" for this suite. @ 04/28/23 17:24:13.111
• [0.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 04/28/23 17:24:13.12
  Apr 28 17:24:13.120: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename daemonsets @ 04/28/23 17:24:13.121
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:24:13.14
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:24:13.143
  STEP: Creating simple DaemonSet "daemon-set" @ 04/28/23 17:24:13.16
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/28/23 17:24:13.164
  Apr 28 17:24:13.172: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 17:24:13.172: INFO: Node ip-172-31-11-161.us-east-2.compute.internal is running 0 daemon pod, expected 1
  E0428 17:24:13.526497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:24:14.179: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 17:24:14.179: INFO: Node ip-172-31-11-161.us-east-2.compute.internal is running 0 daemon pod, expected 1
  E0428 17:24:14.527502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:24:15.179: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
  Apr 28 17:24:15.179: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: listing all DaemonSets @ 04/28/23 17:24:15.181
  STEP: DeleteCollection of the DaemonSets @ 04/28/23 17:24:15.184
  STEP: Verify that ReplicaSets have been deleted @ 04/28/23 17:24:15.189
  Apr 28 17:24:15.202: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"277951"},"items":null}

  Apr 28 17:24:15.206: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"277951"},"items":[{"metadata":{"name":"daemon-set-2qqdt","generateName":"daemon-set-","namespace":"daemonsets-2289","uid":"de936af9-6d04-4f13-a54b-ece05d9c8027","resourceVersion":"277936","creationTimestamp":"2023-04-28T17:24:13Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"5681f125de85d9df3c1c44d46b1a6fe08bd2036617fa106191c7e616530c111f","cni.projectcalico.org/podIP":"10.42.0.192/32","cni.projectcalico.org/podIPs":"10.42.0.192/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"329b9214-cc14-4b8b-a13e-4763991f1214","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-04-28T17:24:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-28T17:24:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"329b9214-cc14-4b8b-a13e-4763991f1214\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-28T17:24:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.0.192\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-wctzc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-wctzc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-14-195.us-east-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-14-195.us-east-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T17:24:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T17:24:14Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T17:24:14Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T17:24:13Z"}],"hostIP":"172.31.14.195","podIP":"10.42.0.192","podIPs":[{"ip":"10.42.0.192"}],"startTime":"2023-04-28T17:24:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-28T17:24:14Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://e6fd06104cfee8a7aa98033385d886874f61b424aacf12dcd34a5d9e99145333","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-kfp9m","generateName":"daemon-set-","namespace":"daemonsets-2289","uid":"a2af7349-1c14-4cac-8c3e-b5191251c2e2","resourceVersion":"277948","creationTimestamp":"2023-04-28T17:24:13Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"3b4d88f96c48a37e5486522a499d73fbde8c61e3ac765eed7d1aa9ff12183623","cni.projectcalico.org/podIP":"10.42.2.236/32","cni.projectcalico.org/podIPs":"10.42.2.236/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"329b9214-cc14-4b8b-a13e-4763991f1214","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-04-28T17:24:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-28T17:24:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"329b9214-cc14-4b8b-a13e-4763991f1214\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-28T17:24:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.236\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-8vs6t","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-8vs6t","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-6-71.us-east-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-6-71.us-east-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T17:24:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T17:24:15Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T17:24:15Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T17:24:13Z"}],"hostIP":"172.31.6.71","podIP":"10.42.2.236","podIPs":[{"ip":"10.42.2.236"}],"startTime":"2023-04-28T17:24:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-28T17:24:14Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://5e084fdc2326cc055060dd7b79cbe92b1876c88d3e58f5147ec803b65e3d52bd","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-pzhh6","generateName":"daemon-set-","namespace":"daemonsets-2289","uid":"342fd9b6-90dc-435a-ac72-5e0ec5c6fcb5","resourceVersion":"277938","creationTimestamp":"2023-04-28T17:24:13Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"104ea6d6fe03b366601296e82d34bf81fec60e700eb8aabc208904fb2c100b4f","cni.projectcalico.org/podIP":"10.42.1.206/32","cni.projectcalico.org/podIPs":"10.42.1.206/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"329b9214-cc14-4b8b-a13e-4763991f1214","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-04-28T17:24:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-28T17:24:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"329b9214-cc14-4b8b-a13e-4763991f1214\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-28T17:24:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.206\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-zchk9","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-zchk9","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-11-161.us-east-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-11-161.us-east-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T17:24:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T17:24:14Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T17:24:14Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T17:24:13Z"}],"hostIP":"172.31.11.161","podIP":"10.42.1.206","podIPs":[{"ip":"10.42.1.206"}],"startTime":"2023-04-28T17:24:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-28T17:24:14Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://df620da96190c7490373a70d3d3830114063b3d4ed8d82eaf9c4aabba3c3ac7d","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vcbjs","generateName":"daemon-set-","namespace":"daemonsets-2289","uid":"03c1b214-9fd8-47fa-8978-3758551669f6","resourceVersion":"277946","creationTimestamp":"2023-04-28T17:24:13Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"62eb3fd5e2aa48c72933c1e4b90814ca7ebcca878c3a5d1ffd0ee87ec0b41327","cni.projectcalico.org/podIP":"10.42.3.166/32","cni.projectcalico.org/podIPs":"10.42.3.166/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"329b9214-cc14-4b8b-a13e-4763991f1214","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-04-28T17:24:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-28T17:24:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"329b9214-cc14-4b8b-a13e-4763991f1214\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-28T17:24:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.166\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-l8f84","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-l8f84","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-9-127.us-east-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-9-127.us-east-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T17:24:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T17:24:14Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T17:24:14Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-28T17:24:13Z"}],"hostIP":"172.31.9.127","podIP":"10.42.3.166","podIPs":[{"ip":"10.42.3.166"}],"startTime":"2023-04-28T17:24:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-28T17:24:14Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://aa7697278fae17c0eb5504ef8a998f2d4627753fe57cf608e4264cffc46f2210","started":true}],"qosClass":"BestEffort"}}]}

  Apr 28 17:24:15.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-2289" for this suite. @ 04/28/23 17:24:15.254
• [2.140 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 04/28/23 17:24:15.263
  Apr 28 17:24:15.264: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename webhook @ 04/28/23 17:24:15.266
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:24:15.281
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:24:15.284
  STEP: Setting up server cert @ 04/28/23 17:24:15.303
  E0428 17:24:15.527758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 17:24:15.992
  STEP: Deploying the webhook pod @ 04/28/23 17:24:16.007
  STEP: Wait for the deployment to be ready @ 04/28/23 17:24:16.019
  Apr 28 17:24:16.028: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0428 17:24:16.528001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:17.528274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 17:24:18.039
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 17:24:18.052
  E0428 17:24:18.529319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:24:19.053: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 04/28/23 17:24:19.056
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 04/28/23 17:24:19.076
  STEP: Creating a configMap that should not be mutated @ 04/28/23 17:24:19.081
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 04/28/23 17:24:19.089
  STEP: Creating a configMap that should be mutated @ 04/28/23 17:24:19.095
  Apr 28 17:24:19.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6769" for this suite. @ 04/28/23 17:24:19.157
  STEP: Destroying namespace "webhook-markers-8311" for this suite. @ 04/28/23 17:24:19.164
• [3.914 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 04/28/23 17:24:19.191
  Apr 28 17:24:19.191: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 17:24:19.202
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:24:19.23
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:24:19.236
  STEP: Creating a ResourceQuota with terminating scope @ 04/28/23 17:24:19.239
  STEP: Ensuring ResourceQuota status is calculated @ 04/28/23 17:24:19.243
  E0428 17:24:19.529895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:20.530468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not terminating scope @ 04/28/23 17:24:21.247
  STEP: Ensuring ResourceQuota status is calculated @ 04/28/23 17:24:21.251
  E0428 17:24:21.530804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:22.530914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a long running pod @ 04/28/23 17:24:23.254
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 04/28/23 17:24:23.266
  E0428 17:24:23.531993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:24.532187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 04/28/23 17:24:25.271
  E0428 17:24:25.532900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:26.532985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 04/28/23 17:24:27.274
  STEP: Ensuring resource quota status released the pod usage @ 04/28/23 17:24:27.284
  E0428 17:24:27.533208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:28.534357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a terminating pod @ 04/28/23 17:24:29.288
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 04/28/23 17:24:29.302
  E0428 17:24:29.534410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:30.534514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 04/28/23 17:24:31.305
  E0428 17:24:31.535395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:32.535608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 04/28/23 17:24:33.308
  STEP: Ensuring resource quota status released the pod usage @ 04/28/23 17:24:33.327
  E0428 17:24:33.535727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:34.535958      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:24:35.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1010" for this suite. @ 04/28/23 17:24:35.336
• [16.163 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 04/28/23 17:24:35.347
  Apr 28 17:24:35.347: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename sched-pred @ 04/28/23 17:24:35.348
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:24:35.366
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:24:35.369
  Apr 28 17:24:35.371: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 28 17:24:35.378: INFO: Waiting for terminating namespaces to be deleted...
  Apr 28 17:24:35.380: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-11-161.us-east-2.compute.internal before test
  Apr 28 17:24:35.388: INFO: cloud-controller-manager-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:02 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.388: INFO: 	Container cloud-controller-manager ready: true, restart count 0
  Apr 28 17:24:35.388: INFO: etcd-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:46:40 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.388: INFO: 	Container etcd ready: true, restart count 0
  Apr 28 17:24:35.388: INFO: kube-apiserver-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:46:57 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.388: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 28 17:24:35.388: INFO: kube-controller-manager-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:02 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.388: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 28 17:24:35.388: INFO: kube-proxy-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:05 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.388: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 28 17:24:35.388: INFO: kube-scheduler-ip-172-31-11-161.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:02 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.388: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 28 17:24:35.388: INFO: rke2-canal-8td74 from kube-system started at 2023-04-28 03:47:01 +0000 UTC (2 container statuses recorded)
  Apr 28 17:24:35.388: INFO: 	Container calico-node ready: true, restart count 0
  Apr 28 17:24:35.388: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 28 17:24:35.388: INFO: rke2-coredns-rke2-coredns-5896cccb79-vcgcz from kube-system started at 2023-04-28 03:47:28 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.388: INFO: 	Container coredns ready: true, restart count 0
  Apr 28 17:24:35.388: INFO: rke2-ingress-nginx-controller-6f4sz from kube-system started at 2023-04-28 03:47:28 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.388: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
  Apr 28 17:24:35.388: INFO: sonobuoy-systemd-logs-daemon-set-19d55eb1839b4b26-n6bzk from sonobuoy started at 2023-04-28 16:37:21 +0000 UTC (2 container statuses recorded)
  Apr 28 17:24:35.388: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 17:24:35.388: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 17:24:35.388: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-14-195.us-east-2.compute.internal before test
  Apr 28 17:24:35.398: INFO: cloud-controller-manager-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:44:16 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.398: INFO: 	Container cloud-controller-manager ready: true, restart count 0
  Apr 28 17:24:35.398: INFO: etcd-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:43:55 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.398: INFO: 	Container etcd ready: true, restart count 0
  Apr 28 17:24:35.398: INFO: helm-install-rke2-canal-8zdl7 from kube-system started at 2023-04-28 03:44:29 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.398: INFO: 	Container helm ready: false, restart count 0
  Apr 28 17:24:35.398: INFO: helm-install-rke2-coredns-8tn5j from kube-system started at 2023-04-28 03:44:29 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.398: INFO: 	Container helm ready: false, restart count 1
  Apr 28 17:24:35.398: INFO: helm-install-rke2-ingress-nginx-v5xks from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.398: INFO: 	Container helm ready: false, restart count 0
  Apr 28 17:24:35.398: INFO: helm-install-rke2-metrics-server-66776 from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.398: INFO: 	Container helm ready: false, restart count 0
  Apr 28 17:24:35.398: INFO: helm-install-rke2-snapshot-controller-crd-xc2xl from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.399: INFO: 	Container helm ready: false, restart count 0
  Apr 28 17:24:35.399: INFO: helm-install-rke2-snapshot-controller-g6gdp from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.399: INFO: 	Container helm ready: false, restart count 0
  Apr 28 17:24:35.399: INFO: helm-install-rke2-snapshot-validation-webhook-6rwkq from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.399: INFO: 	Container helm ready: false, restart count 0
  Apr 28 17:24:35.399: INFO: kube-apiserver-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:44:09 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.399: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 28 17:24:35.399: INFO: kube-controller-manager-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:44:14 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.399: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 28 17:24:35.399: INFO: kube-proxy-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:44:19 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.399: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 28 17:24:35.399: INFO: kube-scheduler-ip-172-31-14-195.us-east-2.compute.internal from kube-system started at 2023-04-28 03:44:14 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.399: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 28 17:24:35.399: INFO: rke2-canal-sz474 from kube-system started at 2023-04-28 03:44:35 +0000 UTC (2 container statuses recorded)
  Apr 28 17:24:35.399: INFO: 	Container calico-node ready: true, restart count 0
  Apr 28 17:24:35.399: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 28 17:24:35.399: INFO: rke2-coredns-rke2-coredns-5896cccb79-lb8dr from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.399: INFO: 	Container coredns ready: true, restart count 0
  Apr 28 17:24:35.399: INFO: rke2-coredns-rke2-coredns-autoscaler-f6766cdc9-qrz4d from kube-system started at 2023-04-28 03:44:46 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.399: INFO: 	Container autoscaler ready: true, restart count 0
  Apr 28 17:24:35.399: INFO: rke2-ingress-nginx-controller-l9r4l from kube-system started at 2023-04-28 03:45:10 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.399: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
  Apr 28 17:24:35.399: INFO: rke2-metrics-server-6d45f6cb4d-587zc from kube-system started at 2023-04-28 03:45:06 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.399: INFO: 	Container metrics-server ready: true, restart count 0
  Apr 28 17:24:35.399: INFO: rke2-snapshot-controller-7bf6d7bf5f-swpkx from kube-system started at 2023-04-28 03:45:23 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.399: INFO: 	Container rke2-snapshot-controller ready: true, restart count 0
  Apr 28 17:24:35.399: INFO: rke2-snapshot-validation-webhook-b65d46c9f-8jsvg from kube-system started at 2023-04-28 03:45:24 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.399: INFO: 	Container rke2-snapshot-validation-webhook ready: true, restart count 0
  Apr 28 17:24:35.399: INFO: sonobuoy-systemd-logs-daemon-set-19d55eb1839b4b26-bxxcq from sonobuoy started at 2023-04-28 16:37:21 +0000 UTC (2 container statuses recorded)
  Apr 28 17:24:35.399: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 17:24:35.399: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 17:24:35.399: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-6-71.us-east-2.compute.internal before test
  Apr 28 17:24:35.407: INFO: cloud-controller-manager-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:18 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.407: INFO: 	Container cloud-controller-manager ready: true, restart count 0
  Apr 28 17:24:35.407: INFO: etcd-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:01 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.407: INFO: 	Container etcd ready: true, restart count 0
  Apr 28 17:24:35.407: INFO: kube-apiserver-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:13 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.407: INFO: 	Container kube-apiserver ready: true, restart count 0
  Apr 28 17:24:35.407: INFO: kube-controller-manager-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:18 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.407: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Apr 28 17:24:35.407: INFO: kube-proxy-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:20 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.407: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 28 17:24:35.407: INFO: kube-scheduler-ip-172-31-6-71.us-east-2.compute.internal from kube-system started at 2023-04-28 03:47:18 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.407: INFO: 	Container kube-scheduler ready: true, restart count 0
  Apr 28 17:24:35.407: INFO: rke2-canal-8rz4z from kube-system started at 2023-04-28 03:47:20 +0000 UTC (2 container statuses recorded)
  Apr 28 17:24:35.407: INFO: 	Container calico-node ready: true, restart count 0
  Apr 28 17:24:35.407: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 28 17:24:35.407: INFO: rke2-ingress-nginx-controller-xfwz8 from kube-system started at 2023-04-28 03:47:42 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.407: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
  Apr 28 17:24:35.407: INFO: sonobuoy-systemd-logs-daemon-set-19d55eb1839b4b26-98t5f from sonobuoy started at 2023-04-28 16:37:21 +0000 UTC (2 container statuses recorded)
  Apr 28 17:24:35.407: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 17:24:35.407: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 28 17:24:35.407: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-9-127.us-east-2.compute.internal before test
  Apr 28 17:24:35.415: INFO: kube-proxy-ip-172-31-9-127.us-east-2.compute.internal from kube-system started at 2023-04-28 03:48:58 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.415: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 28 17:24:35.415: INFO: rke2-canal-lc6hn from kube-system started at 2023-04-28 03:48:58 +0000 UTC (2 container statuses recorded)
  Apr 28 17:24:35.415: INFO: 	Container calico-node ready: true, restart count 0
  Apr 28 17:24:35.415: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 28 17:24:35.415: INFO: rke2-ingress-nginx-controller-qjl8l from kube-system started at 2023-04-28 17:06:16 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.415: INFO: 	Container rke2-ingress-nginx-controller ready: true, restart count 0
  Apr 28 17:24:35.415: INFO: sonobuoy from sonobuoy started at 2023-04-28 16:37:20 +0000 UTC (1 container statuses recorded)
  Apr 28 17:24:35.415: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 28 17:24:35.415: INFO: sonobuoy-e2e-job-92a64e7703e04ef2 from sonobuoy started at 2023-04-28 16:37:21 +0000 UTC (2 container statuses recorded)
  Apr 28 17:24:35.415: INFO: 	Container e2e ready: true, restart count 0
  Apr 28 17:24:35.415: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 17:24:35.415: INFO: sonobuoy-systemd-logs-daemon-set-19d55eb1839b4b26-jv86q from sonobuoy started at 2023-04-28 16:37:21 +0000 UTC (2 container statuses recorded)
  Apr 28 17:24:35.415: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 28 17:24:35.415: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/28/23 17:24:35.415
  E0428 17:24:35.536265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:36.536409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/28/23 17:24:37.43
  STEP: Trying to apply a random label on the found node. @ 04/28/23 17:24:37.441
  STEP: verifying the node has the label kubernetes.io/e2e-5d832009-32e6-4783-8e12-7049514bbf70 95 @ 04/28/23 17:24:37.451
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 04/28/23 17:24:37.469
  E0428 17:24:37.536891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:38.537361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.9.127 on the node which pod4 resides and expect not scheduled @ 04/28/23 17:24:39.5
  E0428 17:24:39.538289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:40.538325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:41.538424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:42.538622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:43.539351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:44.539617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:45.540260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:46.540515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:47.541127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:48.541205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:49.541444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:50.542297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:51.542485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:52.542709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:53.543259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:54.543387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:55.543558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:56.543677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:57.544272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:58.544593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:24:59.545549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:00.546392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:01.546607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:02.546721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:03.547395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:04.547843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:05.548084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:06.548282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:07.548442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:08.548659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:09.548761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:10.548898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:11.549003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:12.549632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:13.549741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:14.549831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:15.549940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:16.550037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:17.550243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:18.550912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:19.551125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:20.551267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:21.551385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:22.551501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:23.552060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:24.552280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:25.552394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:26.553182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:27.553204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:28.553241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:29.553335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:30.554472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:31.554695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:32.554797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:33.555387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:34.555512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:35.555716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:36.555813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:37.556050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:38.556094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:39.556303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:40.556328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:41.556581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:42.556753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:43.557363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:44.557481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:45.557594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:46.557727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:47.557841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:48.558410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:49.558519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:50.558628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:51.558735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:52.558854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:53.558963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:54.559069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:55.559371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:56.559367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:57.559711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:58.560256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:25:59.560538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:00.561392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:01.561462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:02.562315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:03.562841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:04.563011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:05.563095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:06.563614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:07.564436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:08.564918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:09.565084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:10.565255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:11.565507      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:12.566511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:13.567000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:14.567141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:15.567254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:16.567363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:17.567472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:18.567597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:19.567749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:20.567936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:21.567994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:22.568211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:23.568645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:24.568747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:25.568842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:26.569014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:27.569239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:28.569428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:29.569518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:30.569637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:31.569732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:32.569856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:33.570838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:34.570958      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:35.571887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:36.572360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:37.572512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:38.572664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:39.572792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:40.574026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:41.574550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:42.574756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:43.574787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:44.574912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:45.575186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:46.575296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:47.575521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:48.575609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:49.575840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:50.576037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:51.576132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:52.576276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:53.576816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:54.577373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:55.577607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:56.577777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:57.578006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:58.578159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:26:59.578728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:00.579358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:01.579437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:02.579724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:03.580253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:04.581247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:05.582261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:06.582379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:07.582571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:08.582720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:09.582906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:10.583034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:11.583241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:12.583352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:13.583893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:14.584017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:15.584217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:16.584762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:17.584991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:18.585894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:19.586034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:20.586134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:21.586367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:22.586548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:23.586614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:24.587564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:25.587778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:26.587883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:27.588000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:28.588098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:29.588306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:30.588514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:31.588556      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:32.588725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:33.589388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:34.589491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:35.589573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:36.590388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:37.590504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:38.591008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:39.591320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:40.591500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:41.591706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:42.591814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:43.591939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:44.593046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:45.593250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:46.594310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:47.594506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:48.594820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:49.595018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:50.596060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:51.596385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:52.597137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:53.597720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:54.597823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:55.597945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:56.598954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:57.599856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:58.599822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:27:59.600398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:00.601335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:01.601588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:02.601716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:03.602273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:04.602679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:05.602780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:06.602899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:07.603135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:08.603717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:09.603851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:10.604155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:11.604354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:12.604498      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:13.605054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:14.605210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:15.606297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:16.606399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:17.606624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:18.606713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:19.606985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:20.607118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:21.607322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:22.607448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:23.608022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:24.608971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:25.609209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:26.609327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:27.609421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:28.609517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:29.609642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:30.609758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:31.609839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:32.610816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:33.611365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:34.611533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:35.611732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:36.611928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:37.612025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:38.612148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:39.612348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:40.613186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:41.613275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:42.614305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:43.614388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:44.614529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:45.614637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:46.615028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:47.615165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:48.615660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:49.615880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:50.615985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:51.616202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:52.616328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:53.616452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:54.616509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:55.616645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:56.616783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:57.616995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:58.617161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:28:59.617511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:00.617325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:01.617534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:02.618527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:03.618997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:04.619074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:05.620061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:06.621058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:07.621198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:08.621297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:09.622275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:10.623112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:11.623322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:12.624290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:13.624782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:14.625210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:15.625415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:16.625982      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:17.626266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:18.626328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:19.626518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:20.627613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:21.627713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:22.628486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:23.628957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:24.629370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:25.630260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:26.630546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:27.630732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:28.631575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:29.631777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:30.632267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:31.632462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:32.632547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:33.632943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:34.633226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:35.633326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:36.633422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:37.633786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:38.634245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-5d832009-32e6-4783-8e12-7049514bbf70 off the node ip-172-31-9-127.us-east-2.compute.internal @ 04/28/23 17:29:39.507
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-5d832009-32e6-4783-8e12-7049514bbf70 @ 04/28/23 17:29:39.521
  Apr 28 17:29:39.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-8811" for this suite. @ 04/28/23 17:29:39.576
• [304.236 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 04/28/23 17:29:39.59
  Apr 28 17:29:39.591: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename secrets @ 04/28/23 17:29:39.593
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:29:39.622
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:29:39.625
  STEP: creating a secret @ 04/28/23 17:29:39.627
  E0428 17:29:39.637274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 04/28/23 17:29:39.642
  STEP: patching the secret @ 04/28/23 17:29:39.646
  STEP: deleting the secret using a LabelSelector @ 04/28/23 17:29:39.654
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 04/28/23 17:29:39.66
  Apr 28 17:29:39.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1627" for this suite. @ 04/28/23 17:29:39.668
• [0.082 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 04/28/23 17:29:39.673
  Apr 28 17:29:39.673: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename container-probe @ 04/28/23 17:29:39.674
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:29:39.688
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:29:39.69
  STEP: Creating pod liveness-2bd88bd5-4c7f-4bc2-bf98-62e4b76d31b8 in namespace container-probe-9791 @ 04/28/23 17:29:39.692
  E0428 17:29:40.638126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:41.638229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:29:41.706: INFO: Started pod liveness-2bd88bd5-4c7f-4bc2-bf98-62e4b76d31b8 in namespace container-probe-9791
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/28/23 17:29:41.706
  Apr 28 17:29:41.708: INFO: Initial restart count of pod liveness-2bd88bd5-4c7f-4bc2-bf98-62e4b76d31b8 is 0
  E0428 17:29:42.638581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:43.638731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:44.639878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:45.639896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:46.639980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:47.640119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:48.640237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:49.640476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:50.641209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:51.642334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:52.642573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:53.643069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:54.643189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:55.643281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:56.643830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:57.644018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:58.644110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:29:59.644318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:00.644461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:01.644612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:30:01.752: INFO: Restart count of pod container-probe-9791/liveness-2bd88bd5-4c7f-4bc2-bf98-62e4b76d31b8 is now 1 (20.044192837s elapsed)
  E0428 17:30:02.645051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:03.645627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:04.645768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:05.646080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:06.646190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:07.646308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:08.646409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:09.646628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:10.646776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:11.646999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:12.647702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:13.647836      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:14.647967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:15.648687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:16.649222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:17.649243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:18.649546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:19.649595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:20.649698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:21.650337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:30:21.793: INFO: Restart count of pod container-probe-9791/liveness-2bd88bd5-4c7f-4bc2-bf98-62e4b76d31b8 is now 2 (40.084851963s elapsed)
  E0428 17:30:22.650759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:23.650972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:24.651089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:25.651205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:26.651305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:27.651435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:28.651663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:29.652044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:30.652155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:31.652365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:32.652735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:33.653009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:34.653157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:35.653227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:36.654325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:37.654677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:38.655338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:39.656990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:40.657123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:41.657227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:30:41.834: INFO: Restart count of pod container-probe-9791/liveness-2bd88bd5-4c7f-4bc2-bf98-62e4b76d31b8 is now 3 (1m0.12565551s elapsed)
  E0428 17:30:42.658225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:43.658355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:44.658470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:45.658576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:46.658810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:47.658817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:48.658950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:49.659109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:50.659235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:51.659418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:52.659491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:53.660180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:54.660718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:55.660931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:56.661037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:57.661474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:58.662276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:30:59.662577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:00.665407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:01.663830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:31:01.873: INFO: Restart count of pod container-probe-9791/liveness-2bd88bd5-4c7f-4bc2-bf98-62e4b76d31b8 is now 4 (1m20.164527308s elapsed)
  E0428 17:31:02.664374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:03.664503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:04.664636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:05.664817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:06.665836      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:07.665873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:08.666174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:09.666345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:10.667274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:11.667398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:12.667862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:13.668066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:14.669187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:15.669230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:16.669403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:17.670402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:18.670937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:19.671132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:20.671235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:21.671436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:22.671881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:23.672050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:24.672298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:25.672496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:26.672706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:27.673206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:28.674262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:29.674359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:30.674783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:31.675015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:32.675511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:33.675651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:34.675979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:35.676270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:36.677676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:37.677910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:38.678026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:39.678233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:40.678308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:41.678442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:42.678981      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:43.679092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:44.680062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:45.680244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:46.680652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:47.680892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:48.681194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:49.681311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:50.682375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:51.682456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:52.683298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:53.683493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:54.683595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:55.683746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:56.683848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:57.684275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:58.684592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:31:59.685434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:00.687576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:01.687830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:02.688698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:03.688799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:32:04.006: INFO: Restart count of pod container-probe-9791/liveness-2bd88bd5-4c7f-4bc2-bf98-62e4b76d31b8 is now 5 (2m22.297385369s elapsed)
  Apr 28 17:32:04.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 17:32:04.01
  STEP: Destroying namespace "container-probe-9791" for this suite. @ 04/28/23 17:32:04.022
• [144.356 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 04/28/23 17:32:04.037
  Apr 28 17:32:04.037: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename crd-webhook @ 04/28/23 17:32:04.039
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:32:04.055
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:32:04.062
  STEP: Setting up server cert @ 04/28/23 17:32:04.064
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 04/28/23 17:32:04.484
  STEP: Deploying the custom resource conversion webhook pod @ 04/28/23 17:32:04.492
  STEP: Wait for the deployment to be ready @ 04/28/23 17:32:04.505
  Apr 28 17:32:04.512: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  E0428 17:32:04.689534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:05.690306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 17:32:06.532
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 17:32:06.543
  E0428 17:32:06.691191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:32:07.543: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Apr 28 17:32:07.546: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  E0428 17:32:07.691863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:08.692760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:09.692973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 04/28/23 17:32:10.092
  STEP: Create a v2 custom resource @ 04/28/23 17:32:10.107
  STEP: List CRs in v1 @ 04/28/23 17:32:10.132
  STEP: List CRs in v2 @ 04/28/23 17:32:10.144
  Apr 28 17:32:10.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0428 17:32:10.693574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-7246" for this suite. @ 04/28/23 17:32:10.731
• [6.706 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 04/28/23 17:32:10.748
  Apr 28 17:32:10.748: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename runtimeclass @ 04/28/23 17:32:10.749
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:32:10.772
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:32:10.777
  Apr 28 17:32:10.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-8377" for this suite. @ 04/28/23 17:32:10.798
• [0.054 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 04/28/23 17:32:10.803
  Apr 28 17:32:10.804: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename runtimeclass @ 04/28/23 17:32:10.804
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:32:10.817
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:32:10.819
  STEP: getting /apis @ 04/28/23 17:32:10.821
  STEP: getting /apis/node.k8s.io @ 04/28/23 17:32:10.826
  STEP: getting /apis/node.k8s.io/v1 @ 04/28/23 17:32:10.826
  STEP: creating @ 04/28/23 17:32:10.827
  STEP: watching @ 04/28/23 17:32:10.839
  Apr 28 17:32:10.839: INFO: starting watch
  STEP: getting @ 04/28/23 17:32:10.844
  STEP: listing @ 04/28/23 17:32:10.846
  STEP: patching @ 04/28/23 17:32:10.849
  STEP: updating @ 04/28/23 17:32:10.853
  Apr 28 17:32:10.857: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 04/28/23 17:32:10.857
  STEP: deleting a collection @ 04/28/23 17:32:10.865
  Apr 28 17:32:10.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-7089" for this suite. @ 04/28/23 17:32:10.897
• [0.113 seconds]
------------------------------
S
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 04/28/23 17:32:10.917
  Apr 28 17:32:10.917: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename container-probe @ 04/28/23 17:32:10.918
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:32:10.933
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:32:10.935
  STEP: Creating pod test-grpc-69f70139-de65-425e-9556-1d80f100bc3a in namespace container-probe-3539 @ 04/28/23 17:32:10.944
  E0428 17:32:11.694094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:12.694685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:32:12.974: INFO: Started pod test-grpc-69f70139-de65-425e-9556-1d80f100bc3a in namespace container-probe-3539
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/28/23 17:32:12.974
  Apr 28 17:32:12.976: INFO: Initial restart count of pod test-grpc-69f70139-de65-425e-9556-1d80f100bc3a is 0
  E0428 17:32:13.694783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:14.695020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:15.695085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:16.695532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:17.695669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:18.695797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:19.695963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:20.696365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:21.697363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:22.697834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:23.697954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:24.698519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:25.699011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:26.699238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:27.699681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:28.699875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:29.699966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:30.700327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:31.701087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:32.702035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:33.703086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:34.703201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:35.703313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:36.703530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:37.704559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:38.704675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:39.705727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:40.705845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:41.705949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:42.706576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:43.707477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:44.707598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:45.707707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:46.707982      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:47.708680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:48.708778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:49.708996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:50.709024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:51.709212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:52.709830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:53.710591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:54.710689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:55.710808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:56.710917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:57.711877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:58.712278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:32:59.712674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:00.712844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:01.712918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:02.714008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:03.714023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:04.714235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:05.715271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:06.715547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:07.716558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:08.716697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:09.716828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:10.717043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:11.717579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:12.717774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:13.718297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:14.718411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:15.719489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:16.720447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:17.720768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:18.721005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:19.721188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:20.721215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:21.722289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:22.722890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:23.723247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:24.723439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:25.724477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:26.724680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:27.725135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:28.725332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:29.726264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:30.726398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:31.726890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:32.727970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:33.728084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:34.728209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:35.728276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:36.729400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:37.730086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:38.730051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:39.730170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:40.730440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:41.730510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:42.730954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:43.731036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:44.731242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:45.732237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:46.732589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:47.733476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:48.733590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:49.734287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:50.734467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:51.734579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:52.735130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:53.735297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:54.735655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:55.736490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:56.736617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:57.737337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:58.737532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:33:59.738065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:00.738256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:01.739369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:02.739874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:03.739973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:04.740091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:05.740192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:06.740294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:07.740587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:08.740838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:09.741113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:10.741196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:11.742275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:12.743161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:13.743265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:14.743497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:15.744542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:16.744729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:17.744855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:18.745047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:19.745879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:20.746083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:21.746487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:22.746922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:23.747946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:24.748145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:25.748821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:26.748924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:27.749914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:28.750034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:29.750856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:30.751053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:31.751409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:32.751727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:33.752197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:34.752398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:35.752549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:36.752939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:37.753896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:38.754257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:39.754678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:40.754957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:41.755015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:42.756009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:43.756421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:44.756716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:45.757623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:46.757763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:47.757900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:48.758111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:49.758263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:50.758579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:51.758923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:52.759378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:53.760094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:54.760185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:55.760585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:56.761386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:57.761619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:58.762273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:34:59.762377      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:00.762874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:01.763765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:02.764485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:03.765416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:04.765628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:05.765673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:06.765876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:07.766011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:08.766620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:09.767573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:10.767680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:11.768741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:12.769281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:13.769382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:14.769664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:15.769791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:16.770267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:17.771114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:18.771219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:19.771984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:20.772092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:21.773158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:22.773478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:23.773622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:24.773748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:25.774293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:26.774424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:27.775476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:28.775576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:29.775670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:30.775869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:31.775997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:32.777533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:33.778363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:34.778571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:35.779597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:36.779708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:37.780523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:38.780714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:39.780830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:40.780931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:41.782009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:42.782618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:43.783607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:44.783736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:45.784255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:46.784462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:47.784958      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:48.785216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:49.785299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:50.786273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:51.786405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:52.787131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:53.787774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:54.787888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:55.788635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:56.788943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:57.789689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:58.789763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:35:59.790661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:00.790885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:01.790998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:02.791616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:03.792654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:04.792831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:05.792975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:06.793200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:07.793898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:08.794034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:09.794858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:10.794985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:11.795180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:12.795737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:13.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 17:36:13.468
  STEP: Destroying namespace "container-probe-3539" for this suite. @ 04/28/23 17:36:13.489
• [242.580 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 04/28/23 17:36:13.498
  Apr 28 17:36:13.498: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 17:36:13.499
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:36:13.518
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:36:13.523
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 04/28/23 17:36:13.533
  E0428 17:36:13.795852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:14.796025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:15.796867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:16.796988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:36:17.552
  Apr 28 17:36:17.556: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-2292292e-5820-4356-9c91-4a74efa29eff container test-container: <nil>
  STEP: delete the pod @ 04/28/23 17:36:17.574
  Apr 28 17:36:17.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4295" for this suite. @ 04/28/23 17:36:17.6
• [4.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 04/28/23 17:36:17.622
  Apr 28 17:36:17.622: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename daemonsets @ 04/28/23 17:36:17.623
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:36:17.637
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:36:17.64
  STEP: Creating a simple DaemonSet "daemon-set" @ 04/28/23 17:36:17.679
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/28/23 17:36:17.684
  Apr 28 17:36:17.695: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 17:36:17.695: INFO: Node ip-172-31-11-161.us-east-2.compute.internal is running 0 daemon pod, expected 1
  E0428 17:36:17.797673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:18.704: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 17:36:18.704: INFO: Node ip-172-31-11-161.us-east-2.compute.internal is running 0 daemon pod, expected 1
  E0428 17:36:18.797804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:19.702: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 28 17:36:19.702: INFO: Node ip-172-31-6-71.us-east-2.compute.internal is running 0 daemon pod, expected 1
  E0428 17:36:19.797861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:20.702: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
  Apr 28 17:36:20.702: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 04/28/23 17:36:20.705
  Apr 28 17:36:20.726: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
  Apr 28 17:36:20.726: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 04/28/23 17:36:20.726
  STEP: Deleting DaemonSet "daemon-set" @ 04/28/23 17:36:20.736
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8345, will wait for the garbage collector to delete the pods @ 04/28/23 17:36:20.736
  Apr 28 17:36:20.794: INFO: Deleting DaemonSet.extensions daemon-set took: 4.847998ms
  E0428 17:36:20.798607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:20.894: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.577172ms
  E0428 17:36:21.801178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:22.801354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:23.698: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 17:36:23.698: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 28 17:36:23.700: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"281992"},"items":null}

  Apr 28 17:36:23.702: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"281992"},"items":null}

  Apr 28 17:36:23.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8345" for this suite. @ 04/28/23 17:36:23.718
• [6.102 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 04/28/23 17:36:23.725
  Apr 28 17:36:23.725: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename statefulset @ 04/28/23 17:36:23.726
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:36:23.741
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:36:23.744
  STEP: Creating service test in namespace statefulset-4268 @ 04/28/23 17:36:23.746
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 04/28/23 17:36:23.75
  STEP: Creating stateful set ss in namespace statefulset-4268 @ 04/28/23 17:36:23.755
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4268 @ 04/28/23 17:36:23.761
  Apr 28 17:36:23.763: INFO: Found 0 stateful pods, waiting for 1
  E0428 17:36:23.801717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:24.802513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:25.803368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:26.803518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:27.803993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:28.804195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:29.804400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:30.804647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:31.804784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:32.804807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:33.768: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 04/28/23 17:36:33.768
  Apr 28 17:36:33.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=statefulset-4268 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0428 17:36:33.805437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:33.933: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 28 17:36:33.933: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 28 17:36:33.933: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 28 17:36:33.936: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0428 17:36:34.805549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:35.805638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:36.805778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:37.806041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:38.806260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:39.806459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:40.806567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:41.806821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:42.807830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:43.808057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:43.940: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 28 17:36:43.940: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 17:36:43.956: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999663s
  E0428 17:36:44.808694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:44.961: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994124697s
  E0428 17:36:45.809213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:45.964: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.989609152s
  E0428 17:36:46.810286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:46.967: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.986440184s
  E0428 17:36:47.810986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:47.971: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.98274924s
  E0428 17:36:48.811664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:48.974: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.979140267s
  E0428 17:36:49.812703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:49.985: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.976119234s
  E0428 17:36:50.812913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:50.990: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.964857893s
  E0428 17:36:51.813216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:51.994: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.95951511s
  E0428 17:36:52.814261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:36:52.998: INFO: Verifying statefulset ss doesn't scale past 1 for another 955.39656ms
  E0428 17:36:53.814350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4268 @ 04/28/23 17:36:53.998
  Apr 28 17:36:54.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=statefulset-4268 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 28 17:36:54.153: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 28 17:36:54.153: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 28 17:36:54.153: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 28 17:36:54.157: INFO: Found 1 stateful pods, waiting for 3
  E0428 17:36:54.814765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:55.815410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:56.815663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:57.816078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:58.816402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:36:59.816655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:00.816953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:01.817099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:02.817634      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:03.817904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:37:04.161: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 17:37:04.161: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 28 17:37:04.161: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 04/28/23 17:37:04.161
  STEP: Scale down will halt with unhealthy stateful pod @ 04/28/23 17:37:04.161
  Apr 28 17:37:04.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=statefulset-4268 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 28 17:37:04.336: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 28 17:37:04.336: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 28 17:37:04.336: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 28 17:37:04.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=statefulset-4268 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 28 17:37:04.529: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 28 17:37:04.529: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 28 17:37:04.529: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 28 17:37:04.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=statefulset-4268 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 28 17:37:04.695: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 28 17:37:04.695: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 28 17:37:04.695: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 28 17:37:04.695: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 17:37:04.697: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0428 17:37:04.818160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:05.818676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:06.818784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:07.818918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:08.819086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:09.819242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:10.820270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:11.821448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:12.821924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:13.822388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:37:14.704: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 28 17:37:14.704: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Apr 28 17:37:14.704: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Apr 28 17:37:14.721: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999659s
  E0428 17:37:14.823346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:37:15.725: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990698516s
  E0428 17:37:15.824376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:37:16.728: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98744121s
  E0428 17:37:16.824597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:37:17.734: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983482969s
  E0428 17:37:17.825027      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:37:18.738: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.97786301s
  E0428 17:37:18.825583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:37:19.742: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.9742878s
  E0428 17:37:19.825702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:37:20.746: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.970116164s
  E0428 17:37:20.829235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:37:21.749: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.966608187s
  E0428 17:37:21.829563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:37:22.753: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.963100739s
  E0428 17:37:22.830024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:37:23.756: INFO: Verifying statefulset ss doesn't scale past 3 for another 959.800947ms
  E0428 17:37:23.830281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4268 @ 04/28/23 17:37:24.757
  Apr 28 17:37:24.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=statefulset-4268 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0428 17:37:24.830327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:37:25.043: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 28 17:37:25.043: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 28 17:37:25.043: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 28 17:37:25.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=statefulset-4268 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 28 17:37:25.304: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 28 17:37:25.304: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 28 17:37:25.304: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 28 17:37:25.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=statefulset-4268 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 28 17:37:25.497: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 28 17:37:25.497: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 28 17:37:25.497: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 28 17:37:25.497: INFO: Scaling statefulset ss to 0
  E0428 17:37:25.831216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:26.831784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:27.832611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:28.832824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:29.833029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:30.833234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:31.833402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:32.833881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:33.834009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:34.834111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 04/28/23 17:37:35.509
  Apr 28 17:37:35.509: INFO: Deleting all statefulset in ns statefulset-4268
  Apr 28 17:37:35.512: INFO: Scaling statefulset ss to 0
  Apr 28 17:37:35.519: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 17:37:35.521: INFO: Deleting statefulset ss
  Apr 28 17:37:35.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4268" for this suite. @ 04/28/23 17:37:35.537
• [71.817 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 04/28/23 17:37:35.543
  Apr 28 17:37:35.543: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename cronjob @ 04/28/23 17:37:35.544
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:37:35.562
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:37:35.565
  STEP: Creating a ForbidConcurrent cronjob @ 04/28/23 17:37:35.567
  STEP: Ensuring a job is scheduled @ 04/28/23 17:37:35.572
  E0428 17:37:35.834919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:36.835245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:37.835493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:38.835744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:39.835798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:40.835897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:41.836639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:42.837178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:43.837282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:44.837389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:45.837483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:46.837577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:47.838532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:48.838657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:49.839576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:50.839694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:51.839795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:52.840163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:53.840268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:54.840409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:55.841434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:56.841718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:57.842263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:58.842336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:37:59.843203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:00.848603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 04/28/23 17:38:01.576
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 04/28/23 17:38:01.582
  STEP: Ensuring no more jobs are scheduled @ 04/28/23 17:38:01.585
  E0428 17:38:01.849390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:02.849903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:03.850631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:04.850798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:05.851666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:06.851867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:07.852477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:08.852652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:09.852778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:10.853452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:11.854421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:12.854979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:13.855949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:14.856209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:15.857034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:16.857233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:17.857347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:18.858273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:19.858778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:20.859118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:21.859213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:22.860042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:23.860798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:24.861027      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:25.863393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:26.863749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:27.863888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:28.864183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:29.864603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:30.865310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:31.865937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:32.866275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:33.867232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:34.867954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:35.868985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:36.869374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:37.869770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:38.869882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:39.870865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:40.871287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:41.871992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:42.872502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:43.873440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:44.873562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:45.874343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:46.875441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:47.875831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:48.876280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:49.876484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:50.877390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:51.877977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:52.878818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:53.879866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:54.879985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:55.880712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:56.880815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:57.881508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:58.881624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:38:59.882281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:00.882482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:01.882599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:02.883164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:03.884159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:04.884375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:05.884476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:06.884575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:07.884683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:08.884796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:09.884847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:10.885366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:11.886468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:12.885909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:13.886025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:14.886061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:15.887097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:16.887289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:17.887468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:18.887618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:19.888456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:20.888718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:21.889222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:22.890719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:23.893351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:24.893430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:25.894282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:26.894488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:27.895232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:28.895521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:29.896353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:30.900205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:31.900316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:32.900887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:33.901239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:34.902281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:35.902385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:36.902637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:37.903617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:38.903767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:39.903877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:40.904053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:41.904093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:42.904621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:43.906422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:44.906519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:45.907396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:46.907522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:47.907784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:48.908322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:49.909316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:50.910447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:51.911178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:52.911701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:53.912618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:54.912854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:55.912944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:56.913094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:57.913215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:58.913206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:39:59.914085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:00.919005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:01.919309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:02.920003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:03.920088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:04.920182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:05.921193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:06.921267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:07.921702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:08.922283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:09.922584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:10.922711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:11.922802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:12.922909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:13.923780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:14.923912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:15.924042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:16.925052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:17.926074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:18.926202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:19.926311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:20.933142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:21.930127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:22.930700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:23.932001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:24.932125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:25.932431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:26.932659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:27.933621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:28.934311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:29.935362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:30.937437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:31.938310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:32.938815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:33.939415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:34.939721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:35.940406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:36.940652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:37.940617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:38.940705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:39.941708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:40.942288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:41.942352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:42.942600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:43.943629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:44.943652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:45.943705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:46.944020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:47.944737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:48.945118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:49.945198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:50.945299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:51.946205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:52.946909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:53.947014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:54.947228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:55.948255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:56.948701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:57.949092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:58.949207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:40:59.949312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:00.949636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:01.949948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:02.950988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:03.951778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:04.951933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:05.953000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:06.953334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:07.954054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:08.954407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:09.954792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:10.958528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:11.958680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:12.958901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:13.958993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:14.959120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:15.960014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:16.960643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:17.960473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:18.960707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:19.961476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:20.962260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:21.962821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:22.962975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:23.963104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:24.963235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:25.964321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:26.964536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:27.964969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:28.965224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:29.966171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:30.966292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:31.967007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:32.967588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:33.967734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:34.967929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:35.968020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:36.968149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:37.969134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:38.969254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:39.970264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:40.970497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:41.970531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:42.971000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:43.971689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:44.972020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:45.973033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:46.973229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:47.973341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:48.973449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:49.974651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:50.974965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:51.975059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:52.975620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:53.976546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:54.976779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:55.976915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:56.977148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:57.978103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:58.978213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:41:59.979262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:00.979467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:01.980342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:02.981033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:03.981657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:04.981773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:05.982009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:06.982197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:07.982250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:08.982439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:09.982725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:10.982908      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:11.983407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:12.984065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:13.985144      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:14.985210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:15.985405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:16.986282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:17.986822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:18.986927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:19.987031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:20.987299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:21.987409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:22.988191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:23.988295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:24.988463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:25.988564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:26.988670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:27.988705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:28.988809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:29.989595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:30.990589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:31.991210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:32.991900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:33.992609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:34.992979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:35.993029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:36.993217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:37.993992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:38.994098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:39.994203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:40.994308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:41.994402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:42.995009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:43.995143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:44.997989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:45.998060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:46.998376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:47.999200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:48.999266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:49.999983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:51.000097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:52.000739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:53.001010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:54.001224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:55.001329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:56.002252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:57.003752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:58.004090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:42:59.004198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:00.007585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:01.007747      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 04/28/23 17:43:01.591
  Apr 28 17:43:01.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5605" for this suite. @ 04/28/23 17:43:01.607
• [326.071 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 04/28/23 17:43:01.626
  Apr 28 17:43:01.626: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:43:01.628
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:43:01.651
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:43:01.655
  STEP: Creating the pod @ 04/28/23 17:43:01.658
  E0428 17:43:02.008809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:03.009355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:04.010274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:43:04.213: INFO: Successfully updated pod "annotationupdateaa33efd9-532b-4965-99b0-2faabd9eb56b"
  E0428 17:43:05.011129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:06.011332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:43:06.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5766" for this suite. @ 04/28/23 17:43:06.234
• [4.615 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 04/28/23 17:43:06.239
  Apr 28 17:43:06.239: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename webhook @ 04/28/23 17:43:06.24
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:43:06.256
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:43:06.262
  STEP: Setting up server cert @ 04/28/23 17:43:06.284
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 17:43:06.614
  STEP: Deploying the webhook pod @ 04/28/23 17:43:06.623
  STEP: Wait for the deployment to be ready @ 04/28/23 17:43:06.634
  Apr 28 17:43:06.644: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0428 17:43:07.012086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:08.012958      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 17:43:08.652
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 17:43:08.66
  E0428 17:43:09.013530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:43:09.661: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 04/28/23 17:43:09.665
  STEP: create a configmap that should be updated by the webhook @ 04/28/23 17:43:09.681
  Apr 28 17:43:09.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6239" for this suite. @ 04/28/23 17:43:09.766
  STEP: Destroying namespace "webhook-markers-511" for this suite. @ 04/28/23 17:43:09.773
• [3.539 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 04/28/23 17:43:09.783
  Apr 28 17:43:09.783: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:43:09.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:43:09.797
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:43:09.799
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 17:43:09.804
  E0428 17:43:10.014450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:11.014749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:12.015492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:13.016495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:43:13.849
  Apr 28 17:43:13.852: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod downwardapi-volume-1f8cd5f2-2096-4395-bb3a-548602bc00df container client-container: <nil>
  STEP: delete the pod @ 04/28/23 17:43:13.857
  Apr 28 17:43:13.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1618" for this suite. @ 04/28/23 17:43:13.881
• [4.103 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 04/28/23 17:43:13.887
  Apr 28 17:43:13.887: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:43:13.888
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:43:13.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:43:13.905
  STEP: Creating secret with name s-test-opt-del-91b0b026-506f-4723-a80a-d6218ac25038 @ 04/28/23 17:43:13.912
  STEP: Creating secret with name s-test-opt-upd-1dd9ddb0-09ec-4c99-a1c5-96c06539dda8 @ 04/28/23 17:43:13.917
  STEP: Creating the pod @ 04/28/23 17:43:13.922
  E0428 17:43:14.017104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:15.022466      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-91b0b026-506f-4723-a80a-d6218ac25038 @ 04/28/23 17:43:15.953
  STEP: Updating secret s-test-opt-upd-1dd9ddb0-09ec-4c99-a1c5-96c06539dda8 @ 04/28/23 17:43:15.959
  STEP: Creating secret with name s-test-opt-create-9ac05f90-ebac-48fc-a833-410888d785c8 @ 04/28/23 17:43:15.964
  STEP: waiting to observe update in volume @ 04/28/23 17:43:15.967
  E0428 17:43:16.023139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:17.023333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:43:17.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4003" for this suite. @ 04/28/23 17:43:17.994
• [4.113 seconds]
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 04/28/23 17:43:18
  Apr 28 17:43:18.000: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename podtemplate @ 04/28/23 17:43:18.001
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:43:18.013
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:43:18.016
  E0428 17:43:18.023620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:43:18.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-7800" for this suite. @ 04/28/23 17:43:18.044
• [0.048 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 04/28/23 17:43:18.049
  Apr 28 17:43:18.049: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename dns @ 04/28/23 17:43:18.05
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:43:18.066
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:43:18.072
  STEP: Creating a test externalName service @ 04/28/23 17:43:18.074
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8172.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local; sleep 1; done
   @ 04/28/23 17:43:18.08
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8172.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local; sleep 1; done
   @ 04/28/23 17:43:18.08
  STEP: creating a pod to probe DNS @ 04/28/23 17:43:18.08
  STEP: submitting the pod to kubernetes @ 04/28/23 17:43:18.08
  E0428 17:43:19.023809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:20.023858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/28/23 17:43:20.101
  STEP: looking for the results for each expected name from probers @ 04/28/23 17:43:20.104
  Apr 28 17:43:20.111: INFO: DNS probes using dns-test-abf42fed-bc01-44cd-a106-ed49905fcf85 succeeded

  STEP: changing the externalName to bar.example.com @ 04/28/23 17:43:20.112
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8172.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local; sleep 1; done
   @ 04/28/23 17:43:20.119
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8172.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local; sleep 1; done
   @ 04/28/23 17:43:20.119
  STEP: creating a second pod to probe DNS @ 04/28/23 17:43:20.119
  STEP: submitting the pod to kubernetes @ 04/28/23 17:43:20.119
  E0428 17:43:21.023975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:22.024085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/28/23 17:43:22.131
  STEP: looking for the results for each expected name from probers @ 04/28/23 17:43:22.134
  Apr 28 17:43:22.140: INFO: File wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-fd96364b-f77b-4abc-92a1-78657a9865e0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 28 17:43:22.143: INFO: File jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-fd96364b-f77b-4abc-92a1-78657a9865e0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 28 17:43:22.143: INFO: Lookups using dns-8172/dns-test-fd96364b-f77b-4abc-92a1-78657a9865e0 failed for: [wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local]

  E0428 17:43:23.025170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:24.025259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:25.025373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:26.025509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:27.026283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:43:27.148: INFO: File wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-fd96364b-f77b-4abc-92a1-78657a9865e0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 28 17:43:27.151: INFO: File jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-fd96364b-f77b-4abc-92a1-78657a9865e0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 28 17:43:27.151: INFO: Lookups using dns-8172/dns-test-fd96364b-f77b-4abc-92a1-78657a9865e0 failed for: [wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local]

  E0428 17:43:28.027313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:29.027671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:30.027785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:31.028005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:32.028196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:43:32.147: INFO: File wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-fd96364b-f77b-4abc-92a1-78657a9865e0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 28 17:43:32.151: INFO: File jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-fd96364b-f77b-4abc-92a1-78657a9865e0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 28 17:43:32.151: INFO: Lookups using dns-8172/dns-test-fd96364b-f77b-4abc-92a1-78657a9865e0 failed for: [wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local]

  E0428 17:43:33.028382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:34.028628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:35.028725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:36.028919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:37.029210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:43:37.147: INFO: File wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-fd96364b-f77b-4abc-92a1-78657a9865e0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 28 17:43:37.150: INFO: File jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-fd96364b-f77b-4abc-92a1-78657a9865e0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 28 17:43:37.150: INFO: Lookups using dns-8172/dns-test-fd96364b-f77b-4abc-92a1-78657a9865e0 failed for: [wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local]

  E0428 17:43:38.029984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:39.030524      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:40.030848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:41.030907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:42.031043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:43:42.148: INFO: File wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-fd96364b-f77b-4abc-92a1-78657a9865e0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 28 17:43:42.152: INFO: File jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-fd96364b-f77b-4abc-92a1-78657a9865e0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 28 17:43:42.152: INFO: Lookups using dns-8172/dns-test-fd96364b-f77b-4abc-92a1-78657a9865e0 failed for: [wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local]

  E0428 17:43:43.031357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:44.031766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:45.031890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:46.032026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:47.032094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:43:47.148: INFO: File wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-fd96364b-f77b-4abc-92a1-78657a9865e0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 28 17:43:47.151: INFO: File jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-fd96364b-f77b-4abc-92a1-78657a9865e0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 28 17:43:47.151: INFO: Lookups using dns-8172/dns-test-fd96364b-f77b-4abc-92a1-78657a9865e0 failed for: [wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local]

  E0428 17:43:48.032828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:49.032961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:50.033137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:51.033217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:52.033306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:43:52.151: INFO: DNS probes using dns-test-fd96364b-f77b-4abc-92a1-78657a9865e0 succeeded

  STEP: changing the service to type=ClusterIP @ 04/28/23 17:43:52.151
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8172.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local; sleep 1; done
   @ 04/28/23 17:43:52.163
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8172.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local; sleep 1; done
   @ 04/28/23 17:43:52.163
  STEP: creating a third pod to probe DNS @ 04/28/23 17:43:52.163
  STEP: submitting the pod to kubernetes @ 04/28/23 17:43:52.172
  E0428 17:43:53.033720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:54.033948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/28/23 17:43:54.189
  STEP: looking for the results for each expected name from probers @ 04/28/23 17:43:54.191
  Apr 28 17:43:54.200: INFO: DNS probes using dns-test-25f21876-fe0d-42ab-824f-549d6db9f102 succeeded

  Apr 28 17:43:54.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 17:43:54.204
  STEP: deleting the pod @ 04/28/23 17:43:54.216
  STEP: deleting the pod @ 04/28/23 17:43:54.228
  STEP: deleting the test externalName service @ 04/28/23 17:43:54.244
  STEP: Destroying namespace "dns-8172" for this suite. @ 04/28/23 17:43:54.289
• [36.252 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 04/28/23 17:43:54.301
  Apr 28 17:43:54.301: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename containers @ 04/28/23 17:43:54.302
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:43:54.314
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:43:54.318
  STEP: Creating a pod to test override arguments @ 04/28/23 17:43:54.32
  E0428 17:43:55.034476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:56.034957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:57.035832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:43:58.035917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:43:58.34
  Apr 28 17:43:58.342: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod client-containers-8aad990b-49c3-4217-aea3-a0d0eb6ef766 container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 17:43:58.347
  Apr 28 17:43:58.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-5355" for this suite. @ 04/28/23 17:43:58.362
• [4.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 04/28/23 17:43:58.368
  Apr 28 17:43:58.368: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename services @ 04/28/23 17:43:58.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:43:58.38
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:43:58.383
  STEP: creating service in namespace services-7606 @ 04/28/23 17:43:58.385
  STEP: creating service affinity-clusterip-transition in namespace services-7606 @ 04/28/23 17:43:58.385
  STEP: creating replication controller affinity-clusterip-transition in namespace services-7606 @ 04/28/23 17:43:58.393
  I0428 17:43:58.401352      19 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-7606, replica count: 3
  E0428 17:43:59.036878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:00.036955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:01.037232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 17:44:01.452452      19 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 28 17:44:01.457: INFO: Creating new exec pod
  E0428 17:44:02.037396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:03.037802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:04.037911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:44:04.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-7606 exec execpod-affinityqfml8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  E0428 17:44:05.038810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:44:05.176: INFO: stderr: "+ + echo hostName\nnc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Apr 28 17:44:05.176: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:44:05.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-7606 exec execpod-affinityqfml8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.118.31 80'
  Apr 28 17:44:05.343: INFO: stderr: "+ nc -v -t -w 2 10.43.118.31 80\nConnection to 10.43.118.31 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Apr 28 17:44:05.343: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:44:05.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-7606 exec execpod-affinityqfml8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.118.31:80/ ; done'
  Apr 28 17:44:05.584: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n"
  Apr 28 17:44:05.584: INFO: stdout: "\naffinity-clusterip-transition-vql9b\naffinity-clusterip-transition-lrpfd\naffinity-clusterip-transition-vql9b\naffinity-clusterip-transition-qk7gg\naffinity-clusterip-transition-vql9b\naffinity-clusterip-transition-vql9b\naffinity-clusterip-transition-lrpfd\naffinity-clusterip-transition-qk7gg\naffinity-clusterip-transition-vql9b\naffinity-clusterip-transition-lrpfd\naffinity-clusterip-transition-qk7gg\naffinity-clusterip-transition-lrpfd\naffinity-clusterip-transition-qk7gg\naffinity-clusterip-transition-vql9b\naffinity-clusterip-transition-vql9b\naffinity-clusterip-transition-lrpfd"
  Apr 28 17:44:05.584: INFO: Received response from host: affinity-clusterip-transition-vql9b
  Apr 28 17:44:05.584: INFO: Received response from host: affinity-clusterip-transition-lrpfd
  Apr 28 17:44:05.584: INFO: Received response from host: affinity-clusterip-transition-vql9b
  Apr 28 17:44:05.584: INFO: Received response from host: affinity-clusterip-transition-qk7gg
  Apr 28 17:44:05.584: INFO: Received response from host: affinity-clusterip-transition-vql9b
  Apr 28 17:44:05.584: INFO: Received response from host: affinity-clusterip-transition-vql9b
  Apr 28 17:44:05.584: INFO: Received response from host: affinity-clusterip-transition-lrpfd
  Apr 28 17:44:05.584: INFO: Received response from host: affinity-clusterip-transition-qk7gg
  Apr 28 17:44:05.584: INFO: Received response from host: affinity-clusterip-transition-vql9b
  Apr 28 17:44:05.584: INFO: Received response from host: affinity-clusterip-transition-lrpfd
  Apr 28 17:44:05.584: INFO: Received response from host: affinity-clusterip-transition-qk7gg
  Apr 28 17:44:05.584: INFO: Received response from host: affinity-clusterip-transition-lrpfd
  Apr 28 17:44:05.584: INFO: Received response from host: affinity-clusterip-transition-qk7gg
  Apr 28 17:44:05.584: INFO: Received response from host: affinity-clusterip-transition-vql9b
  Apr 28 17:44:05.584: INFO: Received response from host: affinity-clusterip-transition-vql9b
  Apr 28 17:44:05.584: INFO: Received response from host: affinity-clusterip-transition-lrpfd
  Apr 28 17:44:05.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-7606 exec execpod-affinityqfml8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.118.31:80/ ; done'
  Apr 28 17:44:05.805: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.118.31:80/\n"
  Apr 28 17:44:05.805: INFO: stdout: "\naffinity-clusterip-transition-vql9b\naffinity-clusterip-transition-vql9b\naffinity-clusterip-transition-vql9b\naffinity-clusterip-transition-vql9b\naffinity-clusterip-transition-vql9b\naffinity-clusterip-transition-vql9b\naffinity-clusterip-transition-vql9b\naffinity-clusterip-transition-vql9b\naffinity-clusterip-transition-vql9b\naffinity-clusterip-transition-vql9b\naffinity-clusterip-transition-vql9b\naffinity-clusterip-transition-vql9b\naffinity-clusterip-transition-vql9b\naffinity-clusterip-transition-vql9b\naffinity-clusterip-transition-vql9b\naffinity-clusterip-transition-vql9b"
  Apr 28 17:44:05.805: INFO: Received response from host: affinity-clusterip-transition-vql9b
  Apr 28 17:44:05.805: INFO: Received response from host: affinity-clusterip-transition-vql9b
  Apr 28 17:44:05.805: INFO: Received response from host: affinity-clusterip-transition-vql9b
  Apr 28 17:44:05.805: INFO: Received response from host: affinity-clusterip-transition-vql9b
  Apr 28 17:44:05.805: INFO: Received response from host: affinity-clusterip-transition-vql9b
  Apr 28 17:44:05.805: INFO: Received response from host: affinity-clusterip-transition-vql9b
  Apr 28 17:44:05.805: INFO: Received response from host: affinity-clusterip-transition-vql9b
  Apr 28 17:44:05.805: INFO: Received response from host: affinity-clusterip-transition-vql9b
  Apr 28 17:44:05.805: INFO: Received response from host: affinity-clusterip-transition-vql9b
  Apr 28 17:44:05.805: INFO: Received response from host: affinity-clusterip-transition-vql9b
  Apr 28 17:44:05.805: INFO: Received response from host: affinity-clusterip-transition-vql9b
  Apr 28 17:44:05.805: INFO: Received response from host: affinity-clusterip-transition-vql9b
  Apr 28 17:44:05.805: INFO: Received response from host: affinity-clusterip-transition-vql9b
  Apr 28 17:44:05.805: INFO: Received response from host: affinity-clusterip-transition-vql9b
  Apr 28 17:44:05.805: INFO: Received response from host: affinity-clusterip-transition-vql9b
  Apr 28 17:44:05.805: INFO: Received response from host: affinity-clusterip-transition-vql9b
  Apr 28 17:44:05.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 17:44:05.809: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7606, will wait for the garbage collector to delete the pods @ 04/28/23 17:44:05.844
  Apr 28 17:44:05.912: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.725389ms
  Apr 28 17:44:06.013: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.513426ms
  E0428 17:44:06.046924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:07.047869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:08.048066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-7606" for this suite. @ 04/28/23 17:44:08.131
• [9.770 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 04/28/23 17:44:08.139
  Apr 28 17:44:08.139: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 17:44:08.14
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:44:08.152
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:44:08.167
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 04/28/23 17:44:08.174
  E0428 17:44:09.048466      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:10.048537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:11.048653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:12.048890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:44:12.204
  Apr 28 17:44:12.207: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-fb82fb7d-5788-4df6-89a1-554017d08da9 container test-container: <nil>
  STEP: delete the pod @ 04/28/23 17:44:12.212
  Apr 28 17:44:12.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9510" for this suite. @ 04/28/23 17:44:12.232
• [4.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 04/28/23 17:44:12.239
  Apr 28 17:44:12.239: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:44:12.24
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:44:12.255
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:44:12.258
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 17:44:12.26
  E0428 17:44:13.049730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:14.050309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:15.052051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:16.052253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:44:16.279
  Apr 28 17:44:16.282: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod downwardapi-volume-0d1b4ee4-ed04-4b2b-9720-a18f11eb0da4 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 17:44:16.288
  Apr 28 17:44:16.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5429" for this suite. @ 04/28/23 17:44:16.305
• [4.071 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 04/28/23 17:44:16.311
  Apr 28 17:44:16.311: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/28/23 17:44:16.312
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:44:16.328
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:44:16.336
  Apr 28 17:44:16.338: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  E0428 17:44:17.053112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:18.053579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/28/23 17:44:19.052
  Apr 28 17:44:19.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-4393 --namespace=crd-publish-openapi-4393 create -f -'
  E0428 17:44:19.053644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:44:19.785: INFO: stderr: ""
  Apr 28 17:44:19.785: INFO: stdout: "e2e-test-crd-publish-openapi-4454-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Apr 28 17:44:19.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-4393 --namespace=crd-publish-openapi-4393 delete e2e-test-crd-publish-openapi-4454-crds test-cr'
  Apr 28 17:44:19.861: INFO: stderr: ""
  Apr 28 17:44:19.861: INFO: stdout: "e2e-test-crd-publish-openapi-4454-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Apr 28 17:44:19.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-4393 --namespace=crd-publish-openapi-4393 apply -f -'
  E0428 17:44:20.055628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:44:20.088: INFO: stderr: ""
  Apr 28 17:44:20.088: INFO: stdout: "e2e-test-crd-publish-openapi-4454-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Apr 28 17:44:20.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-4393 --namespace=crd-publish-openapi-4393 delete e2e-test-crd-publish-openapi-4454-crds test-cr'
  Apr 28 17:44:20.169: INFO: stderr: ""
  Apr 28 17:44:20.169: INFO: stdout: "e2e-test-crd-publish-openapi-4454-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 04/28/23 17:44:20.169
  Apr 28 17:44:20.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-4393 explain e2e-test-crd-publish-openapi-4454-crds'
  E0428 17:44:21.056066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:44:21.104: INFO: stderr: ""
  Apr 28 17:44:21.104: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-4454-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0428 17:44:22.056948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:23.057148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:44:23.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4393" for this suite. @ 04/28/23 17:44:23.21
• [6.904 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 04/28/23 17:44:23.217
  Apr 28 17:44:23.217: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename container-probe @ 04/28/23 17:44:23.218
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:44:23.232
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:44:23.235
  STEP: Creating pod liveness-b9bb45e4-fc1d-4a68-acfd-599ca42dda96 in namespace container-probe-3031 @ 04/28/23 17:44:23.239
  E0428 17:44:24.057953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:25.058005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:44:25.254: INFO: Started pod liveness-b9bb45e4-fc1d-4a68-acfd-599ca42dda96 in namespace container-probe-3031
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/28/23 17:44:25.254
  Apr 28 17:44:25.256: INFO: Initial restart count of pod liveness-b9bb45e4-fc1d-4a68-acfd-599ca42dda96 is 0
  E0428 17:44:26.058115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:27.058239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:28.059281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:29.060222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:30.060864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:31.061213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:32.061429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:33.062544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:34.062661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:35.062873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:36.062973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:37.063250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:38.064283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:39.064420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:40.064534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:41.064628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:42.064740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:43.064823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:44.065910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:45.066028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:44:45.305: INFO: Restart count of pod container-probe-3031/liveness-b9bb45e4-fc1d-4a68-acfd-599ca42dda96 is now 1 (20.048824232s elapsed)
  Apr 28 17:44:45.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 17:44:45.317
  STEP: Destroying namespace "container-probe-3031" for this suite. @ 04/28/23 17:44:45.34
• [22.143 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 04/28/23 17:44:45.361
  Apr 28 17:44:45.361: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:44:45.363
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:44:45.397
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:44:45.4
  STEP: Creating configMap with name projected-configmap-test-volume-9a990bab-fa30-48e0-a0ff-5319a944b993 @ 04/28/23 17:44:45.409
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:44:45.416
  E0428 17:44:46.066367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:47.066494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:48.066534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:49.067796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:44:49.437
  Apr 28 17:44:49.440: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-projected-configmaps-40bfcab4-18e2-4afa-84aa-9a01f7a56f82 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 17:44:49.445
  Apr 28 17:44:49.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2866" for this suite. @ 04/28/23 17:44:49.466
• [4.111 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 04/28/23 17:44:49.473
  Apr 28 17:44:49.473: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename controllerrevisions @ 04/28/23 17:44:49.474
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:44:49.489
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:44:49.496
  STEP: Creating DaemonSet "e2e-bk85g-daemon-set" @ 04/28/23 17:44:49.518
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/28/23 17:44:49.523
  Apr 28 17:44:49.533: INFO: Number of nodes with available pods controlled by daemonset e2e-bk85g-daemon-set: 0
  Apr 28 17:44:49.533: INFO: Node ip-172-31-11-161.us-east-2.compute.internal is running 0 daemon pod, expected 1
  E0428 17:44:50.069860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:44:50.543: INFO: Number of nodes with available pods controlled by daemonset e2e-bk85g-daemon-set: 0
  Apr 28 17:44:50.543: INFO: Node ip-172-31-11-161.us-east-2.compute.internal is running 0 daemon pod, expected 1
  E0428 17:44:51.070398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:44:51.540: INFO: Number of nodes with available pods controlled by daemonset e2e-bk85g-daemon-set: 4
  Apr 28 17:44:51.540: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset e2e-bk85g-daemon-set
  STEP: Confirm DaemonSet "e2e-bk85g-daemon-set" successfully created with "daemonset-name=e2e-bk85g-daemon-set" label @ 04/28/23 17:44:51.544
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-bk85g-daemon-set" @ 04/28/23 17:44:51.549
  Apr 28 17:44:51.552: INFO: Located ControllerRevision: "e2e-bk85g-daemon-set-b6f6d5768"
  STEP: Patching ControllerRevision "e2e-bk85g-daemon-set-b6f6d5768" @ 04/28/23 17:44:51.554
  Apr 28 17:44:51.558: INFO: e2e-bk85g-daemon-set-b6f6d5768 has been patched
  STEP: Create a new ControllerRevision @ 04/28/23 17:44:51.558
  Apr 28 17:44:51.563: INFO: Created ControllerRevision: e2e-bk85g-daemon-set-665696b94
  STEP: Confirm that there are two ControllerRevisions @ 04/28/23 17:44:51.563
  Apr 28 17:44:51.563: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 28 17:44:51.566: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-bk85g-daemon-set-b6f6d5768" @ 04/28/23 17:44:51.566
  STEP: Confirm that there is only one ControllerRevision @ 04/28/23 17:44:51.57
  Apr 28 17:44:51.570: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 28 17:44:51.572: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-bk85g-daemon-set-665696b94" @ 04/28/23 17:44:51.574
  Apr 28 17:44:51.581: INFO: e2e-bk85g-daemon-set-665696b94 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 04/28/23 17:44:51.581
  W0428 17:44:51.588309      19 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 04/28/23 17:44:51.588
  Apr 28 17:44:51.588: INFO: Requesting list of ControllerRevisions to confirm quantity
  E0428 17:44:52.071325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:44:52.593: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 28 17:44:52.596: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-bk85g-daemon-set-665696b94=updated" @ 04/28/23 17:44:52.596
  STEP: Confirm that there is only one ControllerRevision @ 04/28/23 17:44:52.602
  Apr 28 17:44:52.602: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 28 17:44:52.605: INFO: Found 1 ControllerRevisions
  Apr 28 17:44:52.608: INFO: ControllerRevision "e2e-bk85g-daemon-set-76c59f59b7" has revision 3
  STEP: Deleting DaemonSet "e2e-bk85g-daemon-set" @ 04/28/23 17:44:52.61
  STEP: deleting DaemonSet.extensions e2e-bk85g-daemon-set in namespace controllerrevisions-2651, will wait for the garbage collector to delete the pods @ 04/28/23 17:44:52.61
  Apr 28 17:44:52.670: INFO: Deleting DaemonSet.extensions e2e-bk85g-daemon-set took: 6.2645ms
  Apr 28 17:44:52.770: INFO: Terminating DaemonSet.extensions e2e-bk85g-daemon-set pods took: 100.35179ms
  E0428 17:44:53.072086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:54.072175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:44:54.273: INFO: Number of nodes with available pods controlled by daemonset e2e-bk85g-daemon-set: 0
  Apr 28 17:44:54.274: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-bk85g-daemon-set
  Apr 28 17:44:54.277: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"285419"},"items":null}

  Apr 28 17:44:54.279: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"285419"},"items":null}

  Apr 28 17:44:54.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-2651" for this suite. @ 04/28/23 17:44:54.292
• [4.825 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 04/28/23 17:44:54.3
  Apr 28 17:44:54.300: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 17:44:54.3
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:44:54.314
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:44:54.316
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/28/23 17:44:54.318
  Apr 28 17:44:54.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-3920 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Apr 28 17:44:54.386: INFO: stderr: ""
  Apr 28 17:44:54.386: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 04/28/23 17:44:54.386
  E0428 17:44:55.072624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:56.073331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:57.074295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:58.074396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:44:59.074532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 04/28/23 17:44:59.439
  Apr 28 17:44:59.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-3920 get pod e2e-test-httpd-pod -o json'
  Apr 28 17:44:59.522: INFO: stderr: ""
  Apr 28 17:44:59.522: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"2f469395d6f2559a3a086d089762847675976eff73f6a8aa7c98770fd90136e4\",\n            \"cni.projectcalico.org/podIP\": \"10.42.3.194/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.42.3.194/32\"\n        },\n        \"creationTimestamp\": \"2023-04-28T17:44:54Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3920\",\n        \"resourceVersion\": \"285452\",\n        \"uid\": \"bc996c74-4757-45d4-bd00-15052ec2183b\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-c5dcr\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-9-127.us-east-2.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-c5dcr\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-28T17:44:54Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-28T17:44:55Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-28T17:44:55Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-28T17:44:54Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://691607dd8f22cabfdf3af92feb2f612626bb669e484cbdcc6470beca18ab643c\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-04-28T17:44:55Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.9.127\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.42.3.194\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.42.3.194\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-04-28T17:44:54Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 04/28/23 17:44:59.522
  Apr 28 17:44:59.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-3920 replace -f -'
  E0428 17:45:00.074781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:45:00.923: INFO: stderr: ""
  Apr 28 17:45:00.923: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 04/28/23 17:45:00.923
  Apr 28 17:45:00.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-3920 delete pods e2e-test-httpd-pod'
  E0428 17:45:01.075578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:02.076138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:45:02.902: INFO: stderr: ""
  Apr 28 17:45:02.902: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 28 17:45:02.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3920" for this suite. @ 04/28/23 17:45:02.906
• [8.617 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 04/28/23 17:45:02.917
  Apr 28 17:45:02.917: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename configmap @ 04/28/23 17:45:02.919
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:45:02.939
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:45:02.945
  STEP: Creating configMap configmap-7672/configmap-test-562d12e8-3c3f-4647-8fd0-eef4aba1ef3e @ 04/28/23 17:45:02.947
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:45:02.953
  E0428 17:45:03.076510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:04.080161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:05.077144      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:06.077241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:45:06.984
  Apr 28 17:45:06.987: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-configmaps-8c2dbac1-0c04-4275-83c0-08dd115abb16 container env-test: <nil>
  STEP: delete the pod @ 04/28/23 17:45:06.993
  Apr 28 17:45:07.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7672" for this suite. @ 04/28/23 17:45:07.017
• [4.106 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 04/28/23 17:45:07.024
  Apr 28 17:45:07.024: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename services @ 04/28/23 17:45:07.024
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:45:07.038
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:45:07.041
  STEP: creating service multi-endpoint-test in namespace services-9356 @ 04/28/23 17:45:07.044
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9356 to expose endpoints map[] @ 04/28/23 17:45:07.056
  Apr 28 17:45:07.065: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  E0428 17:45:07.077292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:45:08.072: INFO: successfully validated that service multi-endpoint-test in namespace services-9356 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-9356 @ 04/28/23 17:45:08.072
  E0428 17:45:08.077564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:09.078439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:10.078663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9356 to expose endpoints map[pod1:[100]] @ 04/28/23 17:45:10.093
  Apr 28 17:45:10.100: INFO: successfully validated that service multi-endpoint-test in namespace services-9356 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-9356 @ 04/28/23 17:45:10.1
  E0428 17:45:11.079344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:12.079564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9356 to expose endpoints map[pod1:[100] pod2:[101]] @ 04/28/23 17:45:12.117
  Apr 28 17:45:12.129: INFO: successfully validated that service multi-endpoint-test in namespace services-9356 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 04/28/23 17:45:12.129
  Apr 28 17:45:12.129: INFO: Creating new exec pod
  E0428 17:45:13.080557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:14.080907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:15.083121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:45:15.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-9356 exec execpod8hbzx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Apr 28 17:45:15.566: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 80\n+ echo hostName\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Apr 28 17:45:15.566: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:45:15.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-9356 exec execpod8hbzx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.173.104 80'
  Apr 28 17:45:15.982: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.173.104 80\nConnection to 10.43.173.104 80 port [tcp/http] succeeded!\n"
  Apr 28 17:45:15.984: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:45:15.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-9356 exec execpod8hbzx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  E0428 17:45:16.082606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:45:16.151: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 81\n+ echo hostName\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Apr 28 17:45:16.151: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 28 17:45:16.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-9356 exec execpod8hbzx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.173.104 81'
  Apr 28 17:45:16.285: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.173.104 81\nConnection to 10.43.173.104 81 port [tcp/*] succeeded!\n"
  Apr 28 17:45:16.285: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-9356 @ 04/28/23 17:45:16.285
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9356 to expose endpoints map[pod2:[101]] @ 04/28/23 17:45:16.306
  Apr 28 17:45:16.351: INFO: successfully validated that service multi-endpoint-test in namespace services-9356 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-9356 @ 04/28/23 17:45:16.351
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9356 to expose endpoints map[] @ 04/28/23 17:45:16.365
  Apr 28 17:45:16.379: INFO: successfully validated that service multi-endpoint-test in namespace services-9356 exposes endpoints map[]
  Apr 28 17:45:16.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9356" for this suite. @ 04/28/23 17:45:16.403
• [9.386 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 04/28/23 17:45:16.41
  Apr 28 17:45:16.410: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename services @ 04/28/23 17:45:16.411
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:45:16.422
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:45:16.431
  STEP: creating service nodeport-test with type=NodePort in namespace services-9149 @ 04/28/23 17:45:16.434
  STEP: creating replication controller nodeport-test in namespace services-9149 @ 04/28/23 17:45:16.453
  I0428 17:45:16.461475      19 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-9149, replica count: 2
  E0428 17:45:17.083629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:18.084355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:19.084455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 17:45:19.512047      19 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 28 17:45:19.512: INFO: Creating new exec pod
  E0428 17:45:20.084563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:21.084731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:22.085452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:45:22.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-9149 exec execpodvmx5r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Apr 28 17:45:22.688: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Apr 28 17:45:22.688: INFO: stdout: ""
  E0428 17:45:23.086155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:45:23.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-9149 exec execpodvmx5r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Apr 28 17:45:23.965: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Apr 28 17:45:23.965: INFO: stdout: "nodeport-test-7c7w9"
  Apr 28 17:45:23.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-9149 exec execpodvmx5r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.131.5 80'
  E0428 17:45:24.087020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:45:24.238: INFO: stderr: "+ nc -v -t -w 2 10.43.131.5 80\n+ echo hostName\nConnection to 10.43.131.5 80 port [tcp/http] succeeded!\n"
  Apr 28 17:45:24.239: INFO: stdout: "nodeport-test-7c7w9"
  Apr 28 17:45:24.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-9149 exec execpodvmx5r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.14.195 32171'
  Apr 28 17:45:24.498: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.14.195 32171\nConnection to 172.31.14.195 32171 port [tcp/*] succeeded!\n"
  Apr 28 17:45:24.498: INFO: stdout: "nodeport-test-7c7w9"
  Apr 28 17:45:24.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-9149 exec execpodvmx5r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.6.71 32171'
  Apr 28 17:45:24.641: INFO: stderr: "+ nc -v -t -w 2 172.31.6.71 32171\nConnection to 172.31.6.71 32171 port [tcp/*] succeeded!\n+ echo hostName\n"
  Apr 28 17:45:24.641: INFO: stdout: "nodeport-test-d9w7w"
  Apr 28 17:45:24.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9149" for this suite. @ 04/28/23 17:45:24.645
• [8.240 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 04/28/23 17:45:24.651
  Apr 28 17:45:24.651: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename lease-test @ 04/28/23 17:45:24.652
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:45:24.663
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:45:24.665
  Apr 28 17:45:24.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-1407" for this suite. @ 04/28/23 17:45:24.715
• [0.068 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 04/28/23 17:45:24.721
  Apr 28 17:45:24.721: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename sysctl @ 04/28/23 17:45:24.722
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:45:24.732
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:45:24.735
  STEP: Creating a pod with one valid and two invalid sysctls @ 04/28/23 17:45:24.737
  Apr 28 17:45:24.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-977" for this suite. @ 04/28/23 17:45:24.744
• [0.029 seconds]
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 04/28/23 17:45:24.75
  Apr 28 17:45:24.750: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename container-runtime @ 04/28/23 17:45:24.751
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:45:24.765
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:45:24.771
  STEP: create the container @ 04/28/23 17:45:24.774
  W0428 17:45:24.785383      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 04/28/23 17:45:24.786
  E0428 17:45:25.088067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:26.088169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:27.089985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 04/28/23 17:45:27.801
  STEP: the container should be terminated @ 04/28/23 17:45:27.804
  STEP: the termination message should be set @ 04/28/23 17:45:27.804
  Apr 28 17:45:27.804: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 04/28/23 17:45:27.804
  Apr 28 17:45:27.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-5341" for this suite. @ 04/28/23 17:45:27.82
• [3.074 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 04/28/23 17:45:27.825
  Apr 28 17:45:27.825: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename var-expansion @ 04/28/23 17:45:27.826
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:45:27.841
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:45:27.843
  STEP: Creating a pod to test substitution in container's args @ 04/28/23 17:45:27.846
  E0428 17:45:28.094635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:29.095235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:30.095759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:31.095864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:45:31.884
  Apr 28 17:45:31.886: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod var-expansion-95ec7aa0-a96b-4b8b-984f-0e9229810379 container dapi-container: <nil>
  STEP: delete the pod @ 04/28/23 17:45:31.891
  Apr 28 17:45:31.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5895" for this suite. @ 04/28/23 17:45:31.906
• [4.085 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 04/28/23 17:45:31.911
  Apr 28 17:45:31.911: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename gc @ 04/28/23 17:45:31.916
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:45:31.93
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:45:31.932
  STEP: create the rc @ 04/28/23 17:45:31.937
  W0428 17:45:31.941935      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0428 17:45:32.096286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:33.096656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:34.097281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:35.098190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:36.098514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:37.098607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 04/28/23 17:45:37.957
  STEP: wait for the rc to be deleted @ 04/28/23 17:45:37.969
  E0428 17:45:38.099466      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:45:38.999: INFO: 80 pods remaining
  Apr 28 17:45:39.004: INFO: 80 pods has nil DeletionTimestamp
  Apr 28 17:45:39.004: INFO: 
  E0428 17:45:39.100148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:45:40.034: INFO: 73 pods remaining
  Apr 28 17:45:40.034: INFO: 72 pods has nil DeletionTimestamp
  Apr 28 17:45:40.034: INFO: 
  E0428 17:45:40.101436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:45:41.024: INFO: 60 pods remaining
  Apr 28 17:45:41.024: INFO: 60 pods has nil DeletionTimestamp
  Apr 28 17:45:41.024: INFO: 
  E0428 17:45:41.112749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:45:42.059: INFO: 40 pods remaining
  Apr 28 17:45:42.059: INFO: 40 pods has nil DeletionTimestamp
  Apr 28 17:45:42.059: INFO: 
  E0428 17:45:42.113332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:45:43.037: INFO: 33 pods remaining
  Apr 28 17:45:43.037: INFO: 32 pods has nil DeletionTimestamp
  Apr 28 17:45:43.037: INFO: 
  E0428 17:45:43.114149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:45:43.985: INFO: 20 pods remaining
  Apr 28 17:45:43.985: INFO: 20 pods has nil DeletionTimestamp
  Apr 28 17:45:43.985: INFO: 
  E0428 17:45:44.114801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 04/28/23 17:45:44.982
  E0428 17:45:45.116043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:45:45.136: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 28 17:45:45.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1001" for this suite. @ 04/28/23 17:45:45.141
• [13.240 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 04/28/23 17:45:45.151
  Apr 28 17:45:45.151: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/28/23 17:45:45.153
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:45:45.174
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:45:45.182
  Apr 28 17:45:45.190: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  E0428 17:45:46.119218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:47.120134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/28/23 17:45:47.269
  Apr 28 17:45:47.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-7910 --namespace=crd-publish-openapi-7910 create -f -'
  E0428 17:45:48.120732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:45:48.875: INFO: stderr: ""
  Apr 28 17:45:48.875: INFO: stdout: "e2e-test-crd-publish-openapi-9000-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Apr 28 17:45:48.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-7910 --namespace=crd-publish-openapi-7910 delete e2e-test-crd-publish-openapi-9000-crds test-cr'
  Apr 28 17:45:49.065: INFO: stderr: ""
  Apr 28 17:45:49.065: INFO: stdout: "e2e-test-crd-publish-openapi-9000-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Apr 28 17:45:49.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-7910 --namespace=crd-publish-openapi-7910 apply -f -'
  E0428 17:45:49.121206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:45:49.513: INFO: stderr: ""
  Apr 28 17:45:49.513: INFO: stdout: "e2e-test-crd-publish-openapi-9000-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Apr 28 17:45:49.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-7910 --namespace=crd-publish-openapi-7910 delete e2e-test-crd-publish-openapi-9000-crds test-cr'
  Apr 28 17:45:49.622: INFO: stderr: ""
  Apr 28 17:45:49.622: INFO: stdout: "e2e-test-crd-publish-openapi-9000-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 04/28/23 17:45:49.622
  Apr 28 17:45:49.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=crd-publish-openapi-7910 explain e2e-test-crd-publish-openapi-9000-crds'
  Apr 28 17:45:49.981: INFO: stderr: ""
  Apr 28 17:45:49.981: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-9000-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0428 17:45:50.121660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:51.121871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:52.122463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:45:52.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7910" for this suite. @ 04/28/23 17:45:52.802
• [7.662 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 04/28/23 17:45:52.814
  Apr 28 17:45:52.814: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename secrets @ 04/28/23 17:45:52.816
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:45:52.837
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:45:52.846
  STEP: Creating secret with name secret-test-ce9d1fe2-f43c-4404-87cd-f73a50b4a4b6 @ 04/28/23 17:45:52.85
  STEP: Creating a pod to test consume secrets @ 04/28/23 17:45:52.863
  E0428 17:45:53.123258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:54.123492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:55.124279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:56.124523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:45:56.888
  Apr 28 17:45:56.891: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-secrets-e604d30c-000e-4802-8f34-5119207f8993 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 17:45:56.903
  Apr 28 17:45:56.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5291" for this suite. @ 04/28/23 17:45:56.92
• [4.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 04/28/23 17:45:56.927
  Apr 28 17:45:56.927: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename statefulset @ 04/28/23 17:45:56.928
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:45:56.942
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:45:56.945
  STEP: Creating service test in namespace statefulset-8731 @ 04/28/23 17:45:56.947
  STEP: Creating statefulset ss in namespace statefulset-8731 @ 04/28/23 17:45:56.953
  Apr 28 17:45:56.968: INFO: Found 0 stateful pods, waiting for 1
  E0428 17:45:57.125691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:58.125998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:45:59.126100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:00.126537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:01.126817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:02.126989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:03.127134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:04.127338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:05.127547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:06.127667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:46:06.972: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 04/28/23 17:46:06.978
  STEP: updating a scale subresource @ 04/28/23 17:46:06.98
  STEP: verifying the statefulset Spec.Replicas was modified @ 04/28/23 17:46:06.986
  STEP: Patch a scale subresource @ 04/28/23 17:46:06.989
  STEP: verifying the statefulset Spec.Replicas was modified @ 04/28/23 17:46:06.999
  Apr 28 17:46:07.017: INFO: Deleting all statefulset in ns statefulset-8731
  Apr 28 17:46:07.028: INFO: Scaling statefulset ss to 0
  E0428 17:46:07.127967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:08.128137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:09.128218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:10.129263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:11.129292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:12.129541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:13.130199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:14.130395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:15.131361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:16.131450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:46:17.073: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 17:46:17.077: INFO: Deleting statefulset ss
  Apr 28 17:46:17.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8731" for this suite. @ 04/28/23 17:46:17.092
• [20.177 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 04/28/23 17:46:17.106
  Apr 28 17:46:17.106: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 17:46:17.107
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:46:17.128
  E0428 17:46:17.132057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:46:17.132
  STEP: Creating a ResourceQuota with best effort scope @ 04/28/23 17:46:17.146
  STEP: Ensuring ResourceQuota status is calculated @ 04/28/23 17:46:17.15
  E0428 17:46:18.132978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:19.133283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not best effort scope @ 04/28/23 17:46:19.158
  STEP: Ensuring ResourceQuota status is calculated @ 04/28/23 17:46:19.164
  E0428 17:46:20.133893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:21.134011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a best-effort pod @ 04/28/23 17:46:21.168
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 04/28/23 17:46:21.22
  E0428 17:46:22.134174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:23.134852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 04/28/23 17:46:23.224
  E0428 17:46:24.135229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:25.136037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 04/28/23 17:46:25.228
  STEP: Ensuring resource quota status released the pod usage @ 04/28/23 17:46:25.245
  E0428 17:46:26.136164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:27.136306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a not best-effort pod @ 04/28/23 17:46:27.25
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 04/28/23 17:46:27.265
  E0428 17:46:28.137179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:29.137479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 04/28/23 17:46:29.27
  E0428 17:46:30.139438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:31.139618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 04/28/23 17:46:31.274
  STEP: Ensuring resource quota status released the pod usage @ 04/28/23 17:46:31.292
  E0428 17:46:32.139835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:33.140194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:46:33.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6038" for this suite. @ 04/28/23 17:46:33.306
• [16.207 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 04/28/23 17:46:33.314
  Apr 28 17:46:33.314: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 17:46:33.315
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:46:33.33
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:46:33.332
  STEP: create deployment with httpd image @ 04/28/23 17:46:33.335
  Apr 28 17:46:33.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-4292 create -f -'
  E0428 17:46:34.140346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:46:34.291: INFO: stderr: ""
  Apr 28 17:46:34.291: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 04/28/23 17:46:34.291
  Apr 28 17:46:34.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-4292 diff -f -'
  E0428 17:46:35.141327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:46:35.276: INFO: rc: 1
  Apr 28 17:46:35.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-4292 delete -f -'
  Apr 28 17:46:35.342: INFO: stderr: ""
  Apr 28 17:46:35.342: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Apr 28 17:46:35.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4292" for this suite. @ 04/28/23 17:46:35.347
• [2.043 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 04/28/23 17:46:35.358
  Apr 28 17:46:35.358: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/28/23 17:46:35.358
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:46:35.372
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:46:35.375
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 04/28/23 17:46:35.377
  Apr 28 17:46:35.378: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  E0428 17:46:36.141824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:37.142272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:46:37.391: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  E0428 17:46:38.142288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:39.142400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:40.144697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:41.145620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:42.146274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:43.155754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:46:43.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-246" for this suite. @ 04/28/23 17:46:43.779
• [8.427 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 04/28/23 17:46:43.79
  Apr 28 17:46:43.790: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename sched-preemption @ 04/28/23 17:46:43.791
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:46:43.808
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:46:43.81
  Apr 28 17:46:43.825: INFO: Waiting up to 1m0s for all nodes to be ready
  E0428 17:46:44.155652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:45.156450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:46.156527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:47.156641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:48.157546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:49.157673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:50.158119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:51.164386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:52.165397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:53.165459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:54.166285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:55.167242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:56.168056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:57.168214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:58.169150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:46:59.169200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:00.169944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:01.170049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:02.170179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:03.170820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:04.171748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:05.172146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:06.172249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:07.172350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:08.173368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:09.174291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:10.174387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:11.174585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:12.174722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:13.175447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:14.175547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:15.176366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:16.177424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:17.177799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:18.177969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:19.178328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:20.178329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:21.178608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:22.179438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:23.180074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:24.180605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:25.180644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:26.181608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:27.182290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:28.182400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:29.182618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:30.183370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:31.183582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:32.184197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:33.184288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:34.185295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:35.185457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:36.186171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:37.186291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:38.186424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:39.186509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:40.186585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:41.186779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:42.187208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:43.187806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:47:43.866: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 04/28/23 17:47:43.878
  Apr 28 17:47:43.878: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename sched-preemption-path @ 04/28/23 17:47:43.879
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:47:43.904
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:47:43.907
  Apr 28 17:47:43.955: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Apr 28 17:47:43.966: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Apr 28 17:47:43.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 17:47:44.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-963" for this suite. @ 04/28/23 17:47:44.089
  STEP: Destroying namespace "sched-preemption-8244" for this suite. @ 04/28/23 17:47:44.095
• [60.312 seconds]
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 04/28/23 17:47:44.103
  Apr 28 17:47:44.103: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename var-expansion @ 04/28/23 17:47:44.103
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:47:44.118
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:47:44.121
  STEP: Creating a pod to test substitution in container's command @ 04/28/23 17:47:44.123
  E0428 17:47:44.188858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:45.189131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:46.189709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:47.189806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:47:48.156
  Apr 28 17:47:48.159: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod var-expansion-5306df0e-14f2-48af-9375-3f74e5e17cc3 container dapi-container: <nil>
  STEP: delete the pod @ 04/28/23 17:47:48.173
  E0428 17:47:48.190083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:47:48.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5369" for this suite. @ 04/28/23 17:47:48.197
• [4.100 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 04/28/23 17:47:48.205
  Apr 28 17:47:48.205: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename namespaces @ 04/28/23 17:47:48.206
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:47:48.22
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:47:48.223
  STEP: Creating a test namespace @ 04/28/23 17:47:48.226
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:47:48.241
  STEP: Creating a pod in the namespace @ 04/28/23 17:47:48.243
  STEP: Waiting for the pod to have running status @ 04/28/23 17:47:48.25
  E0428 17:47:49.190283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:50.190573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the namespace @ 04/28/23 17:47:50.259
  STEP: Waiting for the namespace to be removed. @ 04/28/23 17:47:50.264
  E0428 17:47:51.190641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:52.191182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:53.192006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:54.192120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:55.192691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:56.193181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:57.193323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:58.194310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:47:59.194302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:00.194407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:01.194568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 04/28/23 17:48:01.268
  STEP: Verifying there are no pods in the namespace @ 04/28/23 17:48:01.285
  Apr 28 17:48:01.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-1762" for this suite. @ 04/28/23 17:48:01.292
  STEP: Destroying namespace "nsdeletetest-1746" for this suite. @ 04/28/23 17:48:01.298
  Apr 28 17:48:01.302: INFO: Namespace nsdeletetest-1746 was already deleted
  STEP: Destroying namespace "nsdeletetest-8831" for this suite. @ 04/28/23 17:48:01.302
• [13.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 04/28/23 17:48:01.31
  Apr 28 17:48:01.310: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename sched-preemption @ 04/28/23 17:48:01.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:48:01.326
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:48:01.328
  Apr 28 17:48:01.348: INFO: Waiting up to 1m0s for all nodes to be ready
  E0428 17:48:02.195294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:03.195381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:04.195880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:05.196104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:06.196479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:07.196550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:08.197587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:09.198320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:10.198901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:11.199028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:12.199148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:13.199926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:14.200554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:15.200820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:16.201239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:17.202282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:18.202339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:19.202563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:20.202701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:21.202976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:22.203854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:23.204509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:24.204617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:25.204835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:26.204944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:27.205202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:28.206290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:29.206678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:30.206813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:31.207102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:32.207612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:33.208333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:34.209322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:35.209391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:36.209494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:37.210093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:38.210232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:39.210421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:40.210835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:41.210946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:42.211620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:43.212154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:44.212330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:45.212535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:46.212683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:47.212869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:48.213182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:49.213226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:50.213329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:51.214337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:52.214477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:53.215040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:54.215156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:55.215592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:56.215721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:57.215960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:58.216362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:48:59.216655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:00.217225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:01.216936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:49:01.386: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 04/28/23 17:49:01.389
  Apr 28 17:49:01.389: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename sched-preemption-path @ 04/28/23 17:49:01.39
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:49:01.409
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:49:01.412
  STEP: Finding an available node @ 04/28/23 17:49:01.416
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/28/23 17:49:01.417
  E0428 17:49:02.217353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:03.217967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/28/23 17:49:03.444
  Apr 28 17:49:03.461: INFO: found a healthy node: ip-172-31-9-127.us-east-2.compute.internal
  E0428 17:49:04.219121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:05.219935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:06.220030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:07.220137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:08.220160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:09.221039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:10.222503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:11.223141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:49:11.545: INFO: pods created so far: [1 1 1]
  Apr 28 17:49:11.545: INFO: length of pods created so far: 3
  E0428 17:49:12.224302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:13.224667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:49:13.554: INFO: pods created so far: [2 2 1]
  E0428 17:49:14.224779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:15.225136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:16.225312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:17.225359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:18.225436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:19.225673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:20.225765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:49:20.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 17:49:20.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-2555" for this suite. @ 04/28/23 17:49:20.65
  STEP: Destroying namespace "sched-preemption-2559" for this suite. @ 04/28/23 17:49:20.655
• [79.353 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 04/28/23 17:49:20.663
  Apr 28 17:49:20.663: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename container-probe @ 04/28/23 17:49:20.664
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:49:20.679
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:49:20.683
  E0428 17:49:21.226366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:22.226433      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:23.226651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:24.227072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:25.227255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:26.227562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:27.227783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:28.227828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:29.228739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:30.228962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:31.230469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:32.230568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:33.231365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:34.231560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:35.231735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:36.231786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:37.232488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:38.232632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:39.232759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:40.232882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:41.233216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:42.233325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:49:42.742: INFO: Container started at 2023-04-28 17:49:21 +0000 UTC, pod became ready at 2023-04-28 17:49:41 +0000 UTC
  Apr 28 17:49:42.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-9495" for this suite. @ 04/28/23 17:49:42.75
• [22.094 seconds]
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 04/28/23 17:49:42.758
  Apr 28 17:49:42.758: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename init-container @ 04/28/23 17:49:42.759
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:49:42.776
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:49:42.78
  STEP: creating the pod @ 04/28/23 17:49:42.784
  Apr 28 17:49:42.784: INFO: PodSpec: initContainers in spec.initContainers
  E0428 17:49:43.233552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:44.239626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:45.240044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:46.240229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:47.240408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:49:47.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-7109" for this suite. @ 04/28/23 17:49:47.357
• [4.607 seconds]
------------------------------
S
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 04/28/23 17:49:47.373
  Apr 28 17:49:47.373: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename pods @ 04/28/23 17:49:47.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:49:47.396
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:49:47.399
  STEP: creating pod @ 04/28/23 17:49:47.401
  E0428 17:49:48.240389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:49.242357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:49:49.424: INFO: Pod pod-hostip-d299eb20-e8fc-46e6-9bf0-424ef64b4603 has hostIP: 172.31.9.127
  Apr 28 17:49:49.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1908" for this suite. @ 04/28/23 17:49:49.428
• [2.065 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 04/28/23 17:49:49.441
  Apr 28 17:49:49.441: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename pods @ 04/28/23 17:49:49.443
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:49:49.461
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:49:49.464
  STEP: creating the pod @ 04/28/23 17:49:49.466
  STEP: setting up watch @ 04/28/23 17:49:49.466
  STEP: submitting the pod to kubernetes @ 04/28/23 17:49:49.569
  STEP: verifying the pod is in kubernetes @ 04/28/23 17:49:49.577
  STEP: verifying pod creation was observed @ 04/28/23 17:49:49.58
  E0428 17:49:50.245919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:51.245973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 04/28/23 17:49:51.593
  STEP: verifying pod deletion was observed @ 04/28/23 17:49:51.601
  E0428 17:49:52.246069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:53.249790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:49:54.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-173" for this suite. @ 04/28/23 17:49:54.114
• [4.678 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 04/28/23 17:49:54.121
  Apr 28 17:49:54.121: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename dns @ 04/28/23 17:49:54.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:49:54.139
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:49:54.143
  STEP: Creating a test headless service @ 04/28/23 17:49:54.145
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5580.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5580.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5580.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5580.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5580.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5580.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5580.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5580.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5580.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5580.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 245.240.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.240.245_udp@PTR;check="$$(dig +tcp +noall +answer +search 245.240.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.240.245_tcp@PTR;sleep 1; done
   @ 04/28/23 17:49:54.163
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5580.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5580.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5580.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5580.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5580.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5580.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5580.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5580.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5580.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5580.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 245.240.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.240.245_udp@PTR;check="$$(dig +tcp +noall +answer +search 245.240.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.240.245_tcp@PTR;sleep 1; done
   @ 04/28/23 17:49:54.163
  STEP: creating a pod to probe DNS @ 04/28/23 17:49:54.163
  STEP: submitting the pod to kubernetes @ 04/28/23 17:49:54.163
  E0428 17:49:54.250774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:55.253290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/28/23 17:49:56.2
  STEP: looking for the results for each expected name from probers @ 04/28/23 17:49:56.203
  Apr 28 17:49:56.209: INFO: Unable to read wheezy_udp@dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:49:56.213: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:49:56.215: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:49:56.218: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:49:56.231: INFO: Unable to read jessie_udp@dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:49:56.234: INFO: Unable to read jessie_tcp@dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:49:56.237: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:49:56.239: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:49:56.251: INFO: Lookups using dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee failed for: [wheezy_udp@dns-test-service.dns-5580.svc.cluster.local wheezy_tcp@dns-test-service.dns-5580.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local jessie_udp@dns-test-service.dns-5580.svc.cluster.local jessie_tcp@dns-test-service.dns-5580.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local]

  E0428 17:49:56.256380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:57.256569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:58.256922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:49:59.257205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:00.258279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:01.259275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:50:01.277: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:50:01.281: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:50:01.333: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:50:01.343: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:50:01.369: INFO: Lookups using dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local]

  E0428 17:50:02.259381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:03.260080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:04.260193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:05.261141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:06.261779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:50:06.262: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:50:06.266: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:50:06.291: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:50:06.294: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:50:06.314: INFO: Lookups using dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local]

  E0428 17:50:07.261882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:08.262270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:09.262313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:10.263961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:50:11.262: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  E0428 17:50:11.264434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:50:11.265: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:50:11.289: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:50:11.293: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:50:11.305: INFO: Lookups using dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local]

  E0428 17:50:12.264986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:13.265143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:14.265292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:15.265566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:50:16.261: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  E0428 17:50:16.265749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:50:16.265: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:50:16.286: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:50:16.289: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:50:16.317: INFO: Lookups using dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local]

  E0428 17:50:17.265871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:18.265987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:19.266512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:20.267087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:50:21.264: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  E0428 17:50:21.268085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:50:21.268: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:50:21.290: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:50:21.293: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:50:21.304: INFO: Lookups using dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local]

  E0428 17:50:22.268183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:23.269237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:24.269365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:25.270254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:26.270322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:50:26.285: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:50:26.288: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local from pod dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee: the server could not find the requested resource (get pods dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee)
  Apr 28 17:50:26.298: INFO: Lookups using dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee failed for: [jessie_udp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5580.svc.cluster.local]

  E0428 17:50:27.271828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:28.272337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:29.272527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:30.272692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:31.273752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:50:31.298: INFO: DNS probes using dns-5580/dns-test-8ef5e7e1-18a5-4f6f-b1c9-4c9ce310e7ee succeeded

  Apr 28 17:50:31.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 17:50:31.302
  STEP: deleting the test service @ 04/28/23 17:50:31.334
  STEP: deleting the test headless service @ 04/28/23 17:50:31.392
  STEP: Destroying namespace "dns-5580" for this suite. @ 04/28/23 17:50:31.406
• [37.292 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 04/28/23 17:50:31.413
  Apr 28 17:50:31.413: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename services @ 04/28/23 17:50:31.415
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:50:31.454
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:50:31.465
  STEP: fetching services @ 04/28/23 17:50:31.467
  Apr 28 17:50:31.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7940" for this suite. @ 04/28/23 17:50:31.476
• [0.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 04/28/23 17:50:31.485
  Apr 28 17:50:31.485: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename watch @ 04/28/23 17:50:31.486
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:50:31.502
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:50:31.507
  STEP: creating a new configmap @ 04/28/23 17:50:31.509
  STEP: modifying the configmap once @ 04/28/23 17:50:31.514
  STEP: modifying the configmap a second time @ 04/28/23 17:50:31.524
  STEP: deleting the configmap @ 04/28/23 17:50:31.533
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 04/28/23 17:50:31.542
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 04/28/23 17:50:31.545
  Apr 28 17:50:31.545: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9692  51b83fa1-22b7-4df8-9acc-2c0b653fc9f9 289777 0 2023-04-28 17:50:31 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-28 17:50:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 17:50:31.545: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9692  51b83fa1-22b7-4df8-9acc-2c0b653fc9f9 289778 0 2023-04-28 17:50:31 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-28 17:50:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 17:50:31.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9692" for this suite. @ 04/28/23 17:50:31.549
• [0.068 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 04/28/23 17:50:31.558
  Apr 28 17:50:31.558: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/28/23 17:50:31.559
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:50:31.575
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:50:31.577
  STEP: create the container to handle the HTTPGet hook request. @ 04/28/23 17:50:31.583
  E0428 17:50:32.274263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:33.275084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 04/28/23 17:50:33.611
  E0428 17:50:34.275285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:35.275396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 04/28/23 17:50:35.624
  E0428 17:50:36.276294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:37.277226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:38.278254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:39.278329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 04/28/23 17:50:39.643
  Apr 28 17:50:39.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-18" for this suite. @ 04/28/23 17:50:39.66
• [8.109 seconds]
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 04/28/23 17:50:39.668
  Apr 28 17:50:39.668: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename subpath @ 04/28/23 17:50:39.669
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:50:39.685
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:50:39.688
  STEP: Setting up data @ 04/28/23 17:50:39.691
  STEP: Creating pod pod-subpath-test-projected-d6j8 @ 04/28/23 17:50:39.703
  STEP: Creating a pod to test atomic-volume-subpath @ 04/28/23 17:50:39.703
  E0428 17:50:40.284464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:41.284555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:42.284682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:43.285244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:44.285320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:45.285687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:46.285773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:47.286283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:48.288721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:49.289718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:50.289814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:51.289950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:52.290010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:53.290370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:54.290483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:55.290585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:56.291264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:57.291379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:58.292431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:50:59.292724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:00.292847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:01.292987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:02.294831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:03.295212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:51:03.766
  Apr 28 17:51:03.769: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-subpath-test-projected-d6j8 container test-container-subpath-projected-d6j8: <nil>
  STEP: delete the pod @ 04/28/23 17:51:03.792
  STEP: Deleting pod pod-subpath-test-projected-d6j8 @ 04/28/23 17:51:03.812
  Apr 28 17:51:03.812: INFO: Deleting pod "pod-subpath-test-projected-d6j8" in namespace "subpath-8402"
  Apr 28 17:51:03.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8402" for this suite. @ 04/28/23 17:51:03.819
• [24.157 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 04/28/23 17:51:03.827
  Apr 28 17:51:03.827: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename svcaccounts @ 04/28/23 17:51:03.829
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:51:03.851
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:51:03.854
  Apr 28 17:51:03.876: INFO: created pod
  E0428 17:51:04.295430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:05.295755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:06.296513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:07.301737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:51:07.89
  E0428 17:51:08.301857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:09.302978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:10.303054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:11.303168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:12.303362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:13.303937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:14.304072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:15.304302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:16.304414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:17.304520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:18.305038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:19.305204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:20.305386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:21.305433      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:22.305521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:23.305819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:24.305902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:25.306902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:26.307042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:27.307248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:28.307501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:29.307594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:30.307752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:31.307963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:32.308254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:33.308894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:34.309200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:35.309232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:36.310313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:37.311024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:51:37.890: INFO: polling logs
  Apr 28 17:51:37.918: INFO: Pod logs: 
  I0428 17:51:04.778974       1 log.go:198] OK: Got token
  I0428 17:51:04.779014       1 log.go:198] validating with in-cluster discovery
  I0428 17:51:04.779413       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
  I0428 17:51:04.779448       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8155:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682704864, NotBefore:1682704264, IssuedAt:1682704264, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8155", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"30fbd47a-8e16-450a-98dc-d2d85a88fc0f"}}}
  I0428 17:51:04.788513       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I0428 17:51:04.792670       1 log.go:198] OK: Validated signature on JWT
  I0428 17:51:04.792800       1 log.go:198] OK: Got valid claims from token!
  I0428 17:51:04.792830       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8155:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682704864, NotBefore:1682704264, IssuedAt:1682704264, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8155", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"30fbd47a-8e16-450a-98dc-d2d85a88fc0f"}}}

  Apr 28 17:51:37.918: INFO: completed pod
  Apr 28 17:51:37.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8155" for this suite. @ 04/28/23 17:51:37.949
• [34.147 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 04/28/23 17:51:37.977
  Apr 28 17:51:37.977: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:51:37.978
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:51:38.019
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:51:38.023
  STEP: Creating configMap with name projected-configmap-test-volume-ded0c8a8-a122-4159-a2c3-469627628309 @ 04/28/23 17:51:38.029
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:51:38.037
  E0428 17:51:38.311143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:39.311401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:40.311849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:41.312032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:51:42.067
  Apr 28 17:51:42.071: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-projected-configmaps-103abd61-3e20-4aa2-b6cc-bef603ed82e6 container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 17:51:42.082
  Apr 28 17:51:42.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-926" for this suite. @ 04/28/23 17:51:42.101
• [4.130 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 04/28/23 17:51:42.108
  Apr 28 17:51:42.108: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename container-runtime @ 04/28/23 17:51:42.108
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:51:42.125
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:51:42.127
  STEP: create the container @ 04/28/23 17:51:42.13
  W0428 17:51:42.139857      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/28/23 17:51:42.14
  E0428 17:51:42.313029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:43.313441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:44.313512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 04/28/23 17:51:45.16
  STEP: the container should be terminated @ 04/28/23 17:51:45.163
  STEP: the termination message should be set @ 04/28/23 17:51:45.163
  Apr 28 17:51:45.163: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 04/28/23 17:51:45.163
  Apr 28 17:51:45.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-8325" for this suite. @ 04/28/23 17:51:45.18
• [3.077 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 04/28/23 17:51:45.185
  Apr 28 17:51:45.185: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename secrets @ 04/28/23 17:51:45.186
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:51:45.205
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:51:45.209
  STEP: Creating secret with name secret-test-be2e34fe-2a23-4f53-af91-7b95687a94e0 @ 04/28/23 17:51:45.211
  STEP: Creating a pod to test consume secrets @ 04/28/23 17:51:45.216
  E0428 17:51:45.314078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:46.314851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:47.315485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:48.316532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:51:49.234
  Apr 28 17:51:49.237: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-secrets-c1e45166-649a-4723-93ba-18d5df72a228 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 17:51:49.245
  Apr 28 17:51:49.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9517" for this suite. @ 04/28/23 17:51:49.259
• [4.079 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 04/28/23 17:51:49.271
  Apr 28 17:51:49.272: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename replicaset @ 04/28/23 17:51:49.272
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:51:49.291
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:51:49.294
  STEP: Create a ReplicaSet @ 04/28/23 17:51:49.296
  STEP: Verify that the required pods have come up @ 04/28/23 17:51:49.301
  Apr 28 17:51:49.304: INFO: Pod name sample-pod: Found 0 pods out of 3
  E0428 17:51:49.316862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:50.317810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:51.317917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:52.318066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:53.318153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:51:54.309: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 04/28/23 17:51:54.31
  Apr 28 17:51:54.314: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 04/28/23 17:51:54.314
  E0428 17:51:54.320645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: DeleteCollection of the ReplicaSets @ 04/28/23 17:51:54.321
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 04/28/23 17:51:54.332
  Apr 28 17:51:54.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2435" for this suite. @ 04/28/23 17:51:54.345
• [5.114 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 04/28/23 17:51:54.404
  Apr 28 17:51:54.404: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename webhook @ 04/28/23 17:51:54.405
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:51:54.434
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:51:54.439
  STEP: Setting up server cert @ 04/28/23 17:51:54.48
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 17:51:54.962
  STEP: Deploying the webhook pod @ 04/28/23 17:51:54.969
  STEP: Wait for the deployment to be ready @ 04/28/23 17:51:54.979
  Apr 28 17:51:54.984: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0428 17:51:55.321226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:56.321294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 17:51:56.995
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 17:51:57.007
  E0428 17:51:57.321881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:51:58.008: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 04/28/23 17:51:58.011
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/28/23 17:51:58.037
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 04/28/23 17:51:58.053
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/28/23 17:51:58.062
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 04/28/23 17:51:58.072
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/28/23 17:51:58.084
  Apr 28 17:51:58.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8671" for this suite. @ 04/28/23 17:51:58.153
  STEP: Destroying namespace "webhook-markers-2951" for this suite. @ 04/28/23 17:51:58.161
• [3.779 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 04/28/23 17:51:58.183
  Apr 28 17:51:58.183: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename var-expansion @ 04/28/23 17:51:58.185
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:51:58.225
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:51:58.227
  STEP: creating the pod with failed condition @ 04/28/23 17:51:58.229
  E0428 17:51:58.324411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:51:59.324297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:00.324792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:01.324938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:02.325409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:03.325816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:04.326001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:05.326175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:06.326579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:07.326861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:08.327538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:09.327775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:10.327846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:11.328060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:12.328886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:13.329238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:14.330287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:15.330518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:16.331295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:17.331519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:18.332238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:19.332367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:20.333353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:21.334266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:22.334861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:23.335104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:24.336178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:25.336933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:26.337206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:27.337322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:28.337960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:29.338290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:30.338704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:31.339439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:32.340243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:33.340785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:34.341233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:35.341325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:36.341507      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:37.341701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:38.342327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:39.342459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:40.343336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:41.343636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:42.343942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:43.344165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:44.344695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:45.344905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:46.345253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:47.345421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:48.345652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:49.345778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:50.345867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:51.346289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:52.346527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:53.347405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:54.347613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:55.347740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:56.347835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:57.348041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:58.348372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:52:59.349305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:00.349887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:01.350028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:02.350242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:03.350408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:04.350973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:05.351088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:06.351206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:07.351299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:08.351553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:09.351839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:10.352116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:11.352238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:12.352338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:13.352485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:14.352665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:15.352899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:16.353007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:17.353239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:18.353481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:19.353579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:20.354263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:21.354415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:22.354658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:23.354757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:24.354958      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:25.355291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:26.355487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:27.355694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:28.355886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:29.356023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:30.356275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:31.356385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:32.356612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:33.356746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:34.356885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:35.357284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:36.357686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:37.357738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:38.358123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:39.358246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:40.358345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:41.358565      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:42.359267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:43.359965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:44.360399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:45.360514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:46.361353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:47.361454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:48.362322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:49.362455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:50.362727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:51.362748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:52.362863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:53.363888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:54.364326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:55.364426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:56.364858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:53:57.364962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pod @ 04/28/23 17:53:58.242
  E0428 17:53:58.365601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:53:58.754: INFO: Successfully updated pod "var-expansion-8c69b1c4-811c-4a69-a99c-fa6a575c0642"
  STEP: waiting for pod running @ 04/28/23 17:53:58.757
  E0428 17:53:59.365733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:00.365847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 04/28/23 17:54:00.765
  Apr 28 17:54:00.765: INFO: Deleting pod "var-expansion-8c69b1c4-811c-4a69-a99c-fa6a575c0642" in namespace "var-expansion-3546"
  Apr 28 17:54:00.774: INFO: Wait up to 5m0s for pod "var-expansion-8c69b1c4-811c-4a69-a99c-fa6a575c0642" to be fully deleted
  E0428 17:54:01.365978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:02.366159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:03.366347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:04.366988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:05.367457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:06.367483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:07.368421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:08.368619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:09.369460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:10.369657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:11.370523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:12.371900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:13.372024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:14.372129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:15.372246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:16.372546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:17.373360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:18.374411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:19.374636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:20.374970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:21.375333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:22.375403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:23.376386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:24.376680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:25.377266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:26.377372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:27.378314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:28.378465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:29.378642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:30.378914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:31.378956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:32.380016      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:54:32.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3546" for this suite. @ 04/28/23 17:54:32.858
• [154.702 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 04/28/23 17:54:32.886
  Apr 28 17:54:32.886: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename dns @ 04/28/23 17:54:32.887
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:54:32.906
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:54:32.909
  STEP: Creating a test headless service @ 04/28/23 17:54:32.911
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8635 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8635;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8635 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8635;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8635.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8635.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8635.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8635.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8635.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8635.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8635.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8635.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8635.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8635.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8635.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8635.svc;check="$$(dig +notcp +noall +answer +search 141.54.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.54.141_udp@PTR;check="$$(dig +tcp +noall +answer +search 141.54.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.54.141_tcp@PTR;sleep 1; done
   @ 04/28/23 17:54:32.932
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8635 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8635;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8635 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8635;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8635.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8635.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8635.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8635.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8635.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8635.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8635.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8635.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8635.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8635.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8635.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8635.svc;check="$$(dig +notcp +noall +answer +search 141.54.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.54.141_udp@PTR;check="$$(dig +tcp +noall +answer +search 141.54.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.54.141_tcp@PTR;sleep 1; done
   @ 04/28/23 17:54:32.932
  STEP: creating a pod to probe DNS @ 04/28/23 17:54:32.932
  STEP: submitting the pod to kubernetes @ 04/28/23 17:54:32.932
  E0428 17:54:33.380886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:34.381327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/28/23 17:54:34.963
  STEP: looking for the results for each expected name from probers @ 04/28/23 17:54:34.965
  Apr 28 17:54:34.976: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:34.983: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:34.995: INFO: Unable to read wheezy_udp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:35.001: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:35.007: INFO: Unable to read wheezy_udp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:35.014: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:35.028: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:35.036: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:35.055: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:35.058: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:35.061: INFO: Unable to read jessie_udp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:35.064: INFO: Unable to read jessie_tcp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:35.067: INFO: Unable to read jessie_udp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:35.070: INFO: Unable to read jessie_tcp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:35.073: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:35.076: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:35.088: INFO: Lookups using dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8635 wheezy_tcp@dns-test-service.dns-8635 wheezy_udp@dns-test-service.dns-8635.svc wheezy_tcp@dns-test-service.dns-8635.svc wheezy_udp@_http._tcp.dns-test-service.dns-8635.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8635.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8635 jessie_tcp@dns-test-service.dns-8635 jessie_udp@dns-test-service.dns-8635.svc jessie_tcp@dns-test-service.dns-8635.svc jessie_udp@_http._tcp.dns-test-service.dns-8635.svc jessie_tcp@_http._tcp.dns-test-service.dns-8635.svc]

  E0428 17:54:35.382008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:36.382113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:37.382348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:38.382425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:39.382567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:54:40.092: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:40.096: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:40.099: INFO: Unable to read wheezy_udp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:40.102: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:40.105: INFO: Unable to read wheezy_udp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:40.113: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:40.116: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:40.119: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:40.132: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:40.135: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:40.138: INFO: Unable to read jessie_udp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:40.140: INFO: Unable to read jessie_tcp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:40.143: INFO: Unable to read jessie_udp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:40.146: INFO: Unable to read jessie_tcp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:40.148: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:40.151: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:40.161: INFO: Lookups using dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8635 wheezy_tcp@dns-test-service.dns-8635 wheezy_udp@dns-test-service.dns-8635.svc wheezy_tcp@dns-test-service.dns-8635.svc wheezy_udp@_http._tcp.dns-test-service.dns-8635.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8635.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8635 jessie_tcp@dns-test-service.dns-8635 jessie_udp@dns-test-service.dns-8635.svc jessie_tcp@dns-test-service.dns-8635.svc jessie_udp@_http._tcp.dns-test-service.dns-8635.svc jessie_tcp@_http._tcp.dns-test-service.dns-8635.svc]

  E0428 17:54:40.383279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:41.384268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:42.384517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:43.384629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:44.384732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:54:45.126: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:45.132: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:45.143: INFO: Unable to read wheezy_udp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:45.147: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:45.152: INFO: Unable to read wheezy_udp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:45.156: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:45.159: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:45.162: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:45.178: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:45.181: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:45.184: INFO: Unable to read jessie_udp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:45.187: INFO: Unable to read jessie_tcp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:45.190: INFO: Unable to read jessie_udp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:45.192: INFO: Unable to read jessie_tcp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:45.195: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:45.197: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:45.207: INFO: Lookups using dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8635 wheezy_tcp@dns-test-service.dns-8635 wheezy_udp@dns-test-service.dns-8635.svc wheezy_tcp@dns-test-service.dns-8635.svc wheezy_udp@_http._tcp.dns-test-service.dns-8635.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8635.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8635 jessie_tcp@dns-test-service.dns-8635 jessie_udp@dns-test-service.dns-8635.svc jessie_tcp@dns-test-service.dns-8635.svc jessie_udp@_http._tcp.dns-test-service.dns-8635.svc jessie_tcp@_http._tcp.dns-test-service.dns-8635.svc]

  E0428 17:54:45.385160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:46.387478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:47.387578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:48.387831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:49.388202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:54:50.093: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:50.104: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:50.109: INFO: Unable to read wheezy_udp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:50.114: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:50.120: INFO: Unable to read wheezy_udp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:50.123: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:50.126: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:50.129: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:50.143: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:50.146: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:50.149: INFO: Unable to read jessie_udp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:50.151: INFO: Unable to read jessie_tcp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:50.158: INFO: Unable to read jessie_udp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:50.160: INFO: Unable to read jessie_tcp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:50.163: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:50.166: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:50.176: INFO: Lookups using dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8635 wheezy_tcp@dns-test-service.dns-8635 wheezy_udp@dns-test-service.dns-8635.svc wheezy_tcp@dns-test-service.dns-8635.svc wheezy_udp@_http._tcp.dns-test-service.dns-8635.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8635.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8635 jessie_tcp@dns-test-service.dns-8635 jessie_udp@dns-test-service.dns-8635.svc jessie_tcp@dns-test-service.dns-8635.svc jessie_udp@_http._tcp.dns-test-service.dns-8635.svc jessie_tcp@_http._tcp.dns-test-service.dns-8635.svc]

  E0428 17:54:50.388897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:51.389129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:52.389284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:53.390145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:54.390208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:54:55.093: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:55.106: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:55.109: INFO: Unable to read wheezy_udp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:55.112: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:55.115: INFO: Unable to read wheezy_udp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:55.117: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:55.122: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:55.134: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:55.149: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:55.153: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:55.156: INFO: Unable to read jessie_udp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:55.163: INFO: Unable to read jessie_tcp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:55.166: INFO: Unable to read jessie_udp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:55.173: INFO: Unable to read jessie_tcp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:55.176: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:55.179: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:54:55.190: INFO: Lookups using dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8635 wheezy_tcp@dns-test-service.dns-8635 wheezy_udp@dns-test-service.dns-8635.svc wheezy_tcp@dns-test-service.dns-8635.svc wheezy_udp@_http._tcp.dns-test-service.dns-8635.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8635.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8635 jessie_tcp@dns-test-service.dns-8635 jessie_udp@dns-test-service.dns-8635.svc jessie_tcp@dns-test-service.dns-8635.svc jessie_udp@_http._tcp.dns-test-service.dns-8635.svc jessie_tcp@_http._tcp.dns-test-service.dns-8635.svc]

  E0428 17:54:55.390991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:56.391101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:57.391178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:58.392056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:54:59.392231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:55:00.106: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:00.115: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:00.124: INFO: Unable to read wheezy_udp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:00.151: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:00.165: INFO: Unable to read wheezy_udp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:00.180: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:00.187: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:00.217: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:00.265: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:00.273: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:00.309: INFO: Unable to read jessie_udp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:00.334: INFO: Unable to read jessie_tcp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:00.346: INFO: Unable to read jessie_udp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:00.356: INFO: Unable to read jessie_tcp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:00.364: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:00.391: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  E0428 17:55:00.392982      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:55:00.449: INFO: Lookups using dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8635 wheezy_tcp@dns-test-service.dns-8635 wheezy_udp@dns-test-service.dns-8635.svc wheezy_tcp@dns-test-service.dns-8635.svc wheezy_udp@_http._tcp.dns-test-service.dns-8635.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8635.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8635 jessie_tcp@dns-test-service.dns-8635 jessie_udp@dns-test-service.dns-8635.svc jessie_tcp@dns-test-service.dns-8635.svc jessie_udp@_http._tcp.dns-test-service.dns-8635.svc jessie_tcp@_http._tcp.dns-test-service.dns-8635.svc]

  E0428 17:55:01.393237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:02.393346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:03.393437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:04.393545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:55:05.093: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:05.097: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:05.107: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:05.111: INFO: Unable to read wheezy_udp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:05.114: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:05.117: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:05.125: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:05.143: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:05.147: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:05.150: INFO: Unable to read jessie_udp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:05.154: INFO: Unable to read jessie_tcp@dns-test-service.dns-8635 from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:05.158: INFO: Unable to read jessie_udp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:05.162: INFO: Unable to read jessie_tcp@dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:05.165: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:05.169: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8635.svc from pod dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6: the server could not find the requested resource (get pods dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6)
  Apr 28 17:55:05.185: INFO: Lookups using dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.dns-8635 wheezy_udp@dns-test-service.dns-8635.svc wheezy_tcp@dns-test-service.dns-8635.svc wheezy_udp@_http._tcp.dns-test-service.dns-8635.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8635.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8635 jessie_tcp@dns-test-service.dns-8635 jessie_udp@dns-test-service.dns-8635.svc jessie_tcp@dns-test-service.dns-8635.svc jessie_udp@_http._tcp.dns-test-service.dns-8635.svc jessie_tcp@_http._tcp.dns-test-service.dns-8635.svc]

  E0428 17:55:05.394283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:06.394412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:07.394548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:08.395064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:09.395179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:55:10.219: INFO: DNS probes using dns-8635/dns-test-d839f94d-7450-4c3b-878c-971baee6d3e6 succeeded

  Apr 28 17:55:10.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 17:55:10.225
  STEP: deleting the test service @ 04/28/23 17:55:10.246
  STEP: deleting the test headless service @ 04/28/23 17:55:10.315
  STEP: Destroying namespace "dns-8635" for this suite. @ 04/28/23 17:55:10.336
• [37.459 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 04/28/23 17:55:10.351
  Apr 28 17:55:10.351: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 17:55:10.352
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:55:10.38
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:55:10.384
  STEP: creating Agnhost RC @ 04/28/23 17:55:10.393
  Apr 28 17:55:10.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-4539 create -f -'
  E0428 17:55:10.395963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:55:10.786: INFO: stderr: ""
  Apr 28 17:55:10.786: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/28/23 17:55:10.786
  E0428 17:55:11.396106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:55:11.791: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 28 17:55:11.791: INFO: Found 0 / 1
  E0428 17:55:12.397009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:55:12.792: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 28 17:55:12.792: INFO: Found 1 / 1
  Apr 28 17:55:12.792: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 04/28/23 17:55:12.792
  Apr 28 17:55:12.796: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 28 17:55:12.796: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 28 17:55:12.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-4539 patch pod agnhost-primary-qc87f -p {"metadata":{"annotations":{"x":"y"}}}'
  Apr 28 17:55:12.882: INFO: stderr: ""
  Apr 28 17:55:12.887: INFO: stdout: "pod/agnhost-primary-qc87f patched\n"
  STEP: checking annotations @ 04/28/23 17:55:12.887
  Apr 28 17:55:12.892: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 28 17:55:12.892: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 28 17:55:12.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4539" for this suite. @ 04/28/23 17:55:12.899
• [2.555 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 04/28/23 17:55:12.908
  Apr 28 17:55:12.909: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename webhook @ 04/28/23 17:55:12.911
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:55:12.926
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:55:12.929
  STEP: Setting up server cert @ 04/28/23 17:55:12.953
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 17:55:13.342
  STEP: Deploying the webhook pod @ 04/28/23 17:55:13.349
  STEP: Wait for the deployment to be ready @ 04/28/23 17:55:13.361
  Apr 28 17:55:13.367: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0428 17:55:13.398657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:14.398813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 17:55:15.376
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 17:55:15.388
  E0428 17:55:15.399940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:55:16.388: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 04/28/23 17:55:16.392
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 04/28/23 17:55:16.394
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 04/28/23 17:55:16.394
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 04/28/23 17:55:16.394
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 04/28/23 17:55:16.395
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 04/28/23 17:55:16.395
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 04/28/23 17:55:16.396
  Apr 28 17:55:16.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0428 17:55:16.400269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-8807" for this suite. @ 04/28/23 17:55:16.461
  STEP: Destroying namespace "webhook-markers-2740" for this suite. @ 04/28/23 17:55:16.483
• [3.589 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 04/28/23 17:55:16.504
  Apr 28 17:55:16.506: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename security-context-test @ 04/28/23 17:55:16.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:55:16.567
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:55:16.572
  E0428 17:55:17.400214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:18.400395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:19.400500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:20.400595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:55:20.610: INFO: Got logs for pod "busybox-privileged-false-e70ed058-49c3-40f5-9cc4-bcef4642e7f5": "ip: RTNETLINK answers: Operation not permitted\n"
  Apr 28 17:55:20.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-7089" for this suite. @ 04/28/23 17:55:20.615
• [4.117 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 04/28/23 17:55:20.622
  Apr 28 17:55:20.622: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename webhook @ 04/28/23 17:55:20.623
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:55:20.641
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:55:20.643
  STEP: Setting up server cert @ 04/28/23 17:55:20.67
  E0428 17:55:21.401205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 17:55:21.41
  STEP: Deploying the webhook pod @ 04/28/23 17:55:21.418
  STEP: Wait for the deployment to be ready @ 04/28/23 17:55:21.433
  Apr 28 17:55:21.443: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0428 17:55:22.401298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:23.402281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 17:55:23.451
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 17:55:23.46
  E0428 17:55:24.402421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:55:24.461: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 28 17:55:24.465: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8473-crds.webhook.example.com via the AdmissionRegistration API @ 04/28/23 17:55:24.973
  STEP: Creating a custom resource that should be mutated by the webhook @ 04/28/23 17:55:24.992
  E0428 17:55:25.402577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:26.402898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:55:27.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0428 17:55:27.403143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-4028" for this suite. @ 04/28/23 17:55:27.662
  STEP: Destroying namespace "webhook-markers-2700" for this suite. @ 04/28/23 17:55:27.675
• [7.060 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 04/28/23 17:55:27.682
  Apr 28 17:55:27.682: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:55:27.683
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:55:27.707
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:55:27.71
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 17:55:27.712
  E0428 17:55:28.403242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:29.403514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:30.403626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:31.403969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:55:31.729
  Apr 28 17:55:31.732: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod downwardapi-volume-bb73f42b-ddaa-4f18-8cd5-4a5fe333d06b container client-container: <nil>
  STEP: delete the pod @ 04/28/23 17:55:31.738
  Apr 28 17:55:31.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4926" for this suite. @ 04/28/23 17:55:31.757
• [4.083 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 04/28/23 17:55:31.765
  Apr 28 17:55:31.765: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename job @ 04/28/23 17:55:31.766
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:55:31.79
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:55:31.793
  STEP: Creating Indexed job @ 04/28/23 17:55:31.795
  STEP: Ensuring job reaches completions @ 04/28/23 17:55:31.801
  E0428 17:55:32.404578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:33.404692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:34.404791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:35.405642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:36.405759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:37.406553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:38.407108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:39.407220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:40.407402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:41.407584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring pods with index for job exist @ 04/28/23 17:55:41.805
  Apr 28 17:55:41.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-4313" for this suite. @ 04/28/23 17:55:41.824
• [10.072 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 04/28/23 17:55:41.839
  Apr 28 17:55:41.839: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename namespaces @ 04/28/23 17:55:41.84
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:55:41.876
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:55:41.882
  STEP: Updating Namespace "namespaces-9280" @ 04/28/23 17:55:41.886
  Apr 28 17:55:41.907: INFO: Namespace "namespaces-9280" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"5e0cf72d-a540-4b84-a10a-53bd9bf0993c", "kubernetes.io/metadata.name":"namespaces-9280", "namespaces-9280":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Apr 28 17:55:41.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9280" for this suite. @ 04/28/23 17:55:41.917
• [0.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 04/28/23 17:55:41.933
  Apr 28 17:55:41.933: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename webhook @ 04/28/23 17:55:41.934
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:55:41.952
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:55:41.961
  STEP: Setting up server cert @ 04/28/23 17:55:42.016
  E0428 17:55:42.408362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 17:55:42.946
  STEP: Deploying the webhook pod @ 04/28/23 17:55:42.95
  STEP: Wait for the deployment to be ready @ 04/28/23 17:55:42.962
  Apr 28 17:55:42.966: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0428 17:55:43.408454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:44.409257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 17:55:44.986
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 17:55:44.997
  E0428 17:55:45.410279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:55:45.998: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 28 17:55:46.001: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  E0428 17:55:46.410388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5669-crds.webhook.example.com via the AdmissionRegistration API @ 04/28/23 17:55:46.513
  STEP: Creating a custom resource that should be mutated by the webhook @ 04/28/23 17:55:46.533
  E0428 17:55:47.410557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:48.410665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:55:48.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2241" for this suite. @ 04/28/23 17:55:49.16
  STEP: Destroying namespace "webhook-markers-8725" for this suite. @ 04/28/23 17:55:49.166
• [7.242 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 04/28/23 17:55:49.177
  Apr 28 17:55:49.177: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename replicaset @ 04/28/23 17:55:49.178
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:55:49.191
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:55:49.194
  STEP: Create a Replicaset @ 04/28/23 17:55:49.199
  STEP: Verify that the required pods have come up. @ 04/28/23 17:55:49.203
  Apr 28 17:55:49.206: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0428 17:55:49.411395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:50.411534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:51.411788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:52.411886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:53.412341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:55:54.210: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/28/23 17:55:54.21
  STEP: Getting /status @ 04/28/23 17:55:54.21
  Apr 28 17:55:54.213: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 04/28/23 17:55:54.213
  Apr 28 17:55:54.221: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 04/28/23 17:55:54.221
  Apr 28 17:55:54.223: INFO: Observed &ReplicaSet event: ADDED
  Apr 28 17:55:54.224: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 28 17:55:54.224: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 28 17:55:54.224: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 28 17:55:54.224: INFO: Found replicaset test-rs in namespace replicaset-5186 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 28 17:55:54.224: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 04/28/23 17:55:54.224
  Apr 28 17:55:54.224: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 28 17:55:54.230: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 04/28/23 17:55:54.23
  Apr 28 17:55:54.232: INFO: Observed &ReplicaSet event: ADDED
  Apr 28 17:55:54.232: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 28 17:55:54.232: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 28 17:55:54.233: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 28 17:55:54.233: INFO: Observed replicaset test-rs in namespace replicaset-5186 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 28 17:55:54.233: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 28 17:55:54.233: INFO: Found replicaset test-rs in namespace replicaset-5186 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Apr 28 17:55:54.233: INFO: Replicaset test-rs has a patched status
  Apr 28 17:55:54.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-5186" for this suite. @ 04/28/23 17:55:54.237
• [5.067 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 04/28/23 17:55:54.246
  Apr 28 17:55:54.246: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename daemonsets @ 04/28/23 17:55:54.247
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:55:54.27
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:55:54.279
  Apr 28 17:55:54.319: INFO: Create a RollingUpdate DaemonSet
  Apr 28 17:55:54.330: INFO: Check that daemon pods launch on every node of the cluster
  Apr 28 17:55:54.338: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 17:55:54.338: INFO: Node ip-172-31-11-161.us-east-2.compute.internal is running 0 daemon pod, expected 1
  E0428 17:55:54.412825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:55:55.347: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 28 17:55:55.347: INFO: Node ip-172-31-11-161.us-east-2.compute.internal is running 0 daemon pod, expected 1
  E0428 17:55:55.412749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:55:56.345: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
  Apr 28 17:55:56.345: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  Apr 28 17:55:56.345: INFO: Update the DaemonSet to trigger a rollout
  Apr 28 17:55:56.358: INFO: Updating DaemonSet daemon-set
  E0428 17:55:56.413203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:57.414268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:55:58.415109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:55:59.377: INFO: Roll back the DaemonSet before rollout is complete
  Apr 28 17:55:59.385: INFO: Updating DaemonSet daemon-set
  Apr 28 17:55:59.386: INFO: Make sure DaemonSet rollback is complete
  Apr 28 17:55:59.389: INFO: Wrong image for pod: daemon-set-dj92z. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Apr 28 17:55:59.390: INFO: Pod daemon-set-dj92z is not available
  E0428 17:55:59.415500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:00.416180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:01.416749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:02.399: INFO: Pod daemon-set-bf6fd is not available
  E0428 17:56:02.416678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting DaemonSet "daemon-set" @ 04/28/23 17:56:02.419
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7204, will wait for the garbage collector to delete the pods @ 04/28/23 17:56:02.419
  Apr 28 17:56:02.483: INFO: Deleting DaemonSet.extensions daemon-set took: 6.493736ms
  Apr 28 17:56:02.583: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.240563ms
  E0428 17:56:03.417121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:04.417583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:05.188: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 17:56:05.188: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 28 17:56:05.190: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"292423"},"items":null}

  Apr 28 17:56:05.193: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"292423"},"items":null}

  Apr 28 17:56:05.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7204" for this suite. @ 04/28/23 17:56:05.219
• [10.979 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 04/28/23 17:56:05.227
  Apr 28 17:56:05.227: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename pods @ 04/28/23 17:56:05.228
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:56:05.243
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:56:05.246
  STEP: Create set of pods @ 04/28/23 17:56:05.248
  Apr 28 17:56:05.256: INFO: created test-pod-1
  Apr 28 17:56:05.262: INFO: created test-pod-2
  Apr 28 17:56:05.272: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 04/28/23 17:56:05.272
  E0428 17:56:05.418323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:06.420584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 04/28/23 17:56:07.333
  Apr 28 17:56:07.338: INFO: Pod quantity 3 is different from expected quantity 0
  E0428 17:56:07.421534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:08.342: INFO: Pod quantity 3 is different from expected quantity 0
  E0428 17:56:08.422310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:09.343: INFO: Pod quantity 3 is different from expected quantity 0
  E0428 17:56:09.423157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:10.343: INFO: Pod quantity 3 is different from expected quantity 0
  E0428 17:56:10.424129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:11.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9217" for this suite. @ 04/28/23 17:56:11.348
• [6.129 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 04/28/23 17:56:11.357
  Apr 28 17:56:11.357: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename daemonsets @ 04/28/23 17:56:11.358
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:56:11.387
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:56:11.397
  E0428 17:56:11.424943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating simple DaemonSet "daemon-set" @ 04/28/23 17:56:11.46
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/28/23 17:56:11.467
  Apr 28 17:56:11.479: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 17:56:11.479: INFO: Node ip-172-31-11-161.us-east-2.compute.internal is running 0 daemon pod, expected 1
  E0428 17:56:12.425213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:12.486: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 28 17:56:12.486: INFO: Node ip-172-31-14-195.us-east-2.compute.internal is running 0 daemon pod, expected 1
  E0428 17:56:13.426158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:13.486: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 28 17:56:13.486: INFO: Node ip-172-31-6-71.us-east-2.compute.internal is running 0 daemon pod, expected 1
  E0428 17:56:14.426517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:14.487: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
  Apr 28 17:56:14.487: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
  STEP: Getting /status @ 04/28/23 17:56:14.489
  Apr 28 17:56:14.492: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 04/28/23 17:56:14.492
  Apr 28 17:56:14.500: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 04/28/23 17:56:14.5
  Apr 28 17:56:14.502: INFO: Observed &DaemonSet event: ADDED
  Apr 28 17:56:14.502: INFO: Observed &DaemonSet event: MODIFIED
  Apr 28 17:56:14.502: INFO: Observed &DaemonSet event: MODIFIED
  Apr 28 17:56:14.502: INFO: Observed &DaemonSet event: MODIFIED
  Apr 28 17:56:14.503: INFO: Observed &DaemonSet event: MODIFIED
  Apr 28 17:56:14.503: INFO: Observed &DaemonSet event: MODIFIED
  Apr 28 17:56:14.503: INFO: Found daemon set daemon-set in namespace daemonsets-2803 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 28 17:56:14.503: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 04/28/23 17:56:14.503
  STEP: watching for the daemon set status to be patched @ 04/28/23 17:56:14.509
  Apr 28 17:56:14.511: INFO: Observed &DaemonSet event: ADDED
  Apr 28 17:56:14.511: INFO: Observed &DaemonSet event: MODIFIED
  Apr 28 17:56:14.511: INFO: Observed &DaemonSet event: MODIFIED
  Apr 28 17:56:14.511: INFO: Observed &DaemonSet event: MODIFIED
  Apr 28 17:56:14.511: INFO: Observed &DaemonSet event: MODIFIED
  Apr 28 17:56:14.511: INFO: Observed &DaemonSet event: MODIFIED
  Apr 28 17:56:14.511: INFO: Observed daemon set daemon-set in namespace daemonsets-2803 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 28 17:56:14.512: INFO: Observed &DaemonSet event: MODIFIED
  Apr 28 17:56:14.512: INFO: Found daemon set daemon-set in namespace daemonsets-2803 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Apr 28 17:56:14.512: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 04/28/23 17:56:14.515
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2803, will wait for the garbage collector to delete the pods @ 04/28/23 17:56:14.515
  Apr 28 17:56:14.574: INFO: Deleting DaemonSet.extensions daemon-set took: 5.630613ms
  Apr 28 17:56:14.675: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.71597ms
  E0428 17:56:15.426778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:16.427214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:17.378: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 28 17:56:17.378: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 28 17:56:17.380: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"292677"},"items":null}

  Apr 28 17:56:17.383: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"292677"},"items":null}

  Apr 28 17:56:17.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-2803" for this suite. @ 04/28/23 17:56:17.398
• [6.046 seconds]
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 04/28/23 17:56:17.404
  Apr 28 17:56:17.404: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename secrets @ 04/28/23 17:56:17.405
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:56:17.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:56:17.421
  STEP: Creating projection with secret that has name secret-emptykey-test-5318a978-32c9-41b3-ba59-01f330d68451 @ 04/28/23 17:56:17.423
  Apr 28 17:56:17.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0428 17:56:17.427752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "secrets-5890" for this suite. @ 04/28/23 17:56:17.429
• [0.031 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 04/28/23 17:56:17.437
  Apr 28 17:56:17.437: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:56:17.438
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:56:17.46
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:56:17.465
  STEP: Creating projection with secret that has name projected-secret-test-a6e2df37-762d-4a99-bbd1-62cde7b5263e @ 04/28/23 17:56:17.467
  STEP: Creating a pod to test consume secrets @ 04/28/23 17:56:17.471
  E0428 17:56:18.429226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:19.429342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:20.429487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:21.429585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:56:21.493
  Apr 28 17:56:21.496: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-projected-secrets-598eff41-6e39-4ac3-beea-4d0230e24f32 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 17:56:21.51
  Apr 28 17:56:21.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9382" for this suite. @ 04/28/23 17:56:21.536
• [4.109 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 04/28/23 17:56:21.549
  Apr 28 17:56:21.549: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename field-validation @ 04/28/23 17:56:21.55
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:56:21.57
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:56:21.573
  Apr 28 17:56:21.575: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  E0428 17:56:22.429855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:23.433291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0428 17:56:24.113931      19 warnings.go:70] unknown field "alpha"
  W0428 17:56:24.113980      19 warnings.go:70] unknown field "beta"
  W0428 17:56:24.113987      19 warnings.go:70] unknown field "delta"
  W0428 17:56:24.113996      19 warnings.go:70] unknown field "epsilon"
  W0428 17:56:24.114003      19 warnings.go:70] unknown field "gamma"
  Apr 28 17:56:24.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5407" for this suite. @ 04/28/23 17:56:24.138
• [2.595 seconds]
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 04/28/23 17:56:24.144
  Apr 28 17:56:24.144: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename endpointslice @ 04/28/23 17:56:24.145
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:56:24.159
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:56:24.162
  E0428 17:56:24.433820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:25.433951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:26.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-3777" for this suite. @ 04/28/23 17:56:26.221
• [2.082 seconds]
------------------------------
SSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 04/28/23 17:56:26.228
  Apr 28 17:56:26.228: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename dns @ 04/28/23 17:56:26.229
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:56:26.251
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:56:26.254
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 04/28/23 17:56:26.257
  Apr 28 17:56:26.265: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1189  ce7b96b1-caba-4436-ae82-fa5659877ef1 292800 0 2023-04-28 17:56:26 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-04-28 17:56:26 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9ffl2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9ffl2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0428 17:56:26.434186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:27.434280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS suffix list is configured on pod... @ 04/28/23 17:56:28.274
  Apr 28 17:56:28.274: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1189 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:56:28.274: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 17:56:28.275: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:56:28.275: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/dns-1189/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 04/28/23 17:56:28.367
  Apr 28 17:56:28.367: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1189 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 17:56:28.367: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 17:56:28.368: INFO: ExecWithOptions: Clientset creation
  Apr 28 17:56:28.368: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/dns-1189/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0428 17:56:28.435008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:56:28.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 17:56:28.467: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-1189" for this suite. @ 04/28/23 17:56:28.479
• [2.256 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 04/28/23 17:56:28.499
  Apr 28 17:56:28.499: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename taint-single-pod @ 04/28/23 17:56:28.501
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:56:28.52
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:56:28.522
  Apr 28 17:56:28.524: INFO: Waiting up to 1m0s for all nodes to be ready
  E0428 17:56:29.435797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:30.435950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:31.436544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:32.436836      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:33.437216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:34.437301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:35.437438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:36.438130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:37.438330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:38.438421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:39.438511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:40.438624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:41.439204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:42.439292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:43.440359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:44.440369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:45.440644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:46.440906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:47.440821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:48.441181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:49.442281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:50.442465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:51.442569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:52.442769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:53.442869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:54.443079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:55.443357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:56.443461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:57.443570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:58.444189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:56:59.444491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:00.444673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:01.445020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:02.445117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:03.445202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:04.445457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:05.445556      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:06.445870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:07.446008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:08.446320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:09.446387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:10.446580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:11.446752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:12.446832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:13.447581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:14.447780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:15.448408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:16.448735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:17.448780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:18.448877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:19.449019      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:20.449232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:21.449412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:22.450082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:23.450180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:24.450840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:25.450967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:26.451058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:27.451186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:28.451273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:57:28.565: INFO: Waiting for terminating namespaces to be deleted...
  Apr 28 17:57:28.569: INFO: Starting informer...
  STEP: Starting pod... @ 04/28/23 17:57:28.569
  Apr 28 17:57:28.782: INFO: Pod is running on ip-172-31-9-127.us-east-2.compute.internal. Tainting Node
  STEP: Trying to apply a taint on the Node @ 04/28/23 17:57:28.782
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/28/23 17:57:28.795
  STEP: Waiting short time to make sure Pod is queued for deletion @ 04/28/23 17:57:28.805
  Apr 28 17:57:28.805: INFO: Pod wasn't evicted. Proceeding
  Apr 28 17:57:28.805: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/28/23 17:57:28.863
  STEP: Waiting some time to make sure that toleration time passed. @ 04/28/23 17:57:28.869
  E0428 17:57:29.452016      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:30.452516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:31.452754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:32.452854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:33.453260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:34.453371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:35.453462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:36.454288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:37.454511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:38.455034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:39.455329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:40.455554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:41.455956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:42.456915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:43.457185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:44.457320      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:45.457524      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:46.458327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:47.458435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:48.459471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:49.459562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:50.460572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:51.460765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:52.460974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:53.461206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:54.461334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:55.461430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:56.462281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:57.462447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:58.462692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:57:59.462924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:00.463776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:01.463966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:02.464192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:03.464298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:04.464488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:05.464726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:06.464903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:07.465110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:08.465198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:09.465378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:10.465570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:11.465874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:12.466025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:13.466467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:14.466904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:15.467128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:16.467248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:17.467470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:18.468544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:19.468661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:20.468771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:21.469549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:22.470930      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:23.472113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:24.472317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:25.472666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:26.473643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:27.473924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:28.474383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:29.474554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:30.475043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:31.475251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:32.475495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:33.475628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:34.475851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:35.475952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:36.476222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:37.476458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:38.476837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:39.477042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:40.477351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:41.477574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:42.477776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:43.478286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:58:43.870: INFO: Pod wasn't evicted. Test successful
  Apr 28 17:58:43.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-6569" for this suite. @ 04/28/23 17:58:43.875
• [135.382 seconds]
------------------------------
SS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 04/28/23 17:58:43.881
  Apr 28 17:58:43.881: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename endpointslice @ 04/28/23 17:58:43.882
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:58:43.899
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:58:43.902
  STEP: getting /apis @ 04/28/23 17:58:43.904
  STEP: getting /apis/discovery.k8s.io @ 04/28/23 17:58:43.908
  STEP: getting /apis/discovery.k8s.iov1 @ 04/28/23 17:58:43.909
  STEP: creating @ 04/28/23 17:58:43.91
  STEP: getting @ 04/28/23 17:58:43.931
  STEP: listing @ 04/28/23 17:58:43.934
  STEP: watching @ 04/28/23 17:58:43.937
  Apr 28 17:58:43.937: INFO: starting watch
  STEP: cluster-wide listing @ 04/28/23 17:58:43.938
  STEP: cluster-wide watching @ 04/28/23 17:58:43.94
  Apr 28 17:58:43.940: INFO: starting watch
  STEP: patching @ 04/28/23 17:58:43.941
  STEP: updating @ 04/28/23 17:58:43.947
  Apr 28 17:58:43.957: INFO: waiting for watch events with expected annotations
  Apr 28 17:58:43.957: INFO: saw patched and updated annotations
  STEP: deleting @ 04/28/23 17:58:43.957
  STEP: deleting a collection @ 04/28/23 17:58:43.969
  Apr 28 17:58:43.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-3150" for this suite. @ 04/28/23 17:58:43.985
• [0.110 seconds]
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 04/28/23 17:58:43.991
  Apr 28 17:58:43.991: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename security-context-test @ 04/28/23 17:58:43.993
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:58:44.011
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:58:44.014
  E0428 17:58:44.478868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:45.479086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:46.479739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:47.479841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:58:48.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-110" for this suite. @ 04/28/23 17:58:48.041
• [4.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 04/28/23 17:58:48.049
  Apr 28 17:58:48.049: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:58:48.05
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:58:48.065
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:58:48.067
  STEP: Creating configMap with name projected-configmap-test-volume-map-503ab414-8e1b-40f9-a097-ba865727c5a1 @ 04/28/23 17:58:48.07
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:58:48.076
  E0428 17:58:48.479934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:49.480059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:50.480619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:51.480713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:58:52.093
  Apr 28 17:58:52.103: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-projected-configmaps-a446b098-667f-4e87-93e7-bf0acd4f0f7e container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 17:58:52.124
  Apr 28 17:58:52.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3844" for this suite. @ 04/28/23 17:58:52.146
• [4.102 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 04/28/23 17:58:52.156
  Apr 28 17:58:52.156: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename svcaccounts @ 04/28/23 17:58:52.156
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:58:52.172
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:58:52.174
  Apr 28 17:58:52.184: INFO: Got root ca configmap in namespace "svcaccounts-4365"
  Apr 28 17:58:52.189: INFO: Deleted root ca configmap in namespace "svcaccounts-4365"
  E0428 17:58:52.480831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for a new root ca configmap created @ 04/28/23 17:58:52.689
  Apr 28 17:58:52.692: INFO: Recreated root ca configmap in namespace "svcaccounts-4365"
  Apr 28 17:58:52.696: INFO: Updated root ca configmap in namespace "svcaccounts-4365"
  STEP: waiting for the root ca configmap reconciled @ 04/28/23 17:58:53.197
  Apr 28 17:58:53.200: INFO: Reconciled root ca configmap in namespace "svcaccounts-4365"
  Apr 28 17:58:53.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4365" for this suite. @ 04/28/23 17:58:53.204
• [1.057 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 04/28/23 17:58:53.214
  Apr 28 17:58:53.214: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename webhook @ 04/28/23 17:58:53.216
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:58:53.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:58:53.25
  STEP: Setting up server cert @ 04/28/23 17:58:53.288
  E0428 17:58:53.481671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 17:58:53.787
  STEP: Deploying the webhook pod @ 04/28/23 17:58:53.794
  STEP: Wait for the deployment to be ready @ 04/28/23 17:58:53.808
  Apr 28 17:58:53.817: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0428 17:58:54.481826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:55.482301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 17:58:55.827
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 17:58:55.838
  E0428 17:58:56.483305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:58:56.838: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 04/28/23 17:58:56.842
  STEP: create a pod that should be updated by the webhook @ 04/28/23 17:58:56.874
  Apr 28 17:58:56.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2186" for this suite. @ 04/28/23 17:58:56.982
  STEP: Destroying namespace "webhook-markers-4458" for this suite. @ 04/28/23 17:58:56.992
• [3.790 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 04/28/23 17:58:57.007
  Apr 28 17:58:57.007: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename security-context-test @ 04/28/23 17:58:57.008
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:58:57.028
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:58:57.031
  E0428 17:58:57.484042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:58.484206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:58:59.484331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:00.484515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:59:01.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-8147" for this suite. @ 04/28/23 17:59:01.061
• [4.060 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 04/28/23 17:59:01.067
  Apr 28 17:59:01.067: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 17:59:01.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:59:01.084
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:59:01.086
  STEP: Creating a ResourceQuota @ 04/28/23 17:59:01.089
  STEP: Getting a ResourceQuota @ 04/28/23 17:59:01.093
  STEP: Updating a ResourceQuota @ 04/28/23 17:59:01.098
  STEP: Verifying a ResourceQuota was modified @ 04/28/23 17:59:01.102
  STEP: Deleting a ResourceQuota @ 04/28/23 17:59:01.105
  STEP: Verifying the deleted ResourceQuota @ 04/28/23 17:59:01.11
  Apr 28 17:59:01.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5941" for this suite. @ 04/28/23 17:59:01.116
• [0.055 seconds]
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 04/28/23 17:59:01.123
  Apr 28 17:59:01.123: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename subpath @ 04/28/23 17:59:01.124
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:59:01.14
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:59:01.142
  STEP: Setting up data @ 04/28/23 17:59:01.144
  STEP: Creating pod pod-subpath-test-configmap-xprd @ 04/28/23 17:59:01.156
  STEP: Creating a pod to test atomic-volume-subpath @ 04/28/23 17:59:01.156
  E0428 17:59:01.485106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:02.485253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:03.485338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:04.486261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:05.486763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:06.487237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:07.487997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:08.488232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:09.488278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:10.488381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:11.488792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:12.489866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:13.489958      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:14.490290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:15.491368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:16.491480      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:17.492005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:18.492530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:19.493207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:20.493311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:21.494026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:22.494614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:23.495604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:24.495814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:59:25.217
  Apr 28 17:59:25.220: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-subpath-test-configmap-xprd container test-container-subpath-configmap-xprd: <nil>
  STEP: delete the pod @ 04/28/23 17:59:25.229
  STEP: Deleting pod pod-subpath-test-configmap-xprd @ 04/28/23 17:59:25.252
  Apr 28 17:59:25.252: INFO: Deleting pod "pod-subpath-test-configmap-xprd" in namespace "subpath-555"
  Apr 28 17:59:25.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-555" for this suite. @ 04/28/23 17:59:25.259
• [24.141 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 04/28/23 17:59:25.265
  Apr 28 17:59:25.265: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/28/23 17:59:25.265
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:59:25.282
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:59:25.285
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 04/28/23 17:59:25.288
  Apr 28 17:59:25.288: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  E0428 17:59:25.496365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:26.496864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:59:26.849: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  E0428 17:59:27.497440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:28.498139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:29.498913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:30.499082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:31.500059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:32.500989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:59:33.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6601" for this suite. @ 04/28/23 17:59:33.028
• [7.769 seconds]
------------------------------
SS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 04/28/23 17:59:33.035
  Apr 28 17:59:33.035: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename configmap @ 04/28/23 17:59:33.037
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:59:33.068
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:59:33.072
  STEP: Creating configMap configmap-5312/configmap-test-5b25114a-f684-4186-ad41-44e0c5352bdf @ 04/28/23 17:59:33.075
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:59:33.081
  E0428 17:59:33.501979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:34.501937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:35.502050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:36.502143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:59:37.097
  Apr 28 17:59:37.100: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-configmaps-655b139e-87f9-4e58-8414-2caa924fcbfb container env-test: <nil>
  STEP: delete the pod @ 04/28/23 17:59:37.106
  Apr 28 17:59:37.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5312" for this suite. @ 04/28/23 17:59:37.124
• [4.096 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 04/28/23 17:59:37.132
  Apr 28 17:59:37.132: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/28/23 17:59:37.133
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:59:37.159
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:59:37.162
  STEP: fetching the /apis discovery document @ 04/28/23 17:59:37.164
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 04/28/23 17:59:37.166
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 04/28/23 17:59:37.166
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 04/28/23 17:59:37.166
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 04/28/23 17:59:37.167
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 04/28/23 17:59:37.167
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 04/28/23 17:59:37.168
  Apr 28 17:59:37.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-597" for this suite. @ 04/28/23 17:59:37.172
• [0.045 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 04/28/23 17:59:37.191
  Apr 28 17:59:37.191: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 17:59:37.192
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:59:37.207
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:59:37.215
  STEP: Creating configMap with name projected-configmap-test-volume-c328fac2-8454-4493-aa92-9d362d678a2c @ 04/28/23 17:59:37.218
  STEP: Creating a pod to test consume configMaps @ 04/28/23 17:59:37.224
  E0428 17:59:37.502891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:38.503822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:39.504894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:40.505129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:59:41.246
  Apr 28 17:59:41.248: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-projected-configmaps-76149fdf-a7da-4452-9b7b-a2da817c7ae1 container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 17:59:41.254
  Apr 28 17:59:41.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5896" for this suite. @ 04/28/23 17:59:41.27
• [4.087 seconds]
------------------------------
S
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 04/28/23 17:59:41.276
  Apr 28 17:59:41.276: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename proxy @ 04/28/23 17:59:41.277
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:59:41.291
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:59:41.294
  STEP: starting an echo server on multiple ports @ 04/28/23 17:59:41.306
  STEP: creating replication controller proxy-service-9ll8m in namespace proxy-8587 @ 04/28/23 17:59:41.309
  I0428 17:59:41.318607      19 runners.go:194] Created replication controller with name: proxy-service-9ll8m, namespace: proxy-8587, replica count: 1
  E0428 17:59:41.506098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 17:59:42.369991      19 runners.go:194] proxy-service-9ll8m Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0428 17:59:42.506236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 17:59:43.370135      19 runners.go:194] proxy-service-9ll8m Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0428 17:59:43.506411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 17:59:44.370277      19 runners.go:194] proxy-service-9ll8m Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 28 17:59:44.374: INFO: setup took 3.076858714s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 04/28/23 17:59:44.374
  Apr 28 17:59:44.402: INFO: (0) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/rewriteme">test</a> (200; 28.138944ms)
  Apr 28 17:59:44.410: INFO: (0) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 35.497308ms)
  Apr 28 17:59:44.410: INFO: (0) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 35.429486ms)
  Apr 28 17:59:44.410: INFO: (0) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">test<... (200; 35.871378ms)
  Apr 28 17:59:44.415: INFO: (0) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 40.434911ms)
  Apr 28 17:59:44.424: INFO: (0) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">... (200; 49.116223ms)
  Apr 28 17:59:44.424: INFO: (0) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname1/proxy/: foo (200; 49.10915ms)
  Apr 28 17:59:44.424: INFO: (0) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname2/proxy/: bar (200; 49.741165ms)
  Apr 28 17:59:44.424: INFO: (0) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname2/proxy/: tls qux (200; 49.634261ms)
  Apr 28 17:59:44.424: INFO: (0) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:462/proxy/: tls qux (200; 49.899593ms)
  Apr 28 17:59:44.424: INFO: (0) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 49.533528ms)
  Apr 28 17:59:44.429: INFO: (0) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/tlsrewritem... (200; 53.934753ms)
  Apr 28 17:59:44.431: INFO: (0) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname2/proxy/: bar (200; 56.666577ms)
  Apr 28 17:59:44.431: INFO: (0) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:460/proxy/: tls baz (200; 56.05658ms)
  Apr 28 17:59:44.431: INFO: (0) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname1/proxy/: tls baz (200; 56.436402ms)
  Apr 28 17:59:44.431: INFO: (0) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname1/proxy/: foo (200; 56.407474ms)
  Apr 28 17:59:44.437: INFO: (1) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/rewriteme">test</a> (200; 5.931833ms)
  Apr 28 17:59:44.438: INFO: (1) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">test<... (200; 6.211284ms)
  Apr 28 17:59:44.441: INFO: (1) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 9.897517ms)
  Apr 28 17:59:44.442: INFO: (1) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:460/proxy/: tls baz (200; 9.879437ms)
  Apr 28 17:59:44.442: INFO: (1) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">... (200; 10.078761ms)
  Apr 28 17:59:44.442: INFO: (1) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 10.299294ms)
  Apr 28 17:59:44.442: INFO: (1) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:462/proxy/: tls qux (200; 10.000807ms)
  Apr 28 17:59:44.442: INFO: (1) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/tlsrewritem... (200; 10.098001ms)
  Apr 28 17:59:44.442: INFO: (1) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 10.162744ms)
  Apr 28 17:59:44.442: INFO: (1) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 10.367689ms)
  Apr 28 17:59:44.444: INFO: (1) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname2/proxy/: bar (200; 12.863921ms)
  Apr 28 17:59:44.445: INFO: (1) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname1/proxy/: foo (200; 13.650323ms)
  Apr 28 17:59:44.446: INFO: (1) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname1/proxy/: foo (200; 14.031632ms)
  Apr 28 17:59:44.446: INFO: (1) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname2/proxy/: tls qux (200; 13.936769ms)
  Apr 28 17:59:44.446: INFO: (1) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname2/proxy/: bar (200; 14.124767ms)
  Apr 28 17:59:44.446: INFO: (1) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname1/proxy/: tls baz (200; 14.41378ms)
  Apr 28 17:59:44.452: INFO: (2) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:462/proxy/: tls qux (200; 6.482252ms)
  Apr 28 17:59:44.456: INFO: (2) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 9.790383ms)
  Apr 28 17:59:44.457: INFO: (2) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/rewriteme">test</a> (200; 10.385454ms)
  Apr 28 17:59:44.457: INFO: (2) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 10.003502ms)
  Apr 28 17:59:44.457: INFO: (2) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/tlsrewritem... (200; 9.95112ms)
  Apr 28 17:59:44.457: INFO: (2) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname2/proxy/: bar (200; 10.613824ms)
  Apr 28 17:59:44.458: INFO: (2) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">... (200; 11.561461ms)
  Apr 28 17:59:44.458: INFO: (2) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 11.768609ms)
  Apr 28 17:59:44.458: INFO: (2) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">test<... (200; 11.844115ms)
  Apr 28 17:59:44.458: INFO: (2) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:460/proxy/: tls baz (200; 11.447844ms)
  Apr 28 17:59:44.458: INFO: (2) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 11.831856ms)
  Apr 28 17:59:44.459: INFO: (2) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname2/proxy/: tls qux (200; 12.276502ms)
  Apr 28 17:59:44.459: INFO: (2) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname1/proxy/: tls baz (200; 12.977459ms)
  Apr 28 17:59:44.460: INFO: (2) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname1/proxy/: foo (200; 13.469944ms)
  Apr 28 17:59:44.460: INFO: (2) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname1/proxy/: foo (200; 13.825617ms)
  Apr 28 17:59:44.461: INFO: (2) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname2/proxy/: bar (200; 14.544035ms)
  Apr 28 17:59:44.471: INFO: (3) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 9.876861ms)
  Apr 28 17:59:44.471: INFO: (3) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 10.113612ms)
  Apr 28 17:59:44.472: INFO: (3) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">test<... (200; 10.606776ms)
  Apr 28 17:59:44.472: INFO: (3) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/rewriteme">test</a> (200; 10.66467ms)
  Apr 28 17:59:44.472: INFO: (3) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 10.611393ms)
  Apr 28 17:59:44.472: INFO: (3) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:462/proxy/: tls qux (200; 10.444218ms)
  Apr 28 17:59:44.472: INFO: (3) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 10.486739ms)
  Apr 28 17:59:44.472: INFO: (3) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:460/proxy/: tls baz (200; 10.593038ms)
  Apr 28 17:59:44.472: INFO: (3) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/tlsrewritem... (200; 10.636204ms)
  Apr 28 17:59:44.473: INFO: (3) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname1/proxy/: tls baz (200; 12.329626ms)
  Apr 28 17:59:44.475: INFO: (3) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname1/proxy/: foo (200; 13.572982ms)
  Apr 28 17:59:44.475: INFO: (3) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname2/proxy/: bar (200; 14.09586ms)
  Apr 28 17:59:44.475: INFO: (3) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname2/proxy/: bar (200; 14.24546ms)
  Apr 28 17:59:44.475: INFO: (3) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname2/proxy/: tls qux (200; 14.613123ms)
  Apr 28 17:59:44.476: INFO: (3) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">... (200; 14.464967ms)
  Apr 28 17:59:44.476: INFO: (3) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname1/proxy/: foo (200; 14.513936ms)
  Apr 28 17:59:44.482: INFO: (4) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:460/proxy/: tls baz (200; 5.952125ms)
  Apr 28 17:59:44.485: INFO: (4) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/rewriteme">test</a> (200; 8.939372ms)
  Apr 28 17:59:44.486: INFO: (4) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 9.929277ms)
  Apr 28 17:59:44.486: INFO: (4) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:462/proxy/: tls qux (200; 10.061453ms)
  Apr 28 17:59:44.487: INFO: (4) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">... (200; 10.611366ms)
  Apr 28 17:59:44.487: INFO: (4) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 10.473344ms)
  Apr 28 17:59:44.487: INFO: (4) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 10.531305ms)
  Apr 28 17:59:44.487: INFO: (4) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/tlsrewritem... (200; 11.154927ms)
  Apr 28 17:59:44.487: INFO: (4) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">test<... (200; 10.812434ms)
  Apr 28 17:59:44.487: INFO: (4) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 11.082153ms)
  Apr 28 17:59:44.490: INFO: (4) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname2/proxy/: tls qux (200; 13.349087ms)
  Apr 28 17:59:44.490: INFO: (4) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname2/proxy/: bar (200; 13.766174ms)
  Apr 28 17:59:44.490: INFO: (4) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname1/proxy/: foo (200; 13.789281ms)
  Apr 28 17:59:44.490: INFO: (4) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname1/proxy/: foo (200; 14.166265ms)
  Apr 28 17:59:44.491: INFO: (4) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname1/proxy/: tls baz (200; 14.319561ms)
  Apr 28 17:59:44.491: INFO: (4) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname2/proxy/: bar (200; 14.147556ms)
  Apr 28 17:59:44.498: INFO: (5) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:462/proxy/: tls qux (200; 7.289787ms)
  Apr 28 17:59:44.501: INFO: (5) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 9.362775ms)
  Apr 28 17:59:44.501: INFO: (5) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname2/proxy/: bar (200; 9.940387ms)
  Apr 28 17:59:44.503: INFO: (5) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/rewriteme">test</a> (200; 11.247758ms)
  Apr 28 17:59:44.503: INFO: (5) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 12.495204ms)
  Apr 28 17:59:44.503: INFO: (5) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">... (200; 12.40549ms)
  Apr 28 17:59:44.504: INFO: (5) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 12.811803ms)
  Apr 28 17:59:44.504: INFO: (5) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname2/proxy/: tls qux (200; 12.695202ms)
  Apr 28 17:59:44.504: INFO: (5) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 13.04291ms)
  Apr 28 17:59:44.504: INFO: (5) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname1/proxy/: foo (200; 12.870427ms)
  Apr 28 17:59:44.505: INFO: (5) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname2/proxy/: bar (200; 13.660287ms)
  Apr 28 17:59:44.505: INFO: (5) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname1/proxy/: foo (200; 13.631271ms)
  Apr 28 17:59:44.505: INFO: (5) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname1/proxy/: tls baz (200; 14.265266ms)
  Apr 28 17:59:44.505: INFO: (5) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:460/proxy/: tls baz (200; 14.156088ms)
  Apr 28 17:59:44.506: INFO: (5) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/tlsrewritem... (200; 14.415824ms)
  Apr 28 17:59:44.506: INFO: (5) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">test<... (200; 14.852319ms)
  E0428 17:59:44.507321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:59:44.515: INFO: (6) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 8.883014ms)
  Apr 28 17:59:44.521: INFO: (6) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/rewriteme">test</a> (200; 14.896659ms)
  Apr 28 17:59:44.521: INFO: (6) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">... (200; 14.727533ms)
  Apr 28 17:59:44.521: INFO: (6) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 14.878112ms)
  Apr 28 17:59:44.521: INFO: (6) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:462/proxy/: tls qux (200; 15.179406ms)
  Apr 28 17:59:44.521: INFO: (6) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:460/proxy/: tls baz (200; 15.242044ms)
  Apr 28 17:59:44.521: INFO: (6) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/tlsrewritem... (200; 15.29991ms)
  Apr 28 17:59:44.522: INFO: (6) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 15.401851ms)
  Apr 28 17:59:44.522: INFO: (6) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 14.91743ms)
  Apr 28 17:59:44.522: INFO: (6) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">test<... (200; 15.133453ms)
  Apr 28 17:59:44.524: INFO: (6) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname2/proxy/: bar (200; 17.294247ms)
  Apr 28 17:59:44.524: INFO: (6) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname1/proxy/: foo (200; 17.056642ms)
  Apr 28 17:59:44.524: INFO: (6) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname2/proxy/: tls qux (200; 17.251143ms)
  Apr 28 17:59:44.524: INFO: (6) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname1/proxy/: tls baz (200; 17.123692ms)
  Apr 28 17:59:44.524: INFO: (6) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname1/proxy/: foo (200; 17.6808ms)
  Apr 28 17:59:44.524: INFO: (6) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname2/proxy/: bar (200; 17.44193ms)
  Apr 28 17:59:44.537: INFO: (7) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/rewriteme">test</a> (200; 12.964541ms)
  Apr 28 17:59:44.537: INFO: (7) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 13.437824ms)
  Apr 28 17:59:44.538: INFO: (7) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 13.135995ms)
  Apr 28 17:59:44.538: INFO: (7) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 13.855917ms)
  Apr 28 17:59:44.538: INFO: (7) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname1/proxy/: foo (200; 13.981227ms)
  Apr 28 17:59:44.538: INFO: (7) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">... (200; 13.329958ms)
  Apr 28 17:59:44.538: INFO: (7) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:462/proxy/: tls qux (200; 13.98835ms)
  Apr 28 17:59:44.538: INFO: (7) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:460/proxy/: tls baz (200; 14.148951ms)
  Apr 28 17:59:44.538: INFO: (7) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">test<... (200; 13.977022ms)
  Apr 28 17:59:44.539: INFO: (7) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/tlsrewritem... (200; 14.724349ms)
  Apr 28 17:59:44.541: INFO: (7) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 16.491581ms)
  Apr 28 17:59:44.543: INFO: (7) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname1/proxy/: foo (200; 18.881722ms)
  Apr 28 17:59:44.543: INFO: (7) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname2/proxy/: tls qux (200; 19.038685ms)
  Apr 28 17:59:44.543: INFO: (7) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname1/proxy/: tls baz (200; 18.85716ms)
  Apr 28 17:59:44.543: INFO: (7) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname2/proxy/: bar (200; 19.157577ms)
  Apr 28 17:59:44.544: INFO: (7) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname2/proxy/: bar (200; 19.78719ms)
  Apr 28 17:59:44.557: INFO: (8) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:460/proxy/: tls baz (200; 11.700675ms)
  Apr 28 17:59:44.558: INFO: (8) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 13.712794ms)
  Apr 28 17:59:44.558: INFO: (8) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname1/proxy/: foo (200; 13.669313ms)
  Apr 28 17:59:44.558: INFO: (8) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:462/proxy/: tls qux (200; 13.2984ms)
  Apr 28 17:59:44.558: INFO: (8) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">... (200; 13.862401ms)
  Apr 28 17:59:44.559: INFO: (8) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 14.007104ms)
  Apr 28 17:59:44.559: INFO: (8) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 14.822719ms)
  Apr 28 17:59:44.559: INFO: (8) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/tlsrewritem... (200; 14.477424ms)
  Apr 28 17:59:44.560: INFO: (8) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">test<... (200; 14.886911ms)
  Apr 28 17:59:44.560: INFO: (8) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/rewriteme">test</a> (200; 15.561881ms)
  Apr 28 17:59:44.560: INFO: (8) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 15.532461ms)
  Apr 28 17:59:44.560: INFO: (8) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname2/proxy/: bar (200; 15.520284ms)
  Apr 28 17:59:44.561: INFO: (8) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname2/proxy/: bar (200; 16.286422ms)
  Apr 28 17:59:44.562: INFO: (8) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname1/proxy/: foo (200; 16.854298ms)
  Apr 28 17:59:44.562: INFO: (8) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname2/proxy/: tls qux (200; 17.081111ms)
  Apr 28 17:59:44.562: INFO: (8) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname1/proxy/: tls baz (200; 16.884492ms)
  Apr 28 17:59:44.569: INFO: (9) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/tlsrewritem... (200; 7.175312ms)
  Apr 28 17:59:44.570: INFO: (9) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 8.337816ms)
  Apr 28 17:59:44.571: INFO: (9) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 9.180553ms)
  Apr 28 17:59:44.572: INFO: (9) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:462/proxy/: tls qux (200; 9.761702ms)
  Apr 28 17:59:44.572: INFO: (9) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:460/proxy/: tls baz (200; 9.731731ms)
  Apr 28 17:59:44.572: INFO: (9) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">test<... (200; 9.757257ms)
  Apr 28 17:59:44.573: INFO: (9) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 10.459715ms)
  Apr 28 17:59:44.573: INFO: (9) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 10.39787ms)
  Apr 28 17:59:44.574: INFO: (9) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">... (200; 11.888174ms)
  Apr 28 17:59:44.575: INFO: (9) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/rewriteme">test</a> (200; 11.993431ms)
  Apr 28 17:59:44.575: INFO: (9) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname1/proxy/: foo (200; 12.871755ms)
  Apr 28 17:59:44.577: INFO: (9) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname2/proxy/: tls qux (200; 14.705964ms)
  Apr 28 17:59:44.579: INFO: (9) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname1/proxy/: tls baz (200; 16.151896ms)
  Apr 28 17:59:44.579: INFO: (9) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname2/proxy/: bar (200; 16.571006ms)
  Apr 28 17:59:44.579: INFO: (9) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname2/proxy/: bar (200; 16.720845ms)
  Apr 28 17:59:44.579: INFO: (9) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname1/proxy/: foo (200; 16.492159ms)
  Apr 28 17:59:44.586: INFO: (10) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 5.873099ms)
  Apr 28 17:59:44.593: INFO: (10) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 13.367496ms)
  Apr 28 17:59:44.594: INFO: (10) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 13.291647ms)
  Apr 28 17:59:44.594: INFO: (10) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:462/proxy/: tls qux (200; 13.463769ms)
  Apr 28 17:59:44.594: INFO: (10) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 13.579439ms)
  Apr 28 17:59:44.594: INFO: (10) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:460/proxy/: tls baz (200; 13.443722ms)
  Apr 28 17:59:44.594: INFO: (10) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/tlsrewritem... (200; 13.456787ms)
  Apr 28 17:59:44.598: INFO: (10) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">test<... (200; 18.036941ms)
  Apr 28 17:59:44.598: INFO: (10) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">... (200; 18.167524ms)
  Apr 28 17:59:44.602: INFO: (10) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname1/proxy/: foo (200; 22.078709ms)
  Apr 28 17:59:44.602: INFO: (10) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname2/proxy/: bar (200; 22.008264ms)
  Apr 28 17:59:44.602: INFO: (10) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname1/proxy/: foo (200; 21.619607ms)
  Apr 28 17:59:44.602: INFO: (10) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/rewriteme">test</a> (200; 21.959617ms)
  Apr 28 17:59:44.602: INFO: (10) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname2/proxy/: bar (200; 22.413244ms)
  Apr 28 17:59:44.602: INFO: (10) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname1/proxy/: tls baz (200; 21.805334ms)
  Apr 28 17:59:44.602: INFO: (10) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname2/proxy/: tls qux (200; 22.292142ms)
  Apr 28 17:59:44.613: INFO: (11) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 10.579274ms)
  Apr 28 17:59:44.613: INFO: (11) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">test<... (200; 10.715792ms)
  Apr 28 17:59:44.613: INFO: (11) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 10.866141ms)
  Apr 28 17:59:44.613: INFO: (11) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/rewriteme">test</a> (200; 10.809887ms)
  Apr 28 17:59:44.615: INFO: (11) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/tlsrewritem... (200; 12.242403ms)
  Apr 28 17:59:44.615: INFO: (11) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 12.613107ms)
  Apr 28 17:59:44.615: INFO: (11) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:460/proxy/: tls baz (200; 12.173128ms)
  Apr 28 17:59:44.615: INFO: (11) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">... (200; 12.58924ms)
  Apr 28 17:59:44.615: INFO: (11) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 12.637009ms)
  Apr 28 17:59:44.616: INFO: (11) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname2/proxy/: tls qux (200; 13.311757ms)
  Apr 28 17:59:44.616: INFO: (11) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:462/proxy/: tls qux (200; 12.898167ms)
  Apr 28 17:59:44.618: INFO: (11) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname1/proxy/: foo (200; 15.751802ms)
  Apr 28 17:59:44.619: INFO: (11) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname2/proxy/: bar (200; 16.554566ms)
  Apr 28 17:59:44.619: INFO: (11) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname1/proxy/: tls baz (200; 16.81918ms)
  Apr 28 17:59:44.620: INFO: (11) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname2/proxy/: bar (200; 16.674498ms)
  Apr 28 17:59:44.620: INFO: (11) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname1/proxy/: foo (200; 16.895253ms)
  Apr 28 17:59:44.631: INFO: (12) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 10.528843ms)
  Apr 28 17:59:44.631: INFO: (12) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/rewriteme">test</a> (200; 10.681129ms)
  Apr 28 17:59:44.631: INFO: (12) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">... (200; 10.67943ms)
  Apr 28 17:59:44.631: INFO: (12) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">test<... (200; 11.05284ms)
  Apr 28 17:59:44.631: INFO: (12) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 11.212102ms)
  Apr 28 17:59:44.631: INFO: (12) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 11.204328ms)
  Apr 28 17:59:44.632: INFO: (12) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 11.996943ms)
  Apr 28 17:59:44.632: INFO: (12) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:462/proxy/: tls qux (200; 11.908313ms)
  Apr 28 17:59:44.632: INFO: (12) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:460/proxy/: tls baz (200; 11.76616ms)
  Apr 28 17:59:44.632: INFO: (12) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname2/proxy/: bar (200; 11.65342ms)
  Apr 28 17:59:44.632: INFO: (12) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/tlsrewritem... (200; 11.913721ms)
  Apr 28 17:59:44.632: INFO: (12) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname1/proxy/: foo (200; 12.327709ms)
  Apr 28 17:59:44.634: INFO: (12) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname1/proxy/: foo (200; 13.449806ms)
  Apr 28 17:59:44.635: INFO: (12) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname2/proxy/: bar (200; 14.895544ms)
  Apr 28 17:59:44.635: INFO: (12) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname1/proxy/: tls baz (200; 15.21102ms)
  Apr 28 17:59:44.636: INFO: (12) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname2/proxy/: tls qux (200; 15.521354ms)
  Apr 28 17:59:44.643: INFO: (13) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">... (200; 7.019356ms)
  Apr 28 17:59:44.646: INFO: (13) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 9.31879ms)
  Apr 28 17:59:44.646: INFO: (13) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 9.532884ms)
  Apr 28 17:59:44.647: INFO: (13) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">test<... (200; 11.100329ms)
  Apr 28 17:59:44.648: INFO: (13) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/rewriteme">test</a> (200; 11.493499ms)
  Apr 28 17:59:44.648: INFO: (13) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 11.507495ms)
  Apr 28 17:59:44.648: INFO: (13) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:462/proxy/: tls qux (200; 11.365359ms)
  Apr 28 17:59:44.648: INFO: (13) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 11.663125ms)
  Apr 28 17:59:44.648: INFO: (13) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname2/proxy/: bar (200; 12.001455ms)
  Apr 28 17:59:44.649: INFO: (13) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname2/proxy/: bar (200; 12.688935ms)
  Apr 28 17:59:44.650: INFO: (13) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/tlsrewritem... (200; 13.238368ms)
  Apr 28 17:59:44.651: INFO: (13) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:460/proxy/: tls baz (200; 14.295992ms)
  Apr 28 17:59:44.653: INFO: (13) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname1/proxy/: foo (200; 17.115652ms)
  Apr 28 17:59:44.655: INFO: (13) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname2/proxy/: tls qux (200; 18.922659ms)
  Apr 28 17:59:44.655: INFO: (13) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname1/proxy/: tls baz (200; 18.705218ms)
  Apr 28 17:59:44.655: INFO: (13) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname1/proxy/: foo (200; 19.332525ms)
  Apr 28 17:59:44.666: INFO: (14) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 10.792173ms)
  Apr 28 17:59:44.666: INFO: (14) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/rewriteme">test</a> (200; 10.609961ms)
  Apr 28 17:59:44.667: INFO: (14) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">... (200; 10.530699ms)
  Apr 28 17:59:44.667: INFO: (14) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">test<... (200; 11.316758ms)
  Apr 28 17:59:44.667: INFO: (14) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 11.274117ms)
  Apr 28 17:59:44.668: INFO: (14) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/tlsrewritem... (200; 12.412067ms)
  Apr 28 17:59:44.668: INFO: (14) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:462/proxy/: tls qux (200; 12.373548ms)
  Apr 28 17:59:44.669: INFO: (14) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:460/proxy/: tls baz (200; 13.868959ms)
  Apr 28 17:59:44.669: INFO: (14) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 13.328873ms)
  Apr 28 17:59:44.670: INFO: (14) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 14.273276ms)
  Apr 28 17:59:44.670: INFO: (14) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname2/proxy/: bar (200; 14.815928ms)
  Apr 28 17:59:44.672: INFO: (14) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname1/proxy/: foo (200; 16.019347ms)
  Apr 28 17:59:44.672: INFO: (14) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname2/proxy/: tls qux (200; 16.329222ms)
  Apr 28 17:59:44.672: INFO: (14) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname2/proxy/: bar (200; 16.184494ms)
  Apr 28 17:59:44.672: INFO: (14) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname1/proxy/: foo (200; 16.128894ms)
  Apr 28 17:59:44.672: INFO: (14) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname1/proxy/: tls baz (200; 16.274271ms)
  Apr 28 17:59:44.685: INFO: (15) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">test<... (200; 11.402857ms)
  Apr 28 17:59:44.685: INFO: (15) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/tlsrewritem... (200; 11.920335ms)
  Apr 28 17:59:44.685: INFO: (15) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname1/proxy/: tls baz (200; 11.536378ms)
  Apr 28 17:59:44.685: INFO: (15) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 12.380562ms)
  Apr 28 17:59:44.685: INFO: (15) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:462/proxy/: tls qux (200; 11.900988ms)
  Apr 28 17:59:44.685: INFO: (15) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">... (200; 12.465144ms)
  Apr 28 17:59:44.685: INFO: (15) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:460/proxy/: tls baz (200; 12.025514ms)
  Apr 28 17:59:44.685: INFO: (15) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 12.339857ms)
  Apr 28 17:59:44.685: INFO: (15) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/rewriteme">test</a> (200; 11.997026ms)
  Apr 28 17:59:44.685: INFO: (15) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 12.330437ms)
  Apr 28 17:59:44.685: INFO: (15) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 12.218132ms)
  Apr 28 17:59:44.685: INFO: (15) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname2/proxy/: tls qux (200; 12.276551ms)
  Apr 28 17:59:44.687: INFO: (15) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname2/proxy/: bar (200; 13.590666ms)
  Apr 28 17:59:44.687: INFO: (15) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname2/proxy/: bar (200; 13.787403ms)
  Apr 28 17:59:44.687: INFO: (15) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname1/proxy/: foo (200; 13.492411ms)
  Apr 28 17:59:44.687: INFO: (15) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname1/proxy/: foo (200; 14.260065ms)
  Apr 28 17:59:44.695: INFO: (16) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">... (200; 7.950596ms)
  Apr 28 17:59:44.699: INFO: (16) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 11.464017ms)
  Apr 28 17:59:44.699: INFO: (16) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 11.847851ms)
  Apr 28 17:59:44.700: INFO: (16) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/rewriteme">test</a> (200; 12.079415ms)
  Apr 28 17:59:44.700: INFO: (16) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname1/proxy/: tls baz (200; 12.565532ms)
  Apr 28 17:59:44.700: INFO: (16) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 12.785192ms)
  Apr 28 17:59:44.700: INFO: (16) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:460/proxy/: tls baz (200; 13.050113ms)
  Apr 28 17:59:44.700: INFO: (16) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">test<... (200; 12.903198ms)
  Apr 28 17:59:44.701: INFO: (16) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 13.509132ms)
  Apr 28 17:59:44.701: INFO: (16) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:462/proxy/: tls qux (200; 13.545185ms)
  Apr 28 17:59:44.701: INFO: (16) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname2/proxy/: bar (200; 14.000915ms)
  Apr 28 17:59:44.702: INFO: (16) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/tlsrewritem... (200; 14.464854ms)
  Apr 28 17:59:44.702: INFO: (16) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname2/proxy/: tls qux (200; 14.699808ms)
  Apr 28 17:59:44.703: INFO: (16) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname2/proxy/: bar (200; 15.34939ms)
  Apr 28 17:59:44.703: INFO: (16) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname1/proxy/: foo (200; 15.578585ms)
  Apr 28 17:59:44.703: INFO: (16) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname1/proxy/: foo (200; 15.90178ms)
  Apr 28 17:59:44.707: INFO: (17) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:460/proxy/: tls baz (200; 3.254731ms)
  Apr 28 17:59:44.710: INFO: (17) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">... (200; 6.112371ms)
  Apr 28 17:59:44.711: INFO: (17) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname1/proxy/: tls baz (200; 7.54528ms)
  Apr 28 17:59:44.715: INFO: (17) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 10.708687ms)
  Apr 28 17:59:44.715: INFO: (17) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 10.82876ms)
  Apr 28 17:59:44.715: INFO: (17) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 10.23768ms)
  Apr 28 17:59:44.715: INFO: (17) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">test<... (200; 10.496556ms)
  Apr 28 17:59:44.715: INFO: (17) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 11.221972ms)
  Apr 28 17:59:44.715: INFO: (17) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/rewriteme">test</a> (200; 10.527634ms)
  Apr 28 17:59:44.715: INFO: (17) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:462/proxy/: tls qux (200; 10.951229ms)
  Apr 28 17:59:44.718: INFO: (17) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname2/proxy/: bar (200; 14.548273ms)
  Apr 28 17:59:44.718: INFO: (17) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname1/proxy/: foo (200; 13.606301ms)
  Apr 28 17:59:44.718: INFO: (17) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname2/proxy/: bar (200; 13.444368ms)
  Apr 28 17:59:44.719: INFO: (17) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/tlsrewritem... (200; 15.162844ms)
  Apr 28 17:59:44.719: INFO: (17) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname2/proxy/: tls qux (200; 15.082022ms)
  Apr 28 17:59:44.719: INFO: (17) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname1/proxy/: foo (200; 15.583658ms)
  Apr 28 17:59:44.735: INFO: (18) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 14.367521ms)
  Apr 28 17:59:44.735: INFO: (18) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/rewriteme">test</a> (200; 14.762991ms)
  Apr 28 17:59:44.735: INFO: (18) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 15.196939ms)
  Apr 28 17:59:44.735: INFO: (18) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">test<... (200; 15.757312ms)
  Apr 28 17:59:44.736: INFO: (18) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:462/proxy/: tls qux (200; 15.25355ms)
  Apr 28 17:59:44.736: INFO: (18) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:460/proxy/: tls baz (200; 15.39155ms)
  Apr 28 17:59:44.736: INFO: (18) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 15.51831ms)
  Apr 28 17:59:44.736: INFO: (18) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">... (200; 15.779166ms)
  Apr 28 17:59:44.736: INFO: (18) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname2/proxy/: bar (200; 16.102608ms)
  Apr 28 17:59:44.736: INFO: (18) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/tlsrewritem... (200; 16.155452ms)
  Apr 28 17:59:44.736: INFO: (18) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 16.250267ms)
  Apr 28 17:59:44.737: INFO: (18) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname2/proxy/: bar (200; 17.354714ms)
  Apr 28 17:59:44.738: INFO: (18) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname1/proxy/: foo (200; 17.411879ms)
  Apr 28 17:59:44.738: INFO: (18) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname1/proxy/: tls baz (200; 18.05192ms)
  Apr 28 17:59:44.738: INFO: (18) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname1/proxy/: foo (200; 18.21902ms)
  Apr 28 17:59:44.738: INFO: (18) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname2/proxy/: tls qux (200; 18.461834ms)
  Apr 28 17:59:44.749: INFO: (19) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 10.291191ms)
  Apr 28 17:59:44.749: INFO: (19) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg/proxy/rewriteme">test</a> (200; 10.563829ms)
  Apr 28 17:59:44.750: INFO: (19) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">... (200; 10.643796ms)
  Apr 28 17:59:44.750: INFO: (19) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:462/proxy/: tls qux (200; 10.951898ms)
  Apr 28 17:59:44.750: INFO: (19) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:460/proxy/: tls baz (200; 11.389483ms)
  Apr 28 17:59:44.750: INFO: (19) /api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/https:proxy-service-9ll8m-nq7kg:443/proxy/tlsrewritem... (200; 11.364262ms)
  Apr 28 17:59:44.750: INFO: (19) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 11.539029ms)
  Apr 28 17:59:44.751: INFO: (19) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:160/proxy/: foo (200; 11.922881ms)
  Apr 28 17:59:44.751: INFO: (19) /api/v1/namespaces/proxy-8587/pods/http:proxy-service-9ll8m-nq7kg:162/proxy/: bar (200; 12.352991ms)
  Apr 28 17:59:44.751: INFO: (19) /api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/: <a href="/api/v1/namespaces/proxy-8587/pods/proxy-service-9ll8m-nq7kg:1080/proxy/rewriteme">test<... (200; 12.404938ms)
  Apr 28 17:59:44.752: INFO: (19) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname1/proxy/: foo (200; 12.740568ms)
  Apr 28 17:59:44.754: INFO: (19) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname1/proxy/: foo (200; 14.84319ms)
  Apr 28 17:59:44.755: INFO: (19) /api/v1/namespaces/proxy-8587/services/http:proxy-service-9ll8m:portname2/proxy/: bar (200; 16.422088ms)
  Apr 28 17:59:44.755: INFO: (19) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname2/proxy/: tls qux (200; 16.456426ms)
  Apr 28 17:59:44.756: INFO: (19) /api/v1/namespaces/proxy-8587/services/https:proxy-service-9ll8m:tlsportname1/proxy/: tls baz (200; 16.639091ms)
  Apr 28 17:59:44.756: INFO: (19) /api/v1/namespaces/proxy-8587/services/proxy-service-9ll8m:portname2/proxy/: bar (200; 16.598709ms)
  Apr 28 17:59:44.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-9ll8m in namespace proxy-8587, will wait for the garbage collector to delete the pods @ 04/28/23 17:59:44.759
  Apr 28 17:59:44.827: INFO: Deleting ReplicationController proxy-service-9ll8m took: 15.098898ms
  Apr 28 17:59:44.928: INFO: Terminating ReplicationController proxy-service-9ll8m pods took: 100.360541ms
  E0428 17:59:45.507868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:46.508761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:47.509375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-8587" for this suite. @ 04/28/23 17:59:47.829
• [6.578 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 04/28/23 17:59:47.855
  Apr 28 17:59:47.855: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 17:59:47.856
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:59:47.885
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:59:47.889
  STEP: Creating a pod to test downward api env vars @ 04/28/23 17:59:47.891
  E0428 17:59:48.510124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:49.510198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:50.511047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:51.511105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 17:59:51.916
  Apr 28 17:59:51.920: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod downward-api-f3a7fc06-7326-4b2f-8a30-cc3a7eb5ac9b container dapi-container: <nil>
  STEP: delete the pod @ 04/28/23 17:59:51.927
  Apr 28 17:59:51.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4756" for this suite. @ 04/28/23 17:59:51.951
• [4.103 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 04/28/23 17:59:51.963
  Apr 28 17:59:51.963: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 17:59:51.964
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:59:51.98
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:59:51.982
  Apr 28 17:59:51.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-2081 create -f -'
  E0428 17:59:52.512036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:59:52.878: INFO: stderr: ""
  Apr 28 17:59:52.878: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Apr 28 17:59:52.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-2081 create -f -'
  E0428 17:59:53.512662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:59:54.261: INFO: stderr: ""
  Apr 28 17:59:54.261: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/28/23 17:59:54.261
  E0428 17:59:54.513032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:59:55.266: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 28 17:59:55.266: INFO: Found 1 / 1
  Apr 28 17:59:55.266: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Apr 28 17:59:55.269: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 28 17:59:55.269: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 28 17:59:55.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-2081 describe pod agnhost-primary-bswsm'
  Apr 28 17:59:55.353: INFO: stderr: ""
  Apr 28 17:59:55.353: INFO: stdout: "Name:             agnhost-primary-bswsm\nNamespace:        kubectl-2081\nPriority:         0\nService Account:  default\nNode:             ip-172-31-9-127.us-east-2.compute.internal/172.31.9.127\nStart Time:       Fri, 28 Apr 2023 17:59:52 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 66c80e0ed6966fddeea0ac0a14c94b3cff80325c0e5fdf37244042aba4dc617f\n                  cni.projectcalico.org/podIP: 10.42.3.30/32\n                  cni.projectcalico.org/podIPs: 10.42.3.30/32\nStatus:           Running\nIP:               10.42.3.30\nIPs:\n  IP:           10.42.3.30\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://2d8e57c5cfdc03b02ec1a27dd078407524086076bbb697e353220e855c431f87\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 28 Apr 2023 17:59:53 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2qf7t (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-2qf7t:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-2081/agnhost-primary-bswsm to ip-172-31-9-127.us-east-2.compute.internal\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
  Apr 28 17:59:55.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-2081 describe rc agnhost-primary'
  Apr 28 17:59:55.438: INFO: stderr: ""
  Apr 28 17:59:55.438: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2081\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-bswsm\n"
  Apr 28 17:59:55.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-2081 describe service agnhost-primary'
  E0428 17:59:55.513193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 17:59:55.520: INFO: stderr: ""
  Apr 28 17:59:55.520: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2081\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.43.208.234\nIPs:               10.43.208.234\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.42.3.30:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Apr 28 17:59:55.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-2081 describe node ip-172-31-11-161.us-east-2.compute.internal'
  Apr 28 17:59:55.663: INFO: stderr: ""
  Apr 28 17:59:55.663: INFO: stdout: "Name:               ip-172-31-11-161.us-east-2.compute.internal\nRoles:              control-plane,etcd,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=rke2\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-11-161.us-east-2.compute.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=true\n                    node-role.kubernetes.io/etcd=true\n                    node-role.kubernetes.io/master=true\n                    node.kubernetes.io/instance-type=rke2\nAnnotations:        etcd.rke2.cattle.io/node-address: 172.31.11.161\n                    etcd.rke2.cattle.io/node-name: ip-172-31-11-161.us-east-2.compute.internal-78a28d93\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"d6:77:4f:98:39:cb\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.31.11.161\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.31.11.161/20\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.42.1.1\n                    rke2.io/encryption-config-hash: start-29e5a662398ac743779a613b0a94e399f058b0d272aa404906adeaa1df76fd45\n                    rke2.io/external-ip: 52.14.222.189\n                    rke2.io/hostname: ip-172-31-11-161.us-east-2.compute.internal\n                    rke2.io/internal-ip: 172.31.11.161\n                    rke2.io/node-args:\n                      [\"server\",\"--write-kubeconfig-mode\",\"0644\",\"--tls-san\",\"3.138.137.181\",\"--server\",\"https://3.138.137.181:9345\",\"--token\",\"********\",\"--nod...\n                    rke2.io/node-config-hash: X3S57VMMU62KX3QGSH66ZMXWPFSKMGDDCBCW2IBBF7WEKMKZSY2Q====\n                    rke2.io/node-env: {}\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 28 Apr 2023 03:47:00 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-11-161.us-east-2.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 28 Apr 2023 17:59:52 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 28 Apr 2023 03:47:40 +0000   Fri, 28 Apr 2023 03:47:40 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Fri, 28 Apr 2023 17:55:41 +0000   Fri, 28 Apr 2023 03:46:59 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 28 Apr 2023 17:55:41 +0000   Fri, 28 Apr 2023 03:46:59 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 28 Apr 2023 17:55:41 +0000   Fri, 28 Apr 2023 03:46:59 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 28 Apr 2023 17:55:41 +0000   Fri, 28 Apr 2023 03:47:28 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.11.161\n  ExternalIP:  52.14.222.189\n  Hostname:    ip-172-31-11-161.us-east-2.compute.internal\nCapacity:\n  cpu:                2\n  ephemeral-storage:  20937708Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3857Mi\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  20368202327\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3857Mi\n  pods:               110\nSystem Info:\n  Machine ID:                 ec2a73b5f0196c7fbff0bd52e6188a84\n  System UUID:                ec2a73b5-f019-6c7f-bff0-bd52e6188a84\n  Boot ID:                    1db9cd42-7b9c-47ee-b369-b91e1f1d300a\n  Kernel Version:             5.14.21-150400.22-default\n  OS Image:                   SUSE Linux Enterprise Server 15 SP4\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.19-k3s1\n  Kubelet Version:            v1.27.1+rke2r1\n  Kube-Proxy Version:         v1.27.1+rke2r1\nPodCIDR:                      10.42.1.0/24\nPodCIDRs:                     10.42.1.0/24\nProviderID:                   rke2://ip-172-31-11-161.us-east-2.compute.internal\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                                    ------------  ----------  ---------------  -------------  ---\n  kube-system                 cloud-controller-manager-ip-172-31-11-161.us-east-2.compute.internal    100m (5%)     0 (0%)      128Mi (3%)       0 (0%)         14h\n  kube-system                 etcd-ip-172-31-11-161.us-east-2.compute.internal                        200m (10%)    0 (0%)      512Mi (13%)      0 (0%)         14h\n  kube-system                 kube-apiserver-ip-172-31-11-161.us-east-2.compute.internal              250m (12%)    0 (0%)      1Gi (26%)        0 (0%)         14h\n  kube-system                 kube-controller-manager-ip-172-31-11-161.us-east-2.compute.internal     200m (10%)    0 (0%)      256Mi (6%)       0 (0%)         14h\n  kube-system                 kube-proxy-ip-172-31-11-161.us-east-2.compute.internal                  250m (12%)    0 (0%)      128Mi (3%)       0 (0%)         14h\n  kube-system                 kube-scheduler-ip-172-31-11-161.us-east-2.compute.internal              100m (5%)     0 (0%)      128Mi (3%)       0 (0%)         14h\n  kube-system                 rke2-canal-8td74                                                        250m (12%)    0 (0%)      0 (0%)           0 (0%)         14h\n  kube-system                 rke2-coredns-rke2-coredns-5896cccb79-vcgcz                              100m (5%)     100m (5%)   128Mi (3%)       128Mi (3%)     14h\n  kube-system                 rke2-ingress-nginx-controller-6f4sz                                     100m (5%)     0 (0%)      90Mi (2%)        0 (0%)         14h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-19d55eb1839b4b26-n6bzk                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         82m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests      Limits\n  --------           --------      ------\n  cpu                1550m (77%)   100m (5%)\n  memory             2394Mi (62%)  128Mi (3%)\n  ephemeral-storage  0 (0%)        0 (0%)\n  hugepages-1Gi      0 (0%)        0 (0%)\n  hugepages-2Mi      0 (0%)        0 (0%)\nEvents:              <none>\n"
  Apr 28 17:59:55.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-2081 describe namespace kubectl-2081'
  Apr 28 17:59:55.740: INFO: stderr: ""
  Apr 28 17:59:55.740: INFO: stdout: "Name:         kubectl-2081\nLabels:       e2e-framework=kubectl\n              e2e-run=5e0cf72d-a540-4b84-a10a-53bd9bf0993c\n              kubernetes.io/metadata.name=kubectl-2081\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Apr 28 17:59:55.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2081" for this suite. @ 04/28/23 17:59:55.744
• [3.786 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 04/28/23 17:59:55.751
  Apr 28 17:59:55.751: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 17:59:55.752
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:59:55.778
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:59:55.781
  Apr 28 17:59:55.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-6414 version'
  Apr 28 17:59:55.847: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Apr 28 17:59:55.847: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:21:19Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1+rke2r1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-18T00:13:25Z\", GoVersion:\"go1.20.3 X:boringcrypto\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Apr 28 17:59:55.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6414" for this suite. @ 04/28/23 17:59:55.852
• [0.113 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 04/28/23 17:59:55.874
  Apr 28 17:59:55.874: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename configmap @ 04/28/23 17:59:55.874
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 17:59:55.891
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 17:59:55.894
  STEP: Creating configMap with name configmap-test-upd-6d8870d4-2eff-4dc9-b999-bbde47cd71c5 @ 04/28/23 17:59:55.899
  STEP: Creating the pod @ 04/28/23 17:59:55.903
  E0428 17:59:56.514325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:57.514340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap configmap-test-upd-6d8870d4-2eff-4dc9-b999-bbde47cd71c5 @ 04/28/23 17:59:57.938
  STEP: waiting to observe update in volume @ 04/28/23 17:59:57.944
  E0428 17:59:58.514487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 17:59:59.514705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:00.515724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:01.519220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:02.519318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:03.519633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:04.519836      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:05.520164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:06.520252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:07.520465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:08.521519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:09.521622      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:10.521775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:11.522300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:12.522429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:13.522526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:14.523221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:15.523340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:16.523465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:17.524122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:18.524285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:19.524542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:20.525542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:21.525676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:22.525763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:23.526011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:24.526283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:25.526403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:26.526771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:27.527032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:28.527805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:29.527949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:30.528025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:31.528129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:32.528317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:33.528861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:34.528941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:35.529097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:36.529592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:37.530634      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:38.530675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:39.530869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:40.531149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:41.531260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:42.531915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:43.532476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:00:44.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3361" for this suite. @ 04/28/23 18:00:44.207
• [48.340 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 04/28/23 18:00:44.215
  Apr 28 18:00:44.215: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename pods @ 04/28/23 18:00:44.216
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:00:44.23
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:00:44.232
  Apr 28 18:00:44.234: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: creating the pod @ 04/28/23 18:00:44.235
  STEP: submitting the pod to kubernetes @ 04/28/23 18:00:44.235
  E0428 18:00:44.532616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:45.532717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:00:46.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4222" for this suite. @ 04/28/23 18:00:46.335
• [2.126 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 04/28/23 18:00:46.342
  Apr 28 18:00:46.342: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename webhook @ 04/28/23 18:00:46.342
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:00:46.355
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:00:46.358
  STEP: Setting up server cert @ 04/28/23 18:00:46.392
  E0428 18:00:46.533340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 18:00:46.827
  STEP: Deploying the webhook pod @ 04/28/23 18:00:46.835
  STEP: Wait for the deployment to be ready @ 04/28/23 18:00:46.853
  Apr 28 18:00:46.866: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0428 18:00:47.533928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:48.534163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 18:00:48.874
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 18:00:48.885
  E0428 18:00:49.534996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:00:49.885: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 04/28/23 18:00:49.889
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 04/28/23 18:00:49.907
  STEP: Creating a dummy validating-webhook-configuration object @ 04/28/23 18:00:49.926
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 04/28/23 18:00:49.94
  STEP: Creating a dummy mutating-webhook-configuration object @ 04/28/23 18:00:49.955
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 04/28/23 18:00:49.963
  Apr 28 18:00:49.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3875" for this suite. @ 04/28/23 18:00:50.083
  STEP: Destroying namespace "webhook-markers-9205" for this suite. @ 04/28/23 18:00:50.097
• [3.766 seconds]
------------------------------
S
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 04/28/23 18:00:50.108
  Apr 28 18:00:50.108: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 18:00:50.109
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:00:50.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:00:50.136
  STEP: Creating a pod to test downward api env vars @ 04/28/23 18:00:50.14
  E0428 18:00:50.535891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:51.536096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:52.536142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:53.536612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 18:00:54.156
  Apr 28 18:00:54.164: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod downward-api-5b117e4f-2124-4fd3-92c1-73f4be3f14de container dapi-container: <nil>
  STEP: delete the pod @ 04/28/23 18:00:54.18
  Apr 28 18:00:54.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2318" for this suite. @ 04/28/23 18:00:54.213
• [4.118 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 04/28/23 18:00:54.232
  Apr 28 18:00:54.232: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 18:00:54.233
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:00:54.251
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:00:54.254
  STEP: Creating resourceQuota "e2e-rq-status-q5z6t" @ 04/28/23 18:00:54.262
  Apr 28 18:00:54.275: INFO: Resource quota "e2e-rq-status-q5z6t" reports spec: hard cpu limit of 500m
  Apr 28 18:00:54.275: INFO: Resource quota "e2e-rq-status-q5z6t" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-q5z6t" /status @ 04/28/23 18:00:54.275
  STEP: Confirm /status for "e2e-rq-status-q5z6t" resourceQuota via watch @ 04/28/23 18:00:54.285
  Apr 28 18:00:54.286: INFO: observed resourceQuota "e2e-rq-status-q5z6t" in namespace "resourcequota-2983" with hard status: v1.ResourceList(nil)
  Apr 28 18:00:54.286: INFO: Found resourceQuota "e2e-rq-status-q5z6t" in namespace "resourcequota-2983" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Apr 28 18:00:54.286: INFO: ResourceQuota "e2e-rq-status-q5z6t" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 04/28/23 18:00:54.289
  Apr 28 18:00:54.295: INFO: Resource quota "e2e-rq-status-q5z6t" reports spec: hard cpu limit of 1
  Apr 28 18:00:54.295: INFO: Resource quota "e2e-rq-status-q5z6t" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-q5z6t" /status @ 04/28/23 18:00:54.295
  STEP: Confirm /status for "e2e-rq-status-q5z6t" resourceQuota via watch @ 04/28/23 18:00:54.302
  Apr 28 18:00:54.305: INFO: observed resourceQuota "e2e-rq-status-q5z6t" in namespace "resourcequota-2983" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Apr 28 18:00:54.305: INFO: Found resourceQuota "e2e-rq-status-q5z6t" in namespace "resourcequota-2983" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Apr 28 18:00:54.305: INFO: ResourceQuota "e2e-rq-status-q5z6t" /status was patched
  STEP: Get "e2e-rq-status-q5z6t" /status @ 04/28/23 18:00:54.305
  Apr 28 18:00:54.308: INFO: Resourcequota "e2e-rq-status-q5z6t" reports status: hard cpu of 1
  Apr 28 18:00:54.308: INFO: Resourcequota "e2e-rq-status-q5z6t" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-q5z6t" /status before checking Spec is unchanged @ 04/28/23 18:00:54.311
  Apr 28 18:00:54.316: INFO: Resourcequota "e2e-rq-status-q5z6t" reports status: hard cpu of 2
  Apr 28 18:00:54.316: INFO: Resourcequota "e2e-rq-status-q5z6t" reports status: hard memory of 2Gi
  Apr 28 18:00:54.318: INFO: Found resourceQuota "e2e-rq-status-q5z6t" in namespace "resourcequota-2983" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  E0428 18:00:54.537368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:55.537586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:56.537749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:57.537984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:58.538432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:00:59.539242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:00.539361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:01.539420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:02.540064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:03.540517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:04.541263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:05.541680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:06.541774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:07.542308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:08.542396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:09.542522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:10.542796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:11.543616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:12.543810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:13.544296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:14.544403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:15.544674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:16.544917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:17.545281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:18.546277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:19.546432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:20.547145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:21.547486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:22.547781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:23.548428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:24.549474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:25.549609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:26.549787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:27.549877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:28.550104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:29.551012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:30.551110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:31.551290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:32.551685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:33.552257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:34.552843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:35.552967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:36.553522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:37.554326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:38.554824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:39.555768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:40.555878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:41.556066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:42.556265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:43.556994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:44.557906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:45.558043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:46.558175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:47.558314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:48.558697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:49.559304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:50.559550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:51.559614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:52.559643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:53.559905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:54.560363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:55.560600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:56.561158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:57.561223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:58.561305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:01:59.562271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:00.562386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:01.562497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:02.562611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:03.563159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:04.563435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:05.563611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:06.563890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:07.564165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:08.564590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:09.565376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:10.565518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:11.565593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:12.565819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:13.566384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:14.566956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:15.567081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:16.567173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:17.567376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:18.567507      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:19.568309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:20.568428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:21.568625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:22.568749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:23.569038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:24.569209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:25.570297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:26.570873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:27.571062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:28.571177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:29.572105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:30.572257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:31.572461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:32.572580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:33.572693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:34.573443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:35.574449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:36.574628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:37.575005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:38.575427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:39.576196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:40.576409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:41.577362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:42.578265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:43.579220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:44.580397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:45.580857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:46.580965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:47.581188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:48.581220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:49.581449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:50.581536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:51.581812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:52.582297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:53.582915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:54.583239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:55.583450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:56.583669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:57.583898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:58.584349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:02:59.584377      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:00.584483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:01.584693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:02.585014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:03.585196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:04.585381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:05.585484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:06.586305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:07.586548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:08.586646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:09.587702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:10.587863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:11.588103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:12.588349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:13.588865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:14.588927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:15.589209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:16.589310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:17.589424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:18.589616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:19.590603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:20.590697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:21.591843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:22.592238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:23.592303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:24.592873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:25.593635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:26.594270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:27.598434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:28.598871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:29.599424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:30.599479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:31.600368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:32.600506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:33.600730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:34.601338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:35.602284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:36.602481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:37.602684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:38.602795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:39.603532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:40.603651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:41.603832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:42.604064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:43.604245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:44.604406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:45.604626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:46.605256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:47.605472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:48.605569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:49.605676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:50.605896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:51.606266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:52.606551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:53.607167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:54.607311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:55.607536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:56.607764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:57.608181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:58.608630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:03:59.609470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:00.609570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:01.609722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:02.610658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:03.611219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:04.611728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:05.611852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:06.612057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:07.612250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:08.612654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:09.612761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:10.612942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:11.613184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:12.613221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:13.614299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:14.615071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:15.615206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:16.615436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:17.615666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:18.616011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:19.616210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:20.616324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:21.616517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:22.617606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:23.618223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:24.619008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:25.619203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:26.619403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:27.619506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:28.619631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:04:29.324: INFO: ResourceQuota "e2e-rq-status-q5z6t" Spec was unchanged and /status reset
  Apr 28 18:04:29.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2983" for this suite. @ 04/28/23 18:04:29.328
• [215.102 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 04/28/23 18:04:29.334
  Apr 28 18:04:29.334: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename watch @ 04/28/23 18:04:29.335
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:04:29.35
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:04:29.352
  STEP: creating a watch on configmaps with label A @ 04/28/23 18:04:29.355
  STEP: creating a watch on configmaps with label B @ 04/28/23 18:04:29.355
  STEP: creating a watch on configmaps with label A or B @ 04/28/23 18:04:29.357
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 04/28/23 18:04:29.358
  Apr 28 18:04:29.362: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9288  330f8bb3-c511-4fe8-82a5-729189470a24 295841 0 2023-04-28 18:04:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-28 18:04:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 18:04:29.362: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9288  330f8bb3-c511-4fe8-82a5-729189470a24 295841 0 2023-04-28 18:04:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-28 18:04:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 04/28/23 18:04:29.363
  Apr 28 18:04:29.371: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9288  330f8bb3-c511-4fe8-82a5-729189470a24 295842 0 2023-04-28 18:04:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-28 18:04:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 18:04:29.371: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9288  330f8bb3-c511-4fe8-82a5-729189470a24 295842 0 2023-04-28 18:04:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-28 18:04:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 04/28/23 18:04:29.371
  Apr 28 18:04:29.380: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9288  330f8bb3-c511-4fe8-82a5-729189470a24 295843 0 2023-04-28 18:04:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-28 18:04:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 18:04:29.381: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9288  330f8bb3-c511-4fe8-82a5-729189470a24 295843 0 2023-04-28 18:04:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-28 18:04:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 04/28/23 18:04:29.381
  Apr 28 18:04:29.386: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9288  330f8bb3-c511-4fe8-82a5-729189470a24 295844 0 2023-04-28 18:04:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-28 18:04:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 18:04:29.386: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9288  330f8bb3-c511-4fe8-82a5-729189470a24 295844 0 2023-04-28 18:04:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-28 18:04:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 04/28/23 18:04:29.386
  Apr 28 18:04:29.391: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9288  934cc9a4-722b-49d3-ad4e-eeaff61f1d70 295845 0 2023-04-28 18:04:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-28 18:04:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 18:04:29.392: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9288  934cc9a4-722b-49d3-ad4e-eeaff61f1d70 295845 0 2023-04-28 18:04:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-28 18:04:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0428 18:04:29.620799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:30.620722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:31.620939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:32.621035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:33.621741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:34.622270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:35.622374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:36.622648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:37.622881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:38.623371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 04/28/23 18:04:39.392
  Apr 28 18:04:39.398: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9288  934cc9a4-722b-49d3-ad4e-eeaff61f1d70 295899 0 2023-04-28 18:04:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-28 18:04:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 18:04:39.398: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9288  934cc9a4-722b-49d3-ad4e-eeaff61f1d70 295899 0 2023-04-28 18:04:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-28 18:04:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0428 18:04:39.623538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:40.623770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:41.623988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:42.624194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:43.624704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:44.624931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:45.625182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:46.625313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:47.626287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:48.626398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:04:49.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9288" for this suite. @ 04/28/23 18:04:49.403
• [20.083 seconds]
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 04/28/23 18:04:49.417
  Apr 28 18:04:49.417: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename svcaccounts @ 04/28/23 18:04:49.418
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:04:49.439
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:04:49.446
  Apr 28 18:04:49.493: INFO: created pod pod-service-account-defaultsa
  Apr 28 18:04:49.493: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Apr 28 18:04:49.513: INFO: created pod pod-service-account-mountsa
  Apr 28 18:04:49.513: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Apr 28 18:04:49.537: INFO: created pod pod-service-account-nomountsa
  Apr 28 18:04:49.537: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Apr 28 18:04:49.548: INFO: created pod pod-service-account-defaultsa-mountspec
  Apr 28 18:04:49.548: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Apr 28 18:04:49.565: INFO: created pod pod-service-account-mountsa-mountspec
  Apr 28 18:04:49.565: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Apr 28 18:04:49.586: INFO: created pod pod-service-account-nomountsa-mountspec
  Apr 28 18:04:49.586: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Apr 28 18:04:49.599: INFO: created pod pod-service-account-defaultsa-nomountspec
  Apr 28 18:04:49.599: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Apr 28 18:04:49.621: INFO: created pod pod-service-account-mountsa-nomountspec
  Apr 28 18:04:49.621: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  E0428 18:04:49.626690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:04:49.633: INFO: created pod pod-service-account-nomountsa-nomountspec
  Apr 28 18:04:49.633: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Apr 28 18:04:49.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1714" for this suite. @ 04/28/23 18:04:49.65
• [0.265 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 04/28/23 18:04:49.684
  Apr 28 18:04:49.684: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename init-container @ 04/28/23 18:04:49.686
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:04:49.748
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:04:49.753
  STEP: creating the pod @ 04/28/23 18:04:49.762
  Apr 28 18:04:49.762: INFO: PodSpec: initContainers in spec.initContainers
  E0428 18:04:50.627168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:51.627101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:52.627339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:04:53.627465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:04:53.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-8367" for this suite. @ 04/28/23 18:04:53.911
• [4.237 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 04/28/23 18:04:53.922
  Apr 28 18:04:53.922: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename svc-latency @ 04/28/23 18:04:53.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:04:53.961
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:04:53.963
  Apr 28 18:04:53.965: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-5079 @ 04/28/23 18:04:53.966
  I0428 18:04:53.972382      19 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5079, replica count: 1
  E0428 18:04:54.628458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 18:04:55.023036      19 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0428 18:04:55.629234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 18:04:56.023912      19 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 28 18:04:56.136: INFO: Created: latency-svc-ggzt6
  Apr 28 18:04:56.140: INFO: Got endpoints: latency-svc-ggzt6 [15.860341ms]
  Apr 28 18:04:56.162: INFO: Created: latency-svc-rlrzt
  Apr 28 18:04:56.164: INFO: Got endpoints: latency-svc-rlrzt [23.047011ms]
  Apr 28 18:04:56.167: INFO: Created: latency-svc-kzdj5
  Apr 28 18:04:56.176: INFO: Got endpoints: latency-svc-kzdj5 [35.45992ms]
  Apr 28 18:04:56.179: INFO: Created: latency-svc-lzfxw
  Apr 28 18:04:56.186: INFO: Got endpoints: latency-svc-lzfxw [44.52959ms]
  Apr 28 18:04:56.190: INFO: Created: latency-svc-jlqgr
  Apr 28 18:04:56.197: INFO: Got endpoints: latency-svc-jlqgr [55.672407ms]
  Apr 28 18:04:56.202: INFO: Created: latency-svc-94mzz
  Apr 28 18:04:56.208: INFO: Got endpoints: latency-svc-94mzz [65.691584ms]
  Apr 28 18:04:56.212: INFO: Created: latency-svc-mh7hq
  Apr 28 18:04:56.219: INFO: Got endpoints: latency-svc-mh7hq [77.507244ms]
  Apr 28 18:04:56.227: INFO: Created: latency-svc-g2bwm
  Apr 28 18:04:56.232: INFO: Got endpoints: latency-svc-g2bwm [90.616224ms]
  Apr 28 18:04:56.237: INFO: Created: latency-svc-mzsh8
  Apr 28 18:04:56.241: INFO: Got endpoints: latency-svc-mzsh8 [98.935306ms]
  Apr 28 18:04:56.246: INFO: Created: latency-svc-fswzw
  Apr 28 18:04:56.251: INFO: Got endpoints: latency-svc-fswzw [110.20354ms]
  Apr 28 18:04:56.254: INFO: Created: latency-svc-5w22h
  Apr 28 18:04:56.260: INFO: Got endpoints: latency-svc-5w22h [118.646141ms]
  Apr 28 18:04:56.264: INFO: Created: latency-svc-rv8mm
  Apr 28 18:04:56.269: INFO: Got endpoints: latency-svc-rv8mm [126.62308ms]
  Apr 28 18:04:56.273: INFO: Created: latency-svc-r44gk
  Apr 28 18:04:56.281: INFO: Got endpoints: latency-svc-r44gk [139.962229ms]
  Apr 28 18:04:56.283: INFO: Created: latency-svc-wwq4c
  Apr 28 18:04:56.287: INFO: Got endpoints: latency-svc-wwq4c [145.222028ms]
  Apr 28 18:04:56.296: INFO: Created: latency-svc-z9h2d
  Apr 28 18:04:56.307: INFO: Got endpoints: latency-svc-z9h2d [165.852861ms]
  Apr 28 18:04:56.309: INFO: Created: latency-svc-v5nvl
  Apr 28 18:04:56.318: INFO: Got endpoints: latency-svc-v5nvl [175.388179ms]
  Apr 28 18:04:56.322: INFO: Created: latency-svc-vpqnd
  Apr 28 18:04:56.329: INFO: Got endpoints: latency-svc-vpqnd [165.032661ms]
  Apr 28 18:04:56.333: INFO: Created: latency-svc-5nwhp
  Apr 28 18:04:56.339: INFO: Got endpoints: latency-svc-5nwhp [162.912326ms]
  Apr 28 18:04:56.343: INFO: Created: latency-svc-t45hl
  Apr 28 18:04:56.349: INFO: Got endpoints: latency-svc-t45hl [162.759079ms]
  Apr 28 18:04:56.352: INFO: Created: latency-svc-z8d4b
  Apr 28 18:04:56.368: INFO: Got endpoints: latency-svc-z8d4b [170.894342ms]
  Apr 28 18:04:56.372: INFO: Created: latency-svc-blsqn
  Apr 28 18:04:56.388: INFO: Created: latency-svc-7r7v2
  Apr 28 18:04:56.392: INFO: Got endpoints: latency-svc-7r7v2 [173.504282ms]
  Apr 28 18:04:56.392: INFO: Got endpoints: latency-svc-blsqn [184.511256ms]
  Apr 28 18:04:56.400: INFO: Created: latency-svc-wtwvf
  Apr 28 18:04:56.405: INFO: Got endpoints: latency-svc-wtwvf [172.622311ms]
  Apr 28 18:04:56.409: INFO: Created: latency-svc-xxw9d
  Apr 28 18:04:56.413: INFO: Got endpoints: latency-svc-xxw9d [171.959032ms]
  Apr 28 18:04:56.418: INFO: Created: latency-svc-c8ff4
  Apr 28 18:04:56.424: INFO: Got endpoints: latency-svc-c8ff4 [172.523217ms]
  Apr 28 18:04:56.428: INFO: Created: latency-svc-r48r6
  Apr 28 18:04:56.436: INFO: Got endpoints: latency-svc-r48r6 [175.934297ms]
  Apr 28 18:04:56.440: INFO: Created: latency-svc-5wv6h
  Apr 28 18:04:56.443: INFO: Got endpoints: latency-svc-5wv6h [174.200344ms]
  Apr 28 18:04:56.449: INFO: Created: latency-svc-gmhr6
  Apr 28 18:04:56.459: INFO: Got endpoints: latency-svc-gmhr6 [177.616183ms]
  Apr 28 18:04:56.465: INFO: Created: latency-svc-pvxwl
  Apr 28 18:04:56.469: INFO: Got endpoints: latency-svc-pvxwl [181.83225ms]
  Apr 28 18:04:56.488: INFO: Created: latency-svc-sdjg2
  Apr 28 18:04:56.488: INFO: Got endpoints: latency-svc-sdjg2 [181.243602ms]
  Apr 28 18:04:56.511: INFO: Created: latency-svc-gdxtl
  Apr 28 18:04:56.522: INFO: Got endpoints: latency-svc-gdxtl [204.485583ms]
  Apr 28 18:04:56.535: INFO: Created: latency-svc-8xfct
  Apr 28 18:04:56.546: INFO: Got endpoints: latency-svc-8xfct [217.085554ms]
  Apr 28 18:04:56.553: INFO: Created: latency-svc-pjjrz
  Apr 28 18:04:56.558: INFO: Got endpoints: latency-svc-pjjrz [218.089271ms]
  Apr 28 18:04:56.564: INFO: Created: latency-svc-9c5l4
  Apr 28 18:04:56.570: INFO: Got endpoints: latency-svc-9c5l4 [221.407418ms]
  Apr 28 18:04:56.578: INFO: Created: latency-svc-9qr58
  Apr 28 18:04:56.584: INFO: Got endpoints: latency-svc-9qr58 [216.51246ms]
  Apr 28 18:04:56.599: INFO: Created: latency-svc-sfswd
  Apr 28 18:04:56.606: INFO: Got endpoints: latency-svc-sfswd [213.45189ms]
  Apr 28 18:04:56.612: INFO: Created: latency-svc-lct5v
  Apr 28 18:04:56.622: INFO: Got endpoints: latency-svc-lct5v [229.463787ms]
  E0428 18:04:56.629513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:04:56.631: INFO: Created: latency-svc-42sx4
  Apr 28 18:04:56.639: INFO: Got endpoints: latency-svc-42sx4 [234.087064ms]
  Apr 28 18:04:56.644: INFO: Created: latency-svc-2wxgp
  Apr 28 18:04:56.654: INFO: Got endpoints: latency-svc-2wxgp [240.651967ms]
  Apr 28 18:04:56.662: INFO: Created: latency-svc-bdmgr
  Apr 28 18:04:56.666: INFO: Got endpoints: latency-svc-bdmgr [241.432909ms]
  Apr 28 18:04:56.676: INFO: Created: latency-svc-nhthp
  Apr 28 18:04:56.683: INFO: Created: latency-svc-48927
  Apr 28 18:04:56.692: INFO: Got endpoints: latency-svc-nhthp [254.22019ms]
  Apr 28 18:04:56.705: INFO: Created: latency-svc-rg2xr
  Apr 28 18:04:56.712: INFO: Created: latency-svc-ds58x
  Apr 28 18:04:56.720: INFO: Created: latency-svc-xhfgp
  Apr 28 18:04:56.727: INFO: Created: latency-svc-g9gvx
  Apr 28 18:04:56.734: INFO: Created: latency-svc-sfmrh
  Apr 28 18:04:56.741: INFO: Got endpoints: latency-svc-48927 [297.996187ms]
  Apr 28 18:04:56.745: INFO: Created: latency-svc-dp9vh
  Apr 28 18:04:56.760: INFO: Created: latency-svc-kvnkz
  Apr 28 18:04:56.761: INFO: Created: latency-svc-9ms7r
  Apr 28 18:04:56.765: INFO: Created: latency-svc-z4l2f
  Apr 28 18:04:56.771: INFO: Created: latency-svc-swll5
  Apr 28 18:04:56.779: INFO: Created: latency-svc-mjmwx
  Apr 28 18:04:56.786: INFO: Created: latency-svc-57dgc
  Apr 28 18:04:56.791: INFO: Got endpoints: latency-svc-rg2xr [331.928477ms]
  Apr 28 18:04:56.795: INFO: Created: latency-svc-d2qmz
  Apr 28 18:04:56.802: INFO: Created: latency-svc-jnh7x
  Apr 28 18:04:56.811: INFO: Created: latency-svc-vmg7w
  Apr 28 18:04:56.816: INFO: Created: latency-svc-7p6n4
  Apr 28 18:04:56.844: INFO: Got endpoints: latency-svc-ds58x [374.020688ms]
  Apr 28 18:04:56.854: INFO: Created: latency-svc-g5f6q
  Apr 28 18:04:56.893: INFO: Got endpoints: latency-svc-xhfgp [404.846672ms]
  Apr 28 18:04:56.910: INFO: Created: latency-svc-flhf4
  Apr 28 18:04:56.941: INFO: Got endpoints: latency-svc-g9gvx [418.053674ms]
  Apr 28 18:04:56.959: INFO: Created: latency-svc-6frrh
  Apr 28 18:04:56.990: INFO: Got endpoints: latency-svc-sfmrh [443.377964ms]
  Apr 28 18:04:57.006: INFO: Created: latency-svc-nj2zh
  Apr 28 18:04:57.041: INFO: Got endpoints: latency-svc-dp9vh [483.683063ms]
  Apr 28 18:04:57.056: INFO: Created: latency-svc-mstmx
  Apr 28 18:04:57.091: INFO: Got endpoints: latency-svc-kvnkz [520.29651ms]
  Apr 28 18:04:57.104: INFO: Created: latency-svc-2w9vr
  Apr 28 18:04:57.142: INFO: Got endpoints: latency-svc-9ms7r [557.991802ms]
  Apr 28 18:04:57.157: INFO: Created: latency-svc-pfqgd
  Apr 28 18:04:57.192: INFO: Got endpoints: latency-svc-z4l2f [586.395385ms]
  Apr 28 18:04:57.203: INFO: Created: latency-svc-mdkqb
  Apr 28 18:04:57.240: INFO: Got endpoints: latency-svc-swll5 [618.323544ms]
  Apr 28 18:04:57.251: INFO: Created: latency-svc-m5tg7
  Apr 28 18:04:57.290: INFO: Got endpoints: latency-svc-mjmwx [651.551507ms]
  Apr 28 18:04:57.300: INFO: Created: latency-svc-6m7z9
  Apr 28 18:04:57.342: INFO: Got endpoints: latency-svc-57dgc [688.211409ms]
  Apr 28 18:04:57.351: INFO: Created: latency-svc-dn7hw
  Apr 28 18:04:57.392: INFO: Got endpoints: latency-svc-d2qmz [726.280236ms]
  Apr 28 18:04:57.402: INFO: Created: latency-svc-srt6k
  Apr 28 18:04:57.441: INFO: Got endpoints: latency-svc-jnh7x [748.699335ms]
  Apr 28 18:04:57.451: INFO: Created: latency-svc-xx89h
  Apr 28 18:04:57.489: INFO: Got endpoints: latency-svc-vmg7w [747.963771ms]
  Apr 28 18:04:57.501: INFO: Created: latency-svc-rb7hb
  Apr 28 18:04:57.539: INFO: Got endpoints: latency-svc-7p6n4 [748.000289ms]
  Apr 28 18:04:57.551: INFO: Created: latency-svc-fjplp
  Apr 28 18:04:57.590: INFO: Got endpoints: latency-svc-g5f6q [746.285052ms]
  Apr 28 18:04:57.600: INFO: Created: latency-svc-wcw5p
  E0428 18:04:57.629788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:04:57.646: INFO: Got endpoints: latency-svc-flhf4 [752.317653ms]
  Apr 28 18:04:57.657: INFO: Created: latency-svc-dk8m5
  Apr 28 18:04:57.691: INFO: Got endpoints: latency-svc-6frrh [750.350986ms]
  Apr 28 18:04:57.700: INFO: Created: latency-svc-gpfr4
  Apr 28 18:04:57.743: INFO: Got endpoints: latency-svc-nj2zh [752.798051ms]
  Apr 28 18:04:57.760: INFO: Created: latency-svc-5flqn
  Apr 28 18:04:57.792: INFO: Got endpoints: latency-svc-mstmx [750.158061ms]
  Apr 28 18:04:57.806: INFO: Created: latency-svc-j229n
  Apr 28 18:04:57.839: INFO: Got endpoints: latency-svc-2w9vr [748.139486ms]
  Apr 28 18:04:57.852: INFO: Created: latency-svc-26mm6
  Apr 28 18:04:57.890: INFO: Got endpoints: latency-svc-pfqgd [746.642439ms]
  Apr 28 18:04:57.903: INFO: Created: latency-svc-bv5kh
  Apr 28 18:04:57.941: INFO: Got endpoints: latency-svc-mdkqb [749.19143ms]
  Apr 28 18:04:57.953: INFO: Created: latency-svc-mb2nl
  Apr 28 18:04:57.992: INFO: Got endpoints: latency-svc-m5tg7 [751.23301ms]
  Apr 28 18:04:58.002: INFO: Created: latency-svc-cmg4z
  Apr 28 18:04:58.041: INFO: Got endpoints: latency-svc-6m7z9 [749.782097ms]
  Apr 28 18:04:58.051: INFO: Created: latency-svc-cjnth
  Apr 28 18:04:58.091: INFO: Got endpoints: latency-svc-dn7hw [748.778844ms]
  Apr 28 18:04:58.106: INFO: Created: latency-svc-95b45
  Apr 28 18:04:58.139: INFO: Got endpoints: latency-svc-srt6k [747.302769ms]
  Apr 28 18:04:58.153: INFO: Created: latency-svc-b556h
  Apr 28 18:04:58.190: INFO: Got endpoints: latency-svc-xx89h [748.16646ms]
  Apr 28 18:04:58.200: INFO: Created: latency-svc-9s62p
  Apr 28 18:04:58.239: INFO: Got endpoints: latency-svc-rb7hb [750.392633ms]
  Apr 28 18:04:58.250: INFO: Created: latency-svc-prxtf
  Apr 28 18:04:58.290: INFO: Got endpoints: latency-svc-fjplp [750.186057ms]
  Apr 28 18:04:58.304: INFO: Created: latency-svc-z7z8d
  Apr 28 18:04:58.341: INFO: Got endpoints: latency-svc-wcw5p [750.588899ms]
  Apr 28 18:04:58.350: INFO: Created: latency-svc-zhd2w
  Apr 28 18:04:58.391: INFO: Got endpoints: latency-svc-dk8m5 [745.665227ms]
  Apr 28 18:04:58.403: INFO: Created: latency-svc-w2wzq
  Apr 28 18:04:58.441: INFO: Got endpoints: latency-svc-gpfr4 [749.829586ms]
  Apr 28 18:04:58.451: INFO: Created: latency-svc-s98tn
  Apr 28 18:04:58.489: INFO: Got endpoints: latency-svc-5flqn [746.289975ms]
  Apr 28 18:04:58.501: INFO: Created: latency-svc-mwcbd
  Apr 28 18:04:58.542: INFO: Got endpoints: latency-svc-j229n [750.092292ms]
  Apr 28 18:04:58.558: INFO: Created: latency-svc-zlj5j
  Apr 28 18:04:58.590: INFO: Got endpoints: latency-svc-26mm6 [750.740582ms]
  Apr 28 18:04:58.600: INFO: Created: latency-svc-kpt62
  E0428 18:04:58.637832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:04:58.641: INFO: Got endpoints: latency-svc-bv5kh [750.936322ms]
  Apr 28 18:04:58.651: INFO: Created: latency-svc-m87bh
  Apr 28 18:04:58.692: INFO: Got endpoints: latency-svc-mb2nl [750.336781ms]
  Apr 28 18:04:58.701: INFO: Created: latency-svc-74fgb
  Apr 28 18:04:58.745: INFO: Got endpoints: latency-svc-cmg4z [753.128819ms]
  Apr 28 18:04:58.755: INFO: Created: latency-svc-69tps
  Apr 28 18:04:58.790: INFO: Got endpoints: latency-svc-cjnth [748.66045ms]
  Apr 28 18:04:58.801: INFO: Created: latency-svc-4hx8k
  Apr 28 18:04:58.841: INFO: Got endpoints: latency-svc-95b45 [750.038006ms]
  Apr 28 18:04:58.855: INFO: Created: latency-svc-wwt9h
  Apr 28 18:04:58.891: INFO: Got endpoints: latency-svc-b556h [751.248772ms]
  Apr 28 18:04:58.901: INFO: Created: latency-svc-m4tlv
  Apr 28 18:04:58.941: INFO: Got endpoints: latency-svc-9s62p [750.82447ms]
  Apr 28 18:04:58.951: INFO: Created: latency-svc-75kb5
  Apr 28 18:04:58.992: INFO: Got endpoints: latency-svc-prxtf [752.249077ms]
  Apr 28 18:04:59.002: INFO: Created: latency-svc-bqcs2
  Apr 28 18:04:59.041: INFO: Got endpoints: latency-svc-z7z8d [751.671374ms]
  Apr 28 18:04:59.055: INFO: Created: latency-svc-n8hcs
  Apr 28 18:04:59.091: INFO: Got endpoints: latency-svc-zhd2w [749.987055ms]
  Apr 28 18:04:59.101: INFO: Created: latency-svc-7n95s
  Apr 28 18:04:59.139: INFO: Got endpoints: latency-svc-w2wzq [747.805519ms]
  Apr 28 18:04:59.165: INFO: Created: latency-svc-lqp67
  Apr 28 18:04:59.192: INFO: Got endpoints: latency-svc-s98tn [750.895777ms]
  Apr 28 18:04:59.207: INFO: Created: latency-svc-zbx75
  Apr 28 18:04:59.242: INFO: Got endpoints: latency-svc-mwcbd [752.757566ms]
  Apr 28 18:04:59.253: INFO: Created: latency-svc-cswkw
  Apr 28 18:04:59.291: INFO: Got endpoints: latency-svc-zlj5j [748.661557ms]
  Apr 28 18:04:59.301: INFO: Created: latency-svc-48s29
  Apr 28 18:04:59.339: INFO: Got endpoints: latency-svc-kpt62 [749.563563ms]
  Apr 28 18:04:59.351: INFO: Created: latency-svc-s6grm
  Apr 28 18:04:59.394: INFO: Got endpoints: latency-svc-m87bh [752.77731ms]
  Apr 28 18:04:59.405: INFO: Created: latency-svc-jwjkk
  Apr 28 18:04:59.442: INFO: Got endpoints: latency-svc-74fgb [750.145825ms]
  Apr 28 18:04:59.452: INFO: Created: latency-svc-l2bvn
  Apr 28 18:04:59.491: INFO: Got endpoints: latency-svc-69tps [745.755424ms]
  Apr 28 18:04:59.503: INFO: Created: latency-svc-9td7v
  Apr 28 18:04:59.541: INFO: Got endpoints: latency-svc-4hx8k [750.675269ms]
  Apr 28 18:04:59.552: INFO: Created: latency-svc-ddpjc
  Apr 28 18:04:59.591: INFO: Got endpoints: latency-svc-wwt9h [749.952875ms]
  Apr 28 18:04:59.602: INFO: Created: latency-svc-j9wgm
  E0428 18:04:59.638046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:04:59.641: INFO: Got endpoints: latency-svc-m4tlv [749.849998ms]
  Apr 28 18:04:59.652: INFO: Created: latency-svc-mbwzd
  Apr 28 18:04:59.690: INFO: Got endpoints: latency-svc-75kb5 [749.433918ms]
  Apr 28 18:04:59.700: INFO: Created: latency-svc-zrrbx
  Apr 28 18:04:59.743: INFO: Got endpoints: latency-svc-bqcs2 [751.529174ms]
  Apr 28 18:04:59.754: INFO: Created: latency-svc-7lqdx
  Apr 28 18:04:59.790: INFO: Got endpoints: latency-svc-n8hcs [748.548404ms]
  Apr 28 18:04:59.799: INFO: Created: latency-svc-gq9nl
  Apr 28 18:04:59.840: INFO: Got endpoints: latency-svc-7n95s [748.960269ms]
  Apr 28 18:04:59.855: INFO: Created: latency-svc-kpf4b
  Apr 28 18:04:59.898: INFO: Got endpoints: latency-svc-lqp67 [759.108372ms]
  Apr 28 18:04:59.922: INFO: Created: latency-svc-tk9zf
  Apr 28 18:04:59.946: INFO: Got endpoints: latency-svc-zbx75 [754.007476ms]
  Apr 28 18:04:59.961: INFO: Created: latency-svc-wwf6x
  Apr 28 18:04:59.995: INFO: Got endpoints: latency-svc-cswkw [752.745328ms]
  Apr 28 18:05:00.015: INFO: Created: latency-svc-f44mn
  Apr 28 18:05:00.041: INFO: Got endpoints: latency-svc-48s29 [750.233463ms]
  Apr 28 18:05:00.062: INFO: Created: latency-svc-6zlq4
  Apr 28 18:05:00.097: INFO: Got endpoints: latency-svc-s6grm [757.033344ms]
  Apr 28 18:05:00.118: INFO: Created: latency-svc-589gs
  Apr 28 18:05:00.148: INFO: Got endpoints: latency-svc-jwjkk [754.570075ms]
  Apr 28 18:05:00.173: INFO: Created: latency-svc-lx8jv
  Apr 28 18:05:00.191: INFO: Got endpoints: latency-svc-l2bvn [749.198149ms]
  Apr 28 18:05:00.202: INFO: Created: latency-svc-fmdzv
  Apr 28 18:05:00.242: INFO: Got endpoints: latency-svc-9td7v [751.421698ms]
  Apr 28 18:05:00.254: INFO: Created: latency-svc-kqj98
  Apr 28 18:05:00.291: INFO: Got endpoints: latency-svc-ddpjc [749.519222ms]
  Apr 28 18:05:00.304: INFO: Created: latency-svc-b6cmp
  Apr 28 18:05:00.342: INFO: Got endpoints: latency-svc-j9wgm [750.688514ms]
  Apr 28 18:05:00.354: INFO: Created: latency-svc-s9dx7
  Apr 28 18:05:00.391: INFO: Got endpoints: latency-svc-mbwzd [749.419533ms]
  Apr 28 18:05:00.402: INFO: Created: latency-svc-vnlbw
  Apr 28 18:05:00.440: INFO: Got endpoints: latency-svc-zrrbx [749.137945ms]
  Apr 28 18:05:00.459: INFO: Created: latency-svc-99mvz
  Apr 28 18:05:00.490: INFO: Got endpoints: latency-svc-7lqdx [745.666977ms]
  Apr 28 18:05:00.500: INFO: Created: latency-svc-cgj5n
  Apr 28 18:05:00.541: INFO: Got endpoints: latency-svc-gq9nl [751.245613ms]
  Apr 28 18:05:00.553: INFO: Created: latency-svc-rwrdk
  Apr 28 18:05:00.591: INFO: Got endpoints: latency-svc-kpf4b [749.247889ms]
  Apr 28 18:05:00.602: INFO: Created: latency-svc-rsn9b
  E0428 18:05:00.638533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:05:00.642: INFO: Got endpoints: latency-svc-tk9zf [743.456649ms]
  Apr 28 18:05:00.655: INFO: Created: latency-svc-jrnqw
  Apr 28 18:05:00.691: INFO: Got endpoints: latency-svc-wwf6x [745.331904ms]
  Apr 28 18:05:00.702: INFO: Created: latency-svc-lvqnd
  Apr 28 18:05:00.740: INFO: Got endpoints: latency-svc-f44mn [744.954589ms]
  Apr 28 18:05:00.754: INFO: Created: latency-svc-hcbws
  Apr 28 18:05:00.792: INFO: Got endpoints: latency-svc-6zlq4 [750.357426ms]
  Apr 28 18:05:00.805: INFO: Created: latency-svc-qb6nh
  Apr 28 18:05:00.845: INFO: Got endpoints: latency-svc-589gs [748.122533ms]
  Apr 28 18:05:00.859: INFO: Created: latency-svc-rvv4p
  Apr 28 18:05:00.891: INFO: Got endpoints: latency-svc-lx8jv [741.71477ms]
  Apr 28 18:05:00.903: INFO: Created: latency-svc-dm6lx
  Apr 28 18:05:00.942: INFO: Got endpoints: latency-svc-fmdzv [750.623128ms]
  Apr 28 18:05:00.953: INFO: Created: latency-svc-w7b9f
  Apr 28 18:05:00.990: INFO: Got endpoints: latency-svc-kqj98 [747.229501ms]
  Apr 28 18:05:01.000: INFO: Created: latency-svc-c22lw
  Apr 28 18:05:01.040: INFO: Got endpoints: latency-svc-b6cmp [749.193976ms]
  Apr 28 18:05:01.053: INFO: Created: latency-svc-tjszz
  Apr 28 18:05:01.090: INFO: Got endpoints: latency-svc-s9dx7 [747.657143ms]
  Apr 28 18:05:01.102: INFO: Created: latency-svc-7ns4d
  Apr 28 18:05:01.150: INFO: Got endpoints: latency-svc-vnlbw [759.439535ms]
  Apr 28 18:05:01.177: INFO: Created: latency-svc-pwbnh
  Apr 28 18:05:01.200: INFO: Got endpoints: latency-svc-99mvz [760.130478ms]
  Apr 28 18:05:01.257: INFO: Got endpoints: latency-svc-cgj5n [766.661602ms]
  Apr 28 18:05:01.273: INFO: Created: latency-svc-h5nz4
  Apr 28 18:05:01.302: INFO: Got endpoints: latency-svc-rwrdk [760.439487ms]
  Apr 28 18:05:01.311: INFO: Created: latency-svc-dww2b
  Apr 28 18:05:01.354: INFO: Created: latency-svc-b5s46
  Apr 28 18:05:01.362: INFO: Got endpoints: latency-svc-rsn9b [771.468192ms]
  Apr 28 18:05:01.394: INFO: Created: latency-svc-lt4h8
  Apr 28 18:05:01.398: INFO: Got endpoints: latency-svc-jrnqw [756.226834ms]
  Apr 28 18:05:01.418: INFO: Created: latency-svc-wtbqn
  Apr 28 18:05:01.443: INFO: Got endpoints: latency-svc-lvqnd [751.376593ms]
  Apr 28 18:05:01.461: INFO: Created: latency-svc-rf4fl
  Apr 28 18:05:01.493: INFO: Got endpoints: latency-svc-hcbws [752.030973ms]
  Apr 28 18:05:01.505: INFO: Created: latency-svc-l5gjk
  Apr 28 18:05:01.539: INFO: Got endpoints: latency-svc-qb6nh [746.691836ms]
  Apr 28 18:05:01.550: INFO: Created: latency-svc-fwf25
  Apr 28 18:05:01.589: INFO: Got endpoints: latency-svc-rvv4p [744.168184ms]
  Apr 28 18:05:01.602: INFO: Created: latency-svc-bjtzt
  E0428 18:05:01.639262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:05:01.643: INFO: Got endpoints: latency-svc-dm6lx [752.281786ms]
  Apr 28 18:05:01.658: INFO: Created: latency-svc-9lbl6
  Apr 28 18:05:01.692: INFO: Got endpoints: latency-svc-w7b9f [750.561149ms]
  Apr 28 18:05:01.711: INFO: Created: latency-svc-b7gjl
  Apr 28 18:05:01.747: INFO: Got endpoints: latency-svc-c22lw [756.930644ms]
  Apr 28 18:05:01.784: INFO: Created: latency-svc-6zrz2
  Apr 28 18:05:01.803: INFO: Got endpoints: latency-svc-tjszz [762.578086ms]
  Apr 28 18:05:01.829: INFO: Created: latency-svc-jmcq2
  Apr 28 18:05:01.849: INFO: Got endpoints: latency-svc-7ns4d [759.180309ms]
  Apr 28 18:05:01.872: INFO: Created: latency-svc-g2bmz
  Apr 28 18:05:01.898: INFO: Got endpoints: latency-svc-pwbnh [747.934507ms]
  Apr 28 18:05:01.909: INFO: Created: latency-svc-ngtcp
  Apr 28 18:05:01.944: INFO: Got endpoints: latency-svc-h5nz4 [743.672291ms]
  Apr 28 18:05:01.961: INFO: Created: latency-svc-pj7vj
  Apr 28 18:05:01.994: INFO: Got endpoints: latency-svc-dww2b [736.554738ms]
  Apr 28 18:05:02.011: INFO: Created: latency-svc-cjcv6
  Apr 28 18:05:02.046: INFO: Got endpoints: latency-svc-b5s46 [739.305795ms]
  Apr 28 18:05:02.065: INFO: Created: latency-svc-27k9b
  Apr 28 18:05:02.091: INFO: Got endpoints: latency-svc-lt4h8 [727.570444ms]
  Apr 28 18:05:02.102: INFO: Created: latency-svc-xxsgv
  Apr 28 18:05:02.140: INFO: Got endpoints: latency-svc-wtbqn [741.882381ms]
  Apr 28 18:05:02.164: INFO: Created: latency-svc-5gwpl
  Apr 28 18:05:02.195: INFO: Got endpoints: latency-svc-rf4fl [751.72335ms]
  Apr 28 18:05:02.206: INFO: Created: latency-svc-6vl5r
  Apr 28 18:05:02.241: INFO: Got endpoints: latency-svc-l5gjk [748.314804ms]
  Apr 28 18:05:02.253: INFO: Created: latency-svc-4bsxl
  Apr 28 18:05:02.293: INFO: Got endpoints: latency-svc-fwf25 [753.940081ms]
  Apr 28 18:05:02.304: INFO: Created: latency-svc-bjljf
  Apr 28 18:05:02.341: INFO: Got endpoints: latency-svc-bjtzt [751.487634ms]
  Apr 28 18:05:02.352: INFO: Created: latency-svc-pfpqx
  Apr 28 18:05:02.389: INFO: Got endpoints: latency-svc-9lbl6 [746.284258ms]
  Apr 28 18:05:02.402: INFO: Created: latency-svc-2r4kk
  Apr 28 18:05:02.442: INFO: Got endpoints: latency-svc-b7gjl [749.409624ms]
  Apr 28 18:05:02.453: INFO: Created: latency-svc-9nnw8
  Apr 28 18:05:02.490: INFO: Got endpoints: latency-svc-6zrz2 [743.111635ms]
  Apr 28 18:05:02.503: INFO: Created: latency-svc-mlt8p
  Apr 28 18:05:02.541: INFO: Got endpoints: latency-svc-jmcq2 [737.714257ms]
  Apr 28 18:05:02.561: INFO: Created: latency-svc-lmwd9
  Apr 28 18:05:02.592: INFO: Got endpoints: latency-svc-g2bmz [742.914281ms]
  Apr 28 18:05:02.606: INFO: Created: latency-svc-j9nxl
  E0428 18:05:02.640497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:05:02.642: INFO: Got endpoints: latency-svc-ngtcp [743.093896ms]
  Apr 28 18:05:02.653: INFO: Created: latency-svc-2qt86
  Apr 28 18:05:02.689: INFO: Got endpoints: latency-svc-pj7vj [745.834282ms]
  Apr 28 18:05:02.700: INFO: Created: latency-svc-hv8jf
  Apr 28 18:05:02.743: INFO: Got endpoints: latency-svc-cjcv6 [743.079581ms]
  Apr 28 18:05:02.757: INFO: Created: latency-svc-g6694
  Apr 28 18:05:02.790: INFO: Got endpoints: latency-svc-27k9b [744.177011ms]
  Apr 28 18:05:02.807: INFO: Created: latency-svc-c2lr8
  Apr 28 18:05:02.845: INFO: Got endpoints: latency-svc-xxsgv [753.543864ms]
  Apr 28 18:05:02.871: INFO: Created: latency-svc-64fq9
  Apr 28 18:05:02.893: INFO: Got endpoints: latency-svc-5gwpl [752.835459ms]
  Apr 28 18:05:02.907: INFO: Created: latency-svc-m6t9d
  Apr 28 18:05:02.948: INFO: Got endpoints: latency-svc-6vl5r [752.844988ms]
  Apr 28 18:05:02.961: INFO: Created: latency-svc-qw9cv
  Apr 28 18:05:02.990: INFO: Got endpoints: latency-svc-4bsxl [748.402645ms]
  Apr 28 18:05:03.001: INFO: Created: latency-svc-mqbjv
  Apr 28 18:05:03.040: INFO: Got endpoints: latency-svc-bjljf [747.237495ms]
  Apr 28 18:05:03.053: INFO: Created: latency-svc-8qw58
  Apr 28 18:05:03.091: INFO: Got endpoints: latency-svc-pfpqx [749.266853ms]
  Apr 28 18:05:03.103: INFO: Created: latency-svc-5c9x8
  Apr 28 18:05:03.140: INFO: Got endpoints: latency-svc-2r4kk [749.866841ms]
  Apr 28 18:05:03.157: INFO: Created: latency-svc-mqgxk
  Apr 28 18:05:03.191: INFO: Got endpoints: latency-svc-9nnw8 [748.724294ms]
  Apr 28 18:05:03.205: INFO: Created: latency-svc-mpwmr
  Apr 28 18:05:03.240: INFO: Got endpoints: latency-svc-mlt8p [750.029289ms]
  Apr 28 18:05:03.251: INFO: Created: latency-svc-cp66l
  Apr 28 18:05:03.291: INFO: Got endpoints: latency-svc-lmwd9 [749.980674ms]
  Apr 28 18:05:03.305: INFO: Created: latency-svc-2fzvm
  Apr 28 18:05:03.339: INFO: Got endpoints: latency-svc-j9nxl [747.178506ms]
  Apr 28 18:05:03.356: INFO: Created: latency-svc-pwcfc
  Apr 28 18:05:03.390: INFO: Got endpoints: latency-svc-2qt86 [748.173205ms]
  Apr 28 18:05:03.401: INFO: Created: latency-svc-rnjcl
  Apr 28 18:05:03.442: INFO: Got endpoints: latency-svc-hv8jf [752.410786ms]
  Apr 28 18:05:03.454: INFO: Created: latency-svc-pss6x
  Apr 28 18:05:03.493: INFO: Got endpoints: latency-svc-g6694 [750.360991ms]
  Apr 28 18:05:03.506: INFO: Created: latency-svc-nx8w2
  Apr 28 18:05:03.540: INFO: Got endpoints: latency-svc-c2lr8 [749.500139ms]
  Apr 28 18:05:03.552: INFO: Created: latency-svc-ffrl8
  Apr 28 18:05:03.589: INFO: Got endpoints: latency-svc-64fq9 [744.279871ms]
  Apr 28 18:05:03.599: INFO: Created: latency-svc-nwvls
  E0428 18:05:03.641330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:05:03.650: INFO: Got endpoints: latency-svc-m6t9d [756.679712ms]
  Apr 28 18:05:03.673: INFO: Created: latency-svc-z4x9d
  Apr 28 18:05:03.694: INFO: Got endpoints: latency-svc-qw9cv [746.073587ms]
  Apr 28 18:05:03.717: INFO: Created: latency-svc-pmqmf
  Apr 28 18:05:03.741: INFO: Got endpoints: latency-svc-mqbjv [751.332859ms]
  Apr 28 18:05:03.756: INFO: Created: latency-svc-t6bgn
  Apr 28 18:05:03.794: INFO: Got endpoints: latency-svc-8qw58 [753.478355ms]
  Apr 28 18:05:03.809: INFO: Created: latency-svc-nszdl
  Apr 28 18:05:03.841: INFO: Got endpoints: latency-svc-5c9x8 [749.185785ms]
  Apr 28 18:05:03.853: INFO: Created: latency-svc-vbzdj
  Apr 28 18:05:03.891: INFO: Got endpoints: latency-svc-mqgxk [750.816668ms]
  Apr 28 18:05:03.903: INFO: Created: latency-svc-87r57
  Apr 28 18:05:03.941: INFO: Got endpoints: latency-svc-mpwmr [750.246541ms]
  Apr 28 18:05:03.957: INFO: Created: latency-svc-8vwbf
  Apr 28 18:05:03.993: INFO: Got endpoints: latency-svc-cp66l [752.3014ms]
  Apr 28 18:05:04.040: INFO: Got endpoints: latency-svc-2fzvm [749.115792ms]
  Apr 28 18:05:04.091: INFO: Got endpoints: latency-svc-pwcfc [751.599517ms]
  Apr 28 18:05:04.143: INFO: Got endpoints: latency-svc-rnjcl [752.965371ms]
  Apr 28 18:05:04.193: INFO: Got endpoints: latency-svc-pss6x [751.117735ms]
  Apr 28 18:05:04.240: INFO: Got endpoints: latency-svc-nx8w2 [746.870395ms]
  Apr 28 18:05:04.292: INFO: Got endpoints: latency-svc-ffrl8 [752.054546ms]
  Apr 28 18:05:04.341: INFO: Got endpoints: latency-svc-nwvls [751.612188ms]
  Apr 28 18:05:04.393: INFO: Got endpoints: latency-svc-z4x9d [742.814554ms]
  Apr 28 18:05:04.441: INFO: Got endpoints: latency-svc-pmqmf [747.402523ms]
  Apr 28 18:05:04.492: INFO: Got endpoints: latency-svc-t6bgn [750.937105ms]
  Apr 28 18:05:04.543: INFO: Got endpoints: latency-svc-nszdl [749.07931ms]
  Apr 28 18:05:04.593: INFO: Got endpoints: latency-svc-vbzdj [752.871742ms]
  E0428 18:05:04.642284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:05:04.643: INFO: Got endpoints: latency-svc-87r57 [751.319668ms]
  Apr 28 18:05:04.690: INFO: Got endpoints: latency-svc-8vwbf [749.405547ms]
  Apr 28 18:05:04.690: INFO: Latencies: [23.047011ms 35.45992ms 44.52959ms 55.672407ms 65.691584ms 77.507244ms 90.616224ms 98.935306ms 110.20354ms 118.646141ms 126.62308ms 139.962229ms 145.222028ms 162.759079ms 162.912326ms 165.032661ms 165.852861ms 170.894342ms 171.959032ms 172.523217ms 172.622311ms 173.504282ms 174.200344ms 175.388179ms 175.934297ms 177.616183ms 181.243602ms 181.83225ms 184.511256ms 204.485583ms 213.45189ms 216.51246ms 217.085554ms 218.089271ms 221.407418ms 229.463787ms 234.087064ms 240.651967ms 241.432909ms 254.22019ms 297.996187ms 331.928477ms 374.020688ms 404.846672ms 418.053674ms 443.377964ms 483.683063ms 520.29651ms 557.991802ms 586.395385ms 618.323544ms 651.551507ms 688.211409ms 726.280236ms 727.570444ms 736.554738ms 737.714257ms 739.305795ms 741.71477ms 741.882381ms 742.814554ms 742.914281ms 743.079581ms 743.093896ms 743.111635ms 743.456649ms 743.672291ms 744.168184ms 744.177011ms 744.279871ms 744.954589ms 745.331904ms 745.665227ms 745.666977ms 745.755424ms 745.834282ms 746.073587ms 746.284258ms 746.285052ms 746.289975ms 746.642439ms 746.691836ms 746.870395ms 747.178506ms 747.229501ms 747.237495ms 747.302769ms 747.402523ms 747.657143ms 747.805519ms 747.934507ms 747.963771ms 748.000289ms 748.122533ms 748.139486ms 748.16646ms 748.173205ms 748.314804ms 748.402645ms 748.548404ms 748.66045ms 748.661557ms 748.699335ms 748.724294ms 748.778844ms 748.960269ms 749.07931ms 749.115792ms 749.137945ms 749.185785ms 749.19143ms 749.193976ms 749.198149ms 749.247889ms 749.266853ms 749.405547ms 749.409624ms 749.419533ms 749.433918ms 749.500139ms 749.519222ms 749.563563ms 749.782097ms 749.829586ms 749.849998ms 749.866841ms 749.952875ms 749.980674ms 749.987055ms 750.029289ms 750.038006ms 750.092292ms 750.145825ms 750.158061ms 750.186057ms 750.233463ms 750.246541ms 750.336781ms 750.350986ms 750.357426ms 750.360991ms 750.392633ms 750.561149ms 750.588899ms 750.623128ms 750.675269ms 750.688514ms 750.740582ms 750.816668ms 750.82447ms 750.895777ms 750.936322ms 750.937105ms 751.117735ms 751.23301ms 751.245613ms 751.248772ms 751.319668ms 751.332859ms 751.376593ms 751.421698ms 751.487634ms 751.529174ms 751.599517ms 751.612188ms 751.671374ms 751.72335ms 752.030973ms 752.054546ms 752.249077ms 752.281786ms 752.3014ms 752.317653ms 752.410786ms 752.745328ms 752.757566ms 752.77731ms 752.798051ms 752.835459ms 752.844988ms 752.871742ms 752.965371ms 753.128819ms 753.478355ms 753.543864ms 753.940081ms 754.007476ms 754.570075ms 756.226834ms 756.679712ms 756.930644ms 757.033344ms 759.108372ms 759.180309ms 759.439535ms 760.130478ms 760.439487ms 762.578086ms 766.661602ms 771.468192ms]
  Apr 28 18:05:04.691: INFO: 50 %ile: 748.66045ms
  Apr 28 18:05:04.691: INFO: 90 %ile: 752.871742ms
  Apr 28 18:05:04.691: INFO: 99 %ile: 766.661602ms
  Apr 28 18:05:04.691: INFO: Total sample count: 200
  Apr 28 18:05:04.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-5079" for this suite. @ 04/28/23 18:05:04.697
• [10.780 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 04/28/23 18:05:04.704
  Apr 28 18:05:04.704: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename job @ 04/28/23 18:05:04.705
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:05:04.724
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:05:04.729
  STEP: Creating a job @ 04/28/23 18:05:04.733
  STEP: Ensuring job reaches completions @ 04/28/23 18:05:04.74
  E0428 18:05:05.643367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:06.648184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:07.648285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:08.648722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:09.649200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:10.649300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:11.649467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:12.649546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:13.649719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:14.649756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:05:14.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7875" for this suite. @ 04/28/23 18:05:14.747
• [10.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 04/28/23 18:05:14.756
  Apr 28 18:05:14.756: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename dns @ 04/28/23 18:05:14.757
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:05:14.776
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:05:14.783
  STEP: Creating a test headless service @ 04/28/23 18:05:14.785
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4416.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4416.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 04/28/23 18:05:14.79
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4416.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4416.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 04/28/23 18:05:14.79
  STEP: creating a pod to probe DNS @ 04/28/23 18:05:14.79
  STEP: submitting the pod to kubernetes @ 04/28/23 18:05:14.79
  E0428 18:05:15.650001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:16.650091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/28/23 18:05:16.814
  STEP: looking for the results for each expected name from probers @ 04/28/23 18:05:16.817
  Apr 28 18:05:16.832: INFO: DNS probes using dns-4416/dns-test-257825ac-9a29-45bd-b11c-4108ca0019f1 succeeded

  Apr 28 18:05:16.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 18:05:16.837
  STEP: deleting the test headless service @ 04/28/23 18:05:16.851
  STEP: Destroying namespace "dns-4416" for this suite. @ 04/28/23 18:05:16.864
• [2.116 seconds]
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 04/28/23 18:05:16.872
  Apr 28 18:05:16.872: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename container-runtime @ 04/28/23 18:05:16.873
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:05:16.888
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:05:16.895
  STEP: create the container @ 04/28/23 18:05:16.898
  W0428 18:05:16.906198      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/28/23 18:05:16.906
  E0428 18:05:17.653208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:18.653244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:19.653375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 04/28/23 18:05:19.932
  STEP: the container should be terminated @ 04/28/23 18:05:19.935
  STEP: the termination message should be set @ 04/28/23 18:05:19.935
  Apr 28 18:05:19.935: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 04/28/23 18:05:19.935
  Apr 28 18:05:19.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-1132" for this suite. @ 04/28/23 18:05:19.994
• [3.133 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 04/28/23 18:05:20.009
  Apr 28 18:05:20.009: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename secrets @ 04/28/23 18:05:20.013
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:05:20.04
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:05:20.044
  STEP: Creating secret with name s-test-opt-del-a2db86a8-0117-4726-afe5-0e20468de6f6 @ 04/28/23 18:05:20.057
  STEP: Creating secret with name s-test-opt-upd-32b124ca-64be-417f-b393-bc289a875850 @ 04/28/23 18:05:20.065
  STEP: Creating the pod @ 04/28/23 18:05:20.072
  E0428 18:05:20.654707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:21.654032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-a2db86a8-0117-4726-afe5-0e20468de6f6 @ 04/28/23 18:05:22.145
  STEP: Updating secret s-test-opt-upd-32b124ca-64be-417f-b393-bc289a875850 @ 04/28/23 18:05:22.152
  STEP: Creating secret with name s-test-opt-create-63c4f4f8-3c3f-4e48-a7ca-0adc58d541de @ 04/28/23 18:05:22.156
  STEP: waiting to observe update in volume @ 04/28/23 18:05:22.16
  E0428 18:05:22.654735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:23.654874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:05:24.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6501" for this suite. @ 04/28/23 18:05:24.191
• [4.190 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 04/28/23 18:05:24.203
  Apr 28 18:05:24.204: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename secrets @ 04/28/23 18:05:24.21
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:05:24.228
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:05:24.231
  STEP: Creating secret with name secret-test-map-123dc001-08f7-4dfd-9816-d52c2c25f023 @ 04/28/23 18:05:24.233
  STEP: Creating a pod to test consume secrets @ 04/28/23 18:05:24.237
  E0428 18:05:24.655678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:25.656644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:26.657084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:27.657418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 18:05:28.259
  Apr 28 18:05:28.262: INFO: Trying to get logs from node ip-172-31-6-71.us-east-2.compute.internal pod pod-secrets-3a28e3a8-aecd-4029-9946-f98c3f352483 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 18:05:28.277
  Apr 28 18:05:28.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8413" for this suite. @ 04/28/23 18:05:28.293
• [4.094 seconds]
------------------------------
SS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 04/28/23 18:05:28.298
  Apr 28 18:05:28.298: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename security-context @ 04/28/23 18:05:28.299
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:05:28.317
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:05:28.319
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 04/28/23 18:05:28.321
  E0428 18:05:28.658289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:29.659273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:30.659516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:31.660299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 18:05:32.342
  Apr 28 18:05:32.345: INFO: Trying to get logs from node ip-172-31-6-71.us-east-2.compute.internal pod security-context-f3995b22-2d22-49a6-9d05-2cc4386abf02 container test-container: <nil>
  STEP: delete the pod @ 04/28/23 18:05:32.351
  Apr 28 18:05:32.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-8644" for this suite. @ 04/28/23 18:05:32.366
• [4.073 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 04/28/23 18:05:32.374
  Apr 28 18:05:32.374: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 18:05:32.375
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:05:32.401
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:05:32.406
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 04/28/23 18:05:32.409
  E0428 18:05:32.660501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:33.660821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:34.661802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:35.661804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 18:05:36.426
  Apr 28 18:05:36.428: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-f498301d-a12f-49ba-8cb7-e760691b1c8f container test-container: <nil>
  STEP: delete the pod @ 04/28/23 18:05:36.434
  Apr 28 18:05:36.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9559" for this suite. @ 04/28/23 18:05:36.45
• [4.082 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 04/28/23 18:05:36.458
  Apr 28 18:05:36.458: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 18:05:36.459
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:05:36.477
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:05:36.479
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 18:05:36.481
  E0428 18:05:36.662156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:37.662472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:38.662637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:39.662656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 18:05:40.498
  Apr 28 18:05:40.501: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod downwardapi-volume-24c9e19e-7074-49f7-b564-f470ff3a5369 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 18:05:40.515
  Apr 28 18:05:40.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6137" for this suite. @ 04/28/23 18:05:40.548
• [4.098 seconds]
------------------------------
SS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 04/28/23 18:05:40.557
  Apr 28 18:05:40.557: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename ingress @ 04/28/23 18:05:40.558
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:05:40.587
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:05:40.602
  STEP: getting /apis @ 04/28/23 18:05:40.604
  STEP: getting /apis/networking.k8s.io @ 04/28/23 18:05:40.611
  STEP: getting /apis/networking.k8s.iov1 @ 04/28/23 18:05:40.612
  STEP: creating @ 04/28/23 18:05:40.613
  E0428 18:05:40.663428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting @ 04/28/23 18:05:41.114
  STEP: listing @ 04/28/23 18:05:41.122
  STEP: watching @ 04/28/23 18:05:41.126
  Apr 28 18:05:41.126: INFO: starting watch
  STEP: cluster-wide listing @ 04/28/23 18:05:41.127
  STEP: cluster-wide watching @ 04/28/23 18:05:41.13
  Apr 28 18:05:41.130: INFO: starting watch
  STEP: patching @ 04/28/23 18:05:41.131
  STEP: updating @ 04/28/23 18:05:41.217
  Apr 28 18:05:41.270: INFO: waiting for watch events with expected annotations
  Apr 28 18:05:41.270: INFO: saw patched and updated annotations
  STEP: patching /status @ 04/28/23 18:05:41.27
  STEP: updating /status @ 04/28/23 18:05:41.276
  STEP: get /status @ 04/28/23 18:05:41.285
  STEP: deleting @ 04/28/23 18:05:41.29
  STEP: deleting a collection @ 04/28/23 18:05:41.303
  Apr 28 18:05:41.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-6751" for this suite. @ 04/28/23 18:05:41.319
• [0.767 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 04/28/23 18:05:41.326
  Apr 28 18:05:41.326: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename pods @ 04/28/23 18:05:41.327
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:05:41.34
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:05:41.343
  STEP: creating the pod @ 04/28/23 18:05:41.345
  STEP: submitting the pod to kubernetes @ 04/28/23 18:05:41.345
  W0428 18:05:41.351559      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0428 18:05:41.664416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:42.664795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 04/28/23 18:05:43.36
  STEP: updating the pod @ 04/28/23 18:05:43.363
  E0428 18:05:43.665617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:05:43.879: INFO: Successfully updated pod "pod-update-activedeadlineseconds-f15a5d88-992e-4bc4-9e51-d3dfe7926b08"
  E0428 18:05:44.665725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:45.665817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:46.666369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:47.667169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:05:47.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1992" for this suite. @ 04/28/23 18:05:47.908
• [6.594 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 04/28/23 18:05:47.921
  Apr 28 18:05:47.921: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename statefulset @ 04/28/23 18:05:47.922
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:05:47.949
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:05:47.951
  STEP: Creating service test in namespace statefulset-8191 @ 04/28/23 18:05:47.956
  STEP: Looking for a node to schedule stateful set and pod @ 04/28/23 18:05:47.962
  STEP: Creating pod with conflicting port in namespace statefulset-8191 @ 04/28/23 18:05:47.974
  STEP: Waiting until pod test-pod will start running in namespace statefulset-8191 @ 04/28/23 18:05:47.994
  E0428 18:05:48.668283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:49.668256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating statefulset with conflicting port in namespace statefulset-8191 @ 04/28/23 18:05:50.006
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8191 @ 04/28/23 18:05:50.012
  Apr 28 18:05:50.030: INFO: Observed stateful pod in namespace: statefulset-8191, name: ss-0, uid: bf9aeea8-669e-4851-af6f-d7e3aeab5533, status phase: Pending. Waiting for statefulset controller to delete.
  Apr 28 18:05:50.081: INFO: Observed stateful pod in namespace: statefulset-8191, name: ss-0, uid: bf9aeea8-669e-4851-af6f-d7e3aeab5533, status phase: Failed. Waiting for statefulset controller to delete.
  Apr 28 18:05:50.159: INFO: Observed stateful pod in namespace: statefulset-8191, name: ss-0, uid: bf9aeea8-669e-4851-af6f-d7e3aeab5533, status phase: Failed. Waiting for statefulset controller to delete.
  Apr 28 18:05:50.164: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8191
  STEP: Removing pod with conflicting port in namespace statefulset-8191 @ 04/28/23 18:05:50.165
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8191 and will be in running state @ 04/28/23 18:05:50.201
  E0428 18:05:50.668559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:51.668751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:05:52.211: INFO: Deleting all statefulset in ns statefulset-8191
  Apr 28 18:05:52.214: INFO: Scaling statefulset ss to 0
  E0428 18:05:52.669824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:53.670352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:54.670565      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:55.670844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:56.670940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:57.671256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:58.671343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:05:59.671559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:00.671740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:01.671989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:06:02.228: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 28 18:06:02.231: INFO: Deleting statefulset ss
  Apr 28 18:06:02.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8191" for this suite. @ 04/28/23 18:06:02.253
• [14.357 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 04/28/23 18:06:02.292
  Apr 28 18:06:02.292: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 18:06:02.293
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:06:02.32
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:06:02.323
  STEP: Creating a pod to test downward api env vars @ 04/28/23 18:06:02.325
  E0428 18:06:02.672776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:03.672980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:04.673823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:05.674272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 18:06:06.353
  Apr 28 18:06:06.356: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod downward-api-f966d29f-8d21-4a2b-b260-b47099cfc702 container dapi-container: <nil>
  STEP: delete the pod @ 04/28/23 18:06:06.364
  Apr 28 18:06:06.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1954" for this suite. @ 04/28/23 18:06:06.385
• [4.099 seconds]
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 04/28/23 18:06:06.391
  Apr 28 18:06:06.391: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename watch @ 04/28/23 18:06:06.392
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:06:06.408
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:06:06.411
  STEP: creating a watch on configmaps with a certain label @ 04/28/23 18:06:06.413
  STEP: creating a new configmap @ 04/28/23 18:06:06.414
  STEP: modifying the configmap once @ 04/28/23 18:06:06.42
  STEP: changing the label value of the configmap @ 04/28/23 18:06:06.426
  STEP: Expecting to observe a delete notification for the watched object @ 04/28/23 18:06:06.433
  Apr 28 18:06:06.434: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2031  8ce2955b-e501-45fc-8ca9-a1c4fdb10472 298620 0 2023-04-28 18:06:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-28 18:06:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 18:06:06.434: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2031  8ce2955b-e501-45fc-8ca9-a1c4fdb10472 298621 0 2023-04-28 18:06:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-28 18:06:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 18:06:06.434: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2031  8ce2955b-e501-45fc-8ca9-a1c4fdb10472 298622 0 2023-04-28 18:06:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-28 18:06:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 04/28/23 18:06:06.434
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 04/28/23 18:06:06.443
  E0428 18:06:06.674948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:07.677312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:08.677147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:09.677437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:10.677686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:11.678047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:12.678563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:13.678684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:14.678945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:15.679030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 04/28/23 18:06:16.443
  STEP: modifying the configmap a third time @ 04/28/23 18:06:16.45
  STEP: deleting the configmap @ 04/28/23 18:06:16.458
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 04/28/23 18:06:16.464
  Apr 28 18:06:16.464: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2031  8ce2955b-e501-45fc-8ca9-a1c4fdb10472 298701 0 2023-04-28 18:06:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-28 18:06:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 18:06:16.464: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2031  8ce2955b-e501-45fc-8ca9-a1c4fdb10472 298702 0 2023-04-28 18:06:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-28 18:06:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 18:06:16.464: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2031  8ce2955b-e501-45fc-8ca9-a1c4fdb10472 298703 0 2023-04-28 18:06:06 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-28 18:06:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 28 18:06:16.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2031" for this suite. @ 04/28/23 18:06:16.469
• [10.083 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 04/28/23 18:06:16.476
  Apr 28 18:06:16.476: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename podtemplate @ 04/28/23 18:06:16.477
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:06:16.491
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:06:16.493
  STEP: Create set of pod templates @ 04/28/23 18:06:16.496
  Apr 28 18:06:16.501: INFO: created test-podtemplate-1
  Apr 28 18:06:16.505: INFO: created test-podtemplate-2
  Apr 28 18:06:16.510: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 04/28/23 18:06:16.51
  STEP: delete collection of pod templates @ 04/28/23 18:06:16.513
  Apr 28 18:06:16.513: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 04/28/23 18:06:16.532
  Apr 28 18:06:16.532: INFO: requesting list of pod templates to confirm quantity
  Apr 28 18:06:16.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-947" for this suite. @ 04/28/23 18:06:16.539
• [0.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 04/28/23 18:06:16.554
  Apr 28 18:06:16.554: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/28/23 18:06:16.555
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:06:16.569
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:06:16.571
  STEP: set up a multi version CRD @ 04/28/23 18:06:16.573
  Apr 28 18:06:16.574: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  E0428 18:06:16.679725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:17.680718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:18.681348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:19.683291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:20.684176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: rename a version @ 04/28/23 18:06:20.79
  STEP: check the new version name is served @ 04/28/23 18:06:20.804
  E0428 18:06:21.684215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the old version name is removed @ 04/28/23 18:06:22.158
  E0428 18:06:22.684248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 04/28/23 18:06:22.895
  E0428 18:06:23.684845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:24.685602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:25.686628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:06:26.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2496" for this suite. @ 04/28/23 18:06:26.188
• [9.642 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 04/28/23 18:06:26.196
  Apr 28 18:06:26.196: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename replication-controller @ 04/28/23 18:06:26.197
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:06:26.22
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:06:26.223
  Apr 28 18:06:26.225: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 04/28/23 18:06:26.247
  STEP: Checking rc "condition-test" has the desired failure condition set @ 04/28/23 18:06:26.252
  E0428 18:06:26.687336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 04/28/23 18:06:27.259
  Apr 28 18:06:27.267: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 04/28/23 18:06:27.267
  E0428 18:06:27.686973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:06:28.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-4378" for this suite. @ 04/28/23 18:06:28.279
• [2.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 04/28/23 18:06:28.291
  Apr 28 18:06:28.291: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename pods @ 04/28/23 18:06:28.292
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:06:28.307
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:06:28.31
  Apr 28 18:06:28.312: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: creating the pod @ 04/28/23 18:06:28.313
  STEP: submitting the pod to kubernetes @ 04/28/23 18:06:28.313
  E0428 18:06:28.687463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:29.687510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:06:30.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4836" for this suite. @ 04/28/23 18:06:30.345
• [2.060 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 04/28/23 18:06:30.351
  Apr 28 18:06:30.351: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/28/23 18:06:30.352
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:06:30.365
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:06:30.368
  STEP: create the container to handle the HTTPGet hook request. @ 04/28/23 18:06:30.374
  E0428 18:06:30.688149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:31.688252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 04/28/23 18:06:32.392
  E0428 18:06:32.689252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:33.689249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 04/28/23 18:06:34.409
  STEP: delete the pod with lifecycle hook @ 04/28/23 18:06:34.433
  E0428 18:06:34.689367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:35.690379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:36.690500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:37.690983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:06:38.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-7391" for this suite. @ 04/28/23 18:06:38.463
• [8.124 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 04/28/23 18:06:38.478
  Apr 28 18:06:38.478: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 18:06:38.479
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:06:38.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:06:38.503
  STEP: Creating the pod @ 04/28/23 18:06:38.507
  E0428 18:06:38.692023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:39.692119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:40.692416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:06:41.082: INFO: Successfully updated pod "labelsupdate91579185-077c-43c6-b74b-6ec0bdbc440b"
  E0428 18:06:41.693283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:42.693761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:43.694273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:44.694387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:06:45.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9638" for this suite. @ 04/28/23 18:06:45.144
• [6.674 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 04/28/23 18:06:45.153
  Apr 28 18:06:45.153: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename svcaccounts @ 04/28/23 18:06:45.154
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:06:45.179
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:06:45.182
  STEP: Creating a pod to test service account token:  @ 04/28/23 18:06:45.189
  E0428 18:06:45.698002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:46.698170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:47.698915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:48.698958      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 18:06:49.214
  Apr 28 18:06:49.218: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod test-pod-b559f3a8-3aad-45c2-87ab-23f563fdcc72 container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 18:06:49.228
  Apr 28 18:06:49.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1971" for this suite. @ 04/28/23 18:06:49.246
• [4.098 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 04/28/23 18:06:49.251
  Apr 28 18:06:49.251: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename projected @ 04/28/23 18:06:49.252
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:06:49.272
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:06:49.275
  STEP: Creating secret with name projected-secret-test-58a0203d-5b4f-43b1-8f70-f73f30f88676 @ 04/28/23 18:06:49.277
  STEP: Creating a pod to test consume secrets @ 04/28/23 18:06:49.281
  E0428 18:06:49.699522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:50.699639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:51.699744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:52.700440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 18:06:53.308
  Apr 28 18:06:53.315: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-projected-secrets-64f667ab-0d2d-4a0b-ad49-82db2bacfa9e container secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 18:06:53.325
  Apr 28 18:06:53.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5523" for this suite. @ 04/28/23 18:06:53.352
• [4.111 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 04/28/23 18:06:53.362
  Apr 28 18:06:53.362: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename services @ 04/28/23 18:06:53.363
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:06:53.381
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:06:53.384
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-3113 @ 04/28/23 18:06:53.391
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 04/28/23 18:06:53.417
  STEP: creating service externalsvc in namespace services-3113 @ 04/28/23 18:06:53.417
  STEP: creating replication controller externalsvc in namespace services-3113 @ 04/28/23 18:06:53.442
  I0428 18:06:53.463272      19 runners.go:194] Created replication controller with name: externalsvc, namespace: services-3113, replica count: 2
  E0428 18:06:53.700470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:54.700926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:55.701148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0428 18:06:56.513917      19 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 04/28/23 18:06:56.517
  Apr 28 18:06:56.533: INFO: Creating new exec pod
  E0428 18:06:56.701628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:06:57.702456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:06:58.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=services-3113 exec execpodkvclg -- /bin/sh -x -c nslookup nodeport-service.services-3113.svc.cluster.local'
  E0428 18:06:58.703319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:06:58.909: INFO: stderr: "+ nslookup nodeport-service.services-3113.svc.cluster.local\n"
  Apr 28 18:06:58.909: INFO: stdout: "Server:\t\t10.43.0.10\nAddress:\t10.43.0.10#53\n\nnodeport-service.services-3113.svc.cluster.local\tcanonical name = externalsvc.services-3113.svc.cluster.local.\nName:\texternalsvc.services-3113.svc.cluster.local\nAddress: 10.43.199.53\n\n"
  Apr 28 18:06:58.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-3113, will wait for the garbage collector to delete the pods @ 04/28/23 18:06:58.914
  Apr 28 18:06:58.975: INFO: Deleting ReplicationController externalsvc took: 7.140476ms
  Apr 28 18:06:59.075: INFO: Terminating ReplicationController externalsvc pods took: 100.978052ms
  E0428 18:06:59.703434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:00.703519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:07:01.110: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-3113" for this suite. @ 04/28/23 18:07:01.127
• [7.776 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 04/28/23 18:07:01.139
  Apr 28 18:07:01.139: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 18:07:01.14
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:07:01.157
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:07:01.16
  STEP: creating a replication controller @ 04/28/23 18:07:01.163
  Apr 28 18:07:01.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-4128 create -f -'
  E0428 18:07:01.703544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:07:02.231: INFO: stderr: ""
  Apr 28 18:07:02.231: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/28/23 18:07:02.231
  Apr 28 18:07:02.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-4128 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 28 18:07:02.314: INFO: stderr: ""
  Apr 28 18:07:02.314: INFO: stdout: "update-demo-nautilus-kbc8j update-demo-nautilus-z2qv6 "
  Apr 28 18:07:02.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-4128 get pods update-demo-nautilus-kbc8j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 28 18:07:02.420: INFO: stderr: ""
  Apr 28 18:07:02.420: INFO: stdout: ""
  Apr 28 18:07:02.420: INFO: update-demo-nautilus-kbc8j is created but not running
  E0428 18:07:02.703978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:03.704065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:04.704274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:05.705421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:06.705538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:07:07.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-4128 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 28 18:07:07.496: INFO: stderr: ""
  Apr 28 18:07:07.496: INFO: stdout: "update-demo-nautilus-kbc8j update-demo-nautilus-z2qv6 "
  Apr 28 18:07:07.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-4128 get pods update-demo-nautilus-kbc8j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 28 18:07:07.565: INFO: stderr: ""
  Apr 28 18:07:07.565: INFO: stdout: "true"
  Apr 28 18:07:07.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-4128 get pods update-demo-nautilus-kbc8j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 28 18:07:07.630: INFO: stderr: ""
  Apr 28 18:07:07.630: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 28 18:07:07.630: INFO: validating pod update-demo-nautilus-kbc8j
  Apr 28 18:07:07.636: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 28 18:07:07.636: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 28 18:07:07.636: INFO: update-demo-nautilus-kbc8j is verified up and running
  Apr 28 18:07:07.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-4128 get pods update-demo-nautilus-z2qv6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 28 18:07:07.704: INFO: stderr: ""
  Apr 28 18:07:07.705: INFO: stdout: "true"
  Apr 28 18:07:07.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-4128 get pods update-demo-nautilus-z2qv6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  E0428 18:07:07.705675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:07:07.777: INFO: stderr: ""
  Apr 28 18:07:07.777: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 28 18:07:07.777: INFO: validating pod update-demo-nautilus-z2qv6
  Apr 28 18:07:07.784: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 28 18:07:07.784: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 28 18:07:07.784: INFO: update-demo-nautilus-z2qv6 is verified up and running
  STEP: using delete to clean up resources @ 04/28/23 18:07:07.784
  Apr 28 18:07:07.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-4128 delete --grace-period=0 --force -f -'
  Apr 28 18:07:07.856: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 28 18:07:07.856: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Apr 28 18:07:07.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-4128 get rc,svc -l name=update-demo --no-headers'
  Apr 28 18:07:08.030: INFO: stderr: "No resources found in kubectl-4128 namespace.\n"
  Apr 28 18:07:08.030: INFO: stdout: ""
  Apr 28 18:07:08.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-4128 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 28 18:07:08.187: INFO: stderr: ""
  Apr 28 18:07:08.187: INFO: stdout: ""
  Apr 28 18:07:08.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4128" for this suite. @ 04/28/23 18:07:08.193
• [7.066 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 04/28/23 18:07:08.206
  Apr 28 18:07:08.206: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename pods @ 04/28/23 18:07:08.206
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:07:08.225
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:07:08.228
  STEP: creating the pod @ 04/28/23 18:07:08.231
  STEP: submitting the pod to kubernetes @ 04/28/23 18:07:08.231
  E0428 18:07:08.706228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:09.706595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 04/28/23 18:07:10.255
  STEP: updating the pod @ 04/28/23 18:07:10.258
  E0428 18:07:10.706685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:07:10.769: INFO: Successfully updated pod "pod-update-f43a6647-fb29-4648-8b97-bc1480ff5479"
  STEP: verifying the updated pod is in kubernetes @ 04/28/23 18:07:10.779
  Apr 28 18:07:10.782: INFO: Pod update OK
  Apr 28 18:07:10.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-263" for this suite. @ 04/28/23 18:07:10.785
• [2.585 seconds]
------------------------------
SS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 04/28/23 18:07:10.791
  Apr 28 18:07:10.791: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename cronjob @ 04/28/23 18:07:10.792
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:07:10.816
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:07:10.819
  STEP: Creating a suspended cronjob @ 04/28/23 18:07:10.822
  STEP: Ensuring no jobs are scheduled @ 04/28/23 18:07:10.827
  E0428 18:07:11.706788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:12.708177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:13.707989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:14.708114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:15.708241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:16.708609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:17.709216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:18.709329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:19.710052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:20.710718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:21.712142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:22.712718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:23.713221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:24.713319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:25.713453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:26.713610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:27.714729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:28.714841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:29.714965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:30.715171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:31.715370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:32.715758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:33.716245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:34.716404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:35.716510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:36.716705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:37.716985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:38.717246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:39.717454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:40.718304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:41.718429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:42.719265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:43.719353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:44.720125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:45.720552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:46.720592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:47.720985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:48.721222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:49.722258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:50.722401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:51.722498      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:52.722809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:53.722948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:54.723020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:55.723128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:56.723323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:57.724054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:58.724994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:07:59.725136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:00.725229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:01.725346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:02.726101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:03.727067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:04.727217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:05.727322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:06.727497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:07.727956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:08.728904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:09.729027      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:10.729253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:11.729312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:12.729386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:13.729447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:14.729535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:15.732791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:16.732937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:17.733448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:18.734388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:19.734483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:20.734594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:21.734714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:22.734778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:23.734861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:24.735081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:25.735196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:26.735624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:27.736011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:28.737044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:29.737310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:30.737495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:31.738197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:32.738789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:33.738881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:34.739691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:35.739790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:36.740018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:37.740841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:38.740944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:39.741187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:40.741964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:41.742071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:42.742713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:43.742785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:44.742996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:45.743112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:46.743317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:47.743883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:48.744033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:49.744684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:50.744861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:51.745342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:52.745892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:53.746403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:54.746599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:55.746721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:56.746890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:57.747653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:58.747866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:08:59.748007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:00.748208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:01.748534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:02.748968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:03.749285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:04.750284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:05.750381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:06.751435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:07.752512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:08.752608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:09.752800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:10.753039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:11.753208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:12.753814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:13.754274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:14.754500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:15.755215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:16.755420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:17.756009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:18.756117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:19.756220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:20.756327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:21.756645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:22.757708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:23.757826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:24.758701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:25.759757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:26.759867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:27.760510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:28.760907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:29.761268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:30.762280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:31.762398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:32.762977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:33.763973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:34.764256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:35.764448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:36.764649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:37.764958      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:38.765329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:39.765384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:40.765499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:41.766326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:42.767014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:43.767050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:44.767178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:45.767291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:46.767518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:47.768037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:48.768257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:49.768377      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:50.768611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:51.768829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:52.769220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:53.770104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:54.770213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:55.770309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:56.770535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:57.771006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:58.771244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:09:59.771343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:00.771538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:01.771652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:02.771785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:03.771912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:04.772038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:05.772144      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:06.772343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:07.773213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:08.774297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:09.774427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:10.774533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:11.774607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:12.775530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:13.775497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:14.775702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:15.775834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:16.775952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:17.776273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:18.776474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:19.776907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:20.777432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:21.777525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:22.777812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:23.777963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:24.778208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:25.778274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:26.778459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:27.779237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:28.779792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:29.780304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:30.780537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:31.780642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:32.781428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:33.782270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:34.782538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:35.782756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:36.783265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:37.784208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:38.787351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:39.787469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:40.787678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:41.788449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:42.788934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:43.789758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:44.789795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:45.790326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:46.790392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:47.790849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:48.791252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:49.792131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:50.793183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:51.793210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:52.793776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:53.794023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:54.794242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:55.794345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:56.794555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:57.795570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:58.795817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:10:59.796495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:00.796651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:01.796739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:02.797532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:03.798256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:04.798460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:05.799287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:06.799472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:07.800534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:08.800632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:09.801204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:10.801375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:11.801458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:12.801839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:13.801937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:14.802960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:15.803386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:16.803546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:17.804021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:18.804145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:19.804244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:20.804381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:21.805201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:22.806197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:23.806565      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:24.806656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:25.806829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:26.806945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:27.807903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:28.808211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:29.808776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:30.809516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:31.809595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:32.810392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:33.811189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:34.811275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:35.811669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:36.811872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:37.812021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:38.812744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:39.813017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:40.813489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:41.813718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:42.814312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:43.814424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:44.815852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:45.815944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:46.816208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:47.816739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:48.816980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:49.817134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:50.822881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:51.824086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:52.824569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:53.824727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:54.824952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:55.825094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:56.826006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:57.826566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:58.826704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:11:59.826985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:00.827409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:01.827264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:02.827381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:03.827505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:04.827616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:05.827728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:06.827848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:07.827983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:08.828283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:09.828456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:10.828999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring no job exists by listing jobs explicitly @ 04/28/23 18:12:10.836
  STEP: Removing cronjob @ 04/28/23 18:12:10.843
  Apr 28 18:12:10.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-7071" for this suite. @ 04/28/23 18:12:10.862
• [300.079 seconds]
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 04/28/23 18:12:10.87
  Apr 28 18:12:10.871: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename server-version @ 04/28/23 18:12:10.872
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:12:10.893
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:12:10.898
  STEP: Request ServerVersion @ 04/28/23 18:12:10.903
  STEP: Confirm major version @ 04/28/23 18:12:10.905
  Apr 28 18:12:10.905: INFO: Major version: 1
  STEP: Confirm minor version @ 04/28/23 18:12:10.905
  Apr 28 18:12:10.905: INFO: cleanMinorVersion: 27
  Apr 28 18:12:10.905: INFO: Minor version: 27
  Apr 28 18:12:10.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-3817" for this suite. @ 04/28/23 18:12:10.913
• [0.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 04/28/23 18:12:10.922
  Apr 28 18:12:10.922: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/28/23 18:12:10.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:12:10.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:12:10.963
  Apr 28 18:12:10.968: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  E0428 18:12:11.828732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:12.828842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:13.829867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:14.830469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:15.830569      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:16.830581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:12:17.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-1799" for this suite. @ 04/28/23 18:12:17.406
• [6.489 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 04/28/23 18:12:17.412
  Apr 28 18:12:17.412: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename configmap @ 04/28/23 18:12:17.413
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:12:17.434
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:12:17.437
  STEP: Creating configMap with name configmap-test-volume-f8e15017-8bcf-4b9b-abdc-4fa15d583dc8 @ 04/28/23 18:12:17.439
  STEP: Creating a pod to test consume configMaps @ 04/28/23 18:12:17.443
  E0428 18:12:17.831856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:18.831932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:19.832815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:20.833301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 18:12:21.463
  Apr 28 18:12:21.467: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-configmaps-9aa4d64b-18cd-4239-ad1f-592304667ca1 container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 18:12:21.486
  Apr 28 18:12:21.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1707" for this suite. @ 04/28/23 18:12:21.507
• [4.104 seconds]
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 04/28/23 18:12:21.516
  Apr 28 18:12:21.516: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename svcaccounts @ 04/28/23 18:12:21.517
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:12:21.533
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:12:21.536
  STEP: Creating ServiceAccount "e2e-sa-qxzbz"  @ 04/28/23 18:12:21.538
  Apr 28 18:12:21.542: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-qxzbz"  @ 04/28/23 18:12:21.543
  Apr 28 18:12:21.550: INFO: AutomountServiceAccountToken: true
  Apr 28 18:12:21.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3792" for this suite. @ 04/28/23 18:12:21.554
• [0.044 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 04/28/23 18:12:21.561
  Apr 28 18:12:21.561: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename configmap @ 04/28/23 18:12:21.562
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:12:21.58
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:12:21.594
  STEP: Creating configMap with name configmap-test-volume-map-c9f79051-92d3-452f-b054-7af6e657a800 @ 04/28/23 18:12:21.597
  STEP: Creating a pod to test consume configMaps @ 04/28/23 18:12:21.602
  E0428 18:12:21.833827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:22.834859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:23.835427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:24.835460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 18:12:25.625
  Apr 28 18:12:25.629: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-configmaps-14e7ca9e-165d-4aa0-be8f-889f6d4cae6e container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 18:12:25.638
  Apr 28 18:12:25.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3217" for this suite. @ 04/28/23 18:12:25.668
• [4.114 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 04/28/23 18:12:25.676
  Apr 28 18:12:25.676: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename field-validation @ 04/28/23 18:12:25.677
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:12:25.701
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:12:25.706
  STEP: apply creating a deployment @ 04/28/23 18:12:25.71
  Apr 28 18:12:25.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9098" for this suite. @ 04/28/23 18:12:25.743
• [0.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 04/28/23 18:12:25.752
  Apr 28 18:12:25.752: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 18:12:25.753
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:12:25.78
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:12:25.784
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 18:12:25.787
  E0428 18:12:25.835506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:26.835671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:27.836217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:28.836296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 18:12:29.829
  Apr 28 18:12:29.833: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod downwardapi-volume-2dbca927-6004-4f1a-967e-9cdf8f4d9d33 container client-container: <nil>
  E0428 18:12:29.836474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod @ 04/28/23 18:12:29.84
  Apr 28 18:12:29.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1123" for this suite. @ 04/28/23 18:12:29.863
• [4.117 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 04/28/23 18:12:29.869
  Apr 28 18:12:29.869: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename services @ 04/28/23 18:12:29.87
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:12:29.892
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:12:29.894
  STEP: creating an Endpoint @ 04/28/23 18:12:29.901
  STEP: waiting for available Endpoint @ 04/28/23 18:12:29.906
  STEP: listing all Endpoints @ 04/28/23 18:12:29.907
  STEP: updating the Endpoint @ 04/28/23 18:12:29.91
  STEP: fetching the Endpoint @ 04/28/23 18:12:29.922
  STEP: patching the Endpoint @ 04/28/23 18:12:29.926
  STEP: fetching the Endpoint @ 04/28/23 18:12:29.932
  STEP: deleting the Endpoint by Collection @ 04/28/23 18:12:29.935
  STEP: waiting for Endpoint deletion @ 04/28/23 18:12:29.964
  STEP: fetching the Endpoint @ 04/28/23 18:12:29.966
  Apr 28 18:12:29.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3361" for this suite. @ 04/28/23 18:12:29.981
• [0.122 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 04/28/23 18:12:29.993
  Apr 28 18:12:29.993: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 18:12:29.994
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:12:30.02
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:12:30.022
  STEP: Counting existing ResourceQuota @ 04/28/23 18:12:30.025
  E0428 18:12:30.837219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:31.839440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:32.840378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:33.843292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:34.844218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/28/23 18:12:35.028
  STEP: Ensuring resource quota status is calculated @ 04/28/23 18:12:35.034
  E0428 18:12:35.845059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:36.845597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 04/28/23 18:12:37.041
  STEP: Creating a NodePort Service @ 04/28/23 18:12:37.066
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 04/28/23 18:12:37.118
  STEP: Ensuring resource quota status captures service creation @ 04/28/23 18:12:37.138
  E0428 18:12:37.846553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:38.846665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 04/28/23 18:12:39.142
  STEP: Ensuring resource quota status released usage @ 04/28/23 18:12:39.204
  E0428 18:12:39.846731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:40.846834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:12:41.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1059" for this suite. @ 04/28/23 18:12:41.213
• [11.226 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 04/28/23 18:12:41.221
  Apr 28 18:12:41.221: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/28/23 18:12:41.223
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:12:41.247
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:12:41.255
  Apr 28 18:12:41.263: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  E0428 18:12:41.847680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:12:42.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6469" for this suite. @ 04/28/23 18:12:42.301
• [1.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 04/28/23 18:12:42.311
  Apr 28 18:12:42.311: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename webhook @ 04/28/23 18:12:42.313
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:12:42.33
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:12:42.336
  STEP: Setting up server cert @ 04/28/23 18:12:42.358
  E0428 18:12:42.848452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 18:12:43.337
  STEP: Deploying the webhook pod @ 04/28/23 18:12:43.346
  STEP: Wait for the deployment to be ready @ 04/28/23 18:12:43.357
  Apr 28 18:12:43.363: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0428 18:12:43.850669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:44.850767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 18:12:45.379
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 18:12:45.39
  E0428 18:12:45.850864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:12:46.390: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 04/28/23 18:12:46.394
  STEP: Creating a custom resource definition that should be denied by the webhook @ 04/28/23 18:12:46.416
  Apr 28 18:12:46.416: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 18:12:46.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6491" for this suite. @ 04/28/23 18:12:46.559
  STEP: Destroying namespace "webhook-markers-9067" for this suite. @ 04/28/23 18:12:46.566
• [4.261 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 04/28/23 18:12:46.573
  Apr 28 18:12:46.573: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename svcaccounts @ 04/28/23 18:12:46.578
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:12:46.593
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:12:46.596
  E0428 18:12:46.851394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:47.851504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 04/28/23 18:12:48.635
  Apr 28 18:12:48.635: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1303 pod-service-account-c40d7a9a-a477-4567-a6e2-d71b3d097e10 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  E0428 18:12:48.852228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 04/28/23 18:12:48.855
  Apr 28 18:12:48.855: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1303 pod-service-account-c40d7a9a-a477-4567-a6e2-d71b3d097e10 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 04/28/23 18:12:49.06
  Apr 28 18:12:49.060: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1303 pod-service-account-c40d7a9a-a477-4567-a6e2-d71b3d097e10 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Apr 28 18:12:49.214: INFO: Got root ca configmap in namespace "svcaccounts-1303"
  Apr 28 18:12:49.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1303" for this suite. @ 04/28/23 18:12:49.226
• [2.659 seconds]
------------------------------
S
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 04/28/23 18:12:49.232
  Apr 28 18:12:49.232: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename deployment @ 04/28/23 18:12:49.232
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:12:49.254
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:12:49.262
  STEP: creating a Deployment @ 04/28/23 18:12:49.268
  STEP: waiting for Deployment to be created @ 04/28/23 18:12:49.273
  STEP: waiting for all Replicas to be Ready @ 04/28/23 18:12:49.275
  Apr 28 18:12:49.277: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 28 18:12:49.277: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 28 18:12:49.283: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 28 18:12:49.284: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 28 18:12:49.303: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 28 18:12:49.303: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 28 18:12:49.322: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 28 18:12:49.322: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E0428 18:12:49.852891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:12:50.663: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Apr 28 18:12:50.663: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Apr 28 18:12:50.782: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 04/28/23 18:12:50.782
  W0428 18:12:50.791916      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 28 18:12:50.793: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 04/28/23 18:12:50.794
  Apr 28 18:12:50.796: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 0
  Apr 28 18:12:50.797: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 0
  Apr 28 18:12:50.797: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 0
  Apr 28 18:12:50.801: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 0
  Apr 28 18:12:50.801: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 0
  Apr 28 18:12:50.801: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 0
  Apr 28 18:12:50.802: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 0
  Apr 28 18:12:50.802: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 0
  Apr 28 18:12:50.802: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 1
  Apr 28 18:12:50.802: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 1
  Apr 28 18:12:50.803: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 2
  Apr 28 18:12:50.803: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 2
  Apr 28 18:12:50.803: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 2
  Apr 28 18:12:50.803: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 2
  Apr 28 18:12:50.839: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 2
  Apr 28 18:12:50.839: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 2
  E0428 18:12:50.854721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:12:50.972: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 2
  Apr 28 18:12:50.972: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 2
  Apr 28 18:12:50.989: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 1
  Apr 28 18:12:50.989: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 1
  Apr 28 18:12:50.996: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 1
  Apr 28 18:12:50.996: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 1
  E0428 18:12:51.854085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:12:52.681: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 2
  Apr 28 18:12:52.681: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 2
  Apr 28 18:12:52.707: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 1
  STEP: listing Deployments @ 04/28/23 18:12:52.708
  Apr 28 18:12:52.711: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 04/28/23 18:12:52.711
  Apr 28 18:12:52.725: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 04/28/23 18:12:52.725
  Apr 28 18:12:52.752: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 28 18:12:52.753: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 28 18:12:52.772: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 28 18:12:52.780: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  E0428 18:12:52.854808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:12:53.692: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 28 18:12:53.725: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 28 18:12:53.791: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 28 18:12:53.824: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0428 18:12:53.855388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:12:54.804: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 04/28/23 18:12:54.838
  STEP: fetching the DeploymentStatus @ 04/28/23 18:12:54.845
  Apr 28 18:12:54.849: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 1
  Apr 28 18:12:54.850: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 1
  Apr 28 18:12:54.850: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 1
  Apr 28 18:12:54.850: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 1
  Apr 28 18:12:54.850: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 2
  Apr 28 18:12:54.850: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 3
  Apr 28 18:12:54.850: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 2
  Apr 28 18:12:54.850: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 2
  Apr 28 18:12:54.850: INFO: observed Deployment test-deployment in namespace deployment-5793 with ReadyReplicas 3
  STEP: deleting the Deployment @ 04/28/23 18:12:54.85
  E0428 18:12:54.855391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:12:54.859: INFO: observed event type MODIFIED
  Apr 28 18:12:54.859: INFO: observed event type MODIFIED
  Apr 28 18:12:54.859: INFO: observed event type MODIFIED
  Apr 28 18:12:54.859: INFO: observed event type MODIFIED
  Apr 28 18:12:54.859: INFO: observed event type MODIFIED
  Apr 28 18:12:54.859: INFO: observed event type MODIFIED
  Apr 28 18:12:54.859: INFO: observed event type MODIFIED
  Apr 28 18:12:54.859: INFO: observed event type MODIFIED
  Apr 28 18:12:54.859: INFO: observed event type MODIFIED
  Apr 28 18:12:54.859: INFO: observed event type MODIFIED
  Apr 28 18:12:54.864: INFO: Log out all the ReplicaSets if there is no deployment created
  Apr 28 18:12:54.868: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-5793  e572f4fb-0a04-4c4f-9e9d-47aede8e21de 301469 3 2023-04-28 18:12:49 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 6190f373-25e4-4016-81fd-d43e7a3806fa 0xc004a96af7 0xc004a96af8}] [] [{kube-controller-manager Update apps/v1 2023-04-28 18:12:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6190f373-25e4-4016-81fd-d43e7a3806fa\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-28 18:12:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a96b80 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Apr 28 18:12:54.872: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-5793  bce78df8-4d03-48bb-b24d-19bae961261c 301566 4 2023-04-28 18:12:50 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 6190f373-25e4-4016-81fd-d43e7a3806fa 0xc004a96be7 0xc004a96be8}] [] [{kube-controller-manager Update apps/v1 2023-04-28 18:12:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6190f373-25e4-4016-81fd-d43e7a3806fa\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-28 18:12:54 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a96c70 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Apr 28 18:12:54.876: INFO: pod: "test-deployment-5b5dcbcd95-gtvmk":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-gtvmk test-deployment-5b5dcbcd95- deployment-5793  5d5a6a34-41e9-41c1-896b-13cb4d1bc288 301563 0 2023-04-28 18:12:52 +0000 UTC 2023-04-28 18:12:55 +0000 UTC 0xc004a97008 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[cni.projectcalico.org/containerID:d95bbf67ed6ea9b9eb347186935a3d5c18bc19ef0d9a716a7f2f0b5d0d0ab30b cni.projectcalico.org/podIP:10.42.1.240/32 cni.projectcalico.org/podIPs:10.42.1.240/32] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 bce78df8-4d03-48bb-b24d-19bae961261c 0xc004a97057 0xc004a97058}] [] [{kube-controller-manager Update v1 2023-04-28 18:12:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bce78df8-4d03-48bb-b24d-19bae961261c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-28 18:12:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-28 18:12:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.1.240\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7zrzf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7zrzf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-161.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 18:12:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 18:12:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 18:12:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 18:12:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.161,PodIP:10.42.1.240,StartTime:2023-04-28 18:12:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 18:12:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://81bf3035775af1d6241a539d5e354402825ae972a988739578511a66c727bbec,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.1.240,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 28 18:12:54.877: INFO: pod: "test-deployment-5b5dcbcd95-ltfvq":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-ltfvq test-deployment-5b5dcbcd95- deployment-5793  8ce3e509-fd93-49cb-8875-8deb98272bcf 301522 0 2023-04-28 18:12:50 +0000 UTC 2023-04-28 18:12:54 +0000 UTC 0xc004a97260 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[cni.projectcalico.org/containerID:58d4d69386d41754e30588e97ea740f7a64bd144f286b901b30a4226e2c9db16 cni.projectcalico.org/podIP:10.42.3.61/32 cni.projectcalico.org/podIPs:10.42.3.61/32] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 bce78df8-4d03-48bb-b24d-19bae961261c 0xc004a972b7 0xc004a972b8}] [] [{kube-controller-manager Update v1 2023-04-28 18:12:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bce78df8-4d03-48bb-b24d-19bae961261c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-28 18:12:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-28 18:12:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-frl2g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-frl2g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-127.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 18:12:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 18:12:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 18:12:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 18:12:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.127,PodIP:10.42.3.61,StartTime:2023-04-28 18:12:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 18:12:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://dcb8021077fce94edc97065062c9d77a197e859845e91cd21d4f32284ffe4b10,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.61,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 28 18:12:54.877: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-5793  2d6ae753-2074-4b9c-ae5f-4502fe568e9c 301559 2 2023-04-28 18:12:52 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 6190f373-25e4-4016-81fd-d43e7a3806fa 0xc004a96cd7 0xc004a96cd8}] [] [{kube-controller-manager Update apps/v1 2023-04-28 18:12:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6190f373-25e4-4016-81fd-d43e7a3806fa\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-28 18:12:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a96d70 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  Apr 28 18:12:54.881: INFO: pod: "test-deployment-6fc78d85c6-lnqn4":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-lnqn4 test-deployment-6fc78d85c6- deployment-5793  75d85b19-00d8-42bb-8f3e-cb13016f9538 301517 0 2023-04-28 18:12:52 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[cni.projectcalico.org/containerID:0b7d52b617a24b7eb841a9b03b23ab9004fd48165926efc908a1678a968d0ce1 cni.projectcalico.org/podIP:10.42.3.62/32 cni.projectcalico.org/podIPs:10.42.3.62/32] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 2d6ae753-2074-4b9c-ae5f-4502fe568e9c 0xc006fdcf27 0xc006fdcf28}] [] [{kube-controller-manager Update v1 2023-04-28 18:12:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d6ae753-2074-4b9c-ae5f-4502fe568e9c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-28 18:12:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-28 18:12:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-888gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-888gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-9-127.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 18:12:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 18:12:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 18:12:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 18:12:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.9.127,PodIP:10.42.3.62,StartTime:2023-04-28 18:12:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 18:12:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://80d99b0debb88f401b4f1cc7ba578d00f4093550b220685c3acbd906d47b8f07,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.62,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 28 18:12:54.882: INFO: pod: "test-deployment-6fc78d85c6-nmgvt":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-nmgvt test-deployment-6fc78d85c6- deployment-5793  1ad800d2-b780-433e-91f4-fc7c9b5be5ac 301558 0 2023-04-28 18:12:53 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[cni.projectcalico.org/containerID:637fd78d50a771c624db6eba2ccc5ee75037d1207ad647543242469f75231b59 cni.projectcalico.org/podIP:10.42.2.32/32 cni.projectcalico.org/podIPs:10.42.2.32/32] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 2d6ae753-2074-4b9c-ae5f-4502fe568e9c 0xc006fdd147 0xc006fdd148}] [] [{kube-controller-manager Update v1 2023-04-28 18:12:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2d6ae753-2074-4b9c-ae5f-4502fe568e9c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-28 18:12:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-28 18:12:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2jdps,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2jdps,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-6-71.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 18:12:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 18:12:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 18:12:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-28 18:12:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.6.71,PodIP:10.42.2.32,StartTime:2023-04-28 18:12:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-28 18:12:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1b37706ba13709bb950ea98da968cfee4ede5e6a84afba795de9d127e362304c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.32,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 28 18:12:54.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5793" for this suite. @ 04/28/23 18:12:54.887
• [5.667 seconds]
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 04/28/23 18:12:54.899
  Apr 28 18:12:54.899: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename field-validation @ 04/28/23 18:12:54.9
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:12:54.951
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:12:54.954
  Apr 28 18:12:54.957: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  W0428 18:12:54.958465      19 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc00559dd50 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E0428 18:12:55.855555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:56.855650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0428 18:12:57.028876      19 warnings.go:70] unknown field "alpha"
  W0428 18:12:57.028899      19 warnings.go:70] unknown field "beta"
  W0428 18:12:57.028905      19 warnings.go:70] unknown field "delta"
  W0428 18:12:57.028911      19 warnings.go:70] unknown field "epsilon"
  W0428 18:12:57.028917      19 warnings.go:70] unknown field "gamma"
  Apr 28 18:12:57.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5696" for this suite. @ 04/28/23 18:12:57.059
• [2.169 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 04/28/23 18:12:57.071
  Apr 28 18:12:57.071: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename gc @ 04/28/23 18:12:57.072
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:12:57.09
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:12:57.093
  STEP: create the rc @ 04/28/23 18:12:57.096
  W0428 18:12:57.105004      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0428 18:12:57.855911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:58.856344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:12:59.856913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:00.857042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:01.857244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 04/28/23 18:13:02.11
  STEP: wait for all pods to be garbage collected @ 04/28/23 18:13:02.118
  E0428 18:13:02.857858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:03.858005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:04.858076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:05.858227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:06.858308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 04/28/23 18:13:07.126
  Apr 28 18:13:07.203: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 28 18:13:07.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8196" for this suite. @ 04/28/23 18:13:07.21
• [10.151 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 04/28/23 18:13:07.223
  Apr 28 18:13:07.223: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename kubelet-test @ 04/28/23 18:13:07.224
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:13:07.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:13:07.244
  E0428 18:13:07.858482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:08.858577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:13:09.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8872" for this suite. @ 04/28/23 18:13:09.279
• [2.180 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 04/28/23 18:13:09.403
  Apr 28 18:13:09.403: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename webhook @ 04/28/23 18:13:09.404
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:13:09.42
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:13:09.424
  STEP: Setting up server cert @ 04/28/23 18:13:09.448
  E0428 18:13:09.859236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 18:13:10.333
  E0428 18:13:10.859384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook pod @ 04/28/23 18:13:10.969
  STEP: Wait for the deployment to be ready @ 04/28/23 18:13:10.982
  Apr 28 18:13:10.992: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0428 18:13:11.860319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:12.860408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 18:13:13.001
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 18:13:13.011
  E0428 18:13:13.860628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:13:14.011: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 04/28/23 18:13:14.072
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/28/23 18:13:14.139
  STEP: Deleting the collection of validation webhooks @ 04/28/23 18:13:14.278
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/28/23 18:13:14.333
  Apr 28 18:13:14.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3287" for this suite. @ 04/28/23 18:13:14.397
  STEP: Destroying namespace "webhook-markers-8587" for this suite. @ 04/28/23 18:13:14.407
• [5.012 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 04/28/23 18:13:14.432
  Apr 28 18:13:14.432: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename dns @ 04/28/23 18:13:14.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:13:14.452
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:13:14.455
  STEP: Creating a test headless service @ 04/28/23 18:13:14.457
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5745.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5745.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5745.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5745.svc.cluster.local;sleep 1; done
   @ 04/28/23 18:13:14.464
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5745.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5745.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5745.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5745.svc.cluster.local;sleep 1; done
   @ 04/28/23 18:13:14.464
  STEP: creating a pod to probe DNS @ 04/28/23 18:13:14.464
  STEP: submitting the pod to kubernetes @ 04/28/23 18:13:14.464
  E0428 18:13:14.861679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:15.861843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/28/23 18:13:16.49
  STEP: looking for the results for each expected name from probers @ 04/28/23 18:13:16.493
  Apr 28 18:13:16.506: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:16.510: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:16.514: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:16.517: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:16.520: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:16.523: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:16.526: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:16.529: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:16.529: INFO: Lookups using dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5745.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5745.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local jessie_udp@dns-test-service-2.dns-5745.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5745.svc.cluster.local]

  E0428 18:13:16.862700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:17.863526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:18.863657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:19.863948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:20.871762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:13:21.534: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:21.537: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:21.541: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:21.544: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:21.548: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:21.552: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:21.555: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:21.559: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:21.559: INFO: Lookups using dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5745.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5745.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local jessie_udp@dns-test-service-2.dns-5745.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5745.svc.cluster.local]

  E0428 18:13:21.869366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:22.869481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:23.869570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:24.869718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:25.869830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:13:26.534: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:26.538: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:26.542: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:26.549: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:26.554: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:26.559: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:26.566: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:26.570: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:26.570: INFO: Lookups using dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5745.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5745.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local jessie_udp@dns-test-service-2.dns-5745.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5745.svc.cluster.local]

  E0428 18:13:26.870278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:27.871342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:28.871736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:29.871608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:30.871823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:13:31.533: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:31.536: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:31.539: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:31.542: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:31.545: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:31.548: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:31.550: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:31.553: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:31.553: INFO: Lookups using dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5745.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5745.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local jessie_udp@dns-test-service-2.dns-5745.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5745.svc.cluster.local]

  E0428 18:13:31.871983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:32.872677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:33.872740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:34.873590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:35.873577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:13:36.537: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:36.546: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:36.552: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:36.556: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:36.560: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:36.563: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:36.567: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:36.571: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:36.571: INFO: Lookups using dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5745.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5745.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local jessie_udp@dns-test-service-2.dns-5745.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5745.svc.cluster.local]

  E0428 18:13:36.874449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:37.874772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:38.874894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:39.875168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:40.877379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:13:41.533: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:41.536: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:41.540: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:41.544: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:41.547: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:41.550: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:41.553: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:41.556: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5745.svc.cluster.local from pod dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad: the server could not find the requested resource (get pods dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad)
  Apr 28 18:13:41.556: INFO: Lookups using dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5745.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5745.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5745.svc.cluster.local jessie_udp@dns-test-service-2.dns-5745.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5745.svc.cluster.local]

  E0428 18:13:41.878179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:42.878957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:43.879576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:44.881361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:45.881327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:13:46.555: INFO: DNS probes using dns-5745/dns-test-c966c792-77f3-48ca-ae35-2ecb6d6885ad succeeded

  Apr 28 18:13:46.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/28/23 18:13:46.559
  STEP: deleting the test headless service @ 04/28/23 18:13:46.582
  STEP: Destroying namespace "dns-5745" for this suite. @ 04/28/23 18:13:46.646
• [32.252 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 04/28/23 18:13:46.69
  Apr 28 18:13:46.690: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename proxy @ 04/28/23 18:13:46.692
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:13:46.714
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:13:46.719
  Apr 28 18:13:46.723: INFO: Creating pod...
  E0428 18:13:46.882304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:47.882868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:13:48.749: INFO: Creating service...
  Apr 28 18:13:48.758: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-9760/pods/agnhost/proxy?method=DELETE
  Apr 28 18:13:48.771: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 28 18:13:48.771: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-9760/pods/agnhost/proxy?method=OPTIONS
  Apr 28 18:13:48.775: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 28 18:13:48.775: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-9760/pods/agnhost/proxy?method=PATCH
  Apr 28 18:13:48.778: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 28 18:13:48.778: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-9760/pods/agnhost/proxy?method=POST
  Apr 28 18:13:48.781: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 28 18:13:48.781: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-9760/pods/agnhost/proxy?method=PUT
  Apr 28 18:13:48.783: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 28 18:13:48.783: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-9760/services/e2e-proxy-test-service/proxy?method=DELETE
  Apr 28 18:13:48.788: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 28 18:13:48.788: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-9760/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Apr 28 18:13:48.794: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 28 18:13:48.794: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-9760/services/e2e-proxy-test-service/proxy?method=PATCH
  Apr 28 18:13:48.799: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 28 18:13:48.799: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-9760/services/e2e-proxy-test-service/proxy?method=POST
  Apr 28 18:13:48.803: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 28 18:13:48.803: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-9760/services/e2e-proxy-test-service/proxy?method=PUT
  Apr 28 18:13:48.807: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 28 18:13:48.807: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-9760/pods/agnhost/proxy?method=GET
  Apr 28 18:13:48.809: INFO: http.Client request:GET StatusCode:301
  Apr 28 18:13:48.809: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-9760/services/e2e-proxy-test-service/proxy?method=GET
  Apr 28 18:13:48.813: INFO: http.Client request:GET StatusCode:301
  Apr 28 18:13:48.813: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-9760/pods/agnhost/proxy?method=HEAD
  Apr 28 18:13:48.815: INFO: http.Client request:HEAD StatusCode:301
  Apr 28 18:13:48.815: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-9760/services/e2e-proxy-test-service/proxy?method=HEAD
  Apr 28 18:13:48.820: INFO: http.Client request:HEAD StatusCode:301
  Apr 28 18:13:48.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-9760" for this suite. @ 04/28/23 18:13:48.823
• [2.140 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 04/28/23 18:13:48.836
  Apr 28 18:13:48.836: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename pod-network-test @ 04/28/23 18:13:48.837
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:13:48.855
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:13:48.858
  STEP: Performing setup for networking test in namespace pod-network-test-5238 @ 04/28/23 18:13:48.86
  STEP: creating a selector @ 04/28/23 18:13:48.86
  STEP: Creating the service pods in kubernetes @ 04/28/23 18:13:48.86
  Apr 28 18:13:48.860: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0428 18:13:48.883180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:49.883315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:50.883499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:51.885547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:52.885681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:53.885774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:54.885976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:55.886112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:56.886275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:57.887102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:58.887224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:13:59.887367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:00.887516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 04/28/23 18:14:00.984
  E0428 18:14:01.888633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:02.888878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:14:03.004: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
  Apr 28 18:14:03.004: INFO: Breadth first check of 10.42.1.241 on host 172.31.11.161...
  Apr 28 18:14:03.010: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.69:9080/dial?request=hostname&protocol=http&host=10.42.1.241&port=8083&tries=1'] Namespace:pod-network-test-5238 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 18:14:03.010: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 18:14:03.011: INFO: ExecWithOptions: Clientset creation
  Apr 28 18:14:03.011: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-5238/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.69%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.42.1.241%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 28 18:14:03.092: INFO: Waiting for responses: map[]
  Apr 28 18:14:03.092: INFO: reached 10.42.1.241 after 0/1 tries
  Apr 28 18:14:03.092: INFO: Breadth first check of 10.42.0.227 on host 172.31.14.195...
  Apr 28 18:14:03.096: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.69:9080/dial?request=hostname&protocol=http&host=10.42.0.227&port=8083&tries=1'] Namespace:pod-network-test-5238 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 18:14:03.096: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 18:14:03.096: INFO: ExecWithOptions: Clientset creation
  Apr 28 18:14:03.097: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-5238/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.69%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.42.0.227%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 28 18:14:03.185: INFO: Waiting for responses: map[]
  Apr 28 18:14:03.185: INFO: reached 10.42.0.227 after 0/1 tries
  Apr 28 18:14:03.185: INFO: Breadth first check of 10.42.2.34 on host 172.31.6.71...
  Apr 28 18:14:03.189: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.69:9080/dial?request=hostname&protocol=http&host=10.42.2.34&port=8083&tries=1'] Namespace:pod-network-test-5238 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 18:14:03.189: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 18:14:03.189: INFO: ExecWithOptions: Clientset creation
  Apr 28 18:14:03.190: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-5238/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.69%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.42.2.34%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 28 18:14:03.275: INFO: Waiting for responses: map[]
  Apr 28 18:14:03.275: INFO: reached 10.42.2.34 after 0/1 tries
  Apr 28 18:14:03.275: INFO: Breadth first check of 10.42.3.68 on host 172.31.9.127...
  Apr 28 18:14:03.278: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.69:9080/dial?request=hostname&protocol=http&host=10.42.3.68&port=8083&tries=1'] Namespace:pod-network-test-5238 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 18:14:03.278: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 18:14:03.279: INFO: ExecWithOptions: Clientset creation
  Apr 28 18:14:03.279: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-5238/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.69%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.42.3.68%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 28 18:14:03.387: INFO: Waiting for responses: map[]
  Apr 28 18:14:03.387: INFO: reached 10.42.3.68 after 0/1 tries
  Apr 28 18:14:03.387: INFO: Going to retry 0 out of 4 pods....
  Apr 28 18:14:03.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-5238" for this suite. @ 04/28/23 18:14:03.396
• [14.568 seconds]
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 04/28/23 18:14:03.404
  Apr 28 18:14:03.404: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 18:14:03.405
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:14:03.421
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:14:03.428
  STEP: validating api versions @ 04/28/23 18:14:03.432
  Apr 28 18:14:03.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-6209 api-versions'
  Apr 28 18:14:03.533: INFO: stderr: ""
  Apr 28 18:14:03.533: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nhelm.cattle.io/v1\nk3s.cattle.io/v1\nmetrics.k8s.io/v1beta1\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nv1\n"
  Apr 28 18:14:03.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6209" for this suite. @ 04/28/23 18:14:03.549
• [0.172 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 04/28/23 18:14:03.581
  Apr 28 18:14:03.581: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename tables @ 04/28/23 18:14:03.581
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:14:03.647
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:14:03.651
  Apr 28 18:14:03.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-6122" for this suite. @ 04/28/23 18:14:03.668
• [0.105 seconds]
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 04/28/23 18:14:03.686
  Apr 28 18:14:03.686: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename var-expansion @ 04/28/23 18:14:03.687
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:14:03.719
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:14:03.731
  STEP: Creating a pod to test substitution in volume subpath @ 04/28/23 18:14:03.734
  E0428 18:14:03.889140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:04.889216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:05.889328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:06.890283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 18:14:07.785
  Apr 28 18:14:07.788: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod var-expansion-86ea86ea-8fcc-4143-9ee9-beef590ebfee container dapi-container: <nil>
  STEP: delete the pod @ 04/28/23 18:14:07.795
  Apr 28 18:14:07.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5788" for this suite. @ 04/28/23 18:14:07.827
• [4.150 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 04/28/23 18:14:07.839
  Apr 28 18:14:07.839: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename init-container @ 04/28/23 18:14:07.841
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:14:07.859
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:14:07.863
  STEP: creating the pod @ 04/28/23 18:14:07.865
  Apr 28 18:14:07.865: INFO: PodSpec: initContainers in spec.initContainers
  E0428 18:14:07.898498      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:08.894648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:09.894732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:10.895632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:11.895535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:14:12.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-9416" for this suite. @ 04/28/23 18:14:12.005
• [4.172 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 04/28/23 18:14:12.012
  Apr 28 18:14:12.012: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename pods @ 04/28/23 18:14:12.013
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:14:12.028
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:14:12.03
  STEP: creating a Pod with a static label @ 04/28/23 18:14:12.041
  STEP: watching for Pod to be ready @ 04/28/23 18:14:12.048
  Apr 28 18:14:12.050: INFO: observed Pod pod-test in namespace pods-1309 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Apr 28 18:14:12.053: INFO: observed Pod pod-test in namespace pods-1309 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 18:14:12 +0000 UTC  }]
  Apr 28 18:14:12.068: INFO: observed Pod pod-test in namespace pods-1309 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 18:14:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 18:14:12 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 18:14:12 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 18:14:12 +0000 UTC  }]
  Apr 28 18:14:12.515: INFO: observed Pod pod-test in namespace pods-1309 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 18:14:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 18:14:12 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-28 18:14:12 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 18:14:12 +0000 UTC  }]
  E0428 18:14:12.895807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:13.896409      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:14:14.015: INFO: Found Pod pod-test in namespace pods-1309 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 18:14:12 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 18:14:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 18:14:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-28 18:14:12 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 04/28/23 18:14:14.022
  STEP: getting the Pod and ensuring that it's patched @ 04/28/23 18:14:14.039
  STEP: replacing the Pod's status Ready condition to False @ 04/28/23 18:14:14.042
  STEP: check the Pod again to ensure its Ready conditions are False @ 04/28/23 18:14:14.053
  STEP: deleting the Pod via a Collection with a LabelSelector @ 04/28/23 18:14:14.053
  STEP: watching for the Pod to be deleted @ 04/28/23 18:14:14.06
  Apr 28 18:14:14.066: INFO: observed event type MODIFIED
  E0428 18:14:14.896575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:15.896887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:14:16.018: INFO: observed event type MODIFIED
  Apr 28 18:14:16.324: INFO: observed event type MODIFIED
  Apr 28 18:14:16.382: INFO: observed event type MODIFIED
  E0428 18:14:16.900202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:14:17.026: INFO: observed event type MODIFIED
  Apr 28 18:14:17.039: INFO: observed event type MODIFIED
  Apr 28 18:14:17.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1309" for this suite. @ 04/28/23 18:14:17.057
• [5.058 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 04/28/23 18:14:17.073
  Apr 28 18:14:17.073: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename downward-api @ 04/28/23 18:14:17.074
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:14:17.103
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:14:17.108
  STEP: Creating a pod to test downward API volume plugin @ 04/28/23 18:14:17.111
  E0428 18:14:17.901367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:18.901380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:19.901515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:20.901616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 18:14:21.142
  Apr 28 18:14:21.145: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod downwardapi-volume-c33e8047-4a74-41dd-aee1-b99d1a3dce86 container client-container: <nil>
  STEP: delete the pod @ 04/28/23 18:14:21.151
  Apr 28 18:14:21.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7241" for this suite. @ 04/28/23 18:14:21.169
• [4.101 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 04/28/23 18:14:21.175
  Apr 28 18:14:21.175: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/28/23 18:14:21.175
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:14:21.19
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:14:21.193
  Apr 28 18:14:21.196: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  E0428 18:14:21.902103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:22.903033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:23.904065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:14:24.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-8212" for this suite. @ 04/28/23 18:14:24.27
• [3.102 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 04/28/23 18:14:24.279
  Apr 28 18:14:24.279: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename var-expansion @ 04/28/23 18:14:24.28
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:14:24.298
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:14:24.3
  E0428 18:14:24.905136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:25.905219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:14:26.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 18:14:26.326: INFO: Deleting pod "var-expansion-9b1d75cb-2d63-4369-9c97-bc5815cb8a9d" in namespace "var-expansion-5327"
  Apr 28 18:14:26.344: INFO: Wait up to 5m0s for pod "var-expansion-9b1d75cb-2d63-4369-9c97-bc5815cb8a9d" to be fully deleted
  E0428 18:14:26.905340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:27.905715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-5327" for this suite. @ 04/28/23 18:14:28.372
• [4.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 04/28/23 18:14:28.382
  Apr 28 18:14:28.382: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename kubectl @ 04/28/23 18:14:28.382
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:14:28.403
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:14:28.406
  STEP: validating cluster-info @ 04/28/23 18:14:28.41
  Apr 28 18:14:28.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-386887975 --namespace=kubectl-859 cluster-info'
  Apr 28 18:14:28.475: INFO: stderr: ""
  Apr 28 18:14:28.475: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.43.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Apr 28 18:14:28.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-859" for this suite. @ 04/28/23 18:14:28.484
• [0.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 04/28/23 18:14:28.499
  Apr 28 18:14:28.499: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 18:14:28.501
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:14:28.575
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:14:28.587
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 04/28/23 18:14:28.59
  E0428 18:14:28.906153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:29.906252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:30.906839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:31.906972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 18:14:32.618
  Apr 28 18:14:32.625: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-dfc24be8-d105-4fe6-9fad-eb4a5137149f container test-container: <nil>
  STEP: delete the pod @ 04/28/23 18:14:32.635
  Apr 28 18:14:32.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5219" for this suite. @ 04/28/23 18:14:32.669
• [4.176 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 04/28/23 18:14:32.679
  Apr 28 18:14:32.679: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename field-validation @ 04/28/23 18:14:32.683
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:14:32.7
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:14:32.702
  STEP: apply creating a deployment @ 04/28/23 18:14:32.706
  Apr 28 18:14:32.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5732" for this suite. @ 04/28/23 18:14:32.723
• [0.052 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 04/28/23 18:14:32.731
  Apr 28 18:14:32.731: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename hostport @ 04/28/23 18:14:32.732
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:14:32.748
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:14:32.752
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 04/28/23 18:14:32.76
  E0428 18:14:32.907364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:33.907918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.14.195 on the node which pod1 resides and expect scheduled @ 04/28/23 18:14:34.781
  E0428 18:14:34.908520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:35.908617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.14.195 but use UDP protocol on the node which pod2 resides @ 04/28/23 18:14:36.792
  E0428 18:14:36.909244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:37.909716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:38.910296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:39.910374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 04/28/23 18:14:40.839
  Apr 28 18:14:40.839: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.14.195 http://127.0.0.1:54323/hostname] Namespace:hostport-1160 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 18:14:40.839: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 18:14:40.839: INFO: ExecWithOptions: Clientset creation
  Apr 28 18:14:40.839: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/hostport-1160/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.14.195+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0428 18:14:40.914914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.14.195, port: 54323 @ 04/28/23 18:14:41.716
  Apr 28 18:14:41.716: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.14.195:54323/hostname] Namespace:hostport-1160 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 18:14:41.716: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 18:14:41.716: INFO: ExecWithOptions: Clientset creation
  Apr 28 18:14:41.716: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/hostport-1160/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.14.195%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.14.195, port: 54323 UDP @ 04/28/23 18:14:41.799
  Apr 28 18:14:41.799: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.14.195 54323] Namespace:hostport-1160 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 28 18:14:41.799: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  Apr 28 18:14:41.800: INFO: ExecWithOptions: Clientset creation
  Apr 28 18:14:41.800: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/hostport-1160/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.14.195+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0428 18:14:41.915185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:42.915773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:43.915978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:44.916102      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:45.916212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:14:46.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-1160" for this suite. @ 04/28/23 18:14:46.901
• [14.176 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 04/28/23 18:14:46.908
  Apr 28 18:14:46.908: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename webhook @ 04/28/23 18:14:46.909
  E0428 18:14:46.916304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:14:46.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:14:46.927
  STEP: Setting up server cert @ 04/28/23 18:14:46.949
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/28/23 18:14:47.548
  STEP: Deploying the webhook pod @ 04/28/23 18:14:47.556
  STEP: Wait for the deployment to be ready @ 04/28/23 18:14:47.569
  Apr 28 18:14:47.576: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0428 18:14:47.917032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:48.917230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/28/23 18:14:49.584
  STEP: Verifying the service has paired with the endpoint @ 04/28/23 18:14:49.593
  E0428 18:14:49.918227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:14:50.594: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 28 18:14:50.597: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  E0428 18:14:50.918788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4798-crds.webhook.example.com via the AdmissionRegistration API @ 04/28/23 18:14:51.11
  STEP: Creating a custom resource while v1 is storage version @ 04/28/23 18:14:51.149
  E0428 18:14:51.918922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:52.919276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 04/28/23 18:14:53.192
  STEP: Patching the custom resource while v2 is storage version @ 04/28/23 18:14:53.212
  Apr 28 18:14:53.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7941" for this suite. @ 04/28/23 18:14:53.881
  STEP: Destroying namespace "webhook-markers-741" for this suite. @ 04/28/23 18:14:53.887
• [6.984 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 04/28/23 18:14:53.893
  Apr 28 18:14:53.893: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename containers @ 04/28/23 18:14:53.896
  E0428 18:14:53.919305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:14:53.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:14:53.927
  STEP: Creating a pod to test override all @ 04/28/23 18:14:53.933
  E0428 18:14:54.919378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:55.919529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:56.919626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:57.920043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 18:14:57.987
  Apr 28 18:14:57.990: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod client-containers-b093b5a9-8077-4384-9a20-c28bc4a5014f container agnhost-container: <nil>
  STEP: delete the pod @ 04/28/23 18:14:57.999
  Apr 28 18:14:58.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-8249" for this suite. @ 04/28/23 18:14:58.03
• [4.153 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 04/28/23 18:14:58.048
  Apr 28 18:14:58.048: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename emptydir @ 04/28/23 18:14:58.05
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:14:58.094
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:14:58.101
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 04/28/23 18:14:58.11
  E0428 18:14:58.920260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:14:59.920367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:00.921305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:01.921376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 18:15:02.175
  Apr 28 18:15:02.179: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-609ec348-7f5c-48c8-90c2-ac78254f6905 container test-container: <nil>
  STEP: delete the pod @ 04/28/23 18:15:02.19
  Apr 28 18:15:02.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6138" for this suite. @ 04/28/23 18:15:02.215
• [4.173 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 04/28/23 18:15:02.223
  Apr 28 18:15:02.224: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename cronjob @ 04/28/23 18:15:02.224
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:15:02.262
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:15:02.265
  STEP: Creating a cronjob @ 04/28/23 18:15:02.268
  STEP: Ensuring more than one job is running at a time @ 04/28/23 18:15:02.273
  E0428 18:15:02.922083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:03.922324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:04.923282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:05.923428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:06.923564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:07.924020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:08.924293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:09.924419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:10.926115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:11.927048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:12.927964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:13.928213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:14.929345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:15.929213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:16.929360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:17.929572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:18.929742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:19.929829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:20.933758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:21.932061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:22.932630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:23.932723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:24.933767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:25.934061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:26.934739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:27.934865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:28.935332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:29.935768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:30.936041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:31.936267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:32.937135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:33.937263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:34.937360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:35.937476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:36.938665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:37.939090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:38.939858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:39.940316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:40.940957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:41.941106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:42.941843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:43.942513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:44.942566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:45.942910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:46.943194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:47.943697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:48.943816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:49.944182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:50.945376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:51.945441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:52.946269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:53.946391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:54.947243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:55.947414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:56.950066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:57.951114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:58.951866      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:15:59.952070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:00.952124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:01.952222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:02.953134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:03.953240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:04.954285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:05.954480      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:06.955267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:07.955429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:08.956466      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:09.956677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:10.957163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:11.957268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:12.958042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:13.958175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:14.959164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:15.959223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:16.960813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:17.960473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:18.960470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:19.960687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:20.961741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:21.961854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:22.962278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:23.962375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:24.962865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:25.962978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:26.963816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:27.964089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:28.964222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:29.964317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:30.964446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:31.964664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:32.964874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:33.965032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:34.965081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:35.965214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:36.966015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:37.966181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:38.966178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:39.966299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:40.967555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:41.967681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:42.968428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:43.968814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:44.969221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:45.969326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:46.970205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:47.971188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:48.972130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:49.972242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:50.973093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:51.973540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:52.974023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:53.974130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:54.974260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:55.974518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:56.975027      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:57.975418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:58.975518      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:16:59.975725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 04/28/23 18:17:00.277
  STEP: Removing cronjob @ 04/28/23 18:17:00.282
  Apr 28 18:17:00.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-9306" for this suite. @ 04/28/23 18:17:00.345
• [118.178 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 04/28/23 18:17:00.465
  Apr 28 18:17:00.465: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename disruption @ 04/28/23 18:17:00.474
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:17:00.503
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:17:00.512
  STEP: Creating a kubernetes client @ 04/28/23 18:17:00.52
  Apr 28 18:17:00.520: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename disruption-2 @ 04/28/23 18:17:00.52
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:17:00.568
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:17:00.585
  STEP: Waiting for the pdb to be processed @ 04/28/23 18:17:00.622
  E0428 18:17:00.976251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:01.976233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 04/28/23 18:17:02.636
  E0428 18:17:02.977209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:03.977338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 04/28/23 18:17:04.647
  E0428 18:17:04.977875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:05.977996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: listing a collection of PDBs across all namespaces @ 04/28/23 18:17:06.659
  STEP: listing a collection of PDBs in namespace disruption-5200 @ 04/28/23 18:17:06.664
  STEP: deleting a collection of PDBs @ 04/28/23 18:17:06.679
  STEP: Waiting for the PDB collection to be deleted @ 04/28/23 18:17:06.701
  Apr 28 18:17:06.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 28 18:17:06.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-7409" for this suite. @ 04/28/23 18:17:06.713
  STEP: Destroying namespace "disruption-5200" for this suite. @ 04/28/23 18:17:06.719
• [6.260 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 04/28/23 18:17:06.726
  Apr 28 18:17:06.726: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename secrets @ 04/28/23 18:17:06.728
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:17:06.745
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:17:06.75
  STEP: Creating secret with name secret-test-70982cda-37ce-4dda-ac59-c656abbcd041 @ 04/28/23 18:17:06.756
  STEP: Creating a pod to test consume secrets @ 04/28/23 18:17:06.762
  E0428 18:17:06.978477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:07.979096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:08.980167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:09.980293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/28/23 18:17:10.794
  Apr 28 18:17:10.802: INFO: Trying to get logs from node ip-172-31-9-127.us-east-2.compute.internal pod pod-secrets-56065fdc-6f13-4559-81d7-df4055dcdae0 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/28/23 18:17:10.82
  Apr 28 18:17:10.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5608" for this suite. @ 04/28/23 18:17:10.848
• [4.130 seconds]
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 04/28/23 18:17:10.857
  Apr 28 18:17:10.858: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename container-runtime @ 04/28/23 18:17:10.859
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:17:10.89
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:17:10.898
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 04/28/23 18:17:10.913
  E0428 18:17:10.980999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:11.981199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:12.981664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:13.981784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:14.982454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:15.983517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:16.983723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:17.983758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:18.984655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:19.984957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:20.985099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:21.985237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:22.985323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:23.985419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:24.985601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:25.986227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:26.986615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 04/28/23 18:17:27.332
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 04/28/23 18:17:27.336
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 04/28/23 18:17:27.342
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 04/28/23 18:17:27.342
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 04/28/23 18:17:27.398
  E0428 18:17:27.987887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:28.988741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:29.989227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 04/28/23 18:17:30.418
  E0428 18:17:30.997682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 04/28/23 18:17:31.445
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 04/28/23 18:17:31.452
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 04/28/23 18:17:31.452
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 04/28/23 18:17:31.471
  E0428 18:17:31.998319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 04/28/23 18:17:32.479
  E0428 18:17:32.999136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:33.999801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 04/28/23 18:17:34.742
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 04/28/23 18:17:34.748
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 04/28/23 18:17:34.748
  Apr 28 18:17:34.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-1755" for this suite. @ 04/28/23 18:17:34.778
• [23.927 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 04/28/23 18:17:34.786
  Apr 28 18:17:34.786: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename resourcequota @ 04/28/23 18:17:34.786
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:17:34.806
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:17:34.809
  STEP: Counting existing ResourceQuota @ 04/28/23 18:17:34.811
  E0428 18:17:34.999868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:36.000056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:37.000596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:38.003596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:39.003983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/28/23 18:17:39.815
  STEP: Ensuring resource quota status is calculated @ 04/28/23 18:17:39.822
  E0428 18:17:40.004402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:41.004628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:17:41.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8840" for this suite. @ 04/28/23 18:17:41.833
• [7.054 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 04/28/23 18:17:41.841
  Apr 28 18:17:41.841: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename services @ 04/28/23 18:17:41.843
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:17:41.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:17:41.87
  STEP: creating a collection of services @ 04/28/23 18:17:41.872
  Apr 28 18:17:41.872: INFO: Creating e2e-svc-a-smg27
  Apr 28 18:17:41.884: INFO: Creating e2e-svc-b-dz2lx
  Apr 28 18:17:41.897: INFO: Creating e2e-svc-c-tknq6
  STEP: deleting service collection @ 04/28/23 18:17:41.926
  Apr 28 18:17:41.959: INFO: Collection of services has been deleted
  Apr 28 18:17:41.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9083" for this suite. @ 04/28/23 18:17:41.963
• [0.129 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 04/28/23 18:17:41.974
  Apr 28 18:17:41.974: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename namespaces @ 04/28/23 18:17:41.975
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:17:41.989
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:17:41.992
  STEP: Read namespace status @ 04/28/23 18:17:41.995
  Apr 28 18:17:41.998: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 04/28/23 18:17:41.998
  Apr 28 18:17:42.004: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 04/28/23 18:17:42.004
  E0428 18:17:42.005164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:17:42.013: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Apr 28 18:17:42.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4938" for this suite. @ 04/28/23 18:17:42.017
• [0.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 04/28/23 18:17:42.028
  Apr 28 18:17:42.028: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename replicaset @ 04/28/23 18:17:42.029
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:17:42.045
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:17:42.048
  Apr 28 18:17:42.068: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0428 18:17:43.005309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:44.005853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:45.006075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:46.006292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0428 18:17:47.006451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:17:47.072: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/28/23 18:17:47.072
  STEP: Scaling up "test-rs" replicaset  @ 04/28/23 18:17:47.072
  Apr 28 18:17:47.088: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 04/28/23 18:17:47.089
  W0428 18:17:47.105839      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 28 18:17:47.120: INFO: observed ReplicaSet test-rs in namespace replicaset-7381 with ReadyReplicas 1, AvailableReplicas 1
  Apr 28 18:17:47.122: INFO: observed ReplicaSet test-rs in namespace replicaset-7381 with ReadyReplicas 1, AvailableReplicas 1
  Apr 28 18:17:47.141: INFO: observed ReplicaSet test-rs in namespace replicaset-7381 with ReadyReplicas 1, AvailableReplicas 1
  Apr 28 18:17:47.149: INFO: observed ReplicaSet test-rs in namespace replicaset-7381 with ReadyReplicas 1, AvailableReplicas 1
  E0428 18:17:48.007094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 28 18:17:48.407: INFO: observed ReplicaSet test-rs in namespace replicaset-7381 with ReadyReplicas 2, AvailableReplicas 2
  Apr 28 18:17:48.433: INFO: observed Replicaset test-rs in namespace replicaset-7381 with ReadyReplicas 3 found true
  Apr 28 18:17:48.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7381" for this suite. @ 04/28/23 18:17:48.438
• [6.415 seconds]
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 04/28/23 18:17:48.443
  Apr 28 18:17:48.443: INFO: >>> kubeConfig: /tmp/kubeconfig-386887975
  STEP: Building a namespace api object, basename kubelet-test @ 04/28/23 18:17:48.444
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/28/23 18:17:48.46
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/28/23 18:17:48.464
  Apr 28 18:17:48.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-2752" for this suite. @ 04/28/23 18:17:48.501
• [0.065 seconds]
------------------------------
SS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Apr 28 18:17:48.510: INFO: Running AfterSuite actions on node 1
  Apr 28 18:17:48.510: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.152 seconds]
------------------------------

Ran 378 of 7207 Specs in 6025.645 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h40m26.422370254s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

