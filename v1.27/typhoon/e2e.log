  I0418 00:15:15.266551      21 e2e.go:117] Starting e2e run "3f27e2f9-bd1e-4da4-8323-1cbff9cd81a4" on Ginkgo node 1
  Apr 18 00:15:15.293: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1681776915 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Apr 18 00:15:15.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 00:15:15.507: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Apr 18 00:15:15.540: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Apr 18 00:15:15.559: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
  Apr 18 00:15:15.559: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
  Apr 18 00:15:15.559: INFO: e2e test version: v1.27.1
  Apr 18 00:15:15.563: INFO: kube-apiserver version: v1.27.1
  Apr 18 00:15:15.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 00:15:15.572: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 04/18/23 00:15:15.873
  Apr 18 00:15:15.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename configmap @ 04/18/23 00:15:15.874
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:15:15.893
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:15:15.898
  STEP: creating a ConfigMap @ 04/18/23 00:15:15.901
  STEP: fetching the ConfigMap @ 04/18/23 00:15:15.906
  STEP: patching the ConfigMap @ 04/18/23 00:15:15.909
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 04/18/23 00:15:15.914
  STEP: deleting the ConfigMap by collection with a label selector @ 04/18/23 00:15:15.918
  STEP: listing all ConfigMaps in test namespace @ 04/18/23 00:15:15.923
  Apr 18 00:15:15.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5622" for this suite. @ 04/18/23 00:15:15.928
• [0.060 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 04/18/23 00:15:15.934
  Apr 18 00:15:15.934: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename discovery @ 04/18/23 00:15:15.935
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:15:15.948
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:15:15.954
  STEP: Setting up server cert @ 04/18/23 00:15:15.958
  Apr 18 00:15:16.237: INFO: Checking APIGroup: apiregistration.k8s.io
  Apr 18 00:15:16.238: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Apr 18 00:15:16.238: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Apr 18 00:15:16.238: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Apr 18 00:15:16.238: INFO: Checking APIGroup: apps
  Apr 18 00:15:16.239: INFO: PreferredVersion.GroupVersion: apps/v1
  Apr 18 00:15:16.239: INFO: Versions found [{apps/v1 v1}]
  Apr 18 00:15:16.239: INFO: apps/v1 matches apps/v1
  Apr 18 00:15:16.239: INFO: Checking APIGroup: events.k8s.io
  Apr 18 00:15:16.240: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Apr 18 00:15:16.240: INFO: Versions found [{events.k8s.io/v1 v1}]
  Apr 18 00:15:16.240: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Apr 18 00:15:16.240: INFO: Checking APIGroup: authentication.k8s.io
  Apr 18 00:15:16.242: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Apr 18 00:15:16.242: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Apr 18 00:15:16.242: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Apr 18 00:15:16.242: INFO: Checking APIGroup: authorization.k8s.io
  Apr 18 00:15:16.243: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Apr 18 00:15:16.243: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Apr 18 00:15:16.243: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Apr 18 00:15:16.243: INFO: Checking APIGroup: autoscaling
  Apr 18 00:15:16.244: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Apr 18 00:15:16.244: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Apr 18 00:15:16.244: INFO: autoscaling/v2 matches autoscaling/v2
  Apr 18 00:15:16.244: INFO: Checking APIGroup: batch
  Apr 18 00:15:16.246: INFO: PreferredVersion.GroupVersion: batch/v1
  Apr 18 00:15:16.246: INFO: Versions found [{batch/v1 v1}]
  Apr 18 00:15:16.246: INFO: batch/v1 matches batch/v1
  Apr 18 00:15:16.246: INFO: Checking APIGroup: certificates.k8s.io
  Apr 18 00:15:16.247: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Apr 18 00:15:16.247: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Apr 18 00:15:16.247: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Apr 18 00:15:16.247: INFO: Checking APIGroup: networking.k8s.io
  Apr 18 00:15:16.249: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Apr 18 00:15:16.249: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Apr 18 00:15:16.249: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Apr 18 00:15:16.249: INFO: Checking APIGroup: policy
  Apr 18 00:15:16.250: INFO: PreferredVersion.GroupVersion: policy/v1
  Apr 18 00:15:16.250: INFO: Versions found [{policy/v1 v1}]
  Apr 18 00:15:16.250: INFO: policy/v1 matches policy/v1
  Apr 18 00:15:16.250: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Apr 18 00:15:16.251: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Apr 18 00:15:16.251: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Apr 18 00:15:16.251: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Apr 18 00:15:16.251: INFO: Checking APIGroup: storage.k8s.io
  Apr 18 00:15:16.253: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Apr 18 00:15:16.253: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Apr 18 00:15:16.253: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Apr 18 00:15:16.253: INFO: Checking APIGroup: admissionregistration.k8s.io
  Apr 18 00:15:16.254: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Apr 18 00:15:16.254: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Apr 18 00:15:16.254: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Apr 18 00:15:16.254: INFO: Checking APIGroup: apiextensions.k8s.io
  Apr 18 00:15:16.255: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Apr 18 00:15:16.255: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Apr 18 00:15:16.255: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Apr 18 00:15:16.255: INFO: Checking APIGroup: scheduling.k8s.io
  Apr 18 00:15:16.256: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Apr 18 00:15:16.257: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Apr 18 00:15:16.257: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Apr 18 00:15:16.257: INFO: Checking APIGroup: coordination.k8s.io
  Apr 18 00:15:16.258: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Apr 18 00:15:16.258: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Apr 18 00:15:16.258: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Apr 18 00:15:16.258: INFO: Checking APIGroup: node.k8s.io
  Apr 18 00:15:16.259: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Apr 18 00:15:16.259: INFO: Versions found [{node.k8s.io/v1 v1}]
  Apr 18 00:15:16.259: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Apr 18 00:15:16.259: INFO: Checking APIGroup: discovery.k8s.io
  Apr 18 00:15:16.260: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Apr 18 00:15:16.260: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Apr 18 00:15:16.260: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Apr 18 00:15:16.260: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Apr 18 00:15:16.262: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Apr 18 00:15:16.262: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Apr 18 00:15:16.262: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Apr 18 00:15:16.262: INFO: Checking APIGroup: crd.projectcalico.org
  Apr 18 00:15:16.263: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
  Apr 18 00:15:16.263: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
  Apr 18 00:15:16.263: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
  Apr 18 00:15:16.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-4152" for this suite. @ 04/18/23 00:15:16.266
• [0.337 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 04/18/23 00:15:16.272
  Apr 18 00:15:16.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename watch @ 04/18/23 00:15:16.273
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:15:16.287
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:15:16.293
  STEP: creating a watch on configmaps @ 04/18/23 00:15:16.296
  STEP: creating a new configmap @ 04/18/23 00:15:16.298
  STEP: modifying the configmap once @ 04/18/23 00:15:16.302
  STEP: closing the watch once it receives two notifications @ 04/18/23 00:15:16.309
  Apr 18 00:15:16.309: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-310  18cfc354-cb05-4b20-927e-fc495ec47eb7 40575 0 2023-04-18 00:15:16 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-18 00:15:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 18 00:15:16.309: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-310  18cfc354-cb05-4b20-927e-fc495ec47eb7 40576 0 2023-04-18 00:15:16 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-18 00:15:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 04/18/23 00:15:16.309
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 04/18/23 00:15:16.315
  STEP: deleting the configmap @ 04/18/23 00:15:16.317
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 04/18/23 00:15:16.322
  Apr 18 00:15:16.322: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-310  18cfc354-cb05-4b20-927e-fc495ec47eb7 40577 0 2023-04-18 00:15:16 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-18 00:15:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 18 00:15:16.322: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-310  18cfc354-cb05-4b20-927e-fc495ec47eb7 40578 0 2023-04-18 00:15:16 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-18 00:15:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 18 00:15:16.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-310" for this suite. @ 04/18/23 00:15:16.325
• [0.058 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 04/18/23 00:15:16.331
  Apr 18 00:15:16.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 00:15:16.331
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:15:16.346
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:15:16.351
  STEP: Creating a pod to test downward API volume plugin @ 04/18/23 00:15:16.354
  STEP: Saw pod success @ 04/18/23 00:15:22.38
  Apr 18 00:15:22.386: INFO: Trying to get logs from node ip-10-0-14-154 pod downwardapi-volume-18c0185e-2bca-4606-8270-c96809a28a38 container client-container: <nil>
  STEP: delete the pod @ 04/18/23 00:15:22.419
  Apr 18 00:15:22.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6255" for this suite. @ 04/18/23 00:15:22.451
• [6.130 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 04/18/23 00:15:22.465
  Apr 18 00:15:22.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename job @ 04/18/23 00:15:22.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:15:22.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:15:22.504
  STEP: Creating Indexed job @ 04/18/23 00:15:22.509
  STEP: Ensuring job reaches completions @ 04/18/23 00:15:22.516
  STEP: Ensuring pods with index for job exist @ 04/18/23 00:15:32.52
  Apr 18 00:15:32.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7934" for this suite. @ 04/18/23 00:15:32.527
• [10.071 seconds]
------------------------------
SS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 04/18/23 00:15:32.536
  Apr 18 00:15:32.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename configmap @ 04/18/23 00:15:32.536
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:15:32.567
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:15:32.571
  STEP: Creating configMap with name configmap-test-volume-5f2bab35-0e6a-46d3-a3c2-10856c421d6a @ 04/18/23 00:15:32.58
  STEP: Creating a pod to test consume configMaps @ 04/18/23 00:15:32.584
  STEP: Saw pod success @ 04/18/23 00:15:36.619
  Apr 18 00:15:36.622: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-configmaps-f72b4bae-3ffd-4e61-8875-629be97f369a container agnhost-container: <nil>
  STEP: delete the pod @ 04/18/23 00:15:36.629
  Apr 18 00:15:36.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2107" for this suite. @ 04/18/23 00:15:36.648
• [4.121 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 04/18/23 00:15:36.661
  Apr 18 00:15:36.661: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename aggregator @ 04/18/23 00:15:36.662
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:15:36.679
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:15:36.683
  Apr 18 00:15:36.687: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Registering the sample API server. @ 04/18/23 00:15:36.687
  Apr 18 00:15:37.898: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Apr 18 00:15:37.935: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
  Apr 18 00:15:40.015: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 0, 15, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 18 00:15:42.020: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 0, 15, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 18 00:15:44.024: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 0, 15, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 18 00:15:46.021: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 0, 15, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 18 00:15:48.020: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 0, 15, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 18 00:15:50.021: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 0, 15, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 18 00:15:52.020: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 0, 15, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 18 00:15:54.021: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 0, 15, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 18 00:15:56.021: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 0, 15, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 18 00:15:58.022: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 0, 15, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 18 00:16:00.021: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 0, 15, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 15, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 18 00:16:02.232: INFO: Waited 198.002227ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 04/18/23 00:16:02.454
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 04/18/23 00:16:02.459
  STEP: List APIServices @ 04/18/23 00:16:02.481
  Apr 18 00:16:02.514: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 04/18/23 00:16:02.515
  Apr 18 00:16:02.560: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 04/18/23 00:16:02.56
  Apr 18 00:16:02.612: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.April, 18, 0, 16, 2, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 04/18/23 00:16:02.612
  Apr 18 00:16:02.642: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-04-18 00:16:02 +0000 UTC Passed all checks passed}
  Apr 18 00:16:02.642: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 18 00:16:02.642: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 04/18/23 00:16:02.642
  Apr 18 00:16:02.672: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-1835399651" @ 04/18/23 00:16:02.672
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 04/18/23 00:16:02.752
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 04/18/23 00:16:02.786
  STEP: Patch APIService Status @ 04/18/23 00:16:02.798
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 04/18/23 00:16:02.82
  Apr 18 00:16:02.846: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-04-18 00:16:02 +0000 UTC Passed all checks passed}
  Apr 18 00:16:02.846: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 18 00:16:02.846: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Apr 18 00:16:02.847: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 04/18/23 00:16:02.847
  STEP: Confirm that the generated APIService has been deleted @ 04/18/23 00:16:02.872
  Apr 18 00:16:02.872: INFO: Requesting list of APIServices to confirm quantity
  Apr 18 00:16:02.880: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Apr 18 00:16:02.880: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Apr 18 00:16:03.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-9254" for this suite. @ 04/18/23 00:16:03.276
• [26.623 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 04/18/23 00:16:03.292
  Apr 18 00:16:03.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename svcaccounts @ 04/18/23 00:16:03.293
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:16:03.337
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:16:03.345
  STEP: creating a ServiceAccount @ 04/18/23 00:16:03.351
  STEP: watching for the ServiceAccount to be added @ 04/18/23 00:16:03.364
  STEP: patching the ServiceAccount @ 04/18/23 00:16:03.367
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 04/18/23 00:16:03.379
  STEP: deleting the ServiceAccount @ 04/18/23 00:16:03.385
  Apr 18 00:16:03.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6272" for this suite. @ 04/18/23 00:16:03.414
• [0.144 seconds]
------------------------------
SS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 04/18/23 00:16:03.437
  Apr 18 00:16:03.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename dns @ 04/18/23 00:16:03.437
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:16:03.482
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:16:03.487
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6752.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6752.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 04/18/23 00:16:03.494
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6752.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6752.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 04/18/23 00:16:03.494
  STEP: creating a pod to probe /etc/hosts @ 04/18/23 00:16:03.494
  STEP: submitting the pod to kubernetes @ 04/18/23 00:16:03.494
  STEP: retrieving the pod @ 04/18/23 00:16:11.528
  STEP: looking for the results for each expected name from probers @ 04/18/23 00:16:11.531
  Apr 18 00:16:11.557: INFO: DNS probes using dns-6752/dns-test-4cda876b-eab3-43d3-b79d-f05d4e8400c2 succeeded

  Apr 18 00:16:11.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/18/23 00:16:11.56
  STEP: Destroying namespace "dns-6752" for this suite. @ 04/18/23 00:16:11.577
• [8.147 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 04/18/23 00:16:11.584
  Apr 18 00:16:11.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename gc @ 04/18/23 00:16:11.585
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:16:11.608
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:16:11.611
  STEP: create the rc @ 04/18/23 00:16:11.619
  W0418 00:16:11.624137      21 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 04/18/23 00:16:15.67
  STEP: wait for the rc to be deleted @ 04/18/23 00:16:15.721
  Apr 18 00:16:16.774: INFO: 80 pods remaining
  Apr 18 00:16:16.774: INFO: 80 pods has nil DeletionTimestamp
  Apr 18 00:16:16.774: INFO: 
  Apr 18 00:16:17.766: INFO: 73 pods remaining
  Apr 18 00:16:17.766: INFO: 72 pods has nil DeletionTimestamp
  Apr 18 00:16:17.766: INFO: 
  Apr 18 00:16:18.740: INFO: 60 pods remaining
  Apr 18 00:16:18.740: INFO: 60 pods has nil DeletionTimestamp
  Apr 18 00:16:18.740: INFO: 
  Apr 18 00:16:19.742: INFO: 40 pods remaining
  Apr 18 00:16:19.742: INFO: 40 pods has nil DeletionTimestamp
  Apr 18 00:16:19.742: INFO: 
  Apr 18 00:16:20.946: INFO: 31 pods remaining
  Apr 18 00:16:21.025: INFO: 31 pods has nil DeletionTimestamp
  Apr 18 00:16:21.025: INFO: 
  Apr 18 00:16:21.754: INFO: 20 pods remaining
  Apr 18 00:16:21.754: INFO: 20 pods has nil DeletionTimestamp
  Apr 18 00:16:21.754: INFO: 
  Apr 18 00:16:22.780: INFO: 0 pods remaining
  Apr 18 00:16:22.780: INFO: 0 pods has nil DeletionTimestamp
  Apr 18 00:16:22.780: INFO: 
  STEP: Gathering metrics @ 04/18/23 00:16:23.729
  Apr 18 00:16:23.970: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 18 00:16:23.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1800" for this suite. @ 04/18/23 00:16:23.988
• [12.422 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 04/18/23 00:16:24.007
  Apr 18 00:16:24.007: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename endpointslice @ 04/18/23 00:16:24.016
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:16:24.049
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:16:24.059
  Apr 18 00:16:24.087: INFO: Endpoints addresses: [10.0.5.176] , ports: [6443]
  Apr 18 00:16:24.087: INFO: EndpointSlices addresses: [10.0.5.176] , ports: [6443]
  Apr 18 00:16:24.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-1450" for this suite. @ 04/18/23 00:16:24.095
• [0.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 04/18/23 00:16:24.109
  Apr 18 00:16:24.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename security-context-test @ 04/18/23 00:16:24.121
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:16:24.145
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:16:24.151
  Apr 18 00:16:40.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-7278" for this suite. @ 04/18/23 00:16:40.257
• [16.157 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 04/18/23 00:16:40.272
  Apr 18 00:16:40.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename configmap @ 04/18/23 00:16:40.273
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:16:40.295
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:16:40.299
  STEP: Creating configMap with name configmap-test-volume-map-9d965548-3d62-43db-8b3e-3e44e57c2ad9 @ 04/18/23 00:16:40.303
  STEP: Creating a pod to test consume configMaps @ 04/18/23 00:16:40.307
  STEP: Saw pod success @ 04/18/23 00:16:44.351
  Apr 18 00:16:44.354: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-configmaps-c02b56cd-573d-4c4d-94ff-e92845c8ac19 container agnhost-container: <nil>
  STEP: delete the pod @ 04/18/23 00:16:44.36
  Apr 18 00:16:44.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5747" for this suite. @ 04/18/23 00:16:44.377
• [4.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 04/18/23 00:16:44.384
  Apr 18 00:16:44.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename webhook @ 04/18/23 00:16:44.389
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:16:44.405
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:16:44.41
  STEP: Setting up server cert @ 04/18/23 00:16:44.437
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/18/23 00:16:45.147
  STEP: Deploying the webhook pod @ 04/18/23 00:16:45.162
  STEP: Wait for the deployment to be ready @ 04/18/23 00:16:45.18
  Apr 18 00:16:45.235: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/18/23 00:16:47.247
  STEP: Verifying the service has paired with the endpoint @ 04/18/23 00:16:47.262
  Apr 18 00:16:48.262: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 04/18/23 00:16:48.265
  STEP: Creating a custom resource definition that should be denied by the webhook @ 04/18/23 00:16:48.287
  Apr 18 00:16:48.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 00:16:48.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3928" for this suite. @ 04/18/23 00:16:48.351
  STEP: Destroying namespace "webhook-markers-4360" for this suite. @ 04/18/23 00:16:48.359
• [3.985 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 04/18/23 00:16:48.371
  Apr 18 00:16:48.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename services @ 04/18/23 00:16:48.372
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:16:48.389
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:16:48.394
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-6033 @ 04/18/23 00:16:48.398
  STEP: changing the ExternalName service to type=NodePort @ 04/18/23 00:16:48.403
  STEP: creating replication controller externalname-service in namespace services-6033 @ 04/18/23 00:16:48.425
  I0418 00:16:48.436705      21 runners.go:194] Created replication controller with name: externalname-service, namespace: services-6033, replica count: 2
  I0418 00:16:51.487888      21 runners.go:194] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0418 00:16:54.488898      21 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 18 00:16:54.489: INFO: Creating new exec pod
  Apr 18 00:16:57.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-6033 exec execpodsdzs2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 18 00:16:57.769: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 18 00:16:57.769: INFO: stdout: "externalname-service-x8klz"
  Apr 18 00:16:57.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-6033 exec execpodsdzs2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.3.250.224 80'
  Apr 18 00:16:57.994: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.3.250.224 80\nConnection to 10.3.250.224 80 port [tcp/http] succeeded!\n"
  Apr 18 00:16:57.994: INFO: stdout: ""
  Apr 18 00:16:58.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-6033 exec execpodsdzs2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.3.250.224 80'
  Apr 18 00:16:59.229: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.3.250.224 80\nConnection to 10.3.250.224 80 port [tcp/http] succeeded!\n"
  Apr 18 00:16:59.229: INFO: stdout: ""
  Apr 18 00:16:59.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-6033 exec execpodsdzs2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.3.250.224 80'
  Apr 18 00:17:00.198: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.3.250.224 80\nConnection to 10.3.250.224 80 port [tcp/http] succeeded!\n"
  Apr 18 00:17:00.198: INFO: stdout: ""
  Apr 18 00:17:00.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-6033 exec execpodsdzs2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.3.250.224 80'
  Apr 18 00:17:01.236: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.3.250.224 80\nConnection to 10.3.250.224 80 port [tcp/http] succeeded!\n"
  Apr 18 00:17:01.237: INFO: stdout: ""
  Apr 18 00:17:01.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-6033 exec execpodsdzs2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.3.250.224 80'
  Apr 18 00:17:02.488: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.3.250.224 80\nConnection to 10.3.250.224 80 port [tcp/http] succeeded!\n"
  Apr 18 00:17:02.488: INFO: stdout: "externalname-service-rcc28"
  Apr 18 00:17:02.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-6033 exec execpodsdzs2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.14.154 32620'
  Apr 18 00:17:02.708: INFO: stderr: "+ nc -v -t -w 2 10.0.14.154 32620\n+ echo hostName\nConnection to 10.0.14.154 32620 port [tcp/*] succeeded!\n"
  Apr 18 00:17:02.708: INFO: stdout: "externalname-service-x8klz"
  Apr 18 00:17:02.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-6033 exec execpodsdzs2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.27.81 32620'
  Apr 18 00:17:02.939: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 10.0.27.81 32620\nConnection to 10.0.27.81 32620 port [tcp/*] succeeded!\n"
  Apr 18 00:17:02.939: INFO: stdout: "externalname-service-x8klz"
  Apr 18 00:17:02.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 18 00:17:02.948: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-6033" for this suite. @ 04/18/23 00:17:03.015
• [14.657 seconds]
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 04/18/23 00:17:03.028
  Apr 18 00:17:03.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename watch @ 04/18/23 00:17:03.029
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:17:03.085
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:17:03.101
  STEP: creating a watch on configmaps with a certain label @ 04/18/23 00:17:03.11
  STEP: creating a new configmap @ 04/18/23 00:17:03.114
  STEP: modifying the configmap once @ 04/18/23 00:17:03.131
  STEP: changing the label value of the configmap @ 04/18/23 00:17:03.17
  STEP: Expecting to observe a delete notification for the watched object @ 04/18/23 00:17:03.202
  Apr 18 00:17:03.202: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9976  c4cbd34f-7a59-4d31-a104-1586426c3ce3 42737 0 2023-04-18 00:17:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-18 00:17:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 18 00:17:03.202: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9976  c4cbd34f-7a59-4d31-a104-1586426c3ce3 42739 0 2023-04-18 00:17:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-18 00:17:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 18 00:17:03.203: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9976  c4cbd34f-7a59-4d31-a104-1586426c3ce3 42740 0 2023-04-18 00:17:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-18 00:17:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 04/18/23 00:17:03.203
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 04/18/23 00:17:03.223
  STEP: changing the label value of the configmap back @ 04/18/23 00:17:13.226
  STEP: modifying the configmap a third time @ 04/18/23 00:17:13.241
  STEP: deleting the configmap @ 04/18/23 00:17:13.248
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 04/18/23 00:17:13.254
  Apr 18 00:17:13.254: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9976  c4cbd34f-7a59-4d31-a104-1586426c3ce3 42804 0 2023-04-18 00:17:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-18 00:17:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 18 00:17:13.254: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9976  c4cbd34f-7a59-4d31-a104-1586426c3ce3 42805 0 2023-04-18 00:17:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-18 00:17:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 18 00:17:13.255: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9976  c4cbd34f-7a59-4d31-a104-1586426c3ce3 42806 0 2023-04-18 00:17:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-18 00:17:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 18 00:17:13.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9976" for this suite. @ 04/18/23 00:17:13.262
• [10.243 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 04/18/23 00:17:13.272
  Apr 18 00:17:13.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename subpath @ 04/18/23 00:17:13.273
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:17:13.292
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:17:13.305
  STEP: Setting up data @ 04/18/23 00:17:13.308
  STEP: Creating pod pod-subpath-test-configmap-5tj4 @ 04/18/23 00:17:13.317
  STEP: Creating a pod to test atomic-volume-subpath @ 04/18/23 00:17:13.317
  STEP: Saw pod success @ 04/18/23 00:17:37.382
  Apr 18 00:17:37.385: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-subpath-test-configmap-5tj4 container test-container-subpath-configmap-5tj4: <nil>
  STEP: delete the pod @ 04/18/23 00:17:37.39
  STEP: Deleting pod pod-subpath-test-configmap-5tj4 @ 04/18/23 00:17:37.402
  Apr 18 00:17:37.402: INFO: Deleting pod "pod-subpath-test-configmap-5tj4" in namespace "subpath-5234"
  Apr 18 00:17:37.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5234" for this suite. @ 04/18/23 00:17:37.41
• [24.145 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 04/18/23 00:17:37.418
  Apr 18 00:17:37.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 00:17:37.421
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:17:37.441
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:17:37.45
  STEP: Creating configMap with name configmap-projected-all-test-volume-381a8005-62f6-4067-aa83-b1bcc11f52cd @ 04/18/23 00:17:37.459
  STEP: Creating secret with name secret-projected-all-test-volume-59d225d7-43f9-4b30-8dc3-9945e4b9c7b0 @ 04/18/23 00:17:37.47
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 04/18/23 00:17:37.481
  STEP: Saw pod success @ 04/18/23 00:17:41.53
  Apr 18 00:17:41.533: INFO: Trying to get logs from node ip-10-0-14-154 pod projected-volume-79e36c88-6092-4a4c-91ff-c55655d4d1ca container projected-all-volume-test: <nil>
  STEP: delete the pod @ 04/18/23 00:17:41.538
  Apr 18 00:17:41.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5402" for this suite. @ 04/18/23 00:17:41.562
• [4.155 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 04/18/23 00:17:41.578
  Apr 18 00:17:41.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename daemonsets @ 04/18/23 00:17:41.579
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:17:41.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:17:41.62
  STEP: Creating a simple DaemonSet "daemon-set" @ 04/18/23 00:17:41.641
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/18/23 00:17:41.648
  Apr 18 00:17:41.653: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:17:41.657: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 00:17:41.657: INFO: Node ip-10-0-14-154 is running 0 daemon pod, expected 1
  Apr 18 00:17:42.662: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:17:42.676: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 00:17:42.676: INFO: Node ip-10-0-14-154 is running 0 daemon pod, expected 1
  Apr 18 00:17:43.661: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:17:43.664: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 00:17:43.664: INFO: Node ip-10-0-14-154 is running 0 daemon pod, expected 1
  Apr 18 00:17:44.661: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:17:44.664: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 00:17:44.664: INFO: Node ip-10-0-14-154 is running 0 daemon pod, expected 1
  Apr 18 00:17:45.663: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:17:45.674: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 00:17:45.674: INFO: Node ip-10-0-14-154 is running 0 daemon pod, expected 1
  Apr 18 00:17:46.663: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:17:46.667: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 18 00:17:46.667: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 04/18/23 00:17:46.671
  Apr 18 00:17:46.719: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:17:46.728: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 18 00:17:46.728: INFO: Node ip-10-0-27-81 is running 0 daemon pod, expected 1
  Apr 18 00:17:47.732: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:17:47.735: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 18 00:17:47.735: INFO: Node ip-10-0-27-81 is running 0 daemon pod, expected 1
  Apr 18 00:17:48.732: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:17:48.735: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 18 00:17:48.736: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 04/18/23 00:17:48.736
  STEP: Deleting DaemonSet "daemon-set" @ 04/18/23 00:17:48.74
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9445, will wait for the garbage collector to delete the pods @ 04/18/23 00:17:48.741
  Apr 18 00:17:48.823: INFO: Deleting DaemonSet.extensions daemon-set took: 10.420452ms
  Apr 18 00:17:48.924: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.126475ms
  Apr 18 00:17:51.529: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 00:17:51.529: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 18 00:17:51.534: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43025"},"items":null}

  Apr 18 00:17:51.537: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43025"},"items":null}

  Apr 18 00:17:51.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9445" for this suite. @ 04/18/23 00:17:51.549
• [9.977 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 04/18/23 00:17:51.556
  Apr 18 00:17:51.556: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename replication-controller @ 04/18/23 00:17:51.556
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:17:51.579
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:17:51.582
  STEP: Creating ReplicationController "e2e-rc-b9khp" @ 04/18/23 00:17:51.585
  Apr 18 00:17:51.590: INFO: Get Replication Controller "e2e-rc-b9khp" to confirm replicas
  Apr 18 00:17:52.595: INFO: Get Replication Controller "e2e-rc-b9khp" to confirm replicas
  Apr 18 00:17:52.598: INFO: Found 1 replicas for "e2e-rc-b9khp" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-b9khp" @ 04/18/23 00:17:52.598
  STEP: Updating a scale subresource @ 04/18/23 00:17:52.603
  STEP: Verifying replicas where modified for replication controller "e2e-rc-b9khp" @ 04/18/23 00:17:52.609
  Apr 18 00:17:52.609: INFO: Get Replication Controller "e2e-rc-b9khp" to confirm replicas
  Apr 18 00:17:53.614: INFO: Get Replication Controller "e2e-rc-b9khp" to confirm replicas
  Apr 18 00:17:53.618: INFO: Found 2 replicas for "e2e-rc-b9khp" replication controller
  Apr 18 00:17:53.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-5701" for this suite. @ 04/18/23 00:17:53.621
• [2.071 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 04/18/23 00:17:53.628
  Apr 18 00:17:53.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 00:17:53.628
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:17:53.642
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:17:53.646
  STEP: Creating projection with secret that has name projected-secret-test-bd30b2dc-8e8a-4a3d-94f4-e347ebf62452 @ 04/18/23 00:17:53.65
  STEP: Creating a pod to test consume secrets @ 04/18/23 00:17:53.654
  STEP: Saw pod success @ 04/18/23 00:17:57.676
  Apr 18 00:17:57.678: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-projected-secrets-d2a10851-7faf-4938-8b41-440cf2f70170 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/18/23 00:17:57.684
  Apr 18 00:17:57.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3911" for this suite. @ 04/18/23 00:17:57.708
• [4.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 04/18/23 00:17:57.717
  Apr 18 00:17:57.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename emptydir @ 04/18/23 00:17:57.718
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:17:57.753
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:17:57.759
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 04/18/23 00:17:57.764
  STEP: Saw pod success @ 04/18/23 00:18:01.783
  Apr 18 00:18:01.787: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-f4758086-bc14-4ddd-864b-e2d6803dc97d container test-container: <nil>
  STEP: delete the pod @ 04/18/23 00:18:01.793
  Apr 18 00:18:01.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4097" for this suite. @ 04/18/23 00:18:01.811
• [4.100 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 04/18/23 00:18:01.818
  Apr 18 00:18:01.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename namespaces @ 04/18/23 00:18:01.819
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:18:01.858
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:18:01.862
  STEP: creating a Namespace @ 04/18/23 00:18:01.866
  STEP: patching the Namespace @ 04/18/23 00:18:01.888
  STEP: get the Namespace and ensuring it has the label @ 04/18/23 00:18:01.894
  Apr 18 00:18:01.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4298" for this suite. @ 04/18/23 00:18:01.913
  STEP: Destroying namespace "nspatchtest-c6a563a0-0f41-4e34-904a-6ffbc9c77761-4184" for this suite. @ 04/18/23 00:18:01.919
• [0.108 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 04/18/23 00:18:01.927
  Apr 18 00:18:01.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename downward-api @ 04/18/23 00:18:01.928
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:18:01.95
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:18:01.955
  STEP: Creating a pod to test downward API volume plugin @ 04/18/23 00:18:01.96
  STEP: Saw pod success @ 04/18/23 00:18:05.988
  Apr 18 00:18:05.991: INFO: Trying to get logs from node ip-10-0-14-154 pod downwardapi-volume-88cd0625-544b-44aa-9763-bd4961a30564 container client-container: <nil>
  STEP: delete the pod @ 04/18/23 00:18:06
  Apr 18 00:18:06.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9191" for this suite. @ 04/18/23 00:18:06.059
• [4.150 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 04/18/23 00:18:06.08
  Apr 18 00:18:06.080: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename conformance-tests @ 04/18/23 00:18:06.081
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:18:06.135
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:18:06.146
  STEP: Getting node addresses @ 04/18/23 00:18:06.154
  Apr 18 00:18:06.154: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Apr 18 00:18:06.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-4818" for this suite. @ 04/18/23 00:18:06.168
• [0.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 04/18/23 00:18:06.183
  Apr 18 00:18:06.183: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename kubectl @ 04/18/23 00:18:06.19
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:18:06.212
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:18:06.216
  STEP: creating a replication controller @ 04/18/23 00:18:06.22
  Apr 18 00:18:06.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-244 create -f -'
  Apr 18 00:18:07.161: INFO: stderr: ""
  Apr 18 00:18:07.161: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/18/23 00:18:07.161
  Apr 18 00:18:07.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-244 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 18 00:18:07.496: INFO: stderr: ""
  Apr 18 00:18:07.496: INFO: stdout: "update-demo-nautilus-28fkv update-demo-nautilus-xttnf "
  Apr 18 00:18:07.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-244 get pods update-demo-nautilus-28fkv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 18 00:18:07.563: INFO: stderr: ""
  Apr 18 00:18:07.563: INFO: stdout: ""
  Apr 18 00:18:07.563: INFO: update-demo-nautilus-28fkv is created but not running
  Apr 18 00:18:12.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-244 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 18 00:18:12.657: INFO: stderr: ""
  Apr 18 00:18:12.657: INFO: stdout: "update-demo-nautilus-28fkv update-demo-nautilus-xttnf "
  Apr 18 00:18:12.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-244 get pods update-demo-nautilus-28fkv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 18 00:18:12.739: INFO: stderr: ""
  Apr 18 00:18:12.739: INFO: stdout: "true"
  Apr 18 00:18:12.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-244 get pods update-demo-nautilus-28fkv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 18 00:18:12.842: INFO: stderr: ""
  Apr 18 00:18:12.842: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 18 00:18:12.842: INFO: validating pod update-demo-nautilus-28fkv
  Apr 18 00:18:12.847: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 18 00:18:12.847: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 18 00:18:12.847: INFO: update-demo-nautilus-28fkv is verified up and running
  Apr 18 00:18:12.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-244 get pods update-demo-nautilus-xttnf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 18 00:18:12.933: INFO: stderr: ""
  Apr 18 00:18:12.933: INFO: stdout: "true"
  Apr 18 00:18:12.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-244 get pods update-demo-nautilus-xttnf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 18 00:18:13.024: INFO: stderr: ""
  Apr 18 00:18:13.024: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 18 00:18:13.024: INFO: validating pod update-demo-nautilus-xttnf
  Apr 18 00:18:13.029: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 18 00:18:13.029: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 18 00:18:13.029: INFO: update-demo-nautilus-xttnf is verified up and running
  STEP: scaling down the replication controller @ 04/18/23 00:18:13.029
  Apr 18 00:18:13.030: INFO: scanned /root for discovery docs: <nil>
  Apr 18 00:18:13.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-244 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  Apr 18 00:18:14.135: INFO: stderr: ""
  Apr 18 00:18:14.135: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/18/23 00:18:14.135
  Apr 18 00:18:14.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-244 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 18 00:18:14.204: INFO: stderr: ""
  Apr 18 00:18:14.204: INFO: stdout: "update-demo-nautilus-28fkv update-demo-nautilus-xttnf "
  STEP: Replicas for name=update-demo: expected=1 actual=2 @ 04/18/23 00:18:14.204
  Apr 18 00:18:19.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-244 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 18 00:18:19.336: INFO: stderr: ""
  Apr 18 00:18:19.336: INFO: stdout: "update-demo-nautilus-28fkv "
  Apr 18 00:18:19.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-244 get pods update-demo-nautilus-28fkv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 18 00:18:19.415: INFO: stderr: ""
  Apr 18 00:18:19.415: INFO: stdout: "true"
  Apr 18 00:18:19.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-244 get pods update-demo-nautilus-28fkv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 18 00:18:19.490: INFO: stderr: ""
  Apr 18 00:18:19.490: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 18 00:18:19.490: INFO: validating pod update-demo-nautilus-28fkv
  Apr 18 00:18:19.494: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 18 00:18:19.494: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 18 00:18:19.494: INFO: update-demo-nautilus-28fkv is verified up and running
  STEP: scaling up the replication controller @ 04/18/23 00:18:19.494
  Apr 18 00:18:19.495: INFO: scanned /root for discovery docs: <nil>
  Apr 18 00:18:19.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-244 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  Apr 18 00:18:20.625: INFO: stderr: ""
  Apr 18 00:18:20.625: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/18/23 00:18:20.625
  Apr 18 00:18:20.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-244 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 18 00:18:20.729: INFO: stderr: ""
  Apr 18 00:18:20.729: INFO: stdout: "update-demo-nautilus-28fkv update-demo-nautilus-fzql5 "
  Apr 18 00:18:20.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-244 get pods update-demo-nautilus-28fkv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 18 00:18:20.802: INFO: stderr: ""
  Apr 18 00:18:20.802: INFO: stdout: "true"
  Apr 18 00:18:20.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-244 get pods update-demo-nautilus-28fkv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 18 00:18:20.883: INFO: stderr: ""
  Apr 18 00:18:20.883: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 18 00:18:20.883: INFO: validating pod update-demo-nautilus-28fkv
  Apr 18 00:18:20.889: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 18 00:18:20.889: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 18 00:18:20.889: INFO: update-demo-nautilus-28fkv is verified up and running
  Apr 18 00:18:20.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-244 get pods update-demo-nautilus-fzql5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 18 00:18:20.966: INFO: stderr: ""
  Apr 18 00:18:20.966: INFO: stdout: "true"
  Apr 18 00:18:20.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-244 get pods update-demo-nautilus-fzql5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 18 00:18:21.057: INFO: stderr: ""
  Apr 18 00:18:21.057: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 18 00:18:21.057: INFO: validating pod update-demo-nautilus-fzql5
  Apr 18 00:18:21.062: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 18 00:18:21.062: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 18 00:18:21.063: INFO: update-demo-nautilus-fzql5 is verified up and running
  STEP: using delete to clean up resources @ 04/18/23 00:18:21.063
  Apr 18 00:18:21.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-244 delete --grace-period=0 --force -f -'
  Apr 18 00:18:21.150: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 18 00:18:21.150: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Apr 18 00:18:21.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-244 get rc,svc -l name=update-demo --no-headers'
  Apr 18 00:18:21.301: INFO: stderr: "No resources found in kubectl-244 namespace.\n"
  Apr 18 00:18:21.301: INFO: stdout: ""
  Apr 18 00:18:21.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-244 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 18 00:18:21.464: INFO: stderr: ""
  Apr 18 00:18:21.465: INFO: stdout: ""
  Apr 18 00:18:21.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-244" for this suite. @ 04/18/23 00:18:21.477
• [15.303 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 04/18/23 00:18:21.491
  Apr 18 00:18:21.491: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename container-probe @ 04/18/23 00:18:21.492
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:18:21.511
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:18:21.516
  STEP: Creating pod liveness-ddd158ca-bc74-4836-9a6b-a9c48d5e6e8a in namespace container-probe-5611 @ 04/18/23 00:18:21.52
  Apr 18 00:18:23.544: INFO: Started pod liveness-ddd158ca-bc74-4836-9a6b-a9c48d5e6e8a in namespace container-probe-5611
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/18/23 00:18:23.545
  Apr 18 00:18:23.548: INFO: Initial restart count of pod liveness-ddd158ca-bc74-4836-9a6b-a9c48d5e6e8a is 0
  Apr 18 00:22:24.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/18/23 00:22:24.173
  STEP: Destroying namespace "container-probe-5611" for this suite. @ 04/18/23 00:22:24.207
• [242.738 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 04/18/23 00:22:24.23
  Apr 18 00:22:24.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 00:22:24.231
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:22:24.249
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:22:24.254
  STEP: Creating configMap with name projected-configmap-test-volume-c01782ce-274a-4b91-8e27-fbd9d47f96fd @ 04/18/23 00:22:24.258
  STEP: Creating a pod to test consume configMaps @ 04/18/23 00:22:24.263
  STEP: Saw pod success @ 04/18/23 00:22:28.291
  Apr 18 00:22:28.294: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-projected-configmaps-cc54cb03-1320-4a2e-87e2-c5511fc62379 container agnhost-container: <nil>
  STEP: delete the pod @ 04/18/23 00:22:28.311
  Apr 18 00:22:28.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3889" for this suite. @ 04/18/23 00:22:28.331
• [4.107 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 04/18/23 00:22:28.339
  Apr 18 00:22:28.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename services @ 04/18/23 00:22:28.341
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:22:28.358
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:22:28.365
  STEP: creating a Service @ 04/18/23 00:22:28.378
  STEP: watching for the Service to be added @ 04/18/23 00:22:28.391
  Apr 18 00:22:28.394: INFO: Found Service test-service-lgqnh in namespace services-9455 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Apr 18 00:22:28.394: INFO: Service test-service-lgqnh created
  STEP: Getting /status @ 04/18/23 00:22:28.394
  Apr 18 00:22:28.400: INFO: Service test-service-lgqnh has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 04/18/23 00:22:28.401
  STEP: watching for the Service to be patched @ 04/18/23 00:22:28.408
  Apr 18 00:22:28.411: INFO: observed Service test-service-lgqnh in namespace services-9455 with annotations: map[] & LoadBalancer: {[]}
  Apr 18 00:22:28.412: INFO: Found Service test-service-lgqnh in namespace services-9455 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Apr 18 00:22:28.412: INFO: Service test-service-lgqnh has service status patched
  STEP: updating the ServiceStatus @ 04/18/23 00:22:28.413
  Apr 18 00:22:28.425: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 04/18/23 00:22:28.425
  Apr 18 00:22:28.429: INFO: Observed Service test-service-lgqnh in namespace services-9455 with annotations: map[] & Conditions: {[]}
  Apr 18 00:22:28.429: INFO: Observed event: &Service{ObjectMeta:{test-service-lgqnh  services-9455  afec7290-5bb8-49a6-b8d3-59d2f027293d 43805 0 2023-04-18 00:22:28 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-04-18 00:22:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-04-18 00:22:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.3.16.8,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.3.16.8],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Apr 18 00:22:28.431: INFO: Found Service test-service-lgqnh in namespace services-9455 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 18 00:22:28.431: INFO: Service test-service-lgqnh has service status updated
  STEP: patching the service @ 04/18/23 00:22:28.432
  STEP: watching for the Service to be patched @ 04/18/23 00:22:28.449
  Apr 18 00:22:28.454: INFO: observed Service test-service-lgqnh in namespace services-9455 with labels: map[test-service-static:true]
  Apr 18 00:22:28.466: INFO: observed Service test-service-lgqnh in namespace services-9455 with labels: map[test-service-static:true]
  Apr 18 00:22:28.466: INFO: observed Service test-service-lgqnh in namespace services-9455 with labels: map[test-service-static:true]
  Apr 18 00:22:28.466: INFO: Found Service test-service-lgqnh in namespace services-9455 with labels: map[test-service:patched test-service-static:true]
  Apr 18 00:22:28.466: INFO: Service test-service-lgqnh patched
  STEP: deleting the service @ 04/18/23 00:22:28.466
  STEP: watching for the Service to be deleted @ 04/18/23 00:22:28.527
  Apr 18 00:22:28.529: INFO: Observed event: ADDED
  Apr 18 00:22:28.529: INFO: Observed event: MODIFIED
  Apr 18 00:22:28.529: INFO: Observed event: MODIFIED
  Apr 18 00:22:28.529: INFO: Observed event: MODIFIED
  Apr 18 00:22:28.529: INFO: Found Service test-service-lgqnh in namespace services-9455 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Apr 18 00:22:28.529: INFO: Service test-service-lgqnh deleted
  Apr 18 00:22:28.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9455" for this suite. @ 04/18/23 00:22:28.534
• [0.207 seconds]
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 04/18/23 00:22:28.547
  Apr 18 00:22:28.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename endpointslice @ 04/18/23 00:22:28.549
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:22:28.57
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:22:28.575
  Apr 18 00:22:32.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-1908" for this suite. @ 04/18/23 00:22:32.709
• [4.169 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 04/18/23 00:22:32.717
  Apr 18 00:22:32.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename resourcequota @ 04/18/23 00:22:32.718
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:22:32.733
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:22:32.737
  STEP: Counting existing ResourceQuota @ 04/18/23 00:22:32.741
  STEP: Creating a ResourceQuota @ 04/18/23 00:22:37.746
  STEP: Ensuring resource quota status is calculated @ 04/18/23 00:22:37.752
  STEP: Creating a ReplicationController @ 04/18/23 00:22:39.755
  STEP: Ensuring resource quota status captures replication controller creation @ 04/18/23 00:22:39.769
  STEP: Deleting a ReplicationController @ 04/18/23 00:22:41.773
  STEP: Ensuring resource quota status released usage @ 04/18/23 00:22:41.779
  Apr 18 00:22:43.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4599" for this suite. @ 04/18/23 00:22:43.789
• [11.077 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 04/18/23 00:22:43.794
  Apr 18 00:22:43.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename deployment @ 04/18/23 00:22:43.795
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:22:43.816
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:22:43.82
  Apr 18 00:22:43.825: INFO: Creating simple deployment test-new-deployment
  Apr 18 00:22:43.859: INFO: deployment "test-new-deployment" doesn't have the required revision set
  STEP: getting scale subresource @ 04/18/23 00:22:45.879
  STEP: updating a scale subresource @ 04/18/23 00:22:45.888
  STEP: verifying the deployment Spec.Replicas was modified @ 04/18/23 00:22:45.898
  STEP: Patch a scale subresource @ 04/18/23 00:22:45.906
  Apr 18 00:22:45.974: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-9215  b9f4eb60-3043-435a-b4c7-f00602690fb2 43913 3 2023-04-18 00:22:43 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-04-18 00:22:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-18 00:22:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046d5158 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-18 00:22:45 +0000 UTC,LastTransitionTime:2023-04-18 00:22:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-04-18 00:22:45 +0000 UTC,LastTransitionTime:2023-04-18 00:22:43 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 18 00:22:45.993: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-9215  14afa9a3-3c03-4ef9-8f3c-3b428f4300ad 43919 3 2023-04-18 00:22:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment b9f4eb60-3043-435a-b4c7-f00602690fb2 0xc0046d5597 0xc0046d5598}] [] [{kube-controller-manager Update apps/v1 2023-04-18 00:22:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9f4eb60-3043-435a-b4c7-f00602690fb2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-18 00:22:45 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046d5628 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 18 00:22:45.999: INFO: Pod "test-new-deployment-67bd4bf6dc-fwvnr" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-fwvnr test-new-deployment-67bd4bf6dc- deployment-9215  33b7fbe2-7211-4481-810a-9ca62fd22ff7 43906 0 2023-04-18 00:22:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:476cefd4e7240e7e5ca80afb3b700b4678ace2024e16074f19eb099d3340e013 cni.projectcalico.org/podIP:10.2.212.73/32 cni.projectcalico.org/podIPs:10.2.212.73/32] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 14afa9a3-3c03-4ef9-8f3c-3b428f4300ad 0xc0047ab867 0xc0047ab868}] [] [{kube-controller-manager Update v1 2023-04-18 00:22:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14afa9a3-3c03-4ef9-8f3c-3b428f4300ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-18 00:22:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-18 00:22:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.212.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t85d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t85d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-14-154,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:22:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:22:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:22:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:22:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.14.154,PodIP:10.2.212.73,StartTime:2023-04-18 00:22:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-18 00:22:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://95a459a7d9fbb37186f8ff71f77491c4812d07fd2f297e8cebb680eb86f1913c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.212.73,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:22:46.000: INFO: Pod "test-new-deployment-67bd4bf6dc-vdsdb" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-vdsdb test-new-deployment-67bd4bf6dc- deployment-9215  39f166e1-eae9-487e-b786-d4e3eeccc275 43921 0 2023-04-18 00:22:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 14afa9a3-3c03-4ef9-8f3c-3b428f4300ad 0xc0047aba70 0xc0047aba71}] [] [{kube-controller-manager Update v1 2023-04-18 00:22:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14afa9a3-3c03-4ef9-8f3c-3b428f4300ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-18 00:22:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-69vdn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-69vdn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-27-81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:22:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:22:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:22:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:22:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.27.81,PodIP:,StartTime:2023-04-18 00:22:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:22:46.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9215" for this suite. @ 04/18/23 00:22:46.015
• [2.235 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 04/18/23 00:22:46.032
  Apr 18 00:22:46.032: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/18/23 00:22:46.034
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:22:46.068
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:22:46.072
  STEP: fetching the /apis discovery document @ 04/18/23 00:22:46.076
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 04/18/23 00:22:46.078
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 04/18/23 00:22:46.079
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 04/18/23 00:22:46.079
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 04/18/23 00:22:46.08
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 04/18/23 00:22:46.08
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 04/18/23 00:22:46.082
  Apr 18 00:22:46.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-7195" for this suite. @ 04/18/23 00:22:46.086
• [0.063 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 04/18/23 00:22:46.097
  Apr 18 00:22:46.097: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 00:22:46.1
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:22:46.124
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:22:46.129
  STEP: Creating a pod to test downward API volume plugin @ 04/18/23 00:22:46.133
  STEP: Saw pod success @ 04/18/23 00:22:50.158
  Apr 18 00:22:50.160: INFO: Trying to get logs from node ip-10-0-14-154 pod downwardapi-volume-7231719f-da4a-4ea8-ae5f-bb80fbf267db container client-container: <nil>
  STEP: delete the pod @ 04/18/23 00:22:50.165
  Apr 18 00:22:50.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7734" for this suite. @ 04/18/23 00:22:50.185
• [4.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 04/18/23 00:22:50.192
  Apr 18 00:22:50.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename namespaces @ 04/18/23 00:22:50.193
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:22:50.214
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:22:50.219
  STEP: Creating a test namespace @ 04/18/23 00:22:50.225
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:22:50.243
  STEP: Creating a pod in the namespace @ 04/18/23 00:22:50.246
  STEP: Waiting for the pod to have running status @ 04/18/23 00:22:50.254
  STEP: Deleting the namespace @ 04/18/23 00:22:52.261
  STEP: Waiting for the namespace to be removed. @ 04/18/23 00:22:52.274
  STEP: Recreating the namespace @ 04/18/23 00:23:04.278
  STEP: Verifying there are no pods in the namespace @ 04/18/23 00:23:04.352
  Apr 18 00:23:04.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3955" for this suite. @ 04/18/23 00:23:04.368
  STEP: Destroying namespace "nsdeletetest-8236" for this suite. @ 04/18/23 00:23:04.375
  Apr 18 00:23:04.380: INFO: Namespace nsdeletetest-8236 was already deleted
  STEP: Destroying namespace "nsdeletetest-4345" for this suite. @ 04/18/23 00:23:04.38
• [14.247 seconds]
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 04/18/23 00:23:04.44
  Apr 18 00:23:04.440: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename statefulset @ 04/18/23 00:23:04.441
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:23:04.592
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:23:04.638
  STEP: Creating service test in namespace statefulset-552 @ 04/18/23 00:23:04.66
  STEP: Creating a new StatefulSet @ 04/18/23 00:23:04.689
  Apr 18 00:23:04.798: INFO: Found 0 stateful pods, waiting for 3
  Apr 18 00:23:14.806: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 18 00:23:14.806: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 18 00:23:14.806: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 04/18/23 00:23:14.816
  Apr 18 00:23:14.835: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 04/18/23 00:23:14.835
  STEP: Not applying an update when the partition is greater than the number of replicas @ 04/18/23 00:23:24.856
  STEP: Performing a canary update @ 04/18/23 00:23:24.856
  Apr 18 00:23:24.877: INFO: Updating stateful set ss2
  Apr 18 00:23:24.890: INFO: Waiting for Pod statefulset-552/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 04/18/23 00:23:34.902
  Apr 18 00:23:34.979: INFO: Found 1 stateful pods, waiting for 3
  Apr 18 00:23:44.986: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 18 00:23:44.986: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 18 00:23:44.986: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 04/18/23 00:23:44.992
  Apr 18 00:23:45.012: INFO: Updating stateful set ss2
  Apr 18 00:23:45.025: INFO: Waiting for Pod statefulset-552/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Apr 18 00:23:55.082: INFO: Updating stateful set ss2
  Apr 18 00:23:55.198: INFO: Waiting for StatefulSet statefulset-552/ss2 to complete update
  Apr 18 00:23:55.198: INFO: Waiting for Pod statefulset-552/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Apr 18 00:24:05.209: INFO: Deleting all statefulset in ns statefulset-552
  Apr 18 00:24:05.213: INFO: Scaling statefulset ss2 to 0
  Apr 18 00:24:15.241: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 18 00:24:15.244: INFO: Deleting statefulset ss2
  Apr 18 00:24:15.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-552" for this suite. @ 04/18/23 00:24:15.26
• [70.839 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 04/18/23 00:24:15.291
  Apr 18 00:24:15.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename ingressclass @ 04/18/23 00:24:15.293
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:24:15.312
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:24:15.316
  STEP: getting /apis @ 04/18/23 00:24:15.32
  STEP: getting /apis/networking.k8s.io @ 04/18/23 00:24:15.328
  STEP: getting /apis/networking.k8s.iov1 @ 04/18/23 00:24:15.33
  STEP: creating @ 04/18/23 00:24:15.332
  STEP: getting @ 04/18/23 00:24:15.35
  STEP: listing @ 04/18/23 00:24:15.354
  STEP: watching @ 04/18/23 00:24:15.358
  Apr 18 00:24:15.358: INFO: starting watch
  STEP: patching @ 04/18/23 00:24:15.36
  STEP: updating @ 04/18/23 00:24:15.365
  Apr 18 00:24:15.370: INFO: waiting for watch events with expected annotations
  Apr 18 00:24:15.370: INFO: saw patched and updated annotations
  STEP: deleting @ 04/18/23 00:24:15.37
  STEP: deleting a collection @ 04/18/23 00:24:15.379
  Apr 18 00:24:15.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-1663" for this suite. @ 04/18/23 00:24:15.397
• [0.113 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 04/18/23 00:24:15.407
  Apr 18 00:24:15.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename downward-api @ 04/18/23 00:24:15.408
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:24:15.446
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:24:15.463
  STEP: Creating a pod to test downward api env vars @ 04/18/23 00:24:15.467
  STEP: Saw pod success @ 04/18/23 00:24:19.514
  Apr 18 00:24:19.517: INFO: Trying to get logs from node ip-10-0-14-154 pod downward-api-c2f37e0c-b2d8-49f2-9274-acbe3c99581c container dapi-container: <nil>
  STEP: delete the pod @ 04/18/23 00:24:19.524
  Apr 18 00:24:19.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6622" for this suite. @ 04/18/23 00:24:19.54
• [4.139 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 04/18/23 00:24:19.547
  Apr 18 00:24:19.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename configmap @ 04/18/23 00:24:19.548
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:24:19.571
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:24:19.575
  STEP: Creating configMap configmap-6416/configmap-test-0e568df5-e2ae-4abb-bac7-58a9d7968797 @ 04/18/23 00:24:19.58
  STEP: Creating a pod to test consume configMaps @ 04/18/23 00:24:19.584
  STEP: Saw pod success @ 04/18/23 00:24:23.606
  Apr 18 00:24:23.608: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-configmaps-bd7168fc-ff77-4030-a586-e8e59d5eae01 container env-test: <nil>
  STEP: delete the pod @ 04/18/23 00:24:23.614
  Apr 18 00:24:23.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6416" for this suite. @ 04/18/23 00:24:23.637
• [4.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 04/18/23 00:24:23.652
  Apr 18 00:24:23.652: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename pods @ 04/18/23 00:24:23.653
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:24:23.674
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:24:23.678
  STEP: Create a pod @ 04/18/23 00:24:23.683
  STEP: patching /status @ 04/18/23 00:24:25.7
  Apr 18 00:24:25.714: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Apr 18 00:24:25.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-460" for this suite. @ 04/18/23 00:24:25.719
• [2.073 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 04/18/23 00:24:25.728
  Apr 18 00:24:25.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename resourcequota @ 04/18/23 00:24:25.729
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:24:25.754
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:24:25.758
  STEP: Discovering how many secrets are in namespace by default @ 04/18/23 00:24:25.762
  STEP: Counting existing ResourceQuota @ 04/18/23 00:24:30.769
  STEP: Creating a ResourceQuota @ 04/18/23 00:24:35.774
  STEP: Ensuring resource quota status is calculated @ 04/18/23 00:24:35.779
  STEP: Creating a Secret @ 04/18/23 00:24:37.783
  STEP: Ensuring resource quota status captures secret creation @ 04/18/23 00:24:37.796
  STEP: Deleting a secret @ 04/18/23 00:24:39.799
  STEP: Ensuring resource quota status released usage @ 04/18/23 00:24:39.807
  Apr 18 00:24:41.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3488" for this suite. @ 04/18/23 00:24:41.816
• [16.093 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 04/18/23 00:24:41.822
  Apr 18 00:24:41.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename container-runtime @ 04/18/23 00:24:41.823
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:24:41.848
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:24:41.853
  STEP: create the container @ 04/18/23 00:24:41.859
  W0418 00:24:41.867822      21 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 04/18/23 00:24:41.868
  STEP: get the container status @ 04/18/23 00:24:44.893
  STEP: the container should be terminated @ 04/18/23 00:24:44.896
  STEP: the termination message should be set @ 04/18/23 00:24:44.896
  Apr 18 00:24:44.896: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 04/18/23 00:24:44.896
  Apr 18 00:24:44.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6035" for this suite. @ 04/18/23 00:24:44.916
• [3.100 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 04/18/23 00:24:44.926
  Apr 18 00:24:44.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename services @ 04/18/23 00:24:44.927
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:24:44.943
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:24:44.948
  STEP: creating service in namespace services-3299 @ 04/18/23 00:24:44.953
  STEP: creating service affinity-clusterip-transition in namespace services-3299 @ 04/18/23 00:24:44.953
  STEP: creating replication controller affinity-clusterip-transition in namespace services-3299 @ 04/18/23 00:24:44.963
  I0418 00:24:44.979956      21 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-3299, replica count: 3
  I0418 00:24:48.033178      21 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 18 00:24:48.040: INFO: Creating new exec pod
  Apr 18 00:24:51.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-3299 exec execpod-affinitynqvh4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Apr 18 00:24:51.506: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Apr 18 00:24:51.506: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 18 00:24:51.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-3299 exec execpod-affinitynqvh4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.3.181.148 80'
  Apr 18 00:24:51.768: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.3.181.148 80\nConnection to 10.3.181.148 80 port [tcp/http] succeeded!\n"
  Apr 18 00:24:51.769: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 18 00:24:51.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-3299 exec execpod-affinitynqvh4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.3.181.148:80/ ; done'
  Apr 18 00:24:52.186: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n"
  Apr 18 00:24:52.186: INFO: stdout: "\naffinity-clusterip-transition-ccjmh\naffinity-clusterip-transition-nxqqb\naffinity-clusterip-transition-wqcrz\naffinity-clusterip-transition-ccjmh\naffinity-clusterip-transition-nxqqb\naffinity-clusterip-transition-wqcrz\naffinity-clusterip-transition-ccjmh\naffinity-clusterip-transition-nxqqb\naffinity-clusterip-transition-wqcrz\naffinity-clusterip-transition-ccjmh\naffinity-clusterip-transition-nxqqb\naffinity-clusterip-transition-wqcrz\naffinity-clusterip-transition-ccjmh\naffinity-clusterip-transition-nxqqb\naffinity-clusterip-transition-wqcrz\naffinity-clusterip-transition-ccjmh"
  Apr 18 00:24:52.186: INFO: Received response from host: affinity-clusterip-transition-ccjmh
  Apr 18 00:24:52.186: INFO: Received response from host: affinity-clusterip-transition-nxqqb
  Apr 18 00:24:52.186: INFO: Received response from host: affinity-clusterip-transition-wqcrz
  Apr 18 00:24:52.186: INFO: Received response from host: affinity-clusterip-transition-ccjmh
  Apr 18 00:24:52.186: INFO: Received response from host: affinity-clusterip-transition-nxqqb
  Apr 18 00:24:52.186: INFO: Received response from host: affinity-clusterip-transition-wqcrz
  Apr 18 00:24:52.186: INFO: Received response from host: affinity-clusterip-transition-ccjmh
  Apr 18 00:24:52.186: INFO: Received response from host: affinity-clusterip-transition-nxqqb
  Apr 18 00:24:52.186: INFO: Received response from host: affinity-clusterip-transition-wqcrz
  Apr 18 00:24:52.186: INFO: Received response from host: affinity-clusterip-transition-ccjmh
  Apr 18 00:24:52.186: INFO: Received response from host: affinity-clusterip-transition-nxqqb
  Apr 18 00:24:52.186: INFO: Received response from host: affinity-clusterip-transition-wqcrz
  Apr 18 00:24:52.186: INFO: Received response from host: affinity-clusterip-transition-ccjmh
  Apr 18 00:24:52.186: INFO: Received response from host: affinity-clusterip-transition-nxqqb
  Apr 18 00:24:52.186: INFO: Received response from host: affinity-clusterip-transition-wqcrz
  Apr 18 00:24:52.186: INFO: Received response from host: affinity-clusterip-transition-ccjmh
  Apr 18 00:24:52.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-3299 exec execpod-affinitynqvh4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.3.181.148:80/ ; done'
  Apr 18 00:24:52.503: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.181.148:80/\n"
  Apr 18 00:24:52.503: INFO: stdout: "\naffinity-clusterip-transition-wqcrz\naffinity-clusterip-transition-wqcrz\naffinity-clusterip-transition-wqcrz\naffinity-clusterip-transition-wqcrz\naffinity-clusterip-transition-wqcrz\naffinity-clusterip-transition-wqcrz\naffinity-clusterip-transition-wqcrz\naffinity-clusterip-transition-wqcrz\naffinity-clusterip-transition-wqcrz\naffinity-clusterip-transition-wqcrz\naffinity-clusterip-transition-wqcrz\naffinity-clusterip-transition-wqcrz\naffinity-clusterip-transition-wqcrz\naffinity-clusterip-transition-wqcrz\naffinity-clusterip-transition-wqcrz\naffinity-clusterip-transition-wqcrz"
  Apr 18 00:24:52.503: INFO: Received response from host: affinity-clusterip-transition-wqcrz
  Apr 18 00:24:52.503: INFO: Received response from host: affinity-clusterip-transition-wqcrz
  Apr 18 00:24:52.503: INFO: Received response from host: affinity-clusterip-transition-wqcrz
  Apr 18 00:24:52.503: INFO: Received response from host: affinity-clusterip-transition-wqcrz
  Apr 18 00:24:52.503: INFO: Received response from host: affinity-clusterip-transition-wqcrz
  Apr 18 00:24:52.503: INFO: Received response from host: affinity-clusterip-transition-wqcrz
  Apr 18 00:24:52.503: INFO: Received response from host: affinity-clusterip-transition-wqcrz
  Apr 18 00:24:52.503: INFO: Received response from host: affinity-clusterip-transition-wqcrz
  Apr 18 00:24:52.503: INFO: Received response from host: affinity-clusterip-transition-wqcrz
  Apr 18 00:24:52.503: INFO: Received response from host: affinity-clusterip-transition-wqcrz
  Apr 18 00:24:52.503: INFO: Received response from host: affinity-clusterip-transition-wqcrz
  Apr 18 00:24:52.503: INFO: Received response from host: affinity-clusterip-transition-wqcrz
  Apr 18 00:24:52.503: INFO: Received response from host: affinity-clusterip-transition-wqcrz
  Apr 18 00:24:52.503: INFO: Received response from host: affinity-clusterip-transition-wqcrz
  Apr 18 00:24:52.503: INFO: Received response from host: affinity-clusterip-transition-wqcrz
  Apr 18 00:24:52.503: INFO: Received response from host: affinity-clusterip-transition-wqcrz
  Apr 18 00:24:52.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 18 00:24:52.507: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3299, will wait for the garbage collector to delete the pods @ 04/18/23 00:24:52.523
  Apr 18 00:24:52.583: INFO: Deleting ReplicationController affinity-clusterip-transition took: 4.77122ms
  Apr 18 00:24:52.683: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.330049ms
  STEP: Destroying namespace "services-3299" for this suite. @ 04/18/23 00:24:54.602
• [9.685 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 04/18/23 00:24:54.612
  Apr 18 00:24:54.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename gc @ 04/18/23 00:24:54.613
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:24:54.633
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:24:54.642
  STEP: create the deployment @ 04/18/23 00:24:54.647
  W0418 00:24:54.653837      21 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 04/18/23 00:24:54.654
  STEP: delete the deployment @ 04/18/23 00:24:55.166
  STEP: wait for all rs to be garbage collected @ 04/18/23 00:24:55.173
  STEP: expected 0 rs, got 1 rs @ 04/18/23 00:24:55.187
  STEP: expected 0 pods, got 2 pods @ 04/18/23 00:24:55.193
  STEP: Gathering metrics @ 04/18/23 00:24:55.941
  Apr 18 00:24:56.040: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 18 00:24:56.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1298" for this suite. @ 04/18/23 00:24:56.044
• [1.438 seconds]
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 04/18/23 00:24:56.051
  Apr 18 00:24:56.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename proxy @ 04/18/23 00:24:56.052
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:24:56.067
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:24:56.071
  Apr 18 00:24:56.075: INFO: Creating pod...
  Apr 18 00:24:58.096: INFO: Creating service...
  Apr 18 00:24:58.110: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8560/pods/agnhost/proxy?method=DELETE
  Apr 18 00:24:58.125: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 18 00:24:58.125: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8560/pods/agnhost/proxy?method=OPTIONS
  Apr 18 00:24:58.132: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 18 00:24:58.133: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8560/pods/agnhost/proxy?method=PATCH
  Apr 18 00:24:58.137: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 18 00:24:58.138: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8560/pods/agnhost/proxy?method=POST
  Apr 18 00:24:58.142: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 18 00:24:58.142: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8560/pods/agnhost/proxy?method=PUT
  Apr 18 00:24:58.147: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 18 00:24:58.147: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8560/services/e2e-proxy-test-service/proxy?method=DELETE
  Apr 18 00:24:58.153: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 18 00:24:58.153: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8560/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Apr 18 00:24:58.160: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 18 00:24:58.161: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8560/services/e2e-proxy-test-service/proxy?method=PATCH
  Apr 18 00:24:58.165: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 18 00:24:58.166: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8560/services/e2e-proxy-test-service/proxy?method=POST
  Apr 18 00:24:58.172: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 18 00:24:58.172: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8560/services/e2e-proxy-test-service/proxy?method=PUT
  Apr 18 00:24:58.180: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 18 00:24:58.182: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8560/pods/agnhost/proxy?method=GET
  Apr 18 00:24:58.187: INFO: http.Client request:GET StatusCode:301
  Apr 18 00:24:58.187: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8560/services/e2e-proxy-test-service/proxy?method=GET
  Apr 18 00:24:58.194: INFO: http.Client request:GET StatusCode:301
  Apr 18 00:24:58.194: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8560/pods/agnhost/proxy?method=HEAD
  Apr 18 00:24:58.199: INFO: http.Client request:HEAD StatusCode:301
  Apr 18 00:24:58.199: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-8560/services/e2e-proxy-test-service/proxy?method=HEAD
  Apr 18 00:24:58.206: INFO: http.Client request:HEAD StatusCode:301
  Apr 18 00:24:58.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-8560" for this suite. @ 04/18/23 00:24:58.214
• [2.178 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 04/18/23 00:24:58.231
  Apr 18 00:24:58.231: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename kubelet-test @ 04/18/23 00:24:58.233
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:24:58.278
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:24:58.289
  Apr 18 00:25:00.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-5640" for this suite. @ 04/18/23 00:25:00.347
• [2.122 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 04/18/23 00:25:00.355
  Apr 18 00:25:00.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename field-validation @ 04/18/23 00:25:00.356
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:25:00.375
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:25:00.379
  Apr 18 00:25:00.382: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  W0418 00:25:02.943345      21 warnings.go:70] unknown field "alpha"
  W0418 00:25:02.943366      21 warnings.go:70] unknown field "beta"
  W0418 00:25:02.943371      21 warnings.go:70] unknown field "delta"
  W0418 00:25:02.943376      21 warnings.go:70] unknown field "epsilon"
  W0418 00:25:02.943380      21 warnings.go:70] unknown field "gamma"
  Apr 18 00:25:03.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5959" for this suite. @ 04/18/23 00:25:03.027
• [2.689 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 04/18/23 00:25:03.045
  Apr 18 00:25:03.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename dns @ 04/18/23 00:25:03.046
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:25:03.112
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:25:03.121
  STEP: Creating a test externalName service @ 04/18/23 00:25:03.132
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3414.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3414.svc.cluster.local; sleep 1; done
   @ 04/18/23 00:25:03.138
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3414.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3414.svc.cluster.local; sleep 1; done
   @ 04/18/23 00:25:03.139
  STEP: creating a pod to probe DNS @ 04/18/23 00:25:03.14
  STEP: submitting the pod to kubernetes @ 04/18/23 00:25:03.14
  STEP: retrieving the pod @ 04/18/23 00:25:11.199
  STEP: looking for the results for each expected name from probers @ 04/18/23 00:25:11.203
  Apr 18 00:25:11.221: INFO: DNS probes using dns-test-800cd22a-13b8-4f7e-9ac2-ffaa83aa4b6f succeeded

  STEP: changing the externalName to bar.example.com @ 04/18/23 00:25:11.221
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3414.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3414.svc.cluster.local; sleep 1; done
   @ 04/18/23 00:25:11.229
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3414.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3414.svc.cluster.local; sleep 1; done
   @ 04/18/23 00:25:11.229
  STEP: creating a second pod to probe DNS @ 04/18/23 00:25:11.229
  STEP: submitting the pod to kubernetes @ 04/18/23 00:25:11.229
  STEP: retrieving the pod @ 04/18/23 00:25:13.244
  STEP: looking for the results for each expected name from probers @ 04/18/23 00:25:13.247
  Apr 18 00:25:13.251: INFO: File wheezy_udp@dns-test-service-3.dns-3414.svc.cluster.local from pod  dns-3414/dns-test-4d01e05f-fa06-4b14-a806-6e612ef5de19 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 18 00:25:13.254: INFO: File jessie_udp@dns-test-service-3.dns-3414.svc.cluster.local from pod  dns-3414/dns-test-4d01e05f-fa06-4b14-a806-6e612ef5de19 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 18 00:25:13.254: INFO: Lookups using dns-3414/dns-test-4d01e05f-fa06-4b14-a806-6e612ef5de19 failed for: [wheezy_udp@dns-test-service-3.dns-3414.svc.cluster.local jessie_udp@dns-test-service-3.dns-3414.svc.cluster.local]

  Apr 18 00:25:18.263: INFO: DNS probes using dns-test-4d01e05f-fa06-4b14-a806-6e612ef5de19 succeeded

  STEP: changing the service to type=ClusterIP @ 04/18/23 00:25:18.263
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3414.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3414.svc.cluster.local; sleep 1; done
   @ 04/18/23 00:25:18.282
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3414.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3414.svc.cluster.local; sleep 1; done
   @ 04/18/23 00:25:18.283
  STEP: creating a third pod to probe DNS @ 04/18/23 00:25:18.283
  STEP: submitting the pod to kubernetes @ 04/18/23 00:25:18.293
  STEP: retrieving the pod @ 04/18/23 00:25:20.325
  STEP: looking for the results for each expected name from probers @ 04/18/23 00:25:20.327
  Apr 18 00:25:20.336: INFO: DNS probes using dns-test-acd8e5c2-1a65-4cff-abc4-308d409533ae succeeded

  Apr 18 00:25:20.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/18/23 00:25:20.339
  STEP: deleting the pod @ 04/18/23 00:25:20.362
  STEP: deleting the pod @ 04/18/23 00:25:20.383
  STEP: deleting the test externalName service @ 04/18/23 00:25:20.401
  STEP: Destroying namespace "dns-3414" for this suite. @ 04/18/23 00:25:20.434
• [17.404 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 04/18/23 00:25:20.503
  Apr 18 00:25:20.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename configmap @ 04/18/23 00:25:20.509
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:25:20.539
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:25:20.549
  STEP: Creating configMap with name configmap-test-volume-0539dad1-c890-4d84-beb4-7cc73a873bf6 @ 04/18/23 00:25:20.554
  STEP: Creating a pod to test consume configMaps @ 04/18/23 00:25:20.57
  STEP: Saw pod success @ 04/18/23 00:25:24.621
  Apr 18 00:25:24.624: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-configmaps-8dd2bd54-6463-4cd7-bbc8-650c7cfa4194 container configmap-volume-test: <nil>
  STEP: delete the pod @ 04/18/23 00:25:24.631
  Apr 18 00:25:24.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8120" for this suite. @ 04/18/23 00:25:24.649
• [4.152 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 04/18/23 00:25:24.656
  Apr 18 00:25:24.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename dns @ 04/18/23 00:25:24.664
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:25:24.698
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:25:24.706
  STEP: Creating a test headless service @ 04/18/23 00:25:24.711
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 138.19.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.19.138_udp@PTR;check="$$(dig +tcp +noall +answer +search 138.19.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.19.138_tcp@PTR;sleep 1; done
   @ 04/18/23 00:25:24.737
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 138.19.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.19.138_udp@PTR;check="$$(dig +tcp +noall +answer +search 138.19.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.19.138_tcp@PTR;sleep 1; done
   @ 04/18/23 00:25:24.738
  STEP: creating a pod to probe DNS @ 04/18/23 00:25:24.739
  STEP: submitting the pod to kubernetes @ 04/18/23 00:25:24.739
  STEP: retrieving the pod @ 04/18/23 00:25:26.763
  STEP: looking for the results for each expected name from probers @ 04/18/23 00:25:26.766
  Apr 18 00:25:26.772: INFO: Unable to read wheezy_udp@dns-test-service.dns-1839.svc.cluster.local from pod dns-1839/dns-test-dbe96cf6-6d65-4d2f-b643-ff749a7adaa1: the server could not find the requested resource (get pods dns-test-dbe96cf6-6d65-4d2f-b643-ff749a7adaa1)
  Apr 18 00:25:26.775: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1839.svc.cluster.local from pod dns-1839/dns-test-dbe96cf6-6d65-4d2f-b643-ff749a7adaa1: the server could not find the requested resource (get pods dns-test-dbe96cf6-6d65-4d2f-b643-ff749a7adaa1)
  Apr 18 00:25:26.784: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1839.svc.cluster.local from pod dns-1839/dns-test-dbe96cf6-6d65-4d2f-b643-ff749a7adaa1: the server could not find the requested resource (get pods dns-test-dbe96cf6-6d65-4d2f-b643-ff749a7adaa1)
  Apr 18 00:25:26.790: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1839.svc.cluster.local from pod dns-1839/dns-test-dbe96cf6-6d65-4d2f-b643-ff749a7adaa1: the server could not find the requested resource (get pods dns-test-dbe96cf6-6d65-4d2f-b643-ff749a7adaa1)
  Apr 18 00:25:26.810: INFO: Unable to read jessie_udp@dns-test-service.dns-1839.svc.cluster.local from pod dns-1839/dns-test-dbe96cf6-6d65-4d2f-b643-ff749a7adaa1: the server could not find the requested resource (get pods dns-test-dbe96cf6-6d65-4d2f-b643-ff749a7adaa1)
  Apr 18 00:25:26.813: INFO: Unable to read jessie_tcp@dns-test-service.dns-1839.svc.cluster.local from pod dns-1839/dns-test-dbe96cf6-6d65-4d2f-b643-ff749a7adaa1: the server could not find the requested resource (get pods dns-test-dbe96cf6-6d65-4d2f-b643-ff749a7adaa1)
  Apr 18 00:25:26.817: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1839.svc.cluster.local from pod dns-1839/dns-test-dbe96cf6-6d65-4d2f-b643-ff749a7adaa1: the server could not find the requested resource (get pods dns-test-dbe96cf6-6d65-4d2f-b643-ff749a7adaa1)
  Apr 18 00:25:26.821: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1839.svc.cluster.local from pod dns-1839/dns-test-dbe96cf6-6d65-4d2f-b643-ff749a7adaa1: the server could not find the requested resource (get pods dns-test-dbe96cf6-6d65-4d2f-b643-ff749a7adaa1)
  Apr 18 00:25:26.836: INFO: Lookups using dns-1839/dns-test-dbe96cf6-6d65-4d2f-b643-ff749a7adaa1 failed for: [wheezy_udp@dns-test-service.dns-1839.svc.cluster.local wheezy_tcp@dns-test-service.dns-1839.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1839.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1839.svc.cluster.local jessie_udp@dns-test-service.dns-1839.svc.cluster.local jessie_tcp@dns-test-service.dns-1839.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1839.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1839.svc.cluster.local]

  Apr 18 00:25:31.935: INFO: DNS probes using dns-1839/dns-test-dbe96cf6-6d65-4d2f-b643-ff749a7adaa1 succeeded

  Apr 18 00:25:31.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/18/23 00:25:31.939
  STEP: deleting the test service @ 04/18/23 00:25:31.971
  STEP: deleting the test headless service @ 04/18/23 00:25:32.093
  STEP: Destroying namespace "dns-1839" for this suite. @ 04/18/23 00:25:32.131
• [7.493 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 04/18/23 00:25:32.152
  Apr 18 00:25:32.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename configmap @ 04/18/23 00:25:32.153
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:25:32.188
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:25:32.193
  STEP: Creating configMap with name configmap-test-upd-0491047e-aeaf-437e-ba3a-e01c4d21f07e @ 04/18/23 00:25:32.204
  STEP: Creating the pod @ 04/18/23 00:25:32.224
  STEP: Updating configmap configmap-test-upd-0491047e-aeaf-437e-ba3a-e01c4d21f07e @ 04/18/23 00:25:34.26
  STEP: waiting to observe update in volume @ 04/18/23 00:25:34.272
  Apr 18 00:25:36.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1244" for this suite. @ 04/18/23 00:25:36.291
• [4.152 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 04/18/23 00:25:36.306
  Apr 18 00:25:36.306: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 00:25:36.307
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:25:36.344
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:25:36.361
  STEP: Creating a pod to test downward API volume plugin @ 04/18/23 00:25:36.367
  STEP: Saw pod success @ 04/18/23 00:25:40.406
  Apr 18 00:25:40.413: INFO: Trying to get logs from node ip-10-0-14-154 pod downwardapi-volume-c48a49b9-bff1-4e70-8189-64bb8e39d72d container client-container: <nil>
  STEP: delete the pod @ 04/18/23 00:25:40.421
  Apr 18 00:25:40.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-869" for this suite. @ 04/18/23 00:25:40.457
• [4.158 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 04/18/23 00:25:40.466
  Apr 18 00:25:40.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename var-expansion @ 04/18/23 00:25:40.467
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:25:40.485
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:25:40.488
  STEP: Creating a pod to test env composition @ 04/18/23 00:25:40.492
  STEP: Saw pod success @ 04/18/23 00:25:44.511
  Apr 18 00:25:44.513: INFO: Trying to get logs from node ip-10-0-14-154 pod var-expansion-ba235019-51ff-40a0-ba1a-a6cb3c19ea14 container dapi-container: <nil>
  STEP: delete the pod @ 04/18/23 00:25:44.519
  Apr 18 00:25:44.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1358" for this suite. @ 04/18/23 00:25:44.534
• [4.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 04/18/23 00:25:44.541
  Apr 18 00:25:44.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename namespaces @ 04/18/23 00:25:44.542
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:25:44.558
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:25:44.562
  STEP: Read namespace status @ 04/18/23 00:25:44.566
  Apr 18 00:25:44.569: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 04/18/23 00:25:44.569
  Apr 18 00:25:44.575: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 04/18/23 00:25:44.575
  Apr 18 00:25:44.584: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Apr 18 00:25:44.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-1369" for this suite. @ 04/18/23 00:25:44.588
• [0.053 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 04/18/23 00:25:44.596
  Apr 18 00:25:44.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename replicaset @ 04/18/23 00:25:44.597
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:25:44.624
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:25:44.628
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 04/18/23 00:25:44.632
  STEP: When a replicaset with a matching selector is created @ 04/18/23 00:25:46.651
  STEP: Then the orphan pod is adopted @ 04/18/23 00:25:46.655
  STEP: When the matched label of one of its pods change @ 04/18/23 00:25:47.665
  Apr 18 00:25:47.668: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 04/18/23 00:25:47.678
  Apr 18 00:25:48.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-6641" for this suite. @ 04/18/23 00:25:48.695
• [4.106 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 04/18/23 00:25:48.707
  Apr 18 00:25:48.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename containers @ 04/18/23 00:25:48.708
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:25:48.73
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:25:48.738
  Apr 18 00:25:50.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-9078" for this suite. @ 04/18/23 00:25:50.87
• [2.168 seconds]
------------------------------
S
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 04/18/23 00:25:50.876
  Apr 18 00:25:50.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename podtemplate @ 04/18/23 00:25:50.878
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:25:50.896
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:25:50.9
  Apr 18 00:25:50.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-3387" for this suite. @ 04/18/23 00:25:50.947
• [0.081 seconds]
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 04/18/23 00:25:50.958
  Apr 18 00:25:50.958: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename kubectl @ 04/18/23 00:25:50.959
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:25:50.982
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:25:50.99
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/18/23 00:25:50.994
  Apr 18 00:25:50.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-8271 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Apr 18 00:25:51.355: INFO: stderr: ""
  Apr 18 00:25:51.355: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 04/18/23 00:25:51.355
  Apr 18 00:25:51.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-8271 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Apr 18 00:25:51.482: INFO: stderr: ""
  Apr 18 00:25:51.482: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/18/23 00:25:51.483
  Apr 18 00:25:51.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-8271 delete pods e2e-test-httpd-pod'
  Apr 18 00:25:53.688: INFO: stderr: ""
  Apr 18 00:25:53.688: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 18 00:25:53.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8271" for this suite. @ 04/18/23 00:25:53.693
• [2.746 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 04/18/23 00:25:53.706
  Apr 18 00:25:53.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename cronjob @ 04/18/23 00:25:53.707
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:25:53.734
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:25:53.751
  STEP: Creating a cronjob @ 04/18/23 00:25:53.759
  STEP: creating @ 04/18/23 00:25:53.759
  STEP: getting @ 04/18/23 00:25:53.777
  STEP: listing @ 04/18/23 00:25:53.782
  STEP: watching @ 04/18/23 00:25:53.788
  Apr 18 00:25:53.788: INFO: starting watch
  STEP: cluster-wide listing @ 04/18/23 00:25:53.791
  STEP: cluster-wide watching @ 04/18/23 00:25:53.796
  Apr 18 00:25:53.796: INFO: starting watch
  STEP: patching @ 04/18/23 00:25:53.798
  STEP: updating @ 04/18/23 00:25:53.814
  Apr 18 00:25:53.830: INFO: waiting for watch events with expected annotations
  Apr 18 00:25:53.830: INFO: saw patched and updated annotations
  STEP: patching /status @ 04/18/23 00:25:53.831
  STEP: updating /status @ 04/18/23 00:25:53.844
  STEP: get /status @ 04/18/23 00:25:53.869
  STEP: deleting @ 04/18/23 00:25:53.874
  STEP: deleting a collection @ 04/18/23 00:25:53.913
  Apr 18 00:25:53.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8717" for this suite. @ 04/18/23 00:25:53.937
• [0.248 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 04/18/23 00:25:53.959
  Apr 18 00:25:53.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename container-probe @ 04/18/23 00:25:53.959
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:25:54.055
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:25:54.074
  STEP: Creating pod test-grpc-993961f4-7123-426e-8e17-0adfcdbaa5aa in namespace container-probe-331 @ 04/18/23 00:25:54.086
  Apr 18 00:25:56.151: INFO: Started pod test-grpc-993961f4-7123-426e-8e17-0adfcdbaa5aa in namespace container-probe-331
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/18/23 00:25:56.151
  Apr 18 00:25:56.158: INFO: Initial restart count of pod test-grpc-993961f4-7123-426e-8e17-0adfcdbaa5aa is 0
  Apr 18 00:29:56.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/18/23 00:29:56.869
  STEP: Destroying namespace "container-probe-331" for this suite. @ 04/18/23 00:29:56.894
• [242.956 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 04/18/23 00:29:56.917
  Apr 18 00:29:56.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 00:29:56.918
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:29:56.939
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:29:56.948
  STEP: Creating a pod to test downward API volume plugin @ 04/18/23 00:29:56.953
  STEP: Saw pod success @ 04/18/23 00:30:00.983
  Apr 18 00:30:00.987: INFO: Trying to get logs from node ip-10-0-14-154 pod downwardapi-volume-ff78fc7f-485c-4365-ba56-454b4e30ae63 container client-container: <nil>
  STEP: delete the pod @ 04/18/23 00:30:01.004
  Apr 18 00:30:01.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8870" for this suite. @ 04/18/23 00:30:01.022
• [4.111 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 04/18/23 00:30:01.029
  Apr 18 00:30:01.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename emptydir @ 04/18/23 00:30:01.03
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:30:01.049
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:30:01.053
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 04/18/23 00:30:01.067
  STEP: Saw pod success @ 04/18/23 00:30:07.15
  Apr 18 00:30:07.153: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-957bc2f7-d900-4025-86f2-2a87c7e1bc33 container test-container: <nil>
  STEP: delete the pod @ 04/18/23 00:30:07.16
  Apr 18 00:30:07.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2129" for this suite. @ 04/18/23 00:30:07.188
• [6.166 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 04/18/23 00:30:07.2
  Apr 18 00:30:07.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 00:30:07.202
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:30:07.237
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:30:07.281
  STEP: Creating a pod to test downward API volume plugin @ 04/18/23 00:30:07.294
  STEP: Saw pod success @ 04/18/23 00:30:11.324
  Apr 18 00:30:11.327: INFO: Trying to get logs from node ip-10-0-14-154 pod downwardapi-volume-ffd56235-66af-4186-8071-6809986f27d2 container client-container: <nil>
  STEP: delete the pod @ 04/18/23 00:30:11.333
  Apr 18 00:30:11.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6136" for this suite. @ 04/18/23 00:30:11.349
• [4.157 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 04/18/23 00:30:11.36
  Apr 18 00:30:11.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename kubectl @ 04/18/23 00:30:11.362
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:30:11.382
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:30:11.386
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/18/23 00:30:11.39
  Apr 18 00:30:11.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-983 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Apr 18 00:30:11.483: INFO: stderr: ""
  Apr 18 00:30:11.483: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 04/18/23 00:30:11.483
  Apr 18 00:30:11.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-983 delete pods e2e-test-httpd-pod'
  Apr 18 00:30:13.328: INFO: stderr: ""
  Apr 18 00:30:13.328: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 18 00:30:13.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-983" for this suite. @ 04/18/23 00:30:13.334
• [1.982 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 04/18/23 00:30:13.344
  Apr 18 00:30:13.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename gc @ 04/18/23 00:30:13.345
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:30:13.364
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:30:13.368
  STEP: create the rc1 @ 04/18/23 00:30:13.374
  STEP: create the rc2 @ 04/18/23 00:30:13.378
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 04/18/23 00:30:19.396
  STEP: delete the rc simpletest-rc-to-be-deleted @ 04/18/23 00:30:20.505
  STEP: wait for the rc to be deleted @ 04/18/23 00:30:20.526
  Apr 18 00:30:25.663: INFO: 71 pods remaining
  Apr 18 00:30:25.663: INFO: 71 pods has nil DeletionTimestamp
  Apr 18 00:30:25.663: INFO: 
  STEP: Gathering metrics @ 04/18/23 00:30:30.585
  Apr 18 00:30:31.184: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 18 00:30:31.185: INFO: Deleting pod "simpletest-rc-to-be-deleted-27m4m" in namespace "gc-4903"
  Apr 18 00:30:31.214: INFO: Deleting pod "simpletest-rc-to-be-deleted-29q6v" in namespace "gc-4903"
  Apr 18 00:30:31.235: INFO: Deleting pod "simpletest-rc-to-be-deleted-2m77k" in namespace "gc-4903"
  Apr 18 00:30:31.247: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vb9n" in namespace "gc-4903"
  Apr 18 00:30:31.267: INFO: Deleting pod "simpletest-rc-to-be-deleted-46h8s" in namespace "gc-4903"
  Apr 18 00:30:31.304: INFO: Deleting pod "simpletest-rc-to-be-deleted-48mfs" in namespace "gc-4903"
  Apr 18 00:30:31.324: INFO: Deleting pod "simpletest-rc-to-be-deleted-52srt" in namespace "gc-4903"
  Apr 18 00:30:31.343: INFO: Deleting pod "simpletest-rc-to-be-deleted-5fjj8" in namespace "gc-4903"
  Apr 18 00:30:31.380: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mxlc" in namespace "gc-4903"
  Apr 18 00:30:31.419: INFO: Deleting pod "simpletest-rc-to-be-deleted-622np" in namespace "gc-4903"
  Apr 18 00:30:31.429: INFO: Deleting pod "simpletest-rc-to-be-deleted-6427w" in namespace "gc-4903"
  Apr 18 00:30:31.446: INFO: Deleting pod "simpletest-rc-to-be-deleted-65l55" in namespace "gc-4903"
  Apr 18 00:30:31.468: INFO: Deleting pod "simpletest-rc-to-be-deleted-686tq" in namespace "gc-4903"
  Apr 18 00:30:31.485: INFO: Deleting pod "simpletest-rc-to-be-deleted-6fd22" in namespace "gc-4903"
  Apr 18 00:30:31.498: INFO: Deleting pod "simpletest-rc-to-be-deleted-797cv" in namespace "gc-4903"
  Apr 18 00:30:31.511: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bbbf" in namespace "gc-4903"
  Apr 18 00:30:31.529: INFO: Deleting pod "simpletest-rc-to-be-deleted-7f65w" in namespace "gc-4903"
  Apr 18 00:30:31.596: INFO: Deleting pod "simpletest-rc-to-be-deleted-7gmrf" in namespace "gc-4903"
  Apr 18 00:30:31.614: INFO: Deleting pod "simpletest-rc-to-be-deleted-85gzd" in namespace "gc-4903"
  Apr 18 00:30:31.629: INFO: Deleting pod "simpletest-rc-to-be-deleted-89kxq" in namespace "gc-4903"
  Apr 18 00:30:31.657: INFO: Deleting pod "simpletest-rc-to-be-deleted-8j24q" in namespace "gc-4903"
  Apr 18 00:30:31.675: INFO: Deleting pod "simpletest-rc-to-be-deleted-92vv7" in namespace "gc-4903"
  Apr 18 00:30:31.688: INFO: Deleting pod "simpletest-rc-to-be-deleted-94mjr" in namespace "gc-4903"
  Apr 18 00:30:31.705: INFO: Deleting pod "simpletest-rc-to-be-deleted-95jrf" in namespace "gc-4903"
  Apr 18 00:30:31.720: INFO: Deleting pod "simpletest-rc-to-be-deleted-9f28b" in namespace "gc-4903"
  Apr 18 00:30:31.743: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8kfh" in namespace "gc-4903"
  Apr 18 00:30:31.775: INFO: Deleting pod "simpletest-rc-to-be-deleted-bp94l" in namespace "gc-4903"
  Apr 18 00:30:31.809: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvct8" in namespace "gc-4903"
  Apr 18 00:30:31.827: INFO: Deleting pod "simpletest-rc-to-be-deleted-ch6zb" in namespace "gc-4903"
  Apr 18 00:30:31.853: INFO: Deleting pod "simpletest-rc-to-be-deleted-ckllt" in namespace "gc-4903"
  Apr 18 00:30:31.876: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmjzg" in namespace "gc-4903"
  Apr 18 00:30:31.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxpl6" in namespace "gc-4903"
  Apr 18 00:30:31.925: INFO: Deleting pod "simpletest-rc-to-be-deleted-d84sf" in namespace "gc-4903"
  Apr 18 00:30:31.939: INFO: Deleting pod "simpletest-rc-to-be-deleted-dqpcz" in namespace "gc-4903"
  Apr 18 00:30:31.949: INFO: Deleting pod "simpletest-rc-to-be-deleted-dv4pw" in namespace "gc-4903"
  Apr 18 00:30:31.961: INFO: Deleting pod "simpletest-rc-to-be-deleted-f468t" in namespace "gc-4903"
  Apr 18 00:30:31.971: INFO: Deleting pod "simpletest-rc-to-be-deleted-f4fjp" in namespace "gc-4903"
  Apr 18 00:30:31.983: INFO: Deleting pod "simpletest-rc-to-be-deleted-f58hs" in namespace "gc-4903"
  Apr 18 00:30:32.003: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvc28" in namespace "gc-4903"
  Apr 18 00:30:32.016: INFO: Deleting pod "simpletest-rc-to-be-deleted-g26nv" in namespace "gc-4903"
  Apr 18 00:30:32.055: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2cmm" in namespace "gc-4903"
  Apr 18 00:30:32.084: INFO: Deleting pod "simpletest-rc-to-be-deleted-glchg" in namespace "gc-4903"
  Apr 18 00:30:32.112: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtt6w" in namespace "gc-4903"
  Apr 18 00:30:32.139: INFO: Deleting pod "simpletest-rc-to-be-deleted-hv8q9" in namespace "gc-4903"
  Apr 18 00:30:32.183: INFO: Deleting pod "simpletest-rc-to-be-deleted-hx9np" in namespace "gc-4903"
  Apr 18 00:30:32.199: INFO: Deleting pod "simpletest-rc-to-be-deleted-j8dhm" in namespace "gc-4903"
  Apr 18 00:30:32.222: INFO: Deleting pod "simpletest-rc-to-be-deleted-jf45j" in namespace "gc-4903"
  Apr 18 00:30:32.248: INFO: Deleting pod "simpletest-rc-to-be-deleted-jfvxd" in namespace "gc-4903"
  Apr 18 00:30:32.360: INFO: Deleting pod "simpletest-rc-to-be-deleted-k29kr" in namespace "gc-4903"
  Apr 18 00:30:32.389: INFO: Deleting pod "simpletest-rc-to-be-deleted-kp6f6" in namespace "gc-4903"
  Apr 18 00:30:32.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4903" for this suite. @ 04/18/23 00:30:32.443
• [19.114 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 04/18/23 00:30:32.458
  Apr 18 00:30:32.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename webhook @ 04/18/23 00:30:32.46
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:30:32.514
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:30:32.519
  STEP: Setting up server cert @ 04/18/23 00:30:32.562
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/18/23 00:30:33.526
  STEP: Deploying the webhook pod @ 04/18/23 00:30:33.533
  STEP: Wait for the deployment to be ready @ 04/18/23 00:30:33.544
  Apr 18 00:30:33.555: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  Apr 18 00:30:35.573: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 18, 0, 30, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 30, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 0, 30, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 30, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 18 00:30:37.578: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 18, 0, 30, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 30, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 0, 30, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 30, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 18 00:30:39.579: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 18, 0, 30, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 30, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 0, 30, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 30, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 04/18/23 00:30:41.582
  STEP: Verifying the service has paired with the endpoint @ 04/18/23 00:30:41.607
  Apr 18 00:30:42.609: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 04/18/23 00:30:42.628
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 04/18/23 00:30:42.637
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 04/18/23 00:30:42.637
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 04/18/23 00:30:42.637
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 04/18/23 00:30:42.644
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 04/18/23 00:30:42.644
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 04/18/23 00:30:42.65
  Apr 18 00:30:42.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2942" for this suite. @ 04/18/23 00:30:42.839
  STEP: Destroying namespace "webhook-markers-8056" for this suite. @ 04/18/23 00:30:42.858
• [10.432 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 04/18/23 00:30:42.892
  Apr 18 00:30:42.892: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename pods @ 04/18/23 00:30:42.894
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:30:42.967
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:30:42.974
  STEP: Saw pod success @ 04/18/23 00:30:49.082
  Apr 18 00:30:49.085: INFO: Trying to get logs from node ip-10-0-14-154 pod client-envvars-1cc35170-d680-4503-b987-da1ec268f5f7 container env3cont: <nil>
  STEP: delete the pod @ 04/18/23 00:30:49.092
  Apr 18 00:30:49.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3814" for this suite. @ 04/18/23 00:30:49.11
• [6.227 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 04/18/23 00:30:49.121
  Apr 18 00:30:49.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename dns @ 04/18/23 00:30:49.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:30:49.138
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:30:49.141
  STEP: Creating a test headless service @ 04/18/23 00:30:49.145
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-149.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-149.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 04/18/23 00:30:49.151
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-149.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-149.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 04/18/23 00:30:49.151
  STEP: creating a pod to probe DNS @ 04/18/23 00:30:49.151
  STEP: submitting the pod to kubernetes @ 04/18/23 00:30:49.152
  STEP: retrieving the pod @ 04/18/23 00:30:51.175
  STEP: looking for the results for each expected name from probers @ 04/18/23 00:30:51.178
  Apr 18 00:30:51.195: INFO: DNS probes using dns-149/dns-test-9105e1d4-7227-4c8c-9c74-f6a5e4c66f12 succeeded

  Apr 18 00:30:51.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/18/23 00:30:51.198
  STEP: deleting the test headless service @ 04/18/23 00:30:51.214
  STEP: Destroying namespace "dns-149" for this suite. @ 04/18/23 00:30:51.246
• [2.155 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 04/18/23 00:30:51.276
  Apr 18 00:30:51.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename replicaset @ 04/18/23 00:30:51.278
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:30:51.304
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:30:51.308
  Apr 18 00:30:51.325: INFO: Pod name sample-pod: Found 0 pods out of 1
  Apr 18 00:30:56.330: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/18/23 00:30:56.33
  STEP: Scaling up "test-rs" replicaset  @ 04/18/23 00:30:56.33
  Apr 18 00:30:56.340: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 04/18/23 00:30:56.34
  W0418 00:30:56.353765      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 18 00:30:56.363: INFO: observed ReplicaSet test-rs in namespace replicaset-1566 with ReadyReplicas 1, AvailableReplicas 1
  Apr 18 00:30:56.421: INFO: observed ReplicaSet test-rs in namespace replicaset-1566 with ReadyReplicas 1, AvailableReplicas 1
  Apr 18 00:30:56.471: INFO: observed ReplicaSet test-rs in namespace replicaset-1566 with ReadyReplicas 1, AvailableReplicas 1
  Apr 18 00:30:56.483: INFO: observed ReplicaSet test-rs in namespace replicaset-1566 with ReadyReplicas 1, AvailableReplicas 1
  Apr 18 00:30:57.657: INFO: observed ReplicaSet test-rs in namespace replicaset-1566 with ReadyReplicas 2, AvailableReplicas 2
  Apr 18 00:30:58.770: INFO: observed Replicaset test-rs in namespace replicaset-1566 with ReadyReplicas 3 found true
  Apr 18 00:30:58.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1566" for this suite. @ 04/18/23 00:30:58.779
• [7.515 seconds]
------------------------------
SSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 04/18/23 00:30:58.793
  Apr 18 00:30:58.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename dns @ 04/18/23 00:30:58.795
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:30:58.817
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:30:58.822
  STEP: Creating a test headless service @ 04/18/23 00:30:58.829
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-325 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-325;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-325 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-325;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-325.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-325.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-325.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-325.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-325.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-325.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-325.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-325.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-325.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-325.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-325.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-325.svc;check="$$(dig +notcp +noall +answer +search 71.173.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.173.71_udp@PTR;check="$$(dig +tcp +noall +answer +search 71.173.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.173.71_tcp@PTR;sleep 1; done
   @ 04/18/23 00:30:58.857
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-325 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-325;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-325 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-325;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-325.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-325.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-325.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-325.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-325.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-325.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-325.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-325.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-325.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-325.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-325.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-325.svc;check="$$(dig +notcp +noall +answer +search 71.173.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.173.71_udp@PTR;check="$$(dig +tcp +noall +answer +search 71.173.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.173.71_tcp@PTR;sleep 1; done
   @ 04/18/23 00:30:58.857
  STEP: creating a pod to probe DNS @ 04/18/23 00:30:58.857
  STEP: submitting the pod to kubernetes @ 04/18/23 00:30:58.857
  STEP: retrieving the pod @ 04/18/23 00:31:00.888
  STEP: looking for the results for each expected name from probers @ 04/18/23 00:31:00.892
  Apr 18 00:31:00.917: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-325/dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13: the server could not find the requested resource (get pods dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13)
  Apr 18 00:31:00.921: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-325/dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13: the server could not find the requested resource (get pods dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13)
  Apr 18 00:31:00.928: INFO: Unable to read wheezy_udp@dns-test-service.dns-325 from pod dns-325/dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13: the server could not find the requested resource (get pods dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13)
  Apr 18 00:31:00.935: INFO: Unable to read wheezy_tcp@dns-test-service.dns-325 from pod dns-325/dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13: the server could not find the requested resource (get pods dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13)
  Apr 18 00:31:00.940: INFO: Unable to read wheezy_udp@dns-test-service.dns-325.svc from pod dns-325/dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13: the server could not find the requested resource (get pods dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13)
  Apr 18 00:31:00.944: INFO: Unable to read wheezy_tcp@dns-test-service.dns-325.svc from pod dns-325/dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13: the server could not find the requested resource (get pods dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13)
  Apr 18 00:31:00.955: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-325.svc from pod dns-325/dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13: the server could not find the requested resource (get pods dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13)
  Apr 18 00:31:00.963: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-325.svc from pod dns-325/dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13: the server could not find the requested resource (get pods dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13)
  Apr 18 00:31:00.990: INFO: Unable to read jessie_udp@dns-test-service from pod dns-325/dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13: the server could not find the requested resource (get pods dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13)
  Apr 18 00:31:00.998: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-325/dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13: the server could not find the requested resource (get pods dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13)
  Apr 18 00:31:01.003: INFO: Unable to read jessie_udp@dns-test-service.dns-325 from pod dns-325/dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13: the server could not find the requested resource (get pods dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13)
  Apr 18 00:31:01.010: INFO: Unable to read jessie_tcp@dns-test-service.dns-325 from pod dns-325/dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13: the server could not find the requested resource (get pods dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13)
  Apr 18 00:31:01.015: INFO: Unable to read jessie_udp@dns-test-service.dns-325.svc from pod dns-325/dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13: the server could not find the requested resource (get pods dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13)
  Apr 18 00:31:01.020: INFO: Unable to read jessie_tcp@dns-test-service.dns-325.svc from pod dns-325/dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13: the server could not find the requested resource (get pods dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13)
  Apr 18 00:31:01.024: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-325.svc from pod dns-325/dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13: the server could not find the requested resource (get pods dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13)
  Apr 18 00:31:01.029: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-325.svc from pod dns-325/dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13: the server could not find the requested resource (get pods dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13)
  Apr 18 00:31:01.045: INFO: Lookups using dns-325/dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-325 wheezy_tcp@dns-test-service.dns-325 wheezy_udp@dns-test-service.dns-325.svc wheezy_tcp@dns-test-service.dns-325.svc wheezy_udp@_http._tcp.dns-test-service.dns-325.svc wheezy_tcp@_http._tcp.dns-test-service.dns-325.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-325 jessie_tcp@dns-test-service.dns-325 jessie_udp@dns-test-service.dns-325.svc jessie_tcp@dns-test-service.dns-325.svc jessie_udp@_http._tcp.dns-test-service.dns-325.svc jessie_tcp@_http._tcp.dns-test-service.dns-325.svc]

  Apr 18 00:31:06.145: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-325.svc from pod dns-325/dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13: the server could not find the requested resource (get pods dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13)
  Apr 18 00:31:06.174: INFO: Lookups using dns-325/dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13 failed for: [jessie_udp@_http._tcp.dns-test-service.dns-325.svc]

  Apr 18 00:31:11.210: INFO: DNS probes using dns-325/dns-test-1b846e2e-d67a-4d5e-88fc-f04cd698cd13 succeeded

  Apr 18 00:31:11.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/18/23 00:31:11.216
  STEP: deleting the test service @ 04/18/23 00:31:11.258
  STEP: deleting the test headless service @ 04/18/23 00:31:11.326
  STEP: Destroying namespace "dns-325" for this suite. @ 04/18/23 00:31:11.344
• [12.566 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 04/18/23 00:31:11.361
  Apr 18 00:31:11.361: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename gc @ 04/18/23 00:31:11.363
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:31:11.38
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:31:11.385
  STEP: create the deployment @ 04/18/23 00:31:11.389
  W0418 00:31:11.395119      21 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 04/18/23 00:31:11.395
  STEP: delete the deployment @ 04/18/23 00:31:11.91
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 04/18/23 00:31:11.915
  STEP: Gathering metrics @ 04/18/23 00:31:12.44
  Apr 18 00:31:12.567: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 18 00:31:12.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2068" for this suite. @ 04/18/23 00:31:12.575
• [1.220 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 04/18/23 00:31:12.582
  Apr 18 00:31:12.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename watch @ 04/18/23 00:31:12.583
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:31:12.612
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:31:12.622
  STEP: getting a starting resourceVersion @ 04/18/23 00:31:12.626
  STEP: starting a background goroutine to produce watch events @ 04/18/23 00:31:12.63
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 04/18/23 00:31:12.63
  Apr 18 00:31:15.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-5714" for this suite. @ 04/18/23 00:31:15.438
• [2.906 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 04/18/23 00:31:15.489
  Apr 18 00:31:15.489: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename emptydir @ 04/18/23 00:31:15.49
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:31:15.505
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:31:15.509
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 04/18/23 00:31:15.512
  STEP: Saw pod success @ 04/18/23 00:31:19.532
  Apr 18 00:31:19.534: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-f5c01499-c57e-4830-ab35-884ac5a16d3f container test-container: <nil>
  STEP: delete the pod @ 04/18/23 00:31:19.544
  Apr 18 00:31:19.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1269" for this suite. @ 04/18/23 00:31:19.575
• [4.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 04/18/23 00:31:19.591
  Apr 18 00:31:19.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename container-probe @ 04/18/23 00:31:19.592
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:31:19.622
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:31:19.638
  STEP: Creating pod test-grpc-e8823e0e-de0c-41c4-96ea-32da3437fe53 in namespace container-probe-3044 @ 04/18/23 00:31:19.659
  Apr 18 00:31:21.678: INFO: Started pod test-grpc-e8823e0e-de0c-41c4-96ea-32da3437fe53 in namespace container-probe-3044
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/18/23 00:31:21.678
  Apr 18 00:31:21.681: INFO: Initial restart count of pod test-grpc-e8823e0e-de0c-41c4-96ea-32da3437fe53 is 0
  Apr 18 00:32:37.864: INFO: Restart count of pod container-probe-3044/test-grpc-e8823e0e-de0c-41c4-96ea-32da3437fe53 is now 1 (1m16.182208858s elapsed)
  Apr 18 00:32:37.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/18/23 00:32:37.867
  STEP: Destroying namespace "container-probe-3044" for this suite. @ 04/18/23 00:32:37.891
• [78.306 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 04/18/23 00:32:37.899
  Apr 18 00:32:37.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename downward-api @ 04/18/23 00:32:37.9
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:32:37.948
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:32:37.953
  STEP: Creating a pod to test downward api env vars @ 04/18/23 00:32:37.963
  STEP: Saw pod success @ 04/18/23 00:32:41.994
  Apr 18 00:32:41.997: INFO: Trying to get logs from node ip-10-0-14-154 pod downward-api-96e615e6-e250-411a-b9bf-23071dceae4e container dapi-container: <nil>
  STEP: delete the pod @ 04/18/23 00:32:42.004
  Apr 18 00:32:42.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1511" for this suite. @ 04/18/23 00:32:42.054
• [4.166 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 04/18/23 00:32:42.065
  Apr 18 00:32:42.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename deployment @ 04/18/23 00:32:42.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:32:42.093
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:32:42.099
  Apr 18 00:32:42.104: INFO: Creating deployment "test-recreate-deployment"
  Apr 18 00:32:42.114: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Apr 18 00:32:42.124: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  Apr 18 00:32:44.130: INFO: Waiting deployment "test-recreate-deployment" to complete
  Apr 18 00:32:44.133: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Apr 18 00:32:44.141: INFO: Updating deployment test-recreate-deployment
  Apr 18 00:32:44.141: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Apr 18 00:32:44.375: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-1183  7c2a4522-2ece-4bb5-bbf9-fffc66ebf4bd 49327 2 2023-04-18 00:32:42 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-18 00:32:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-18 00:32:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00416dda8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-18 00:32:44 +0000 UTC,LastTransitionTime:2023-04-18 00:32:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-04-18 00:32:44 +0000 UTC,LastTransitionTime:2023-04-18 00:32:42 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Apr 18 00:32:44.378: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-1183  1f165e4f-cbb0-4dc7-8839-cbf0791586c3 49326 1 2023-04-18 00:32:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 7c2a4522-2ece-4bb5-bbf9-fffc66ebf4bd 0xc0044ca167 0xc0044ca168}] [] [{kube-controller-manager Update apps/v1 2023-04-18 00:32:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7c2a4522-2ece-4bb5-bbf9-fffc66ebf4bd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-18 00:32:44 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044ca208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 18 00:32:44.378: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Apr 18 00:32:44.378: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-1183  151731e4-1537-4c53-ad74-b2e010ed0762 49315 2 2023-04-18 00:32:42 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 7c2a4522-2ece-4bb5-bbf9-fffc66ebf4bd 0xc0044ca277 0xc0044ca278}] [] [{kube-controller-manager Update apps/v1 2023-04-18 00:32:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7c2a4522-2ece-4bb5-bbf9-fffc66ebf4bd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-18 00:32:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044ca328 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 18 00:32:44.381: INFO: Pod "test-recreate-deployment-54757ffd6c-lgchs" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-lgchs test-recreate-deployment-54757ffd6c- deployment-1183  9c3f8084-3e5f-4aa2-aabb-f8e3fe8043e5 49325 0 2023-04-18 00:32:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 1f165e4f-cbb0-4dc7-8839-cbf0791586c3 0xc0044ca7b7 0xc0044ca7b8}] [] [{kube-controller-manager Update v1 2023-04-18 00:32:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1f165e4f-cbb0-4dc7-8839-cbf0791586c3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-18 00:32:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qd9tt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qd9tt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-14-154,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:32:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:32:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:32:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:32:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.14.154,PodIP:,StartTime:2023-04-18 00:32:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:32:44.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1183" for this suite. @ 04/18/23 00:32:44.385
• [2.324 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 04/18/23 00:32:44.391
  Apr 18 00:32:44.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename secrets @ 04/18/23 00:32:44.391
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:32:44.423
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:32:44.427
  STEP: Creating secret with name secret-test-6a20264c-5d46-4a65-8ed1-aae2cdd04a07 @ 04/18/23 00:32:44.487
  STEP: Creating a pod to test consume secrets @ 04/18/23 00:32:44.492
  STEP: Saw pod success @ 04/18/23 00:32:48.538
  Apr 18 00:32:48.541: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-secrets-f656dd72-54af-4c2b-9c15-ff835dc8f952 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/18/23 00:32:48.546
  Apr 18 00:32:48.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6808" for this suite. @ 04/18/23 00:32:48.563
  STEP: Destroying namespace "secret-namespace-9808" for this suite. @ 04/18/23 00:32:48.568
• [4.189 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 04/18/23 00:32:48.582
  Apr 18 00:32:48.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename subpath @ 04/18/23 00:32:48.583
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:32:48.603
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:32:48.608
  STEP: Setting up data @ 04/18/23 00:32:48.612
  STEP: Creating pod pod-subpath-test-downwardapi-nzht @ 04/18/23 00:32:48.623
  STEP: Creating a pod to test atomic-volume-subpath @ 04/18/23 00:32:48.623
  STEP: Saw pod success @ 04/18/23 00:33:12.701
  Apr 18 00:33:12.703: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-subpath-test-downwardapi-nzht container test-container-subpath-downwardapi-nzht: <nil>
  STEP: delete the pod @ 04/18/23 00:33:12.71
  STEP: Deleting pod pod-subpath-test-downwardapi-nzht @ 04/18/23 00:33:12.722
  Apr 18 00:33:12.723: INFO: Deleting pod "pod-subpath-test-downwardapi-nzht" in namespace "subpath-6194"
  Apr 18 00:33:12.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-6194" for this suite. @ 04/18/23 00:33:12.729
• [24.164 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 04/18/23 00:33:12.748
  Apr 18 00:33:12.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename statefulset @ 04/18/23 00:33:12.749
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:33:12.773
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:33:12.777
  STEP: Creating service test in namespace statefulset-90 @ 04/18/23 00:33:12.781
  STEP: Creating a new StatefulSet @ 04/18/23 00:33:12.789
  Apr 18 00:33:12.807: INFO: Found 0 stateful pods, waiting for 3
  Apr 18 00:33:22.812: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 18 00:33:22.812: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 18 00:33:22.812: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Apr 18 00:33:22.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=statefulset-90 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 18 00:33:23.255: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 18 00:33:23.255: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 18 00:33:23.256: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 04/18/23 00:33:33.269
  Apr 18 00:33:33.290: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 04/18/23 00:33:33.29
  STEP: Updating Pods in reverse ordinal order @ 04/18/23 00:33:43.312
  Apr 18 00:33:43.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=statefulset-90 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 18 00:33:43.483: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 18 00:33:43.483: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 18 00:33:43.483: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  STEP: Rolling back to a previous revision @ 04/18/23 00:33:53.501
  Apr 18 00:33:53.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=statefulset-90 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 18 00:33:53.649: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 18 00:33:53.649: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 18 00:33:53.649: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 18 00:34:03.700: INFO: Updating stateful set ss2
  STEP: Rolling back update in reverse ordinal order @ 04/18/23 00:34:13.724
  Apr 18 00:34:13.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=statefulset-90 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 18 00:34:13.859: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 18 00:34:13.859: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 18 00:34:13.859: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 18 00:34:23.879: INFO: Deleting all statefulset in ns statefulset-90
  Apr 18 00:34:23.884: INFO: Scaling statefulset ss2 to 0
  Apr 18 00:34:33.903: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 18 00:34:33.907: INFO: Deleting statefulset ss2
  Apr 18 00:34:33.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-90" for this suite. @ 04/18/23 00:34:33.922
• [81.182 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 04/18/23 00:34:33.931
  Apr 18 00:34:33.931: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename kubectl @ 04/18/23 00:34:33.932
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:34:33.952
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:34:33.957
  STEP: starting the proxy server @ 04/18/23 00:34:33.96
  Apr 18 00:34:33.960: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-5522 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 04/18/23 00:34:34.028
  Apr 18 00:34:34.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5522" for this suite. @ 04/18/23 00:34:34.049
• [0.123 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 04/18/23 00:34:34.057
  Apr 18 00:34:34.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename kubelet-test @ 04/18/23 00:34:34.057
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:34:34.074
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:34:34.078
  Apr 18 00:34:36.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1353" for this suite. @ 04/18/23 00:34:36.119
• [2.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 04/18/23 00:34:36.129
  Apr 18 00:34:36.129: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename kubectl @ 04/18/23 00:34:36.13
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:34:36.168
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:34:36.173
  STEP: validating api versions @ 04/18/23 00:34:36.177
  Apr 18 00:34:36.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-884 api-versions'
  Apr 18 00:34:36.277: INFO: stderr: ""
  Apr 18 00:34:36.277: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Apr 18 00:34:36.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-884" for this suite. @ 04/18/23 00:34:36.282
• [0.161 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 04/18/23 00:34:36.291
  Apr 18 00:34:36.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 00:34:36.292
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:34:36.311
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:34:36.315
  STEP: Creating secret with name projected-secret-test-95779eca-11d3-462e-8bb6-337a07c67b50 @ 04/18/23 00:34:36.319
  STEP: Creating a pod to test consume secrets @ 04/18/23 00:34:36.324
  STEP: Saw pod success @ 04/18/23 00:34:40.344
  Apr 18 00:34:40.347: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-projected-secrets-df6f02aa-105e-4450-af54-b5083924c7d4 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/18/23 00:34:40.352
  Apr 18 00:34:40.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-769" for this suite. @ 04/18/23 00:34:40.368
• [4.081 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 04/18/23 00:34:40.374
  Apr 18 00:34:40.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename crd-webhook @ 04/18/23 00:34:40.375
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:34:40.39
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:34:40.394
  STEP: Setting up server cert @ 04/18/23 00:34:40.397
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 04/18/23 00:34:40.78
  STEP: Deploying the custom resource conversion webhook pod @ 04/18/23 00:34:40.79
  STEP: Wait for the deployment to be ready @ 04/18/23 00:34:40.802
  Apr 18 00:34:40.813: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/18/23 00:34:42.821
  STEP: Verifying the service has paired with the endpoint @ 04/18/23 00:34:42.833
  Apr 18 00:34:43.833: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Apr 18 00:34:43.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Creating a v1 custom resource @ 04/18/23 00:34:46.41
  STEP: Create a v2 custom resource @ 04/18/23 00:34:46.427
  STEP: List CRs in v1 @ 04/18/23 00:34:46.48
  STEP: List CRs in v2 @ 04/18/23 00:34:46.486
  Apr 18 00:34:46.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-5527" for this suite. @ 04/18/23 00:34:47.1
• [6.736 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 04/18/23 00:34:47.111
  Apr 18 00:34:47.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename replication-controller @ 04/18/23 00:34:47.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:34:47.142
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:34:47.146
  STEP: Creating replication controller my-hostname-basic-d72f88b0-5715-48d7-a9c1-2669ede10780 @ 04/18/23 00:34:47.15
  Apr 18 00:34:47.159: INFO: Pod name my-hostname-basic-d72f88b0-5715-48d7-a9c1-2669ede10780: Found 0 pods out of 1
  Apr 18 00:34:52.166: INFO: Pod name my-hostname-basic-d72f88b0-5715-48d7-a9c1-2669ede10780: Found 1 pods out of 1
  Apr 18 00:34:52.166: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-d72f88b0-5715-48d7-a9c1-2669ede10780" are running
  Apr 18 00:34:52.170: INFO: Pod "my-hostname-basic-d72f88b0-5715-48d7-a9c1-2669ede10780-nw9mg" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-18 00:34:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-18 00:34:48 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-18 00:34:48 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-18 00:34:47 +0000 UTC Reason: Message:}])
  Apr 18 00:34:52.170: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 04/18/23 00:34:52.17
  Apr 18 00:34:52.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-5143" for this suite. @ 04/18/23 00:34:52.189
• [5.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 04/18/23 00:34:52.201
  Apr 18 00:34:52.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename disruption @ 04/18/23 00:34:52.203
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:34:52.235
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:34:52.242
  STEP: Waiting for the pdb to be processed @ 04/18/23 00:34:52.254
  STEP: Updating PodDisruptionBudget status @ 04/18/23 00:34:54.261
  STEP: Waiting for all pods to be running @ 04/18/23 00:34:54.276
  Apr 18 00:34:54.287: INFO: running pods: 0 < 1
  Apr 18 00:34:56.291: INFO: running pods: 0 < 1
  STEP: locating a running pod @ 04/18/23 00:34:58.291
  STEP: Waiting for the pdb to be processed @ 04/18/23 00:34:58.301
  STEP: Patching PodDisruptionBudget status @ 04/18/23 00:34:58.309
  STEP: Waiting for the pdb to be processed @ 04/18/23 00:34:58.317
  Apr 18 00:34:58.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2312" for this suite. @ 04/18/23 00:34:58.324
• [6.129 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 04/18/23 00:34:58.335
  Apr 18 00:34:58.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename daemonsets @ 04/18/23 00:34:58.338
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:34:58.355
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:34:58.359
  STEP: Creating simple DaemonSet "daemon-set" @ 04/18/23 00:34:58.379
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/18/23 00:34:58.384
  Apr 18 00:34:58.389: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:34:58.392: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 00:34:58.392: INFO: Node ip-10-0-14-154 is running 0 daemon pod, expected 1
  Apr 18 00:34:59.412: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:34:59.425: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 18 00:34:59.426: INFO: Node ip-10-0-14-154 is running 0 daemon pod, expected 1
  Apr 18 00:35:00.397: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:35:00.399: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 18 00:35:00.399: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Getting /status @ 04/18/23 00:35:00.402
  Apr 18 00:35:00.405: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 04/18/23 00:35:00.405
  Apr 18 00:35:00.415: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 04/18/23 00:35:00.415
  Apr 18 00:35:00.417: INFO: Observed &DaemonSet event: ADDED
  Apr 18 00:35:00.418: INFO: Observed &DaemonSet event: MODIFIED
  Apr 18 00:35:00.418: INFO: Observed &DaemonSet event: MODIFIED
  Apr 18 00:35:00.418: INFO: Observed &DaemonSet event: MODIFIED
  Apr 18 00:35:00.418: INFO: Found daemon set daemon-set in namespace daemonsets-7678 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 18 00:35:00.418: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 04/18/23 00:35:00.418
  STEP: watching for the daemon set status to be patched @ 04/18/23 00:35:00.425
  Apr 18 00:35:00.428: INFO: Observed &DaemonSet event: ADDED
  Apr 18 00:35:00.428: INFO: Observed &DaemonSet event: MODIFIED
  Apr 18 00:35:00.428: INFO: Observed &DaemonSet event: MODIFIED
  Apr 18 00:35:00.428: INFO: Observed &DaemonSet event: MODIFIED
  Apr 18 00:35:00.428: INFO: Observed daemon set daemon-set in namespace daemonsets-7678 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 18 00:35:00.429: INFO: Observed &DaemonSet event: MODIFIED
  Apr 18 00:35:00.429: INFO: Found daemon set daemon-set in namespace daemonsets-7678 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Apr 18 00:35:00.429: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 04/18/23 00:35:00.433
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7678, will wait for the garbage collector to delete the pods @ 04/18/23 00:35:00.433
  Apr 18 00:35:00.492: INFO: Deleting DaemonSet.extensions daemon-set took: 5.54633ms
  Apr 18 00:35:00.592: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.171994ms
  Apr 18 00:35:03.703: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 00:35:03.704: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 18 00:35:03.710: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"50378"},"items":null}

  Apr 18 00:35:03.718: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"50378"},"items":null}

  Apr 18 00:35:03.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7678" for this suite. @ 04/18/23 00:35:03.747
• [5.425 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 04/18/23 00:35:03.761
  Apr 18 00:35:03.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/18/23 00:35:03.762
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:35:03.835
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:35:03.85
  Apr 18 00:35:03.863: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/18/23 00:35:05.594
  Apr 18 00:35:05.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-3523 --namespace=crd-publish-openapi-3523 create -f -'
  Apr 18 00:35:06.421: INFO: stderr: ""
  Apr 18 00:35:06.421: INFO: stdout: "e2e-test-crd-publish-openapi-9292-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Apr 18 00:35:06.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-3523 --namespace=crd-publish-openapi-3523 delete e2e-test-crd-publish-openapi-9292-crds test-cr'
  Apr 18 00:35:06.545: INFO: stderr: ""
  Apr 18 00:35:06.545: INFO: stdout: "e2e-test-crd-publish-openapi-9292-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Apr 18 00:35:06.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-3523 --namespace=crd-publish-openapi-3523 apply -f -'
  Apr 18 00:35:06.841: INFO: stderr: ""
  Apr 18 00:35:06.842: INFO: stdout: "e2e-test-crd-publish-openapi-9292-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Apr 18 00:35:06.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-3523 --namespace=crd-publish-openapi-3523 delete e2e-test-crd-publish-openapi-9292-crds test-cr'
  Apr 18 00:35:06.944: INFO: stderr: ""
  Apr 18 00:35:06.944: INFO: stdout: "e2e-test-crd-publish-openapi-9292-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 04/18/23 00:35:06.944
  Apr 18 00:35:06.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-3523 explain e2e-test-crd-publish-openapi-9292-crds'
  Apr 18 00:35:07.207: INFO: stderr: ""
  Apr 18 00:35:07.207: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-9292-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  Apr 18 00:35:08.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3523" for this suite. @ 04/18/23 00:35:08.751
• [5.025 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 04/18/23 00:35:08.787
  Apr 18 00:35:08.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 00:35:08.789
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:35:08.823
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:35:08.831
  STEP: Creating configMap with name projected-configmap-test-volume-3b99f1ed-9f92-4726-be26-298a0b7f3dac @ 04/18/23 00:35:08.85
  STEP: Creating a pod to test consume configMaps @ 04/18/23 00:35:08.868
  STEP: Saw pod success @ 04/18/23 00:35:12.902
  Apr 18 00:35:12.905: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-projected-configmaps-5438b324-7b7d-48c4-bf3e-d57a227458ee container agnhost-container: <nil>
  STEP: delete the pod @ 04/18/23 00:35:12.914
  Apr 18 00:35:12.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6903" for this suite. @ 04/18/23 00:35:12.931
• [4.149 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 04/18/23 00:35:12.938
  Apr 18 00:35:12.938: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename events @ 04/18/23 00:35:12.939
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:35:12.964
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:35:12.97
  STEP: creating a test event @ 04/18/23 00:35:12.975
  STEP: listing events in all namespaces @ 04/18/23 00:35:12.981
  STEP: listing events in test namespace @ 04/18/23 00:35:12.99
  STEP: listing events with field selection filtering on source @ 04/18/23 00:35:12.993
  STEP: listing events with field selection filtering on reportingController @ 04/18/23 00:35:12.997
  STEP: getting the test event @ 04/18/23 00:35:13
  STEP: patching the test event @ 04/18/23 00:35:13.005
  STEP: getting the test event @ 04/18/23 00:35:13.02
  STEP: updating the test event @ 04/18/23 00:35:13.023
  STEP: getting the test event @ 04/18/23 00:35:13.03
  STEP: deleting the test event @ 04/18/23 00:35:13.033
  STEP: listing events in all namespaces @ 04/18/23 00:35:13.039
  STEP: listing events in test namespace @ 04/18/23 00:35:13.043
  Apr 18 00:35:13.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-7503" for this suite. @ 04/18/23 00:35:13.05
• [0.117 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 04/18/23 00:35:13.056
  Apr 18 00:35:13.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename resourcequota @ 04/18/23 00:35:13.057
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:35:13.074
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:35:13.08
  STEP: Creating a ResourceQuota with best effort scope @ 04/18/23 00:35:13.084
  STEP: Ensuring ResourceQuota status is calculated @ 04/18/23 00:35:13.089
  STEP: Creating a ResourceQuota with not best effort scope @ 04/18/23 00:35:15.094
  STEP: Ensuring ResourceQuota status is calculated @ 04/18/23 00:35:15.101
  STEP: Creating a best-effort pod @ 04/18/23 00:35:17.109
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 04/18/23 00:35:17.132
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 04/18/23 00:35:19.136
  STEP: Deleting the pod @ 04/18/23 00:35:21.139
  STEP: Ensuring resource quota status released the pod usage @ 04/18/23 00:35:21.151
  STEP: Creating a not best-effort pod @ 04/18/23 00:35:23.155
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 04/18/23 00:35:23.165
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 04/18/23 00:35:25.169
  STEP: Deleting the pod @ 04/18/23 00:35:27.174
  STEP: Ensuring resource quota status released the pod usage @ 04/18/23 00:35:27.186
  Apr 18 00:35:29.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4507" for this suite. @ 04/18/23 00:35:29.194
• [16.147 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 04/18/23 00:35:29.204
  Apr 18 00:35:29.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/18/23 00:35:29.205
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:35:29.227
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:35:29.231
  STEP: create the container to handle the HTTPGet hook request. @ 04/18/23 00:35:29.237
  STEP: create the pod with lifecycle hook @ 04/18/23 00:35:31.261
  STEP: check poststart hook @ 04/18/23 00:35:33.288
  STEP: delete the pod with lifecycle hook @ 04/18/23 00:35:33.294
  Apr 18 00:35:37.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-8771" for this suite. @ 04/18/23 00:35:37.32
• [8.132 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 04/18/23 00:35:37.336
  Apr 18 00:35:37.336: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename sched-preemption @ 04/18/23 00:35:37.338
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:35:37.367
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:35:37.372
  Apr 18 00:35:37.391: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 18 00:36:37.414: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 04/18/23 00:36:37.418
  Apr 18 00:36:37.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename sched-preemption-path @ 04/18/23 00:36:37.419
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:36:37.438
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:36:37.441
  STEP: Finding an available node @ 04/18/23 00:36:37.447
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/18/23 00:36:37.447
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/18/23 00:36:39.472
  Apr 18 00:36:39.484: INFO: found a healthy node: ip-10-0-14-154
  Apr 18 00:36:45.591: INFO: pods created so far: [1 1 1]
  Apr 18 00:36:45.591: INFO: length of pods created so far: 3
  Apr 18 00:36:47.609: INFO: pods created so far: [2 2 1]
  Apr 18 00:36:54.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 18 00:36:54.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-9144" for this suite. @ 04/18/23 00:36:54.786
  STEP: Destroying namespace "sched-preemption-677" for this suite. @ 04/18/23 00:36:54.804
• [77.488 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 04/18/23 00:36:54.827
  Apr 18 00:36:54.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename init-container @ 04/18/23 00:36:54.829
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:36:54.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:36:54.96
  STEP: creating the pod @ 04/18/23 00:36:54.963
  Apr 18 00:36:54.963: INFO: PodSpec: initContainers in spec.initContainers
  Apr 18 00:36:59.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-2186" for this suite. @ 04/18/23 00:36:59.113
• [4.304 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 04/18/23 00:36:59.131
  Apr 18 00:36:59.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename resourcequota @ 04/18/23 00:36:59.132
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:36:59.166
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:36:59.174
  STEP: Counting existing ResourceQuota @ 04/18/23 00:37:16.184
  STEP: Creating a ResourceQuota @ 04/18/23 00:37:21.191
  STEP: Ensuring resource quota status is calculated @ 04/18/23 00:37:21.197
  STEP: Creating a ConfigMap @ 04/18/23 00:37:23.201
  STEP: Ensuring resource quota status captures configMap creation @ 04/18/23 00:37:23.211
  STEP: Deleting a ConfigMap @ 04/18/23 00:37:25.217
  STEP: Ensuring resource quota status released usage @ 04/18/23 00:37:25.221
  Apr 18 00:37:27.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2430" for this suite. @ 04/18/23 00:37:27.232
• [28.107 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 04/18/23 00:37:27.239
  Apr 18 00:37:27.239: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename runtimeclass @ 04/18/23 00:37:27.24
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:37:27.269
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:37:27.275
  STEP: Deleting RuntimeClass runtimeclass-5936-delete-me @ 04/18/23 00:37:27.284
  STEP: Waiting for the RuntimeClass to disappear @ 04/18/23 00:37:27.289
  Apr 18 00:37:27.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5936" for this suite. @ 04/18/23 00:37:27.317
• [0.084 seconds]
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 04/18/23 00:37:27.324
  Apr 18 00:37:27.324: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename configmap @ 04/18/23 00:37:27.325
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:37:27.341
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:37:27.345
  STEP: Creating configMap that has name configmap-test-emptyKey-db50a2d1-3814-4cfc-bc98-bb02ede3487d @ 04/18/23 00:37:27.35
  Apr 18 00:37:27.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1468" for this suite. @ 04/18/23 00:37:27.357
• [0.038 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 04/18/23 00:37:27.365
  Apr 18 00:37:27.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename downward-api @ 04/18/23 00:37:27.366
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:37:27.382
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:37:27.387
  STEP: Creating a pod to test downward API volume plugin @ 04/18/23 00:37:27.392
  STEP: Saw pod success @ 04/18/23 00:37:31.416
  Apr 18 00:37:31.420: INFO: Trying to get logs from node ip-10-0-14-154 pod downwardapi-volume-78b0df20-eeec-4e80-be4f-e3f025a08c4b container client-container: <nil>
  STEP: delete the pod @ 04/18/23 00:37:31.435
  Apr 18 00:37:31.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4623" for this suite. @ 04/18/23 00:37:31.46
• [4.109 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 04/18/23 00:37:31.475
  Apr 18 00:37:31.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename downward-api @ 04/18/23 00:37:31.476
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:37:31.495
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:37:31.499
  STEP: Creating a pod to test downward api env vars @ 04/18/23 00:37:31.503
  STEP: Saw pod success @ 04/18/23 00:37:35.526
  Apr 18 00:37:35.529: INFO: Trying to get logs from node ip-10-0-14-154 pod downward-api-dc0dbb24-67df-4f55-a383-1d02802d75b4 container dapi-container: <nil>
  STEP: delete the pod @ 04/18/23 00:37:35.535
  Apr 18 00:37:35.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9219" for this suite. @ 04/18/23 00:37:35.559
• [4.091 seconds]
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 04/18/23 00:37:35.566
  Apr 18 00:37:35.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 00:37:35.568
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:37:35.586
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:37:35.589
  STEP: Creating configMap with name cm-test-opt-del-50791939-56ed-4df8-89c4-a4dc420af84a @ 04/18/23 00:37:35.598
  STEP: Creating configMap with name cm-test-opt-upd-8b5dd48e-0f4d-4dd7-9228-fe8c03ba1823 @ 04/18/23 00:37:35.604
  STEP: Creating the pod @ 04/18/23 00:37:35.608
  STEP: Deleting configmap cm-test-opt-del-50791939-56ed-4df8-89c4-a4dc420af84a @ 04/18/23 00:37:37.644
  STEP: Updating configmap cm-test-opt-upd-8b5dd48e-0f4d-4dd7-9228-fe8c03ba1823 @ 04/18/23 00:37:37.648
  STEP: Creating configMap with name cm-test-opt-create-e54d00ad-930c-4747-82ae-8a7c1047596e @ 04/18/23 00:37:37.653
  STEP: waiting to observe update in volume @ 04/18/23 00:37:37.658
  Apr 18 00:37:39.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7799" for this suite. @ 04/18/23 00:37:39.691
• [4.133 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 04/18/23 00:37:39.699
  Apr 18 00:37:39.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename var-expansion @ 04/18/23 00:37:39.7
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:37:39.729
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:37:39.732
  STEP: Creating a pod to test substitution in container's args @ 04/18/23 00:37:39.74
  STEP: Saw pod success @ 04/18/23 00:37:43.787
  Apr 18 00:37:43.790: INFO: Trying to get logs from node ip-10-0-27-81 pod var-expansion-c528b67e-cc92-4e18-9ac3-86662a6e7c2c container dapi-container: <nil>
  STEP: delete the pod @ 04/18/23 00:37:43.812
  Apr 18 00:37:43.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4375" for this suite. @ 04/18/23 00:37:43.828
• [4.137 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 04/18/23 00:37:43.846
  Apr 18 00:37:43.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename var-expansion @ 04/18/23 00:37:43.85
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:37:43.866
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:37:43.87
  Apr 18 00:37:45.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 18 00:37:45.896: INFO: Deleting pod "var-expansion-40e30d2f-79f0-40a1-a803-81a627e537d5" in namespace "var-expansion-2464"
  Apr 18 00:37:45.901: INFO: Wait up to 5m0s for pod "var-expansion-40e30d2f-79f0-40a1-a803-81a627e537d5" to be fully deleted
  STEP: Destroying namespace "var-expansion-2464" for this suite. @ 04/18/23 00:37:47.909
• [4.073 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 04/18/23 00:37:47.92
  Apr 18 00:37:47.921: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 00:37:47.921
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:37:47.937
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:37:47.941
  STEP: Creating configMap with name projected-configmap-test-volume-624d55eb-afe3-46c8-9574-6dfde857c3a2 @ 04/18/23 00:37:47.944
  STEP: Creating a pod to test consume configMaps @ 04/18/23 00:37:47.949
  STEP: Saw pod success @ 04/18/23 00:37:51.967
  Apr 18 00:37:51.970: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-projected-configmaps-63a1f942-7ff3-41ff-87d4-5e75bde292aa container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 04/18/23 00:37:51.975
  Apr 18 00:37:51.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1204" for this suite. @ 04/18/23 00:37:51.992
• [4.077 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 04/18/23 00:37:51.998
  Apr 18 00:37:51.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename gc @ 04/18/23 00:37:52
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:37:52.018
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:37:52.032
  Apr 18 00:37:52.145: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"1b1330f2-6014-42b9-87c7-bf1e69d14314", Controller:(*bool)(0xc00502ea96), BlockOwnerDeletion:(*bool)(0xc00502ea97)}}
  Apr 18 00:37:52.248: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"072a86e1-919e-472d-aecb-32ea0111363f", Controller:(*bool)(0xc00502ed26), BlockOwnerDeletion:(*bool)(0xc00502ed27)}}
  Apr 18 00:37:52.343: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"3096f6b7-1c32-4611-9c38-c81debb5e51a", Controller:(*bool)(0xc00502ef36), BlockOwnerDeletion:(*bool)(0xc00502ef37)}}
  Apr 18 00:37:57.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1202" for this suite. @ 04/18/23 00:37:57.395
• [5.403 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 04/18/23 00:37:57.402
  Apr 18 00:37:57.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename csiinlinevolumes @ 04/18/23 00:37:57.403
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:37:57.423
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:37:57.432
  STEP: creating @ 04/18/23 00:37:57.438
  STEP: getting @ 04/18/23 00:37:57.473
  STEP: listing in namespace @ 04/18/23 00:37:57.491
  STEP: patching @ 04/18/23 00:37:57.496
  STEP: deleting @ 04/18/23 00:37:57.507
  Apr 18 00:37:57.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-5307" for this suite. @ 04/18/23 00:37:57.529
• [0.135 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 04/18/23 00:37:57.537
  Apr 18 00:37:57.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename configmap @ 04/18/23 00:37:57.538
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:37:57.562
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:37:57.566
  STEP: Creating configMap with name configmap-test-volume-60c30d35-e76e-47fd-8fcb-fe4e08b71951 @ 04/18/23 00:37:57.569
  STEP: Creating a pod to test consume configMaps @ 04/18/23 00:37:57.573
  STEP: Saw pod success @ 04/18/23 00:38:01.598
  Apr 18 00:38:01.607: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-configmaps-04cc5337-0177-45d5-a88a-9ce29de4294f container agnhost-container: <nil>
  STEP: delete the pod @ 04/18/23 00:38:01.624
  Apr 18 00:38:01.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-54" for this suite. @ 04/18/23 00:38:01.695
• [4.183 seconds]
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 04/18/23 00:38:01.72
  Apr 18 00:38:01.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename deployment @ 04/18/23 00:38:01.721
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:38:01.837
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:38:01.859
  Apr 18 00:38:01.870: INFO: Creating deployment "webserver-deployment"
  Apr 18 00:38:01.917: INFO: Waiting for observed generation 1
  Apr 18 00:38:03.967: INFO: Waiting for all required pods to come up
  Apr 18 00:38:03.973: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 04/18/23 00:38:03.973
  Apr 18 00:38:06.019: INFO: Waiting for deployment "webserver-deployment" to complete
  Apr 18 00:38:06.024: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:10, UpdatedReplicas:10, ReadyReplicas:9, AvailableReplicas:9, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 0, 38, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 38, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 0, 38, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 38, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"webserver-deployment-67bd4bf6dc\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 18 00:38:08.030: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Apr 18 00:38:08.038: INFO: Updating deployment webserver-deployment
  Apr 18 00:38:08.038: INFO: Waiting for observed generation 2
  Apr 18 00:38:10.044: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Apr 18 00:38:10.050: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Apr 18 00:38:10.052: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Apr 18 00:38:10.068: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Apr 18 00:38:10.068: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Apr 18 00:38:10.071: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Apr 18 00:38:10.075: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Apr 18 00:38:10.075: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Apr 18 00:38:10.083: INFO: Updating deployment webserver-deployment
  Apr 18 00:38:10.083: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Apr 18 00:38:10.090: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Apr 18 00:38:10.095: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Apr 18 00:38:10.150: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-9672  f4832ed2-635e-4b47-80ce-bdd30c84478b 51693 3 2023-04-18 00:38:01 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-04-18 00:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-04-18 00:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045f1708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-18 00:38:05 +0000 UTC,LastTransitionTime:2023-04-18 00:38:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-04-18 00:38:08 +0000 UTC,LastTransitionTime:2023-04-18 00:38:01 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Apr 18 00:38:10.184: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-9672  c1337afd-79eb-4b8e-a375-76bee3607850 51696 3 2023-04-18 00:38:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment f4832ed2-635e-4b47-80ce-bdd30c84478b 0xc004e07bf7 0xc004e07bf8}] [] [{kube-controller-manager Update apps/v1 2023-04-18 00:38:08 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-18 00:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4832ed2-635e-4b47-80ce-bdd30c84478b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e07c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 18 00:38:10.184: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Apr 18 00:38:10.184: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-9672  3a75354d-144d-4fb6-afd1-609cfc9ad6f6 51694 3 2023-04-18 00:38:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment f4832ed2-635e-4b47-80ce-bdd30c84478b 0xc004e07ae7 0xc004e07ae8}] [] [{kube-controller-manager Update apps/v1 2023-04-18 00:38:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-18 00:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4832ed2-635e-4b47-80ce-bdd30c84478b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e07b98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Apr 18 00:38:10.248: INFO: Pod "webserver-deployment-67bd4bf6dc-2p2c6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-2p2c6 webserver-deployment-67bd4bf6dc- deployment-9672  e06a2e39-7333-4034-accf-3f6d66527f53 51724 0 2023-04-18 00:38:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 3a75354d-144d-4fb6-afd1-609cfc9ad6f6 0xc0045f1b37 0xc0045f1b38}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a75354d-144d-4fb6-afd1-609cfc9ad6f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-25bll,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-25bll,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.248: INFO: Pod "webserver-deployment-67bd4bf6dc-2w4l6" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-2w4l6 webserver-deployment-67bd4bf6dc- deployment-9672  a04f6748-a6b0-4d57-8e3f-7341cc309bc7 51562 0 2023-04-18 00:38:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:e1120c5be7e9e99df498cd8c0ab391a2649554f0e9b3624597fb070c24b21a47 cni.projectcalico.org/podIP:10.2.129.123/32 cni.projectcalico.org/podIPs:10.2.129.123/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 3a75354d-144d-4fb6-afd1-609cfc9ad6f6 0xc0045f1c97 0xc0045f1c98}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a75354d-144d-4fb6-afd1-609cfc9ad6f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-18 00:38:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-18 00:38:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.129.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6cm5x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6cm5x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-27-81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.27.81,PodIP:10.2.129.123,StartTime:2023-04-18 00:38:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-18 00:38:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a0989e68f4580c613ee75dc591d6d9b39ab531286f5ac773d9e32c9f7e94cdd8,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.129.123,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.248: INFO: Pod "webserver-deployment-67bd4bf6dc-45l44" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-45l44 webserver-deployment-67bd4bf6dc- deployment-9672  9a740159-3ac4-4e6e-a2c3-2677717e858c 51543 0 2023-04-18 00:38:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:1b91c7ec9517b6abc14311a7bc52c3002190fe1ebb448cbd253f672ef54e6a9b cni.projectcalico.org/podIP:10.2.212.192/32 cni.projectcalico.org/podIPs:10.2.212.192/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 3a75354d-144d-4fb6-afd1-609cfc9ad6f6 0xc0045f1ec0 0xc0045f1ec1}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a75354d-144d-4fb6-afd1-609cfc9ad6f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-18 00:38:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-18 00:38:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.212.192\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pbmvm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pbmvm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-14-154,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.14.154,PodIP:10.2.212.192,StartTime:2023-04-18 00:38:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-18 00:38:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0623b1eb422007c82f811abefce8b2cfeec356abbdb1b7a3ae51f5a97f85dbca,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.212.192,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.248: INFO: Pod "webserver-deployment-67bd4bf6dc-4xhps" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-4xhps webserver-deployment-67bd4bf6dc- deployment-9672  e060ce64-3118-4db7-9b52-b0c9bacfeba5 51722 0 2023-04-18 00:38:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 3a75354d-144d-4fb6-afd1-609cfc9ad6f6 0xc003a39c57 0xc003a39c58}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a75354d-144d-4fb6-afd1-609cfc9ad6f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jlq8h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jlq8h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-27-81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.249: INFO: Pod "webserver-deployment-67bd4bf6dc-7nfcf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-7nfcf webserver-deployment-67bd4bf6dc- deployment-9672  6d6deced-2e85-4eab-a121-c0bf5c64d0a5 51721 0 2023-04-18 00:38:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 3a75354d-144d-4fb6-afd1-609cfc9ad6f6 0xc003a39dc0 0xc003a39dc1}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a75354d-144d-4fb6-afd1-609cfc9ad6f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lhkz2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lhkz2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.249: INFO: Pod "webserver-deployment-67bd4bf6dc-7sgmx" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-7sgmx webserver-deployment-67bd4bf6dc- deployment-9672  0c85e097-f044-4973-b7aa-1d3e260e0762 51709 0 2023-04-18 00:38:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 3a75354d-144d-4fb6-afd1-609cfc9ad6f6 0xc003a39ef7 0xc003a39ef8}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a75354d-144d-4fb6-afd1-609cfc9ad6f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vzwl8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vzwl8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.249: INFO: Pod "webserver-deployment-67bd4bf6dc-d2p6n" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-d2p6n webserver-deployment-67bd4bf6dc- deployment-9672  b2010903-55e9-4936-acbf-2a9a08e88040 51723 0 2023-04-18 00:38:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 3a75354d-144d-4fb6-afd1-609cfc9ad6f6 0xc005402037 0xc005402038}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a75354d-144d-4fb6-afd1-609cfc9ad6f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sgltm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sgltm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.249: INFO: Pod "webserver-deployment-67bd4bf6dc-glm8h" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-glm8h webserver-deployment-67bd4bf6dc- deployment-9672  b72f3db2-d3dd-4cd6-a1c9-aecec037e723 51719 0 2023-04-18 00:38:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 3a75354d-144d-4fb6-afd1-609cfc9ad6f6 0xc005402177 0xc005402178}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a75354d-144d-4fb6-afd1-609cfc9ad6f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-18 00:38:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wdl2z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wdl2z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-14-154,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.14.154,PodIP:,StartTime:2023-04-18 00:38:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.250: INFO: Pod "webserver-deployment-67bd4bf6dc-kct9k" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-kct9k webserver-deployment-67bd4bf6dc- deployment-9672  d25d6520-1190-4c35-a17d-5168bb43a3ce 51708 0 2023-04-18 00:38:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 3a75354d-144d-4fb6-afd1-609cfc9ad6f6 0xc005402347 0xc005402348}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a75354d-144d-4fb6-afd1-609cfc9ad6f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dzwwf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dzwwf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-14-154,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.250: INFO: Pod "webserver-deployment-67bd4bf6dc-njd5m" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-njd5m webserver-deployment-67bd4bf6dc- deployment-9672  cf49f38f-eedb-449d-98a0-cae9a3e261e1 51534 0 2023-04-18 00:38:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:99405b41c11cd6bdcfc4887608a45a70b809b2a173d454476cf9c0e3fe2d215c cni.projectcalico.org/podIP:10.2.212.195/32 cni.projectcalico.org/podIPs:10.2.212.195/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 3a75354d-144d-4fb6-afd1-609cfc9ad6f6 0xc0054024d0 0xc0054024d1}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a75354d-144d-4fb6-afd1-609cfc9ad6f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-18 00:38:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-18 00:38:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.212.195\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hbvt9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hbvt9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-14-154,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.14.154,PodIP:10.2.212.195,StartTime:2023-04-18 00:38:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-18 00:38:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a128715848b71a2975c32097156e4773465ade2202bfcd58027e9491cd8b087d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.212.195,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.250: INFO: Pod "webserver-deployment-67bd4bf6dc-nvrjz" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-nvrjz webserver-deployment-67bd4bf6dc- deployment-9672  c8f1b7e3-377b-47da-bd87-11120c2da6c2 51537 0 2023-04-18 00:38:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:bbc218a1c242a6b1ab3d794ea0c22e74901eea08bb576f701d474f6beef5a261 cni.projectcalico.org/podIP:10.2.212.194/32 cni.projectcalico.org/podIPs:10.2.212.194/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 3a75354d-144d-4fb6-afd1-609cfc9ad6f6 0xc0054026f7 0xc0054026f8}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a75354d-144d-4fb6-afd1-609cfc9ad6f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-18 00:38:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-18 00:38:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.212.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kmtw8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kmtw8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-14-154,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.14.154,PodIP:10.2.212.194,StartTime:2023-04-18 00:38:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-18 00:38:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b208dd4b6aee945933e482b9a199770518e15111b2d59d31ed319ee5fe731237,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.212.194,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.251: INFO: Pod "webserver-deployment-67bd4bf6dc-pnkzz" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-pnkzz webserver-deployment-67bd4bf6dc- deployment-9672  1de483d7-54ed-4286-a919-3a6dcfdbd5ae 51707 0 2023-04-18 00:38:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 3a75354d-144d-4fb6-afd1-609cfc9ad6f6 0xc005402907 0xc005402908}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a75354d-144d-4fb6-afd1-609cfc9ad6f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-269z4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-269z4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.251: INFO: Pod "webserver-deployment-67bd4bf6dc-px4dg" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-px4dg webserver-deployment-67bd4bf6dc- deployment-9672  83ca6a5f-20c6-4953-be96-5e99244030fa 51718 0 2023-04-18 00:38:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 3a75354d-144d-4fb6-afd1-609cfc9ad6f6 0xc005402a47 0xc005402a48}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a75354d-144d-4fb6-afd1-609cfc9ad6f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9b6xc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9b6xc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-27-81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.251: INFO: Pod "webserver-deployment-67bd4bf6dc-rtd65" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rtd65 webserver-deployment-67bd4bf6dc- deployment-9672  6b6e0598-614b-4a07-994d-567d5c54a02b 51565 0 2023-04-18 00:38:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:bbc3902752652ebded5cc72326e6c1b7ad0cedb85ee83a47a1a622bf8c6b055f cni.projectcalico.org/podIP:10.2.129.122/32 cni.projectcalico.org/podIPs:10.2.129.122/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 3a75354d-144d-4fb6-afd1-609cfc9ad6f6 0xc005402c00 0xc005402c01}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a75354d-144d-4fb6-afd1-609cfc9ad6f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-18 00:38:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-18 00:38:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.129.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xqmr8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xqmr8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-27-81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.27.81,PodIP:10.2.129.122,StartTime:2023-04-18 00:38:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-18 00:38:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://871e7dc51eb4a46a248947af26e483d5c56d0cc3586d2269a664c5e7ef4b4644,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.129.122,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.251: INFO: Pod "webserver-deployment-67bd4bf6dc-s7s9w" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-s7s9w webserver-deployment-67bd4bf6dc- deployment-9672  475fcc29-303d-42a1-9e63-1e5d95b6e321 51571 0 2023-04-18 00:38:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:fda2627a885504efe993156335ff61ba43f443e9b1334a56d1d99f59b17dce9f cni.projectcalico.org/podIP:10.2.129.125/32 cni.projectcalico.org/podIPs:10.2.129.125/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 3a75354d-144d-4fb6-afd1-609cfc9ad6f6 0xc005402e20 0xc005402e21}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a75354d-144d-4fb6-afd1-609cfc9ad6f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-18 00:38:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-18 00:38:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.129.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tzb6p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tzb6p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-27-81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.27.81,PodIP:10.2.129.125,StartTime:2023-04-18 00:38:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-18 00:38:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f849b9b28a207cda6dff7a8306fa3666935b954ed8da6a8b9959338c0f55396d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.129.125,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.252: INFO: Pod "webserver-deployment-67bd4bf6dc-vmjlw" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vmjlw webserver-deployment-67bd4bf6dc- deployment-9672  b0e58e30-6926-4170-8bc7-f3b0c674ce2c 51569 0 2023-04-18 00:38:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:0286b5f4426555bf83d0b41fc6d16624aced7fbccbb820b2685f76bd7abed14e cni.projectcalico.org/podIP:10.2.129.124/32 cni.projectcalico.org/podIPs:10.2.129.124/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 3a75354d-144d-4fb6-afd1-609cfc9ad6f6 0xc005403040 0xc005403041}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a75354d-144d-4fb6-afd1-609cfc9ad6f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-18 00:38:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-18 00:38:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.129.124\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8642b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8642b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-27-81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.27.81,PodIP:10.2.129.124,StartTime:2023-04-18 00:38:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-18 00:38:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b12428bba83a839ba33cae5585b6f139fa1b0b3ecae271b30cc7f19722423e2d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.129.124,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.252: INFO: Pod "webserver-deployment-67bd4bf6dc-vw5jv" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vw5jv webserver-deployment-67bd4bf6dc- deployment-9672  49f738e1-31e2-4f8b-ace0-423d563cd004 51710 0 2023-04-18 00:38:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 3a75354d-144d-4fb6-afd1-609cfc9ad6f6 0xc005403240 0xc005403241}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a75354d-144d-4fb6-afd1-609cfc9ad6f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9jmhj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9jmhj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-27-81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.252: INFO: Pod "webserver-deployment-67bd4bf6dc-xmgnw" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xmgnw webserver-deployment-67bd4bf6dc- deployment-9672  99661a83-9954-4d74-9275-5c7ea29c810c 51540 0 2023-04-18 00:38:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:f7981ce9a31c6501f39da6c3ededed01328f6324e5f572598e6cd20da9bfd248 cni.projectcalico.org/podIP:10.2.212.193/32 cni.projectcalico.org/podIPs:10.2.212.193/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 3a75354d-144d-4fb6-afd1-609cfc9ad6f6 0xc0054033c0 0xc0054033c1}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a75354d-144d-4fb6-afd1-609cfc9ad6f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-18 00:38:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-18 00:38:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.212.193\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-22ggw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-22ggw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-14-154,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.14.154,PodIP:10.2.212.193,StartTime:2023-04-18 00:38:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-18 00:38:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0b89d3a77c2b1d025b16c87413bc4cbfec23468197352e1e064ab5b165cb4e7b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.212.193,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.252: INFO: Pod "webserver-deployment-67bd4bf6dc-zs5sb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-zs5sb webserver-deployment-67bd4bf6dc- deployment-9672  7c44b3be-207b-4fa8-90ab-caf4a7301603 51725 0 2023-04-18 00:38:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 3a75354d-144d-4fb6-afd1-609cfc9ad6f6 0xc0054035c7 0xc0054035c8}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a75354d-144d-4fb6-afd1-609cfc9ad6f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8v5q6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8v5q6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.253: INFO: Pod "webserver-deployment-67bd4bf6dc-zzl95" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-zzl95 webserver-deployment-67bd4bf6dc- deployment-9672  faee8eb5-5c78-4be7-b4bd-46a925342f0c 51720 0 2023-04-18 00:38:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 3a75354d-144d-4fb6-afd1-609cfc9ad6f6 0xc005403707 0xc005403708}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a75354d-144d-4fb6-afd1-609cfc9ad6f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2zgbr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2zgbr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.254: INFO: Pod "webserver-deployment-7b75d79cf5-6sl28" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-6sl28 webserver-deployment-7b75d79cf5- deployment-9672  db318f92-a5cd-44b0-bf0b-f3eb9497ff46 51714 0 2023-04-18 00:38:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 c1337afd-79eb-4b8e-a375-76bee3607850 0xc005403847 0xc005403848}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1337afd-79eb-4b8e-a375-76bee3607850\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t76pm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t76pm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.255: INFO: Pod "webserver-deployment-7b75d79cf5-6zfxr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-6zfxr webserver-deployment-7b75d79cf5- deployment-9672  093bd427-0bbf-4f06-be8f-b4ae2d3b5b9f 51686 0 2023-04-18 00:38:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:0527edb18f81d4816c1916b3697e59c4905fa14d5e9fbcb18b0d7f2079642506 cni.projectcalico.org/podIP:10.2.129.126/32 cni.projectcalico.org/podIPs:10.2.129.126/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 c1337afd-79eb-4b8e-a375-76bee3607850 0xc0054039b7 0xc0054039b8}] [] [{calico Update v1 2023-04-18 00:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-18 00:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1337afd-79eb-4b8e-a375-76bee3607850\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-18 00:38:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.129.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9sf7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9sf7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-27-81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.27.81,PodIP:10.2.129.126,StartTime:2023-04-18 00:38:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.129.126,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.255: INFO: Pod "webserver-deployment-7b75d79cf5-77rwb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-77rwb webserver-deployment-7b75d79cf5- deployment-9672  a3491c1e-cda1-4b62-ae67-4d1a7d6d5875 51679 0 2023-04-18 00:38:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:6417601bb4f7bd606a4411137251bdf4d62949b01aed5fb5c3eb756fcb09640f cni.projectcalico.org/podIP:10.2.129.128/32 cni.projectcalico.org/podIPs:10.2.129.128/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 c1337afd-79eb-4b8e-a375-76bee3607850 0xc005403c00 0xc005403c01}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1337afd-79eb-4b8e-a375-76bee3607850\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-18 00:38:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-18 00:38:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.129.128\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tqrv2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tqrv2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-27-81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.27.81,PodIP:10.2.129.128,StartTime:2023-04-18 00:38:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.129.128,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.257: INFO: Pod "webserver-deployment-7b75d79cf5-fzl9f" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-fzl9f webserver-deployment-7b75d79cf5- deployment-9672  5b78a614-4fc1-4dc3-936c-1e9de6518ff3 51688 0 2023-04-18 00:38:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:b7b076c6b32e5b8089a19b0e6368741a78e4b984cceebff70e4f761000e69466 cni.projectcalico.org/podIP:10.2.212.198/32 cni.projectcalico.org/podIPs:10.2.212.198/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 c1337afd-79eb-4b8e-a375-76bee3607850 0xc005403e50 0xc005403e51}] [] [{calico Update v1 2023-04-18 00:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-18 00:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1337afd-79eb-4b8e-a375-76bee3607850\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-18 00:38:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.212.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t6lhw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t6lhw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-14-154,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.14.154,PodIP:10.2.212.198,StartTime:2023-04-18 00:38:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.212.198,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.257: INFO: Pod "webserver-deployment-7b75d79cf5-jznhs" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-jznhs webserver-deployment-7b75d79cf5- deployment-9672  f0ee0f90-eb90-4e16-b11a-0d61e177968b 51683 0 2023-04-18 00:38:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:30bbd4cd0f35cdcd154a8193ad6003555d74d76c5c9de978a9b4708f1891995c cni.projectcalico.org/podIP:10.2.129.127/32 cni.projectcalico.org/podIPs:10.2.129.127/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 c1337afd-79eb-4b8e-a375-76bee3607850 0xc0046260a0 0xc0046260a1}] [] [{calico Update v1 2023-04-18 00:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-18 00:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1337afd-79eb-4b8e-a375-76bee3607850\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-18 00:38:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.129.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dtpvw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dtpvw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-27-81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.27.81,PodIP:10.2.129.127,StartTime:2023-04-18 00:38:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.129.127,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.258: INFO: Pod "webserver-deployment-7b75d79cf5-mvffx" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-mvffx webserver-deployment-7b75d79cf5- deployment-9672  d1bfed2d-d559-4495-99e1-97f8f46b9ba8 51691 0 2023-04-18 00:38:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:bd2b1e0c2acadd714b2f1661f75cc5adc6979142183b5395c89e44c4d58c2d9a cni.projectcalico.org/podIP:10.2.212.197/32 cni.projectcalico.org/podIPs:10.2.212.197/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 c1337afd-79eb-4b8e-a375-76bee3607850 0xc0046262f0 0xc0046262f1}] [] [{calico Update v1 2023-04-18 00:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-18 00:38:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1337afd-79eb-4b8e-a375-76bee3607850\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-18 00:38:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.212.197\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nz9rb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nz9rb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-14-154,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.14.154,PodIP:10.2.212.197,StartTime:2023-04-18 00:38:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.212.197,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.258: INFO: Pod "webserver-deployment-7b75d79cf5-nwdhq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-nwdhq webserver-deployment-7b75d79cf5- deployment-9672  59b8be0d-09bb-4c28-b63a-2cda0e58d61a 51713 0 2023-04-18 00:38:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 c1337afd-79eb-4b8e-a375-76bee3607850 0xc004626520 0xc004626521}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1337afd-79eb-4b8e-a375-76bee3607850\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jm522,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jm522,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-14-154,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 00:38:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.259: INFO: Pod "webserver-deployment-7b75d79cf5-z8qtb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-z8qtb webserver-deployment-7b75d79cf5- deployment-9672  d1e93fbd-b7f4-45b4-ae54-ddd7fc0b03d8 51715 0 2023-04-18 00:38:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 c1337afd-79eb-4b8e-a375-76bee3607850 0xc004626690 0xc004626691}] [] [{kube-controller-manager Update v1 2023-04-18 00:38:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1337afd-79eb-4b8e-a375-76bee3607850\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5l4lr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5l4lr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 00:38:10.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9672" for this suite. @ 04/18/23 00:38:10.343
• [8.672 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 04/18/23 00:38:10.397
  Apr 18 00:38:10.397: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename configmap @ 04/18/23 00:38:10.397
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:38:10.539
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:38:10.597
  STEP: Creating configMap with name configmap-test-volume-e8788ffc-db68-49d3-b4e6-c44c1618cbaa @ 04/18/23 00:38:10.64
  STEP: Creating a pod to test consume configMaps @ 04/18/23 00:38:10.666
  STEP: Saw pod success @ 04/18/23 00:38:14.767
  Apr 18 00:38:14.770: INFO: Trying to get logs from node ip-10-0-27-81 pod pod-configmaps-f8516ff1-3dc0-4d74-9ba0-1b3fd660b1dc container agnhost-container: <nil>
  STEP: delete the pod @ 04/18/23 00:38:14.778
  Apr 18 00:38:14.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1710" for this suite. @ 04/18/23 00:38:14.797
• [4.408 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 04/18/23 00:38:14.807
  Apr 18 00:38:14.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 00:38:14.808
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:38:14.85
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:38:14.856
  STEP: Creating projection with secret that has name projected-secret-test-46f2619f-d1bd-4925-8ec5-d7c11d7c97e8 @ 04/18/23 00:38:14.861
  STEP: Creating a pod to test consume secrets @ 04/18/23 00:38:14.867
  STEP: Saw pod success @ 04/18/23 00:38:18.905
  Apr 18 00:38:18.908: INFO: Trying to get logs from node ip-10-0-27-81 pod pod-projected-secrets-f4ea466a-2a57-4a79-8e98-3e5f72c4ec11 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/18/23 00:38:18.914
  Apr 18 00:38:18.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4242" for this suite. @ 04/18/23 00:38:18.93
• [4.130 seconds]
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 04/18/23 00:38:18.937
  Apr 18 00:38:18.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/18/23 00:38:18.938
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:38:18.954
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:38:18.959
  Apr 18 00:38:18.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 00:38:22.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6414" for this suite. @ 04/18/23 00:38:22.279
• [3.355 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 04/18/23 00:38:22.293
  Apr 18 00:38:22.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename container-probe @ 04/18/23 00:38:22.294
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:38:22.332
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:38:22.347
  STEP: Creating pod test-webserver-22c1fe59-cfd6-4f15-801c-adf6c899122e in namespace container-probe-7296 @ 04/18/23 00:38:22.375
  Apr 18 00:38:24.404: INFO: Started pod test-webserver-22c1fe59-cfd6-4f15-801c-adf6c899122e in namespace container-probe-7296
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/18/23 00:38:24.404
  Apr 18 00:38:24.407: INFO: Initial restart count of pod test-webserver-22c1fe59-cfd6-4f15-801c-adf6c899122e is 0
  Apr 18 00:42:25.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/18/23 00:42:25.201
  STEP: Destroying namespace "container-probe-7296" for this suite. @ 04/18/23 00:42:25.216
• [242.934 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 04/18/23 00:42:25.229
  Apr 18 00:42:25.229: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 00:42:25.231
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:42:25.255
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:42:25.259
  STEP: Creating projection with secret that has name projected-secret-test-8bbcd76e-d754-4613-b0af-7252c1d22586 @ 04/18/23 00:42:25.263
  STEP: Creating a pod to test consume secrets @ 04/18/23 00:42:25.269
  STEP: Saw pod success @ 04/18/23 00:42:29.296
  Apr 18 00:42:29.298: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-projected-secrets-d6263a2f-b8d7-44e1-b1ab-561bf6813be1 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/18/23 00:42:29.326
  Apr 18 00:42:29.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1007" for this suite. @ 04/18/23 00:42:29.345
• [4.121 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 04/18/23 00:42:29.351
  Apr 18 00:42:29.351: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename events @ 04/18/23 00:42:29.352
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:42:29.371
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:42:29.376
  STEP: Create set of events @ 04/18/23 00:42:29.379
  STEP: get a list of Events with a label in the current namespace @ 04/18/23 00:42:29.395
  STEP: delete a list of events @ 04/18/23 00:42:29.398
  Apr 18 00:42:29.398: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 04/18/23 00:42:29.415
  Apr 18 00:42:29.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-2974" for this suite. @ 04/18/23 00:42:29.422
• [0.078 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 04/18/23 00:42:29.432
  Apr 18 00:42:29.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename watch @ 04/18/23 00:42:29.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:42:29.46
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:42:29.465
  STEP: creating a new configmap @ 04/18/23 00:42:29.468
  STEP: modifying the configmap once @ 04/18/23 00:42:29.472
  STEP: modifying the configmap a second time @ 04/18/23 00:42:29.479
  STEP: deleting the configmap @ 04/18/23 00:42:29.485
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 04/18/23 00:42:29.498
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 04/18/23 00:42:29.501
  Apr 18 00:42:29.503: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4505  f758af28-6b65-4bfa-b8d3-e144c7bec4d8 52560 0 2023-04-18 00:42:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-18 00:42:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 18 00:42:29.503: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4505  f758af28-6b65-4bfa-b8d3-e144c7bec4d8 52561 0 2023-04-18 00:42:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-18 00:42:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 18 00:42:29.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-4505" for this suite. @ 04/18/23 00:42:29.508
• [0.082 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 04/18/23 00:42:29.515
  Apr 18 00:42:29.515: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename configmap @ 04/18/23 00:42:29.516
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:42:29.548
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:42:29.557
  STEP: Creating configMap with name configmap-test-volume-map-2499d830-cd94-4e6f-8798-bd85b8a0bc9d @ 04/18/23 00:42:29.56
  STEP: Creating a pod to test consume configMaps @ 04/18/23 00:42:29.564
  STEP: Saw pod success @ 04/18/23 00:42:33.594
  Apr 18 00:42:33.596: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-configmaps-01b4adb0-652e-429c-a06e-fb8ee5af3e69 container agnhost-container: <nil>
  STEP: delete the pod @ 04/18/23 00:42:33.601
  Apr 18 00:42:33.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5164" for this suite. @ 04/18/23 00:42:33.617
• [4.108 seconds]
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 04/18/23 00:42:33.624
  Apr 18 00:42:33.624: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename svcaccounts @ 04/18/23 00:42:33.624
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:42:33.641
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:42:33.645
  Apr 18 00:42:33.670: INFO: created pod pod-service-account-defaultsa
  Apr 18 00:42:33.670: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Apr 18 00:42:33.681: INFO: created pod pod-service-account-mountsa
  Apr 18 00:42:33.681: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Apr 18 00:42:33.695: INFO: created pod pod-service-account-nomountsa
  Apr 18 00:42:33.695: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Apr 18 00:42:33.710: INFO: created pod pod-service-account-defaultsa-mountspec
  Apr 18 00:42:33.710: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Apr 18 00:42:33.731: INFO: created pod pod-service-account-mountsa-mountspec
  Apr 18 00:42:33.731: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Apr 18 00:42:33.751: INFO: created pod pod-service-account-nomountsa-mountspec
  Apr 18 00:42:33.751: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Apr 18 00:42:33.763: INFO: created pod pod-service-account-defaultsa-nomountspec
  Apr 18 00:42:33.764: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Apr 18 00:42:33.789: INFO: created pod pod-service-account-mountsa-nomountspec
  Apr 18 00:42:33.789: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Apr 18 00:42:33.802: INFO: created pod pod-service-account-nomountsa-nomountspec
  Apr 18 00:42:33.802: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Apr 18 00:42:33.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8196" for this suite. @ 04/18/23 00:42:33.82
• [0.213 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 04/18/23 00:42:33.84
  Apr 18 00:42:33.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/18/23 00:42:33.841
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:42:33.87
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:42:33.874
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 04/18/23 00:42:33.878
  Apr 18 00:42:33.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 00:42:35.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 00:42:41.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9989" for this suite. @ 04/18/23 00:42:41.348
• [7.520 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 04/18/23 00:42:41.361
  Apr 18 00:42:41.361: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename job @ 04/18/23 00:42:41.361
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:42:41.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:42:41.421
  STEP: Creating a job @ 04/18/23 00:42:41.428
  STEP: Ensuring active pods == parallelism @ 04/18/23 00:42:41.437
  STEP: Orphaning one of the Job's Pods @ 04/18/23 00:42:45.442
  Apr 18 00:42:45.980: INFO: Successfully updated pod "adopt-release-lkmfw"
  STEP: Checking that the Job readopts the Pod @ 04/18/23 00:42:45.98
  STEP: Removing the labels from the Job's Pod @ 04/18/23 00:42:48.032
  Apr 18 00:42:48.549: INFO: Successfully updated pod "adopt-release-lkmfw"
  STEP: Checking that the Job releases the Pod @ 04/18/23 00:42:48.549
  Apr 18 00:42:50.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8461" for this suite. @ 04/18/23 00:42:50.571
• [9.219 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 04/18/23 00:42:50.581
  Apr 18 00:42:50.581: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 00:42:50.582
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:42:50.602
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:42:50.607
  STEP: Creating projection with secret that has name projected-secret-test-map-9648fd1e-ca67-45e3-808a-53b06481f848 @ 04/18/23 00:42:50.612
  STEP: Creating a pod to test consume secrets @ 04/18/23 00:42:50.617
  STEP: Saw pod success @ 04/18/23 00:42:54.638
  Apr 18 00:42:54.643: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-projected-secrets-60adcfa6-5458-4a8f-99ce-f7731c108faf container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/18/23 00:42:54.651
  Apr 18 00:42:54.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7671" for this suite. @ 04/18/23 00:42:54.681
• [4.111 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 04/18/23 00:42:54.694
  Apr 18 00:42:54.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 00:42:54.695
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:42:54.718
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:42:54.731
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-b68dd362-5acd-4649-ae90-01c2684ed434 @ 04/18/23 00:42:54.739
  STEP: Creating the pod @ 04/18/23 00:42:54.743
  STEP: Updating configmap projected-configmap-test-upd-b68dd362-5acd-4649-ae90-01c2684ed434 @ 04/18/23 00:42:56.777
  STEP: waiting to observe update in volume @ 04/18/23 00:42:56.782
  Apr 18 00:42:58.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1947" for this suite. @ 04/18/23 00:42:58.799
• [4.119 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 04/18/23 00:42:58.814
  Apr 18 00:42:58.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename lease-test @ 04/18/23 00:42:58.816
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:42:58.843
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:42:58.857
  Apr 18 00:42:58.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-2180" for this suite. @ 04/18/23 00:42:58.931
• [0.124 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 04/18/23 00:42:58.943
  Apr 18 00:42:58.943: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename pods @ 04/18/23 00:42:58.943
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:42:58.964
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:42:58.968
  STEP: creating the pod @ 04/18/23 00:42:58.972
  STEP: setting up watch @ 04/18/23 00:42:58.972
  STEP: submitting the pod to kubernetes @ 04/18/23 00:42:59.076
  STEP: verifying the pod is in kubernetes @ 04/18/23 00:42:59.086
  STEP: verifying pod creation was observed @ 04/18/23 00:42:59.091
  STEP: deleting the pod gracefully @ 04/18/23 00:43:01.111
  STEP: verifying pod deletion was observed @ 04/18/23 00:43:01.118
  Apr 18 00:43:03.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6943" for this suite. @ 04/18/23 00:43:03.64
• [4.723 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 04/18/23 00:43:03.666
  Apr 18 00:43:03.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename webhook @ 04/18/23 00:43:03.667
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:43:03.69
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:43:03.694
  STEP: Setting up server cert @ 04/18/23 00:43:03.728
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/18/23 00:43:04.177
  STEP: Deploying the webhook pod @ 04/18/23 00:43:04.188
  STEP: Wait for the deployment to be ready @ 04/18/23 00:43:04.219
  Apr 18 00:43:04.249: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  Apr 18 00:43:06.258: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 18, 0, 43, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 43, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 0, 43, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 0, 43, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 04/18/23 00:43:08.27
  STEP: Verifying the service has paired with the endpoint @ 04/18/23 00:43:08.289
  Apr 18 00:43:09.289: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 04/18/23 00:43:09.292
  STEP: create a pod that should be updated by the webhook @ 04/18/23 00:43:09.307
  Apr 18 00:43:09.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8386" for this suite. @ 04/18/23 00:43:09.395
  STEP: Destroying namespace "webhook-markers-691" for this suite. @ 04/18/23 00:43:09.402
• [5.752 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 04/18/23 00:43:09.42
  Apr 18 00:43:09.420: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename pod-network-test @ 04/18/23 00:43:09.421
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:43:09.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:43:09.446
  STEP: Performing setup for networking test in namespace pod-network-test-788 @ 04/18/23 00:43:09.45
  STEP: creating a selector @ 04/18/23 00:43:09.45
  STEP: Creating the service pods in kubernetes @ 04/18/23 00:43:09.45
  Apr 18 00:43:09.450: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 04/18/23 00:43:31.547
  Apr 18 00:43:33.617: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  Apr 18 00:43:33.617: INFO: Going to poll 10.2.212.207 on port 8081 at least 0 times, with a maximum of 34 tries before failing
  Apr 18 00:43:33.619: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.2.212.207 8081 | grep -v '^\s*$'] Namespace:pod-network-test-788 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 00:43:33.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 00:43:33.621: INFO: ExecWithOptions: Clientset creation
  Apr 18 00:43:33.621: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-788/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.2.212.207+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 18 00:43:34.695: INFO: Found all 1 expected endpoints: [netserver-0]
  Apr 18 00:43:34.696: INFO: Going to poll 10.2.129.138 on port 8081 at least 0 times, with a maximum of 34 tries before failing
  Apr 18 00:43:34.699: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.2.129.138 8081 | grep -v '^\s*$'] Namespace:pod-network-test-788 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 00:43:34.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 00:43:34.700: INFO: ExecWithOptions: Clientset creation
  Apr 18 00:43:34.700: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-788/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.2.129.138+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 18 00:43:35.763: INFO: Found all 1 expected endpoints: [netserver-1]
  Apr 18 00:43:35.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-788" for this suite. @ 04/18/23 00:43:35.77
• [26.355 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 04/18/23 00:43:35.776
  Apr 18 00:43:35.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename webhook @ 04/18/23 00:43:35.777
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:43:35.809
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:43:35.826
  STEP: Setting up server cert @ 04/18/23 00:43:35.863
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/18/23 00:43:36.206
  STEP: Deploying the webhook pod @ 04/18/23 00:43:36.212
  STEP: Wait for the deployment to be ready @ 04/18/23 00:43:36.222
  Apr 18 00:43:36.234: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/18/23 00:43:38.244
  STEP: Verifying the service has paired with the endpoint @ 04/18/23 00:43:38.254
  Apr 18 00:43:39.254: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 18 00:43:39.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 04/18/23 00:43:39.774
  STEP: Creating a custom resource that should be denied by the webhook @ 04/18/23 00:43:39.79
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 04/18/23 00:43:41.813
  STEP: Updating the custom resource with disallowed data should be denied @ 04/18/23 00:43:41.824
  STEP: Deleting the custom resource should be denied @ 04/18/23 00:43:41.855
  STEP: Remove the offending key and value from the custom resource data @ 04/18/23 00:43:41.867
  STEP: Deleting the updated custom resource should be successful @ 04/18/23 00:43:41.886
  Apr 18 00:43:41.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2548" for this suite. @ 04/18/23 00:43:42.526
  STEP: Destroying namespace "webhook-markers-9650" for this suite. @ 04/18/23 00:43:42.531
• [6.763 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 04/18/23 00:43:42.54
  Apr 18 00:43:42.540: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename subpath @ 04/18/23 00:43:42.54
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:43:42.559
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:43:42.566
  STEP: Setting up data @ 04/18/23 00:43:42.571
  STEP: Creating pod pod-subpath-test-configmap-mjk8 @ 04/18/23 00:43:42.578
  STEP: Creating a pod to test atomic-volume-subpath @ 04/18/23 00:43:42.578
  STEP: Saw pod success @ 04/18/23 00:44:06.666
  Apr 18 00:44:06.669: INFO: Trying to get logs from node ip-10-0-27-81 pod pod-subpath-test-configmap-mjk8 container test-container-subpath-configmap-mjk8: <nil>
  STEP: delete the pod @ 04/18/23 00:44:06.695
  STEP: Deleting pod pod-subpath-test-configmap-mjk8 @ 04/18/23 00:44:06.71
  Apr 18 00:44:06.710: INFO: Deleting pod "pod-subpath-test-configmap-mjk8" in namespace "subpath-8028"
  Apr 18 00:44:06.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8028" for this suite. @ 04/18/23 00:44:06.717
• [24.191 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 04/18/23 00:44:06.734
  Apr 18 00:44:06.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename pods @ 04/18/23 00:44:06.735
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:44:06.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:44:06.774
  STEP: creating the pod @ 04/18/23 00:44:06.779
  STEP: submitting the pod to kubernetes @ 04/18/23 00:44:06.78
  W0418 00:44:06.786443      21 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: verifying the pod is in kubernetes @ 04/18/23 00:44:08.8
  STEP: updating the pod @ 04/18/23 00:44:08.813
  Apr 18 00:44:09.327: INFO: Successfully updated pod "pod-update-activedeadlineseconds-222aec00-933f-4770-bc32-8fa8e64d785b"
  Apr 18 00:44:13.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8744" for this suite. @ 04/18/23 00:44:13.345
• [6.618 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 04/18/23 00:44:13.353
  Apr 18 00:44:13.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename svc-latency @ 04/18/23 00:44:13.354
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:44:13.372
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:44:13.378
  Apr 18 00:44:13.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-8829 @ 04/18/23 00:44:13.382
  I0418 00:44:13.389809      21 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8829, replica count: 1
  I0418 00:44:14.441123      21 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0418 00:44:15.441339      21 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0418 00:44:16.442361      21 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 18 00:44:16.561: INFO: Created: latency-svc-vgnvs
  Apr 18 00:44:16.572: INFO: Got endpoints: latency-svc-vgnvs [29.156602ms]
  Apr 18 00:44:16.595: INFO: Created: latency-svc-zd475
  Apr 18 00:44:16.612: INFO: Got endpoints: latency-svc-zd475 [39.0927ms]
  Apr 18 00:44:16.617: INFO: Created: latency-svc-9tm4n
  Apr 18 00:44:16.638: INFO: Got endpoints: latency-svc-9tm4n [64.488332ms]
  Apr 18 00:44:16.638: INFO: Created: latency-svc-99kkk
  Apr 18 00:44:16.661: INFO: Created: latency-svc-2ck8w
  Apr 18 00:44:16.669: INFO: Got endpoints: latency-svc-99kkk [95.37565ms]
  Apr 18 00:44:16.680: INFO: Created: latency-svc-6qs7x
  Apr 18 00:44:16.681: INFO: Got endpoints: latency-svc-2ck8w [106.866004ms]
  Apr 18 00:44:16.700: INFO: Created: latency-svc-dcvvq
  Apr 18 00:44:16.701: INFO: Got endpoints: latency-svc-6qs7x [127.198496ms]
  Apr 18 00:44:16.728: INFO: Created: latency-svc-dkwzr
  Apr 18 00:44:16.729: INFO: Got endpoints: latency-svc-dcvvq [155.603467ms]
  Apr 18 00:44:16.738: INFO: Got endpoints: latency-svc-dkwzr [164.265811ms]
  Apr 18 00:44:16.742: INFO: Created: latency-svc-92n6k
  Apr 18 00:44:16.756: INFO: Created: latency-svc-kkcgv
  Apr 18 00:44:16.769: INFO: Got endpoints: latency-svc-92n6k [194.972952ms]
  Apr 18 00:44:16.776: INFO: Got endpoints: latency-svc-kkcgv [201.70389ms]
  Apr 18 00:44:16.780: INFO: Created: latency-svc-w5tdw
  Apr 18 00:44:16.792: INFO: Got endpoints: latency-svc-w5tdw [217.933772ms]
  Apr 18 00:44:16.793: INFO: Created: latency-svc-9xctq
  Apr 18 00:44:16.808: INFO: Created: latency-svc-kwrwc
  Apr 18 00:44:16.812: INFO: Got endpoints: latency-svc-9xctq [238.175942ms]
  Apr 18 00:44:16.822: INFO: Created: latency-svc-z87q9
  Apr 18 00:44:16.827: INFO: Got endpoints: latency-svc-kwrwc [253.147965ms]
  Apr 18 00:44:16.835: INFO: Created: latency-svc-vvntn
  Apr 18 00:44:16.843: INFO: Got endpoints: latency-svc-z87q9 [269.084784ms]
  Apr 18 00:44:16.855: INFO: Created: latency-svc-npxtq
  Apr 18 00:44:16.858: INFO: Got endpoints: latency-svc-vvntn [284.205565ms]
  Apr 18 00:44:16.869: INFO: Created: latency-svc-wr9ln
  Apr 18 00:44:16.874: INFO: Got endpoints: latency-svc-npxtq [299.725103ms]
  Apr 18 00:44:16.900: INFO: Created: latency-svc-vgdm8
  Apr 18 00:44:16.906: INFO: Got endpoints: latency-svc-wr9ln [294.160278ms]
  Apr 18 00:44:16.909: INFO: Created: latency-svc-jxvjh
  Apr 18 00:44:16.918: INFO: Got endpoints: latency-svc-vgdm8 [279.122304ms]
  Apr 18 00:44:16.931: INFO: Created: latency-svc-pqrpr
  Apr 18 00:44:16.932: INFO: Got endpoints: latency-svc-jxvjh [262.019054ms]
  Apr 18 00:44:16.944: INFO: Created: latency-svc-fk7cn
  Apr 18 00:44:16.948: INFO: Got endpoints: latency-svc-pqrpr [265.515246ms]
  Apr 18 00:44:16.963: INFO: Got endpoints: latency-svc-fk7cn [261.769005ms]
  Apr 18 00:44:17.009: INFO: Created: latency-svc-xs29r
  Apr 18 00:44:17.018: INFO: Created: latency-svc-sxqgx
  Apr 18 00:44:17.019: INFO: Created: latency-svc-hszfn
  Apr 18 00:44:17.019: INFO: Created: latency-svc-9wn9n
  Apr 18 00:44:17.019: INFO: Created: latency-svc-x6wmz
  Apr 18 00:44:17.020: INFO: Created: latency-svc-qg95x
  Apr 18 00:44:17.020: INFO: Created: latency-svc-kb2g5
  Apr 18 00:44:17.020: INFO: Created: latency-svc-fw2pb
  Apr 18 00:44:17.020: INFO: Created: latency-svc-s9ddt
  Apr 18 00:44:17.020: INFO: Created: latency-svc-4vlgt
  Apr 18 00:44:17.020: INFO: Created: latency-svc-77rlh
  Apr 18 00:44:17.020: INFO: Created: latency-svc-2glrq
  Apr 18 00:44:17.022: INFO: Created: latency-svc-cc948
  Apr 18 00:44:17.022: INFO: Created: latency-svc-cptn6
  Apr 18 00:44:17.023: INFO: Created: latency-svc-6nrl5
  Apr 18 00:44:17.060: INFO: Got endpoints: latency-svc-xs29r [321.494976ms]
  Apr 18 00:44:17.065: INFO: Got endpoints: latency-svc-6nrl5 [295.503102ms]
  Apr 18 00:44:17.065: INFO: Got endpoints: latency-svc-9wn9n [102.751505ms]
  Apr 18 00:44:17.066: INFO: Got endpoints: latency-svc-sxqgx [147.217277ms]
  Apr 18 00:44:17.066: INFO: Got endpoints: latency-svc-hszfn [133.675972ms]
  Apr 18 00:44:17.096: INFO: Got endpoints: latency-svc-77rlh [303.346128ms]
  Apr 18 00:44:17.096: INFO: Got endpoints: latency-svc-2glrq [320.409315ms]
  Apr 18 00:44:17.096: INFO: Got endpoints: latency-svc-fw2pb [252.644888ms]
  Apr 18 00:44:17.097: INFO: Got endpoints: latency-svc-s9ddt [269.114051ms]
  Apr 18 00:44:17.097: INFO: Got endpoints: latency-svc-4vlgt [284.780544ms]
  Apr 18 00:44:17.123: INFO: Got endpoints: latency-svc-qg95x [248.436278ms]
  Apr 18 00:44:17.123: INFO: Got endpoints: latency-svc-kb2g5 [264.2419ms]
  Apr 18 00:44:17.136: INFO: Created: latency-svc-69ggx
  Apr 18 00:44:17.136: INFO: Got endpoints: latency-svc-cptn6 [406.461989ms]
  Apr 18 00:44:17.136: INFO: Got endpoints: latency-svc-x6wmz [229.986867ms]
  Apr 18 00:44:17.136: INFO: Got endpoints: latency-svc-cc948 [187.565807ms]
  Apr 18 00:44:17.157: INFO: Got endpoints: latency-svc-69ggx [97.0318ms]
  Apr 18 00:44:17.251: INFO: Created: latency-svc-knggn
  Apr 18 00:44:17.252: INFO: Created: latency-svc-qslsq
  Apr 18 00:44:17.251: INFO: Created: latency-svc-8gxnt
  Apr 18 00:44:17.251: INFO: Created: latency-svc-72llq
  Apr 18 00:44:17.251: INFO: Created: latency-svc-d6vfq
  Apr 18 00:44:17.251: INFO: Created: latency-svc-tlgqm
  Apr 18 00:44:17.251: INFO: Created: latency-svc-2dg2s
  Apr 18 00:44:17.251: INFO: Created: latency-svc-h65ns
  Apr 18 00:44:17.251: INFO: Created: latency-svc-6qwhx
  Apr 18 00:44:17.252: INFO: Created: latency-svc-8mnlb
  Apr 18 00:44:17.252: INFO: Created: latency-svc-tlfv5
  Apr 18 00:44:17.252: INFO: Created: latency-svc-lb4f2
  Apr 18 00:44:17.252: INFO: Created: latency-svc-4mrzx
  Apr 18 00:44:17.252: INFO: Created: latency-svc-kbpfq
  Apr 18 00:44:17.253: INFO: Created: latency-svc-cn49t
  Apr 18 00:44:17.298: INFO: Got endpoints: latency-svc-tlgqm [140.531077ms]
  Apr 18 00:44:17.299: INFO: Got endpoints: latency-svc-knggn [202.799541ms]
  Apr 18 00:44:17.299: INFO: Got endpoints: latency-svc-8gxnt [160.778454ms]
  Apr 18 00:44:17.299: INFO: Got endpoints: latency-svc-4mrzx [234.142918ms]
  Apr 18 00:44:17.301: INFO: Got endpoints: latency-svc-72llq [234.170871ms]
  Apr 18 00:44:17.311: INFO: Got endpoints: latency-svc-qslsq [244.352565ms]
  Apr 18 00:44:17.311: INFO: Got endpoints: latency-svc-kbpfq [245.808594ms]
  Apr 18 00:44:17.325: INFO: Got endpoints: latency-svc-cn49t [227.983754ms]
  Apr 18 00:44:17.325: INFO: Created: latency-svc-qnvlt
  Apr 18 00:44:17.362: INFO: Created: latency-svc-txzkk
  Apr 18 00:44:17.366: INFO: Created: latency-svc-ttq6k
  Apr 18 00:44:17.367: INFO: Created: latency-svc-q8ndr
  Apr 18 00:44:17.368: INFO: Created: latency-svc-md997
  Apr 18 00:44:17.368: INFO: Created: latency-svc-zs4qq
  Apr 18 00:44:17.369: INFO: Created: latency-svc-t4rvq
  Apr 18 00:44:17.369: INFO: Created: latency-svc-wds7p
  Apr 18 00:44:17.380: INFO: Got endpoints: latency-svc-2dg2s [284.306112ms]
  Apr 18 00:44:17.393: INFO: Created: latency-svc-pmplp
  Apr 18 00:44:17.422: INFO: Got endpoints: latency-svc-h65ns [326.065801ms]
  Apr 18 00:44:17.434: INFO: Created: latency-svc-tzcmh
  Apr 18 00:44:17.471: INFO: Got endpoints: latency-svc-6qwhx [374.26766ms]
  Apr 18 00:44:17.482: INFO: Created: latency-svc-t9dr2
  Apr 18 00:44:17.525: INFO: Got endpoints: latency-svc-8mnlb [401.574752ms]
  Apr 18 00:44:17.538: INFO: Created: latency-svc-6twj9
  Apr 18 00:44:17.573: INFO: Got endpoints: latency-svc-tlfv5 [433.32011ms]
  Apr 18 00:44:17.588: INFO: Created: latency-svc-gpk7n
  Apr 18 00:44:17.622: INFO: Got endpoints: latency-svc-lb4f2 [498.515894ms]
  Apr 18 00:44:17.634: INFO: Created: latency-svc-5g2rp
  Apr 18 00:44:17.674: INFO: Got endpoints: latency-svc-d6vfq [536.812064ms]
  Apr 18 00:44:17.684: INFO: Created: latency-svc-qx7bw
  Apr 18 00:44:17.720: INFO: Got endpoints: latency-svc-qnvlt [422.640217ms]
  Apr 18 00:44:17.734: INFO: Created: latency-svc-h7p8m
  Apr 18 00:44:17.774: INFO: Got endpoints: latency-svc-txzkk [462.210395ms]
  Apr 18 00:44:17.787: INFO: Created: latency-svc-42djf
  Apr 18 00:44:17.822: INFO: Got endpoints: latency-svc-md997 [522.634434ms]
  Apr 18 00:44:17.836: INFO: Created: latency-svc-2xpgx
  Apr 18 00:44:17.873: INFO: Got endpoints: latency-svc-zs4qq [547.962874ms]
  Apr 18 00:44:17.887: INFO: Created: latency-svc-w5dp5
  Apr 18 00:44:17.921: INFO: Got endpoints: latency-svc-wds7p [610.374679ms]
  Apr 18 00:44:17.931: INFO: Created: latency-svc-q9kf8
  Apr 18 00:44:17.970: INFO: Got endpoints: latency-svc-t4rvq [669.827586ms]
  Apr 18 00:44:17.980: INFO: Created: latency-svc-f6kpm
  Apr 18 00:44:18.020: INFO: Got endpoints: latency-svc-ttq6k [721.195605ms]
  Apr 18 00:44:18.033: INFO: Created: latency-svc-4rbrg
  Apr 18 00:44:18.073: INFO: Got endpoints: latency-svc-q8ndr [773.933304ms]
  Apr 18 00:44:18.084: INFO: Created: latency-svc-rbt8g
  Apr 18 00:44:18.120: INFO: Got endpoints: latency-svc-pmplp [738.917324ms]
  Apr 18 00:44:18.132: INFO: Created: latency-svc-nwwv7
  Apr 18 00:44:18.173: INFO: Got endpoints: latency-svc-tzcmh [750.378766ms]
  Apr 18 00:44:18.185: INFO: Created: latency-svc-74swg
  Apr 18 00:44:18.223: INFO: Got endpoints: latency-svc-t9dr2 [751.800759ms]
  Apr 18 00:44:18.234: INFO: Created: latency-svc-fzpb7
  Apr 18 00:44:18.272: INFO: Got endpoints: latency-svc-6twj9 [747.347748ms]
  Apr 18 00:44:18.283: INFO: Created: latency-svc-2b5pc
  Apr 18 00:44:18.320: INFO: Got endpoints: latency-svc-gpk7n [746.708227ms]
  Apr 18 00:44:18.330: INFO: Created: latency-svc-829p5
  Apr 18 00:44:18.374: INFO: Got endpoints: latency-svc-5g2rp [751.126008ms]
  Apr 18 00:44:18.385: INFO: Created: latency-svc-hn2nj
  Apr 18 00:44:18.424: INFO: Got endpoints: latency-svc-qx7bw [749.326023ms]
  Apr 18 00:44:18.438: INFO: Created: latency-svc-8d7zm
  Apr 18 00:44:18.485: INFO: Got endpoints: latency-svc-h7p8m [764.151593ms]
  Apr 18 00:44:18.499: INFO: Created: latency-svc-tg8zp
  Apr 18 00:44:18.524: INFO: Got endpoints: latency-svc-42djf [749.145846ms]
  Apr 18 00:44:18.536: INFO: Created: latency-svc-vs5zg
  Apr 18 00:44:18.574: INFO: Got endpoints: latency-svc-2xpgx [751.658146ms]
  Apr 18 00:44:18.588: INFO: Created: latency-svc-dmcsw
  Apr 18 00:44:18.624: INFO: Got endpoints: latency-svc-w5dp5 [750.425731ms]
  Apr 18 00:44:18.639: INFO: Created: latency-svc-9phmk
  Apr 18 00:44:18.674: INFO: Got endpoints: latency-svc-q9kf8 [753.018124ms]
  Apr 18 00:44:18.687: INFO: Created: latency-svc-xvhsh
  Apr 18 00:44:18.721: INFO: Got endpoints: latency-svc-f6kpm [750.185641ms]
  Apr 18 00:44:18.740: INFO: Created: latency-svc-rqj9m
  Apr 18 00:44:18.778: INFO: Got endpoints: latency-svc-4rbrg [757.274887ms]
  Apr 18 00:44:18.799: INFO: Created: latency-svc-pvkkt
  Apr 18 00:44:18.824: INFO: Got endpoints: latency-svc-rbt8g [750.863071ms]
  Apr 18 00:44:18.844: INFO: Created: latency-svc-6q5xc
  Apr 18 00:44:18.919: INFO: Got endpoints: latency-svc-nwwv7 [797.649004ms]
  Apr 18 00:44:18.925: INFO: Got endpoints: latency-svc-74swg [750.9228ms]
  Apr 18 00:44:18.941: INFO: Created: latency-svc-f75cb
  Apr 18 00:44:18.953: INFO: Created: latency-svc-52zzt
  Apr 18 00:44:18.972: INFO: Got endpoints: latency-svc-fzpb7 [748.357914ms]
  Apr 18 00:44:18.983: INFO: Created: latency-svc-gdmc2
  Apr 18 00:44:19.024: INFO: Got endpoints: latency-svc-2b5pc [751.541072ms]
  Apr 18 00:44:19.038: INFO: Created: latency-svc-4s2z9
  Apr 18 00:44:19.070: INFO: Got endpoints: latency-svc-829p5 [748.702957ms]
  Apr 18 00:44:19.083: INFO: Created: latency-svc-mf4rb
  Apr 18 00:44:19.130: INFO: Got endpoints: latency-svc-hn2nj [755.351791ms]
  Apr 18 00:44:19.145: INFO: Created: latency-svc-tdchk
  Apr 18 00:44:19.183: INFO: Got endpoints: latency-svc-8d7zm [759.654816ms]
  Apr 18 00:44:19.195: INFO: Created: latency-svc-5js8b
  Apr 18 00:44:19.223: INFO: Got endpoints: latency-svc-tg8zp [738.216084ms]
  Apr 18 00:44:19.234: INFO: Created: latency-svc-kmclv
  Apr 18 00:44:19.271: INFO: Got endpoints: latency-svc-vs5zg [746.243706ms]
  Apr 18 00:44:19.285: INFO: Created: latency-svc-4f47h
  Apr 18 00:44:19.322: INFO: Got endpoints: latency-svc-dmcsw [747.439698ms]
  Apr 18 00:44:19.334: INFO: Created: latency-svc-hc5hb
  Apr 18 00:44:19.379: INFO: Got endpoints: latency-svc-9phmk [754.54363ms]
  Apr 18 00:44:19.390: INFO: Created: latency-svc-tpz6t
  Apr 18 00:44:19.425: INFO: Got endpoints: latency-svc-xvhsh [750.396916ms]
  Apr 18 00:44:19.437: INFO: Created: latency-svc-mstqg
  Apr 18 00:44:19.475: INFO: Got endpoints: latency-svc-rqj9m [753.409151ms]
  Apr 18 00:44:19.485: INFO: Created: latency-svc-4vwcj
  Apr 18 00:44:19.524: INFO: Got endpoints: latency-svc-pvkkt [745.937506ms]
  Apr 18 00:44:19.535: INFO: Created: latency-svc-2kgdp
  Apr 18 00:44:19.573: INFO: Got endpoints: latency-svc-6q5xc [748.267022ms]
  Apr 18 00:44:19.585: INFO: Created: latency-svc-fzdg6
  Apr 18 00:44:19.623: INFO: Got endpoints: latency-svc-f75cb [698.906564ms]
  Apr 18 00:44:19.635: INFO: Created: latency-svc-wj22n
  Apr 18 00:44:19.675: INFO: Got endpoints: latency-svc-52zzt [750.191355ms]
  Apr 18 00:44:19.686: INFO: Created: latency-svc-ljn4k
  Apr 18 00:44:19.720: INFO: Got endpoints: latency-svc-gdmc2 [748.360142ms]
  Apr 18 00:44:19.731: INFO: Created: latency-svc-l7jk4
  Apr 18 00:44:19.774: INFO: Got endpoints: latency-svc-4s2z9 [749.620356ms]
  Apr 18 00:44:19.787: INFO: Created: latency-svc-mxx4w
  Apr 18 00:44:19.819: INFO: Got endpoints: latency-svc-mf4rb [748.613362ms]
  Apr 18 00:44:19.834: INFO: Created: latency-svc-tjqsv
  Apr 18 00:44:19.872: INFO: Got endpoints: latency-svc-tdchk [742.790916ms]
  Apr 18 00:44:19.892: INFO: Created: latency-svc-nqm6h
  Apr 18 00:44:19.923: INFO: Got endpoints: latency-svc-5js8b [739.169593ms]
  Apr 18 00:44:19.934: INFO: Created: latency-svc-dfjq6
  Apr 18 00:44:19.975: INFO: Got endpoints: latency-svc-kmclv [751.596278ms]
  Apr 18 00:44:19.989: INFO: Created: latency-svc-tfwr2
  Apr 18 00:44:20.022: INFO: Got endpoints: latency-svc-4f47h [750.516704ms]
  Apr 18 00:44:20.035: INFO: Created: latency-svc-q2k8r
  Apr 18 00:44:20.073: INFO: Got endpoints: latency-svc-hc5hb [750.230675ms]
  Apr 18 00:44:20.082: INFO: Created: latency-svc-6d64m
  Apr 18 00:44:20.123: INFO: Got endpoints: latency-svc-tpz6t [743.734758ms]
  Apr 18 00:44:20.139: INFO: Created: latency-svc-k7k2m
  Apr 18 00:44:20.173: INFO: Got endpoints: latency-svc-mstqg [748.10121ms]
  Apr 18 00:44:20.185: INFO: Created: latency-svc-zzdxf
  Apr 18 00:44:20.220: INFO: Got endpoints: latency-svc-4vwcj [744.214958ms]
  Apr 18 00:44:20.231: INFO: Created: latency-svc-h7x5m
  Apr 18 00:44:20.274: INFO: Got endpoints: latency-svc-2kgdp [749.834045ms]
  Apr 18 00:44:20.290: INFO: Created: latency-svc-lt2dc
  Apr 18 00:44:20.324: INFO: Got endpoints: latency-svc-fzdg6 [750.955314ms]
  Apr 18 00:44:20.338: INFO: Created: latency-svc-5f5fx
  Apr 18 00:44:20.386: INFO: Got endpoints: latency-svc-wj22n [762.63697ms]
  Apr 18 00:44:20.412: INFO: Created: latency-svc-gv6lj
  Apr 18 00:44:20.421: INFO: Got endpoints: latency-svc-ljn4k [746.175411ms]
  Apr 18 00:44:20.435: INFO: Created: latency-svc-jhdbb
  Apr 18 00:44:20.472: INFO: Got endpoints: latency-svc-l7jk4 [751.692727ms]
  Apr 18 00:44:20.486: INFO: Created: latency-svc-q4srd
  Apr 18 00:44:20.522: INFO: Got endpoints: latency-svc-mxx4w [747.51613ms]
  Apr 18 00:44:20.538: INFO: Created: latency-svc-r8zx9
  Apr 18 00:44:20.573: INFO: Got endpoints: latency-svc-tjqsv [754.390391ms]
  Apr 18 00:44:20.584: INFO: Created: latency-svc-t8rxp
  Apr 18 00:44:20.622: INFO: Got endpoints: latency-svc-nqm6h [749.424161ms]
  Apr 18 00:44:20.639: INFO: Created: latency-svc-2fjww
  Apr 18 00:44:20.673: INFO: Got endpoints: latency-svc-dfjq6 [749.609842ms]
  Apr 18 00:44:20.683: INFO: Created: latency-svc-tbmz7
  Apr 18 00:44:20.722: INFO: Got endpoints: latency-svc-tfwr2 [745.226386ms]
  Apr 18 00:44:20.735: INFO: Created: latency-svc-76xjc
  Apr 18 00:44:20.772: INFO: Got endpoints: latency-svc-q2k8r [749.791357ms]
  Apr 18 00:44:20.782: INFO: Created: latency-svc-l6rvq
  Apr 18 00:44:20.823: INFO: Got endpoints: latency-svc-6d64m [750.63981ms]
  Apr 18 00:44:20.834: INFO: Created: latency-svc-2tsdn
  Apr 18 00:44:20.871: INFO: Got endpoints: latency-svc-k7k2m [747.832217ms]
  Apr 18 00:44:20.882: INFO: Created: latency-svc-f58pf
  Apr 18 00:44:20.923: INFO: Got endpoints: latency-svc-zzdxf [749.980626ms]
  Apr 18 00:44:20.934: INFO: Created: latency-svc-7p2gl
  Apr 18 00:44:20.973: INFO: Got endpoints: latency-svc-h7x5m [752.478104ms]
  Apr 18 00:44:20.982: INFO: Created: latency-svc-svhxw
  Apr 18 00:44:21.022: INFO: Got endpoints: latency-svc-lt2dc [748.207055ms]
  Apr 18 00:44:21.033: INFO: Created: latency-svc-8s9vb
  Apr 18 00:44:21.073: INFO: Got endpoints: latency-svc-5f5fx [748.862951ms]
  Apr 18 00:44:21.083: INFO: Created: latency-svc-vsckw
  Apr 18 00:44:21.124: INFO: Got endpoints: latency-svc-gv6lj [738.227791ms]
  Apr 18 00:44:21.134: INFO: Created: latency-svc-h6jdp
  Apr 18 00:44:21.170: INFO: Got endpoints: latency-svc-jhdbb [747.282971ms]
  Apr 18 00:44:21.184: INFO: Created: latency-svc-rpjpz
  Apr 18 00:44:21.220: INFO: Got endpoints: latency-svc-q4srd [747.828116ms]
  Apr 18 00:44:21.230: INFO: Created: latency-svc-9lzj4
  Apr 18 00:44:21.271: INFO: Got endpoints: latency-svc-r8zx9 [749.795887ms]
  Apr 18 00:44:21.283: INFO: Created: latency-svc-pbvz6
  Apr 18 00:44:21.321: INFO: Got endpoints: latency-svc-t8rxp [747.441154ms]
  Apr 18 00:44:21.333: INFO: Created: latency-svc-mz9ns
  Apr 18 00:44:21.372: INFO: Got endpoints: latency-svc-2fjww [749.270863ms]
  Apr 18 00:44:21.385: INFO: Created: latency-svc-64q4n
  Apr 18 00:44:21.422: INFO: Got endpoints: latency-svc-tbmz7 [749.788781ms]
  Apr 18 00:44:21.435: INFO: Created: latency-svc-hx7km
  Apr 18 00:44:21.475: INFO: Got endpoints: latency-svc-76xjc [752.198713ms]
  Apr 18 00:44:21.485: INFO: Created: latency-svc-2crtg
  Apr 18 00:44:21.525: INFO: Got endpoints: latency-svc-l6rvq [752.464294ms]
  Apr 18 00:44:21.536: INFO: Created: latency-svc-kx64h
  Apr 18 00:44:21.572: INFO: Got endpoints: latency-svc-2tsdn [747.29428ms]
  Apr 18 00:44:21.583: INFO: Created: latency-svc-njhgq
  Apr 18 00:44:21.620: INFO: Got endpoints: latency-svc-f58pf [749.349052ms]
  Apr 18 00:44:21.632: INFO: Created: latency-svc-hltvq
  Apr 18 00:44:21.669: INFO: Got endpoints: latency-svc-7p2gl [745.051598ms]
  Apr 18 00:44:21.681: INFO: Created: latency-svc-kpg78
  Apr 18 00:44:21.723: INFO: Got endpoints: latency-svc-svhxw [749.250359ms]
  Apr 18 00:44:21.732: INFO: Created: latency-svc-8wg6p
  Apr 18 00:44:21.771: INFO: Got endpoints: latency-svc-8s9vb [748.3937ms]
  Apr 18 00:44:21.782: INFO: Created: latency-svc-lq2wh
  Apr 18 00:44:21.820: INFO: Got endpoints: latency-svc-vsckw [747.42085ms]
  Apr 18 00:44:21.832: INFO: Created: latency-svc-gwkhf
  Apr 18 00:44:21.872: INFO: Got endpoints: latency-svc-h6jdp [748.050928ms]
  Apr 18 00:44:21.888: INFO: Created: latency-svc-4zfps
  Apr 18 00:44:21.923: INFO: Got endpoints: latency-svc-rpjpz [752.481804ms]
  Apr 18 00:44:21.936: INFO: Created: latency-svc-p6rw2
  Apr 18 00:44:21.971: INFO: Got endpoints: latency-svc-9lzj4 [751.218955ms]
  Apr 18 00:44:21.985: INFO: Created: latency-svc-mzn9d
  Apr 18 00:44:22.020: INFO: Got endpoints: latency-svc-pbvz6 [748.028352ms]
  Apr 18 00:44:22.031: INFO: Created: latency-svc-w2762
  Apr 18 00:44:22.072: INFO: Got endpoints: latency-svc-mz9ns [751.579411ms]
  Apr 18 00:44:22.083: INFO: Created: latency-svc-nmsxz
  Apr 18 00:44:22.120: INFO: Got endpoints: latency-svc-64q4n [747.74609ms]
  Apr 18 00:44:22.131: INFO: Created: latency-svc-lhzks
  Apr 18 00:44:22.173: INFO: Got endpoints: latency-svc-hx7km [750.631807ms]
  Apr 18 00:44:22.184: INFO: Created: latency-svc-9h65l
  Apr 18 00:44:22.221: INFO: Got endpoints: latency-svc-2crtg [746.040605ms]
  Apr 18 00:44:22.232: INFO: Created: latency-svc-tw8bp
  Apr 18 00:44:22.296: INFO: Got endpoints: latency-svc-kx64h [770.767342ms]
  Apr 18 00:44:22.325: INFO: Created: latency-svc-phzpr
  Apr 18 00:44:22.353: INFO: Got endpoints: latency-svc-njhgq [780.572021ms]
  Apr 18 00:44:22.382: INFO: Got endpoints: latency-svc-hltvq [760.789725ms]
  Apr 18 00:44:22.383: INFO: Created: latency-svc-2cph6
  Apr 18 00:44:22.403: INFO: Created: latency-svc-r2n97
  Apr 18 00:44:22.422: INFO: Got endpoints: latency-svc-kpg78 [753.39667ms]
  Apr 18 00:44:22.439: INFO: Created: latency-svc-s8tl7
  Apr 18 00:44:22.477: INFO: Got endpoints: latency-svc-8wg6p [753.64419ms]
  Apr 18 00:44:22.493: INFO: Created: latency-svc-9qw59
  Apr 18 00:44:22.529: INFO: Got endpoints: latency-svc-lq2wh [757.04371ms]
  Apr 18 00:44:22.542: INFO: Created: latency-svc-rpz2n
  Apr 18 00:44:22.572: INFO: Got endpoints: latency-svc-gwkhf [751.686687ms]
  Apr 18 00:44:22.586: INFO: Created: latency-svc-txqlt
  Apr 18 00:44:22.623: INFO: Got endpoints: latency-svc-4zfps [750.151667ms]
  Apr 18 00:44:22.633: INFO: Created: latency-svc-2pdxx
  Apr 18 00:44:22.673: INFO: Got endpoints: latency-svc-p6rw2 [749.415556ms]
  Apr 18 00:44:22.694: INFO: Created: latency-svc-hjjmd
  Apr 18 00:44:22.724: INFO: Got endpoints: latency-svc-mzn9d [751.990917ms]
  Apr 18 00:44:22.741: INFO: Created: latency-svc-6gtb8
  Apr 18 00:44:22.774: INFO: Got endpoints: latency-svc-w2762 [752.800256ms]
  Apr 18 00:44:22.784: INFO: Created: latency-svc-t87wv
  Apr 18 00:44:22.830: INFO: Got endpoints: latency-svc-nmsxz [757.137685ms]
  Apr 18 00:44:22.842: INFO: Created: latency-svc-2dbcd
  Apr 18 00:44:22.882: INFO: Got endpoints: latency-svc-lhzks [761.369769ms]
  Apr 18 00:44:22.897: INFO: Created: latency-svc-bkrcn
  Apr 18 00:44:22.939: INFO: Got endpoints: latency-svc-9h65l [764.693043ms]
  Apr 18 00:44:22.953: INFO: Created: latency-svc-8kx5h
  Apr 18 00:44:22.970: INFO: Got endpoints: latency-svc-tw8bp [747.866157ms]
  Apr 18 00:44:22.995: INFO: Created: latency-svc-fmhlg
  Apr 18 00:44:23.024: INFO: Got endpoints: latency-svc-phzpr [727.334166ms]
  Apr 18 00:44:23.034: INFO: Created: latency-svc-gkz82
  Apr 18 00:44:23.074: INFO: Got endpoints: latency-svc-2cph6 [719.848494ms]
  Apr 18 00:44:23.085: INFO: Created: latency-svc-ftc9m
  Apr 18 00:44:23.126: INFO: Got endpoints: latency-svc-r2n97 [743.31646ms]
  Apr 18 00:44:23.138: INFO: Created: latency-svc-r49lj
  Apr 18 00:44:23.173: INFO: Got endpoints: latency-svc-s8tl7 [749.797013ms]
  Apr 18 00:44:23.184: INFO: Created: latency-svc-7h8ng
  Apr 18 00:44:23.220: INFO: Got endpoints: latency-svc-9qw59 [743.141333ms]
  Apr 18 00:44:23.231: INFO: Created: latency-svc-6qgkq
  Apr 18 00:44:23.270: INFO: Got endpoints: latency-svc-rpz2n [741.704559ms]
  Apr 18 00:44:23.285: INFO: Created: latency-svc-msh8s
  Apr 18 00:44:23.323: INFO: Got endpoints: latency-svc-txqlt [749.99472ms]
  Apr 18 00:44:23.333: INFO: Created: latency-svc-9psd9
  Apr 18 00:44:23.372: INFO: Got endpoints: latency-svc-2pdxx [749.241179ms]
  Apr 18 00:44:23.388: INFO: Created: latency-svc-44hlk
  Apr 18 00:44:23.425: INFO: Got endpoints: latency-svc-hjjmd [751.246737ms]
  Apr 18 00:44:23.438: INFO: Created: latency-svc-nk4r4
  Apr 18 00:44:23.474: INFO: Got endpoints: latency-svc-6gtb8 [750.298869ms]
  Apr 18 00:44:23.484: INFO: Created: latency-svc-99nvd
  Apr 18 00:44:23.523: INFO: Got endpoints: latency-svc-t87wv [749.559864ms]
  Apr 18 00:44:23.535: INFO: Created: latency-svc-dsgl7
  Apr 18 00:44:23.572: INFO: Got endpoints: latency-svc-2dbcd [741.871621ms]
  Apr 18 00:44:23.584: INFO: Created: latency-svc-k5rr8
  Apr 18 00:44:23.624: INFO: Got endpoints: latency-svc-bkrcn [741.473629ms]
  Apr 18 00:44:23.636: INFO: Created: latency-svc-qzvfc
  Apr 18 00:44:23.671: INFO: Got endpoints: latency-svc-8kx5h [731.462089ms]
  Apr 18 00:44:23.683: INFO: Created: latency-svc-q2zb2
  Apr 18 00:44:23.727: INFO: Got endpoints: latency-svc-fmhlg [756.703065ms]
  Apr 18 00:44:23.742: INFO: Created: latency-svc-tl7vl
  Apr 18 00:44:23.771: INFO: Got endpoints: latency-svc-gkz82 [746.388073ms]
  Apr 18 00:44:23.781: INFO: Created: latency-svc-j7l25
  Apr 18 00:44:23.821: INFO: Got endpoints: latency-svc-ftc9m [746.979496ms]
  Apr 18 00:44:23.833: INFO: Created: latency-svc-dlrp5
  Apr 18 00:44:23.872: INFO: Got endpoints: latency-svc-r49lj [745.920484ms]
  Apr 18 00:44:23.905: INFO: Created: latency-svc-x7sgd
  Apr 18 00:44:23.924: INFO: Got endpoints: latency-svc-7h8ng [750.880538ms]
  Apr 18 00:44:23.948: INFO: Created: latency-svc-dcwfn
  Apr 18 00:44:23.973: INFO: Got endpoints: latency-svc-6qgkq [751.881664ms]
  Apr 18 00:44:23.982: INFO: Created: latency-svc-j6vzf
  Apr 18 00:44:24.023: INFO: Got endpoints: latency-svc-msh8s [752.597857ms]
  Apr 18 00:44:24.037: INFO: Created: latency-svc-pnd59
  Apr 18 00:44:24.072: INFO: Got endpoints: latency-svc-9psd9 [748.495058ms]
  Apr 18 00:44:24.083: INFO: Created: latency-svc-thpgm
  Apr 18 00:44:24.124: INFO: Got endpoints: latency-svc-44hlk [750.857225ms]
  Apr 18 00:44:24.136: INFO: Created: latency-svc-vtr9c
  Apr 18 00:44:24.173: INFO: Got endpoints: latency-svc-nk4r4 [747.334616ms]
  Apr 18 00:44:24.186: INFO: Created: latency-svc-frlgf
  Apr 18 00:44:24.226: INFO: Got endpoints: latency-svc-99nvd [750.359079ms]
  Apr 18 00:44:24.238: INFO: Created: latency-svc-sr486
  Apr 18 00:44:24.271: INFO: Got endpoints: latency-svc-dsgl7 [747.155724ms]
  Apr 18 00:44:24.284: INFO: Created: latency-svc-2h477
  Apr 18 00:44:24.323: INFO: Got endpoints: latency-svc-k5rr8 [750.868644ms]
  Apr 18 00:44:24.353: INFO: Created: latency-svc-pxgjf
  Apr 18 00:44:24.372: INFO: Got endpoints: latency-svc-qzvfc [747.06494ms]
  Apr 18 00:44:24.383: INFO: Created: latency-svc-qkx5m
  Apr 18 00:44:24.422: INFO: Got endpoints: latency-svc-q2zb2 [751.05547ms]
  Apr 18 00:44:24.478: INFO: Got endpoints: latency-svc-tl7vl [750.250385ms]
  Apr 18 00:44:24.521: INFO: Got endpoints: latency-svc-j7l25 [749.724355ms]
  Apr 18 00:44:24.571: INFO: Got endpoints: latency-svc-dlrp5 [749.252266ms]
  Apr 18 00:44:24.621: INFO: Got endpoints: latency-svc-x7sgd [748.179537ms]
  Apr 18 00:44:24.670: INFO: Got endpoints: latency-svc-dcwfn [746.093635ms]
  Apr 18 00:44:24.722: INFO: Got endpoints: latency-svc-j6vzf [749.124385ms]
  Apr 18 00:44:24.770: INFO: Got endpoints: latency-svc-pnd59 [746.792617ms]
  Apr 18 00:44:24.821: INFO: Got endpoints: latency-svc-thpgm [749.196307ms]
  Apr 18 00:44:24.869: INFO: Got endpoints: latency-svc-vtr9c [745.882785ms]
  Apr 18 00:44:24.923: INFO: Got endpoints: latency-svc-frlgf [750.076074ms]
  Apr 18 00:44:24.974: INFO: Got endpoints: latency-svc-sr486 [748.277486ms]
  Apr 18 00:44:25.024: INFO: Got endpoints: latency-svc-2h477 [752.156205ms]
  Apr 18 00:44:25.071: INFO: Got endpoints: latency-svc-pxgjf [748.191983ms]
  Apr 18 00:44:25.123: INFO: Got endpoints: latency-svc-qkx5m [749.335305ms]
  Apr 18 00:44:25.123: INFO: Latencies: [39.0927ms 64.488332ms 95.37565ms 97.0318ms 102.751505ms 106.866004ms 127.198496ms 133.675972ms 140.531077ms 147.217277ms 155.603467ms 160.778454ms 164.265811ms 187.565807ms 194.972952ms 201.70389ms 202.799541ms 217.933772ms 227.983754ms 229.986867ms 234.142918ms 234.170871ms 238.175942ms 244.352565ms 245.808594ms 248.436278ms 252.644888ms 253.147965ms 261.769005ms 262.019054ms 264.2419ms 265.515246ms 269.084784ms 269.114051ms 279.122304ms 284.205565ms 284.306112ms 284.780544ms 294.160278ms 295.503102ms 299.725103ms 303.346128ms 320.409315ms 321.494976ms 326.065801ms 374.26766ms 401.574752ms 406.461989ms 422.640217ms 433.32011ms 462.210395ms 498.515894ms 522.634434ms 536.812064ms 547.962874ms 610.374679ms 669.827586ms 698.906564ms 719.848494ms 721.195605ms 727.334166ms 731.462089ms 738.216084ms 738.227791ms 738.917324ms 739.169593ms 741.473629ms 741.704559ms 741.871621ms 742.790916ms 743.141333ms 743.31646ms 743.734758ms 744.214958ms 745.051598ms 745.226386ms 745.882785ms 745.920484ms 745.937506ms 746.040605ms 746.093635ms 746.175411ms 746.243706ms 746.388073ms 746.708227ms 746.792617ms 746.979496ms 747.06494ms 747.155724ms 747.282971ms 747.29428ms 747.334616ms 747.347748ms 747.42085ms 747.439698ms 747.441154ms 747.51613ms 747.74609ms 747.828116ms 747.832217ms 747.866157ms 748.028352ms 748.050928ms 748.10121ms 748.179537ms 748.191983ms 748.207055ms 748.267022ms 748.277486ms 748.357914ms 748.360142ms 748.3937ms 748.495058ms 748.613362ms 748.702957ms 748.862951ms 749.124385ms 749.145846ms 749.196307ms 749.241179ms 749.250359ms 749.252266ms 749.270863ms 749.326023ms 749.335305ms 749.349052ms 749.415556ms 749.424161ms 749.559864ms 749.609842ms 749.620356ms 749.724355ms 749.788781ms 749.791357ms 749.795887ms 749.797013ms 749.834045ms 749.980626ms 749.99472ms 750.076074ms 750.151667ms 750.185641ms 750.191355ms 750.230675ms 750.250385ms 750.298869ms 750.359079ms 750.378766ms 750.396916ms 750.425731ms 750.516704ms 750.631807ms 750.63981ms 750.857225ms 750.863071ms 750.868644ms 750.880538ms 750.9228ms 750.955314ms 751.05547ms 751.126008ms 751.218955ms 751.246737ms 751.541072ms 751.579411ms 751.596278ms 751.658146ms 751.686687ms 751.692727ms 751.800759ms 751.881664ms 751.990917ms 752.156205ms 752.198713ms 752.464294ms 752.478104ms 752.481804ms 752.597857ms 752.800256ms 753.018124ms 753.39667ms 753.409151ms 753.64419ms 754.390391ms 754.54363ms 755.351791ms 756.703065ms 757.04371ms 757.137685ms 757.274887ms 759.654816ms 760.789725ms 761.369769ms 762.63697ms 764.151593ms 764.693043ms 770.767342ms 773.933304ms 780.572021ms 797.649004ms]
  Apr 18 00:44:25.123: INFO: 50 %ile: 747.866157ms
  Apr 18 00:44:25.124: INFO: 90 %ile: 753.39667ms
  Apr 18 00:44:25.124: INFO: 99 %ile: 780.572021ms
  Apr 18 00:44:25.124: INFO: Total sample count: 200
  Apr 18 00:44:25.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-8829" for this suite. @ 04/18/23 00:44:25.129
• [11.782 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 04/18/23 00:44:25.136
  Apr 18 00:44:25.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename containers @ 04/18/23 00:44:25.138
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:44:25.156
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:44:25.162
  STEP: Creating a pod to test override arguments @ 04/18/23 00:44:25.167
  STEP: Saw pod success @ 04/18/23 00:44:29.189
  Apr 18 00:44:29.200: INFO: Trying to get logs from node ip-10-0-27-81 pod client-containers-0563e543-f4e1-4b4b-b4d6-946ae10cd66b container agnhost-container: <nil>
  STEP: delete the pod @ 04/18/23 00:44:29.207
  Apr 18 00:44:29.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-5857" for this suite. @ 04/18/23 00:44:29.248
• [4.119 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 04/18/23 00:44:29.261
  Apr 18 00:44:29.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename events @ 04/18/23 00:44:29.263
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:44:29.282
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:44:29.288
  STEP: Create set of events @ 04/18/23 00:44:29.292
  Apr 18 00:44:29.296: INFO: created test-event-1
  Apr 18 00:44:29.302: INFO: created test-event-2
  Apr 18 00:44:29.314: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 04/18/23 00:44:29.314
  STEP: delete collection of events @ 04/18/23 00:44:29.317
  Apr 18 00:44:29.317: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 04/18/23 00:44:29.342
  Apr 18 00:44:29.342: INFO: requesting list of events to confirm quantity
  Apr 18 00:44:29.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-718" for this suite. @ 04/18/23 00:44:29.35
• [0.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 04/18/23 00:44:29.357
  Apr 18 00:44:29.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename replication-controller @ 04/18/23 00:44:29.358
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:44:29.376
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:44:29.38
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 04/18/23 00:44:29.384
  STEP: When a replication controller with a matching selector is created @ 04/18/23 00:44:31.409
  STEP: Then the orphan pod is adopted @ 04/18/23 00:44:31.417
  Apr 18 00:44:32.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-25" for this suite. @ 04/18/23 00:44:32.451
• [3.114 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 04/18/23 00:44:32.471
  Apr 18 00:44:32.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 04/18/23 00:44:32.472
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:44:32.527
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:44:32.532
  STEP: Setting up the test @ 04/18/23 00:44:32.541
  STEP: Creating hostNetwork=false pod @ 04/18/23 00:44:32.541
  STEP: Creating hostNetwork=true pod @ 04/18/23 00:44:36.599
  STEP: Running the test @ 04/18/23 00:44:38.615
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 04/18/23 00:44:38.615
  Apr 18 00:44:38.615: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8564 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 00:44:38.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 00:44:38.616: INFO: ExecWithOptions: Clientset creation
  Apr 18 00:44:38.616: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8564/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 18 00:44:38.676: INFO: Exec stderr: ""
  Apr 18 00:44:38.676: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8564 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 00:44:38.676: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 00:44:38.677: INFO: ExecWithOptions: Clientset creation
  Apr 18 00:44:38.677: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8564/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 18 00:44:38.760: INFO: Exec stderr: ""
  Apr 18 00:44:38.761: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8564 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 00:44:38.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 00:44:38.762: INFO: ExecWithOptions: Clientset creation
  Apr 18 00:44:38.765: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8564/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 18 00:44:38.868: INFO: Exec stderr: ""
  Apr 18 00:44:38.868: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8564 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 00:44:38.869: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 00:44:38.870: INFO: ExecWithOptions: Clientset creation
  Apr 18 00:44:38.870: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8564/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 18 00:44:38.940: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 04/18/23 00:44:38.94
  Apr 18 00:44:38.940: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8564 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 00:44:38.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 00:44:38.941: INFO: ExecWithOptions: Clientset creation
  Apr 18 00:44:38.941: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8564/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Apr 18 00:44:39.002: INFO: Exec stderr: ""
  Apr 18 00:44:39.002: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8564 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 00:44:39.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 00:44:39.003: INFO: ExecWithOptions: Clientset creation
  Apr 18 00:44:39.003: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8564/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Apr 18 00:44:39.063: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 04/18/23 00:44:39.063
  Apr 18 00:44:39.063: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8564 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 00:44:39.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 00:44:39.063: INFO: ExecWithOptions: Clientset creation
  Apr 18 00:44:39.063: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8564/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 18 00:44:39.130: INFO: Exec stderr: ""
  Apr 18 00:44:39.130: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8564 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 00:44:39.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 00:44:39.130: INFO: ExecWithOptions: Clientset creation
  Apr 18 00:44:39.130: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8564/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 18 00:44:39.197: INFO: Exec stderr: ""
  Apr 18 00:44:39.197: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8564 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 00:44:39.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 00:44:39.198: INFO: ExecWithOptions: Clientset creation
  Apr 18 00:44:39.198: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8564/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 18 00:44:39.261: INFO: Exec stderr: ""
  Apr 18 00:44:39.261: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8564 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 00:44:39.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 00:44:39.262: INFO: ExecWithOptions: Clientset creation
  Apr 18 00:44:39.262: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8564/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 18 00:44:39.333: INFO: Exec stderr: ""
  Apr 18 00:44:39.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-8564" for this suite. @ 04/18/23 00:44:39.337
• [6.872 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 04/18/23 00:44:39.344
  Apr 18 00:44:39.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/18/23 00:44:39.345
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:44:39.369
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:44:39.373
  STEP: set up a multi version CRD @ 04/18/23 00:44:39.376
  Apr 18 00:44:39.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: mark a version not serverd @ 04/18/23 00:44:43.131
  STEP: check the unserved version gets removed @ 04/18/23 00:44:43.153
  STEP: check the other version is not changed @ 04/18/23 00:44:44.668
  Apr 18 00:44:47.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3142" for this suite. @ 04/18/23 00:44:47.613
• [8.274 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 04/18/23 00:44:47.62
  Apr 18 00:44:47.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename resourcequota @ 04/18/23 00:44:47.621
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:44:47.637
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:44:47.641
  STEP: Creating resourceQuota "e2e-rq-status-5xf4z" @ 04/18/23 00:44:47.647
  Apr 18 00:44:47.656: INFO: Resource quota "e2e-rq-status-5xf4z" reports spec: hard cpu limit of 500m
  Apr 18 00:44:47.656: INFO: Resource quota "e2e-rq-status-5xf4z" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-5xf4z" /status @ 04/18/23 00:44:47.656
  STEP: Confirm /status for "e2e-rq-status-5xf4z" resourceQuota via watch @ 04/18/23 00:44:47.668
  Apr 18 00:44:47.670: INFO: observed resourceQuota "e2e-rq-status-5xf4z" in namespace "resourcequota-1374" with hard status: v1.ResourceList(nil)
  Apr 18 00:44:47.670: INFO: Found resourceQuota "e2e-rq-status-5xf4z" in namespace "resourcequota-1374" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Apr 18 00:44:47.670: INFO: ResourceQuota "e2e-rq-status-5xf4z" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 04/18/23 00:44:47.675
  Apr 18 00:44:47.682: INFO: Resource quota "e2e-rq-status-5xf4z" reports spec: hard cpu limit of 1
  Apr 18 00:44:47.682: INFO: Resource quota "e2e-rq-status-5xf4z" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-5xf4z" /status @ 04/18/23 00:44:47.682
  STEP: Confirm /status for "e2e-rq-status-5xf4z" resourceQuota via watch @ 04/18/23 00:44:47.689
  Apr 18 00:44:47.691: INFO: observed resourceQuota "e2e-rq-status-5xf4z" in namespace "resourcequota-1374" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Apr 18 00:44:47.691: INFO: Found resourceQuota "e2e-rq-status-5xf4z" in namespace "resourcequota-1374" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Apr 18 00:44:47.691: INFO: ResourceQuota "e2e-rq-status-5xf4z" /status was patched
  STEP: Get "e2e-rq-status-5xf4z" /status @ 04/18/23 00:44:47.691
  Apr 18 00:44:47.694: INFO: Resourcequota "e2e-rq-status-5xf4z" reports status: hard cpu of 1
  Apr 18 00:44:47.694: INFO: Resourcequota "e2e-rq-status-5xf4z" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-5xf4z" /status before checking Spec is unchanged @ 04/18/23 00:44:47.697
  Apr 18 00:44:47.710: INFO: Resourcequota "e2e-rq-status-5xf4z" reports status: hard cpu of 2
  Apr 18 00:44:47.710: INFO: Resourcequota "e2e-rq-status-5xf4z" reports status: hard memory of 2Gi
  Apr 18 00:44:47.713: INFO: Found resourceQuota "e2e-rq-status-5xf4z" in namespace "resourcequota-1374" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  Apr 18 00:44:52.745: INFO: ResourceQuota "e2e-rq-status-5xf4z" Spec was unchanged and /status reset
  Apr 18 00:44:52.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1374" for this suite. @ 04/18/23 00:44:52.752
• [5.146 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 04/18/23 00:44:52.766
  Apr 18 00:44:52.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename webhook @ 04/18/23 00:44:52.768
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:44:52.801
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:44:52.806
  STEP: Setting up server cert @ 04/18/23 00:44:52.848
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/18/23 00:44:53.756
  STEP: Deploying the webhook pod @ 04/18/23 00:44:53.766
  STEP: Wait for the deployment to be ready @ 04/18/23 00:44:53.775
  Apr 18 00:44:53.784: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/18/23 00:44:55.986
  STEP: Verifying the service has paired with the endpoint @ 04/18/23 00:44:56.002
  Apr 18 00:44:57.003: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 04/18/23 00:44:57.073
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/18/23 00:44:57.165
  STEP: Deleting the collection of validation webhooks @ 04/18/23 00:44:57.231
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/18/23 00:44:57.264
  Apr 18 00:44:57.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3054" for this suite. @ 04/18/23 00:44:57.33
  STEP: Destroying namespace "webhook-markers-8474" for this suite. @ 04/18/23 00:44:57.338
• [4.580 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 04/18/23 00:44:57.348
  Apr 18 00:44:57.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename emptydir @ 04/18/23 00:44:57.349
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:44:57.383
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:44:57.388
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 04/18/23 00:44:57.393
  STEP: Saw pod success @ 04/18/23 00:45:01.416
  Apr 18 00:45:01.419: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-0aae4d8c-e010-4582-ba95-af7b579f4d98 container test-container: <nil>
  STEP: delete the pod @ 04/18/23 00:45:01.438
  Apr 18 00:45:01.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5125" for this suite. @ 04/18/23 00:45:01.488
• [4.146 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 04/18/23 00:45:01.503
  Apr 18 00:45:01.503: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename webhook @ 04/18/23 00:45:01.508
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:45:01.532
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:45:01.54
  STEP: Setting up server cert @ 04/18/23 00:45:01.573
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/18/23 00:45:02.73
  STEP: Deploying the webhook pod @ 04/18/23 00:45:02.746
  STEP: Wait for the deployment to be ready @ 04/18/23 00:45:02.765
  Apr 18 00:45:02.785: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/18/23 00:45:04.795
  STEP: Verifying the service has paired with the endpoint @ 04/18/23 00:45:04.806
  Apr 18 00:45:05.808: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 04/18/23 00:45:05.812
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 04/18/23 00:45:05.841
  STEP: Creating a configMap that should not be mutated @ 04/18/23 00:45:05.848
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 04/18/23 00:45:05.862
  STEP: Creating a configMap that should be mutated @ 04/18/23 00:45:05.876
  Apr 18 00:45:05.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1987" for this suite. @ 04/18/23 00:45:06.04
  STEP: Destroying namespace "webhook-markers-1865" for this suite. @ 04/18/23 00:45:06.058
• [4.615 seconds]
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 04/18/23 00:45:06.121
  Apr 18 00:45:06.122: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename svcaccounts @ 04/18/23 00:45:06.128
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:45:06.214
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:45:06.226
  Apr 18 00:45:06.255: INFO: created pod
  STEP: Saw pod success @ 04/18/23 00:45:10.271
  Apr 18 00:45:40.271: INFO: polling logs
  Apr 18 00:45:40.285: INFO: Pod logs: 
  I0418 00:45:07.504554       1 log.go:198] OK: Got token
  I0418 00:45:07.504825       1 log.go:198] validating with in-cluster discovery
  I0418 00:45:07.505397       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
  I0418 00:45:07.505442       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-9015:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1681779306, NotBefore:1681778706, IssuedAt:1681778706, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9015", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"1488cc69-0f2a-424d-aea8-49d9342a7049"}}}
  I0418 00:45:07.527756       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I0418 00:45:07.530158       1 log.go:198] OK: Validated signature on JWT
  I0418 00:45:07.532989       1 log.go:198] OK: Got valid claims from token!
  I0418 00:45:07.533032       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-9015:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1681779306, NotBefore:1681778706, IssuedAt:1681778706, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9015", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"1488cc69-0f2a-424d-aea8-49d9342a7049"}}}

  Apr 18 00:45:40.285: INFO: completed pod
  Apr 18 00:45:40.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9015" for this suite. @ 04/18/23 00:45:40.297
• [34.183 seconds]
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 04/18/23 00:45:40.305
  Apr 18 00:45:40.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename services @ 04/18/23 00:45:40.306
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:45:40.339
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:45:40.348
  STEP: creating service in namespace services-2089 @ 04/18/23 00:45:40.354
  STEP: creating service affinity-nodeport-transition in namespace services-2089 @ 04/18/23 00:45:40.354
  STEP: creating replication controller affinity-nodeport-transition in namespace services-2089 @ 04/18/23 00:45:40.373
  I0418 00:45:40.391899      21 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-2089, replica count: 3
  I0418 00:45:43.443988      21 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 18 00:45:43.456: INFO: Creating new exec pod
  Apr 18 00:45:46.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-2089 exec execpod-affinity7l6mp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Apr 18 00:45:46.772: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-transition 80\n+ echo hostName\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Apr 18 00:45:46.772: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 18 00:45:46.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-2089 exec execpod-affinity7l6mp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.3.191.51 80'
  Apr 18 00:45:46.922: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.3.191.51 80\nConnection to 10.3.191.51 80 port [tcp/http] succeeded!\n"
  Apr 18 00:45:46.922: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 18 00:45:46.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-2089 exec execpod-affinity7l6mp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.14.154 30618'
  Apr 18 00:45:47.119: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.14.154 30618\nConnection to 10.0.14.154 30618 port [tcp/*] succeeded!\n"
  Apr 18 00:45:47.120: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 18 00:45:47.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-2089 exec execpod-affinity7l6mp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.27.81 30618'
  Apr 18 00:45:47.295: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.27.81 30618\nConnection to 10.0.27.81 30618 port [tcp/*] succeeded!\n"
  Apr 18 00:45:47.295: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 18 00:45:47.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-2089 exec execpod-affinity7l6mp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.14.154:30618/ ; done'
  Apr 18 00:45:47.633: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n"
  Apr 18 00:45:47.633: INFO: stdout: "\naffinity-nodeport-transition-nrdjw\naffinity-nodeport-transition-vlrv4\naffinity-nodeport-transition-jpskq\naffinity-nodeport-transition-nrdjw\naffinity-nodeport-transition-vlrv4\naffinity-nodeport-transition-jpskq\naffinity-nodeport-transition-nrdjw\naffinity-nodeport-transition-vlrv4\naffinity-nodeport-transition-jpskq\naffinity-nodeport-transition-nrdjw\naffinity-nodeport-transition-vlrv4\naffinity-nodeport-transition-jpskq\naffinity-nodeport-transition-nrdjw\naffinity-nodeport-transition-vlrv4\naffinity-nodeport-transition-jpskq\naffinity-nodeport-transition-nrdjw"
  Apr 18 00:45:47.633: INFO: Received response from host: affinity-nodeport-transition-nrdjw
  Apr 18 00:45:47.633: INFO: Received response from host: affinity-nodeport-transition-vlrv4
  Apr 18 00:45:47.633: INFO: Received response from host: affinity-nodeport-transition-jpskq
  Apr 18 00:45:47.633: INFO: Received response from host: affinity-nodeport-transition-nrdjw
  Apr 18 00:45:47.633: INFO: Received response from host: affinity-nodeport-transition-vlrv4
  Apr 18 00:45:47.633: INFO: Received response from host: affinity-nodeport-transition-jpskq
  Apr 18 00:45:47.633: INFO: Received response from host: affinity-nodeport-transition-nrdjw
  Apr 18 00:45:47.633: INFO: Received response from host: affinity-nodeport-transition-vlrv4
  Apr 18 00:45:47.633: INFO: Received response from host: affinity-nodeport-transition-jpskq
  Apr 18 00:45:47.633: INFO: Received response from host: affinity-nodeport-transition-nrdjw
  Apr 18 00:45:47.633: INFO: Received response from host: affinity-nodeport-transition-vlrv4
  Apr 18 00:45:47.633: INFO: Received response from host: affinity-nodeport-transition-jpskq
  Apr 18 00:45:47.633: INFO: Received response from host: affinity-nodeport-transition-nrdjw
  Apr 18 00:45:47.633: INFO: Received response from host: affinity-nodeport-transition-vlrv4
  Apr 18 00:45:47.633: INFO: Received response from host: affinity-nodeport-transition-jpskq
  Apr 18 00:45:47.633: INFO: Received response from host: affinity-nodeport-transition-nrdjw
  Apr 18 00:45:47.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-2089 exec execpod-affinity7l6mp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.14.154:30618/ ; done'
  Apr 18 00:45:47.888: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:30618/\n"
  Apr 18 00:45:47.888: INFO: stdout: "\naffinity-nodeport-transition-jpskq\naffinity-nodeport-transition-jpskq\naffinity-nodeport-transition-jpskq\naffinity-nodeport-transition-jpskq\naffinity-nodeport-transition-jpskq\naffinity-nodeport-transition-jpskq\naffinity-nodeport-transition-jpskq\naffinity-nodeport-transition-jpskq\naffinity-nodeport-transition-jpskq\naffinity-nodeport-transition-jpskq\naffinity-nodeport-transition-jpskq\naffinity-nodeport-transition-jpskq\naffinity-nodeport-transition-jpskq\naffinity-nodeport-transition-jpskq\naffinity-nodeport-transition-jpskq\naffinity-nodeport-transition-jpskq"
  Apr 18 00:45:47.888: INFO: Received response from host: affinity-nodeport-transition-jpskq
  Apr 18 00:45:47.888: INFO: Received response from host: affinity-nodeport-transition-jpskq
  Apr 18 00:45:47.888: INFO: Received response from host: affinity-nodeport-transition-jpskq
  Apr 18 00:45:47.888: INFO: Received response from host: affinity-nodeport-transition-jpskq
  Apr 18 00:45:47.888: INFO: Received response from host: affinity-nodeport-transition-jpskq
  Apr 18 00:45:47.888: INFO: Received response from host: affinity-nodeport-transition-jpskq
  Apr 18 00:45:47.888: INFO: Received response from host: affinity-nodeport-transition-jpskq
  Apr 18 00:45:47.888: INFO: Received response from host: affinity-nodeport-transition-jpskq
  Apr 18 00:45:47.888: INFO: Received response from host: affinity-nodeport-transition-jpskq
  Apr 18 00:45:47.888: INFO: Received response from host: affinity-nodeport-transition-jpskq
  Apr 18 00:45:47.888: INFO: Received response from host: affinity-nodeport-transition-jpskq
  Apr 18 00:45:47.888: INFO: Received response from host: affinity-nodeport-transition-jpskq
  Apr 18 00:45:47.888: INFO: Received response from host: affinity-nodeport-transition-jpskq
  Apr 18 00:45:47.888: INFO: Received response from host: affinity-nodeport-transition-jpskq
  Apr 18 00:45:47.888: INFO: Received response from host: affinity-nodeport-transition-jpskq
  Apr 18 00:45:47.888: INFO: Received response from host: affinity-nodeport-transition-jpskq
  Apr 18 00:45:47.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 18 00:45:47.892: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2089, will wait for the garbage collector to delete the pods @ 04/18/23 00:45:47.905
  Apr 18 00:45:47.978: INFO: Deleting ReplicationController affinity-nodeport-transition took: 12.508677ms
  Apr 18 00:45:48.079: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.639616ms
  STEP: Destroying namespace "services-2089" for this suite. @ 04/18/23 00:45:50.41
• [10.115 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 04/18/23 00:45:50.421
  Apr 18 00:45:50.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename controllerrevisions @ 04/18/23 00:45:50.423
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:45:50.457
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:45:50.465
  STEP: Creating DaemonSet "e2e-th7tn-daemon-set" @ 04/18/23 00:45:50.488
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/18/23 00:45:50.494
  Apr 18 00:45:50.501: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:45:50.504: INFO: Number of nodes with available pods controlled by daemonset e2e-th7tn-daemon-set: 0
  Apr 18 00:45:50.504: INFO: Node ip-10-0-14-154 is running 0 daemon pod, expected 1
  Apr 18 00:45:51.508: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:45:51.513: INFO: Number of nodes with available pods controlled by daemonset e2e-th7tn-daemon-set: 1
  Apr 18 00:45:51.513: INFO: Node ip-10-0-27-81 is running 0 daemon pod, expected 1
  Apr 18 00:45:52.511: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:45:52.516: INFO: Number of nodes with available pods controlled by daemonset e2e-th7tn-daemon-set: 2
  Apr 18 00:45:52.516: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-th7tn-daemon-set
  STEP: Confirm DaemonSet "e2e-th7tn-daemon-set" successfully created with "daemonset-name=e2e-th7tn-daemon-set" label @ 04/18/23 00:45:52.524
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-th7tn-daemon-set" @ 04/18/23 00:45:52.539
  Apr 18 00:45:52.551: INFO: Located ControllerRevision: "e2e-th7tn-daemon-set-57d4654f46"
  STEP: Patching ControllerRevision "e2e-th7tn-daemon-set-57d4654f46" @ 04/18/23 00:45:52.562
  Apr 18 00:45:52.575: INFO: e2e-th7tn-daemon-set-57d4654f46 has been patched
  STEP: Create a new ControllerRevision @ 04/18/23 00:45:52.575
  Apr 18 00:45:52.589: INFO: Created ControllerRevision: e2e-th7tn-daemon-set-77b88dd59b
  STEP: Confirm that there are two ControllerRevisions @ 04/18/23 00:45:52.589
  Apr 18 00:45:52.589: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 18 00:45:52.592: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-th7tn-daemon-set-57d4654f46" @ 04/18/23 00:45:52.592
  STEP: Confirm that there is only one ControllerRevision @ 04/18/23 00:45:52.597
  Apr 18 00:45:52.597: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 18 00:45:52.600: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-th7tn-daemon-set-77b88dd59b" @ 04/18/23 00:45:52.603
  Apr 18 00:45:52.620: INFO: e2e-th7tn-daemon-set-77b88dd59b has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 04/18/23 00:45:52.62
  W0418 00:45:52.633013      21 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 04/18/23 00:45:52.633
  Apr 18 00:45:52.633: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 18 00:45:53.639: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 18 00:45:53.652: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-th7tn-daemon-set-77b88dd59b=updated" @ 04/18/23 00:45:53.652
  STEP: Confirm that there is only one ControllerRevision @ 04/18/23 00:45:53.665
  Apr 18 00:45:53.665: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 18 00:45:53.670: INFO: Found 1 ControllerRevisions
  Apr 18 00:45:53.674: INFO: ControllerRevision "e2e-th7tn-daemon-set-7949d5b5df" has revision 3
  STEP: Deleting DaemonSet "e2e-th7tn-daemon-set" @ 04/18/23 00:45:53.677
  STEP: deleting DaemonSet.extensions e2e-th7tn-daemon-set in namespace controllerrevisions-1247, will wait for the garbage collector to delete the pods @ 04/18/23 00:45:53.678
  Apr 18 00:45:53.738: INFO: Deleting DaemonSet.extensions e2e-th7tn-daemon-set took: 7.240832ms
  Apr 18 00:45:53.839: INFO: Terminating DaemonSet.extensions e2e-th7tn-daemon-set pods took: 100.913407ms
  Apr 18 00:45:55.143: INFO: Number of nodes with available pods controlled by daemonset e2e-th7tn-daemon-set: 0
  Apr 18 00:45:55.143: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-th7tn-daemon-set
  Apr 18 00:45:55.145: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"55989"},"items":null}

  Apr 18 00:45:55.148: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"55989"},"items":null}

  Apr 18 00:45:55.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-1247" for this suite. @ 04/18/23 00:45:55.159
• [4.742 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 04/18/23 00:45:55.165
  Apr 18 00:45:55.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename var-expansion @ 04/18/23 00:45:55.167
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:45:55.184
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:45:55.188
  Apr 18 00:45:57.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 18 00:45:57.220: INFO: Deleting pod "var-expansion-109148ac-f8b2-45ee-9662-ed0167098581" in namespace "var-expansion-1750"
  Apr 18 00:45:57.231: INFO: Wait up to 5m0s for pod "var-expansion-109148ac-f8b2-45ee-9662-ed0167098581" to be fully deleted
  STEP: Destroying namespace "var-expansion-1750" for this suite. @ 04/18/23 00:46:01.245
• [6.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 04/18/23 00:46:01.251
  Apr 18 00:46:01.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename secrets @ 04/18/23 00:46:01.251
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:46:01.271
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:46:01.275
  STEP: creating secret secrets-3204/secret-test-28f7651a-fdcb-4215-a05d-8647d92c5741 @ 04/18/23 00:46:01.279
  STEP: Creating a pod to test consume secrets @ 04/18/23 00:46:01.285
  STEP: Saw pod success @ 04/18/23 00:46:05.31
  Apr 18 00:46:05.314: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-configmaps-e352ed56-cee9-4da8-b6d9-bf63a86c68a5 container env-test: <nil>
  STEP: delete the pod @ 04/18/23 00:46:05.321
  Apr 18 00:46:05.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3204" for this suite. @ 04/18/23 00:46:05.379
• [4.135 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 04/18/23 00:46:05.386
  Apr 18 00:46:05.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename emptydir @ 04/18/23 00:46:05.387
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:46:05.419
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:46:05.425
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 04/18/23 00:46:05.429
  STEP: Saw pod success @ 04/18/23 00:46:09.464
  Apr 18 00:46:09.467: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-abcd4c51-396b-44ad-8713-44b3823288a9 container test-container: <nil>
  STEP: delete the pod @ 04/18/23 00:46:09.473
  Apr 18 00:46:09.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3326" for this suite. @ 04/18/23 00:46:09.495
• [4.118 seconds]
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 04/18/23 00:46:09.505
  Apr 18 00:46:09.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename events @ 04/18/23 00:46:09.506
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:46:09.534
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:46:09.538
  STEP: creating a test event @ 04/18/23 00:46:09.541
  STEP: listing all events in all namespaces @ 04/18/23 00:46:09.547
  STEP: patching the test event @ 04/18/23 00:46:09.551
  STEP: fetching the test event @ 04/18/23 00:46:09.556
  STEP: updating the test event @ 04/18/23 00:46:09.559
  STEP: getting the test event @ 04/18/23 00:46:09.567
  STEP: deleting the test event @ 04/18/23 00:46:09.574
  STEP: listing all events in all namespaces @ 04/18/23 00:46:09.582
  Apr 18 00:46:09.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-1118" for this suite. @ 04/18/23 00:46:09.592
• [0.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 04/18/23 00:46:09.599
  Apr 18 00:46:09.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename dns @ 04/18/23 00:46:09.6
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:46:09.617
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:46:09.624
  STEP: Creating a test headless service @ 04/18/23 00:46:09.627
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-328.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-328.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-328.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-328.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-328.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-328.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-328.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-328.svc.cluster.local;sleep 1; done
   @ 04/18/23 00:46:09.636
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-328.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-328.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-328.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-328.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-328.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-328.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-328.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-328.svc.cluster.local;sleep 1; done
   @ 04/18/23 00:46:09.636
  STEP: creating a pod to probe DNS @ 04/18/23 00:46:09.636
  STEP: submitting the pod to kubernetes @ 04/18/23 00:46:09.638
  STEP: retrieving the pod @ 04/18/23 00:46:11.671
  STEP: looking for the results for each expected name from probers @ 04/18/23 00:46:11.674
  Apr 18 00:46:11.678: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-328.svc.cluster.local from pod dns-328/dns-test-4659ac12-298f-4857-b5be-3df494d9c078: the server could not find the requested resource (get pods dns-test-4659ac12-298f-4857-b5be-3df494d9c078)
  Apr 18 00:46:11.680: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-328.svc.cluster.local from pod dns-328/dns-test-4659ac12-298f-4857-b5be-3df494d9c078: the server could not find the requested resource (get pods dns-test-4659ac12-298f-4857-b5be-3df494d9c078)
  Apr 18 00:46:11.683: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-328.svc.cluster.local from pod dns-328/dns-test-4659ac12-298f-4857-b5be-3df494d9c078: the server could not find the requested resource (get pods dns-test-4659ac12-298f-4857-b5be-3df494d9c078)
  Apr 18 00:46:11.686: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-328.svc.cluster.local from pod dns-328/dns-test-4659ac12-298f-4857-b5be-3df494d9c078: the server could not find the requested resource (get pods dns-test-4659ac12-298f-4857-b5be-3df494d9c078)
  Apr 18 00:46:11.689: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-328.svc.cluster.local from pod dns-328/dns-test-4659ac12-298f-4857-b5be-3df494d9c078: the server could not find the requested resource (get pods dns-test-4659ac12-298f-4857-b5be-3df494d9c078)
  Apr 18 00:46:11.691: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-328.svc.cluster.local from pod dns-328/dns-test-4659ac12-298f-4857-b5be-3df494d9c078: the server could not find the requested resource (get pods dns-test-4659ac12-298f-4857-b5be-3df494d9c078)
  Apr 18 00:46:11.694: INFO: Unable to read jessie_udp@dns-test-service-2.dns-328.svc.cluster.local from pod dns-328/dns-test-4659ac12-298f-4857-b5be-3df494d9c078: the server could not find the requested resource (get pods dns-test-4659ac12-298f-4857-b5be-3df494d9c078)
  Apr 18 00:46:11.697: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-328.svc.cluster.local from pod dns-328/dns-test-4659ac12-298f-4857-b5be-3df494d9c078: the server could not find the requested resource (get pods dns-test-4659ac12-298f-4857-b5be-3df494d9c078)
  Apr 18 00:46:11.697: INFO: Lookups using dns-328/dns-test-4659ac12-298f-4857-b5be-3df494d9c078 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-328.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-328.svc.cluster.local wheezy_udp@dns-test-service-2.dns-328.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-328.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-328.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-328.svc.cluster.local jessie_udp@dns-test-service-2.dns-328.svc.cluster.local jessie_tcp@dns-test-service-2.dns-328.svc.cluster.local]

  Apr 18 00:46:16.767: INFO: DNS probes using dns-328/dns-test-4659ac12-298f-4857-b5be-3df494d9c078 succeeded

  Apr 18 00:46:16.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/18/23 00:46:16.77
  STEP: deleting the test headless service @ 04/18/23 00:46:16.794
  STEP: Destroying namespace "dns-328" for this suite. @ 04/18/23 00:46:16.824
• [7.238 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 04/18/23 00:46:16.84
  Apr 18 00:46:16.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename downward-api @ 04/18/23 00:46:16.841
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:46:16.89
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:46:16.9
  STEP: Creating a pod to test downward API volume plugin @ 04/18/23 00:46:16.908
  STEP: Saw pod success @ 04/18/23 00:46:20.932
  Apr 18 00:46:20.935: INFO: Trying to get logs from node ip-10-0-14-154 pod downwardapi-volume-359e9ab6-37f2-4a89-b91b-4b2411f02505 container client-container: <nil>
  STEP: delete the pod @ 04/18/23 00:46:20.94
  Apr 18 00:46:20.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6230" for this suite. @ 04/18/23 00:46:20.959
• [4.127 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 04/18/23 00:46:20.968
  Apr 18 00:46:20.968: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename statefulset @ 04/18/23 00:46:20.969
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:46:21.025
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:46:21.039
  STEP: Creating service test in namespace statefulset-3161 @ 04/18/23 00:46:21.043
  STEP: Creating statefulset ss in namespace statefulset-3161 @ 04/18/23 00:46:21.071
  Apr 18 00:46:21.102: INFO: Found 0 stateful pods, waiting for 1
  Apr 18 00:46:31.105: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 04/18/23 00:46:31.116
  STEP: Getting /status @ 04/18/23 00:46:31.131
  Apr 18 00:46:31.139: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 04/18/23 00:46:31.139
  Apr 18 00:46:31.150: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 04/18/23 00:46:31.151
  Apr 18 00:46:31.156: INFO: Observed &StatefulSet event: ADDED
  Apr 18 00:46:31.157: INFO: Found Statefulset ss in namespace statefulset-3161 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 18 00:46:31.157: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 04/18/23 00:46:31.157
  Apr 18 00:46:31.157: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 18 00:46:31.168: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 04/18/23 00:46:31.168
  Apr 18 00:46:31.174: INFO: Observed &StatefulSet event: ADDED
  Apr 18 00:46:31.174: INFO: Observed Statefulset ss in namespace statefulset-3161 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 18 00:46:31.174: INFO: Observed &StatefulSet event: MODIFIED
  Apr 18 00:46:31.174: INFO: Deleting all statefulset in ns statefulset-3161
  Apr 18 00:46:31.178: INFO: Scaling statefulset ss to 0
  Apr 18 00:46:41.223: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 18 00:46:41.229: INFO: Deleting statefulset ss
  Apr 18 00:46:41.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3161" for this suite. @ 04/18/23 00:46:41.252
• [20.295 seconds]
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 04/18/23 00:46:41.263
  Apr 18 00:46:41.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename kubelet-test @ 04/18/23 00:46:41.265
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:46:41.289
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:46:41.293
  Apr 18 00:46:41.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-9211" for this suite. @ 04/18/23 00:46:41.383
• [0.129 seconds]
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 04/18/23 00:46:41.394
  Apr 18 00:46:41.394: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename svcaccounts @ 04/18/23 00:46:41.395
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:46:41.438
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:46:41.449
  Apr 18 00:46:41.457: INFO: Got root ca configmap in namespace "svcaccounts-1013"
  Apr 18 00:46:41.466: INFO: Deleted root ca configmap in namespace "svcaccounts-1013"
  STEP: waiting for a new root ca configmap created @ 04/18/23 00:46:41.966
  Apr 18 00:46:41.970: INFO: Recreated root ca configmap in namespace "svcaccounts-1013"
  Apr 18 00:46:41.974: INFO: Updated root ca configmap in namespace "svcaccounts-1013"
  STEP: waiting for the root ca configmap reconciled @ 04/18/23 00:46:42.475
  Apr 18 00:46:42.479: INFO: Reconciled root ca configmap in namespace "svcaccounts-1013"
  Apr 18 00:46:42.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1013" for this suite. @ 04/18/23 00:46:42.482
• [1.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 04/18/23 00:46:42.489
  Apr 18 00:46:42.489: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 00:46:42.49
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:46:42.509
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:46:42.514
  STEP: Creating a pod to test downward API volume plugin @ 04/18/23 00:46:42.518
  STEP: Saw pod success @ 04/18/23 00:46:46.556
  Apr 18 00:46:46.571: INFO: Trying to get logs from node ip-10-0-14-154 pod downwardapi-volume-7a81d3ba-a4ed-43b7-9959-37a1162e9f77 container client-container: <nil>
  STEP: delete the pod @ 04/18/23 00:46:46.596
  Apr 18 00:46:46.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7195" for this suite. @ 04/18/23 00:46:46.639
• [4.166 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 04/18/23 00:46:46.655
  Apr 18 00:46:46.655: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename var-expansion @ 04/18/23 00:46:46.656
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:46:46.718
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:46:46.742
  STEP: creating the pod with failed condition @ 04/18/23 00:46:46.752
  STEP: updating the pod @ 04/18/23 00:48:46.775
  Apr 18 00:48:47.299: INFO: Successfully updated pod "var-expansion-1e2302fd-eb43-44d2-be40-d773cc8c5930"
  STEP: waiting for pod running @ 04/18/23 00:48:47.299
  STEP: deleting the pod gracefully @ 04/18/23 00:48:49.32
  Apr 18 00:48:49.320: INFO: Deleting pod "var-expansion-1e2302fd-eb43-44d2-be40-d773cc8c5930" in namespace "var-expansion-6666"
  Apr 18 00:48:49.358: INFO: Wait up to 5m0s for pod "var-expansion-1e2302fd-eb43-44d2-be40-d773cc8c5930" to be fully deleted
  Apr 18 00:49:21.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-6666" for this suite. @ 04/18/23 00:49:21.447
• [154.808 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 04/18/23 00:49:21.464
  Apr 18 00:49:21.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/18/23 00:49:21.465
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:49:21.49
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:49:21.495
  STEP: set up a multi version CRD @ 04/18/23 00:49:21.501
  Apr 18 00:49:21.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: rename a version @ 04/18/23 00:49:25.225
  STEP: check the new version name is served @ 04/18/23 00:49:25.262
  STEP: check the old version name is removed @ 04/18/23 00:49:26.022
  STEP: check the other version is not changed @ 04/18/23 00:49:26.774
  Apr 18 00:49:29.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1017" for this suite. @ 04/18/23 00:49:29.705
• [8.246 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 04/18/23 00:49:29.711
  Apr 18 00:49:29.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename secrets @ 04/18/23 00:49:29.713
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:49:29.727
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:49:29.732
  STEP: Creating secret with name secret-test-22f281ef-6ff8-4dfd-9474-a748a5fceed2 @ 04/18/23 00:49:29.735
  STEP: Creating a pod to test consume secrets @ 04/18/23 00:49:29.739
  STEP: Saw pod success @ 04/18/23 00:49:33.76
  Apr 18 00:49:33.763: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-secrets-8eee7d22-47ab-41f9-9a74-a65875f5d4bd container secret-env-test: <nil>
  STEP: delete the pod @ 04/18/23 00:49:33.78
  Apr 18 00:49:33.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7878" for this suite. @ 04/18/23 00:49:33.797
• [4.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 04/18/23 00:49:33.803
  Apr 18 00:49:33.804: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename statefulset @ 04/18/23 00:49:33.804
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:49:33.821
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:49:33.825
  STEP: Creating service test in namespace statefulset-1926 @ 04/18/23 00:49:33.829
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 04/18/23 00:49:33.833
  STEP: Creating stateful set ss in namespace statefulset-1926 @ 04/18/23 00:49:33.838
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1926 @ 04/18/23 00:49:33.85
  Apr 18 00:49:33.855: INFO: Found 0 stateful pods, waiting for 1
  Apr 18 00:49:43.862: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 04/18/23 00:49:43.862
  Apr 18 00:49:43.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=statefulset-1926 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 18 00:49:44.017: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 18 00:49:44.017: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 18 00:49:44.017: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 18 00:49:44.020: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Apr 18 00:49:54.028: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 18 00:49:54.028: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 18 00:49:54.051: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999675s
  Apr 18 00:49:55.055: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.991266345s
  Apr 18 00:49:56.058: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987625774s
  Apr 18 00:49:57.062: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.983548599s
  Apr 18 00:49:58.068: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.980154256s
  Apr 18 00:49:59.078: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.973191499s
  Apr 18 00:50:00.082: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.96418524s
  Apr 18 00:50:01.087: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.959395218s
  Apr 18 00:50:02.091: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.955023332s
  Apr 18 00:50:03.094: INFO: Verifying statefulset ss doesn't scale past 1 for another 951.327699ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1926 @ 04/18/23 00:50:04.095
  Apr 18 00:50:04.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=statefulset-1926 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 18 00:50:04.331: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 18 00:50:04.331: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 18 00:50:04.331: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 18 00:50:04.336: INFO: Found 1 stateful pods, waiting for 3
  Apr 18 00:50:14.344: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 18 00:50:14.344: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 18 00:50:14.344: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 04/18/23 00:50:14.344
  STEP: Scale down will halt with unhealthy stateful pod @ 04/18/23 00:50:14.344
  Apr 18 00:50:14.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=statefulset-1926 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 18 00:50:14.654: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 18 00:50:14.654: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 18 00:50:14.654: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 18 00:50:14.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=statefulset-1926 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 18 00:50:14.898: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 18 00:50:14.898: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 18 00:50:14.898: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 18 00:50:14.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=statefulset-1926 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 18 00:50:15.074: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 18 00:50:15.074: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 18 00:50:15.074: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 18 00:50:15.074: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 18 00:50:15.078: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  Apr 18 00:50:25.089: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 18 00:50:25.089: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Apr 18 00:50:25.089: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Apr 18 00:50:25.123: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999687s
  Apr 18 00:50:26.128: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989070356s
  Apr 18 00:50:27.135: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.984051388s
  Apr 18 00:50:28.139: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.978302927s
  Apr 18 00:50:29.143: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.974636573s
  Apr 18 00:50:30.147: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.970477614s
  Apr 18 00:50:31.151: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.966160989s
  Apr 18 00:50:32.163: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.962774429s
  Apr 18 00:50:33.166: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.950587606s
  Apr 18 00:50:34.170: INFO: Verifying statefulset ss doesn't scale past 3 for another 947.291709ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1926 @ 04/18/23 00:50:35.17
  Apr 18 00:50:35.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=statefulset-1926 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 18 00:50:35.401: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 18 00:50:35.401: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 18 00:50:35.401: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 18 00:50:35.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=statefulset-1926 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 18 00:50:35.586: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 18 00:50:35.586: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 18 00:50:35.586: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 18 00:50:35.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=statefulset-1926 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 18 00:50:35.775: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 18 00:50:35.775: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 18 00:50:35.775: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 18 00:50:35.775: INFO: Scaling statefulset ss to 0
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 04/18/23 00:50:45.791
  Apr 18 00:50:45.791: INFO: Deleting all statefulset in ns statefulset-1926
  Apr 18 00:50:45.794: INFO: Scaling statefulset ss to 0
  Apr 18 00:50:45.805: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 18 00:50:45.808: INFO: Deleting statefulset ss
  Apr 18 00:50:45.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1926" for this suite. @ 04/18/23 00:50:45.832
• [72.038 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 04/18/23 00:50:45.842
  Apr 18 00:50:45.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/18/23 00:50:45.843
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:50:45.865
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:50:45.872
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 04/18/23 00:50:45.876
  Apr 18 00:50:45.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 04/18/23 00:50:52.13
  Apr 18 00:50:52.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 00:50:53.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 00:50:59.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3858" for this suite. @ 04/18/23 00:50:59.409
• [13.573 seconds]
------------------------------
SSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 04/18/23 00:50:59.416
  Apr 18 00:50:59.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename certificates @ 04/18/23 00:50:59.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:50:59.445
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:50:59.456
  STEP: getting /apis @ 04/18/23 00:51:00.062
  STEP: getting /apis/certificates.k8s.io @ 04/18/23 00:51:00.067
  STEP: getting /apis/certificates.k8s.io/v1 @ 04/18/23 00:51:00.068
  STEP: creating @ 04/18/23 00:51:00.07
  STEP: getting @ 04/18/23 00:51:00.088
  STEP: listing @ 04/18/23 00:51:00.09
  STEP: watching @ 04/18/23 00:51:00.093
  Apr 18 00:51:00.093: INFO: starting watch
  STEP: patching @ 04/18/23 00:51:00.094
  STEP: updating @ 04/18/23 00:51:00.1
  Apr 18 00:51:00.106: INFO: waiting for watch events with expected annotations
  Apr 18 00:51:00.106: INFO: saw patched and updated annotations
  STEP: getting /approval @ 04/18/23 00:51:00.107
  STEP: patching /approval @ 04/18/23 00:51:00.11
  STEP: updating /approval @ 04/18/23 00:51:00.115
  STEP: getting /status @ 04/18/23 00:51:00.123
  STEP: patching /status @ 04/18/23 00:51:00.126
  STEP: updating /status @ 04/18/23 00:51:00.133
  STEP: deleting @ 04/18/23 00:51:00.143
  STEP: deleting a collection @ 04/18/23 00:51:00.154
  Apr 18 00:51:00.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-3693" for this suite. @ 04/18/23 00:51:00.167
• [0.756 seconds]
------------------------------
SS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 04/18/23 00:51:00.172
  Apr 18 00:51:00.172: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename var-expansion @ 04/18/23 00:51:00.172
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:51:00.195
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:51:00.199
  STEP: creating the pod @ 04/18/23 00:51:00.205
  STEP: waiting for pod running @ 04/18/23 00:51:00.213
  STEP: creating a file in subpath @ 04/18/23 00:51:02.22
  Apr 18 00:51:02.229: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3533 PodName:var-expansion-4475a45f-a566-4cdb-bc51-45eb4cb9248a ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 00:51:02.229: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 00:51:02.230: INFO: ExecWithOptions: Clientset creation
  Apr 18 00:51:02.230: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/var-expansion-3533/pods/var-expansion-4475a45f-a566-4cdb-bc51-45eb4cb9248a/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 04/18/23 00:51:02.387
  Apr 18 00:51:02.399: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3533 PodName:var-expansion-4475a45f-a566-4cdb-bc51-45eb4cb9248a ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 00:51:02.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 00:51:02.400: INFO: ExecWithOptions: Clientset creation
  Apr 18 00:51:02.400: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/var-expansion-3533/pods/var-expansion-4475a45f-a566-4cdb-bc51-45eb4cb9248a/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 04/18/23 00:51:02.609
  Apr 18 00:51:03.124: INFO: Successfully updated pod "var-expansion-4475a45f-a566-4cdb-bc51-45eb4cb9248a"
  STEP: waiting for annotated pod running @ 04/18/23 00:51:03.124
  STEP: deleting the pod gracefully @ 04/18/23 00:51:03.128
  Apr 18 00:51:03.128: INFO: Deleting pod "var-expansion-4475a45f-a566-4cdb-bc51-45eb4cb9248a" in namespace "var-expansion-3533"
  Apr 18 00:51:03.139: INFO: Wait up to 5m0s for pod "var-expansion-4475a45f-a566-4cdb-bc51-45eb4cb9248a" to be fully deleted
  Apr 18 00:51:35.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3533" for this suite. @ 04/18/23 00:51:35.229
• [35.066 seconds]
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 04/18/23 00:51:35.238
  Apr 18 00:51:35.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename subjectreview @ 04/18/23 00:51:35.239
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:51:35.29
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:51:35.307
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-4243" @ 04/18/23 00:51:35.316
  Apr 18 00:51:35.325: INFO: saUsername: "system:serviceaccount:subjectreview-4243:e2e"
  Apr 18 00:51:35.326: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-4243"}
  Apr 18 00:51:35.326: INFO: saUID: "94c99439-b333-46e8-844b-077411538389"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-4243:e2e" @ 04/18/23 00:51:35.326
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-4243:e2e" @ 04/18/23 00:51:35.326
  Apr 18 00:51:35.331: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-4243:e2e" api 'list' configmaps in "subjectreview-4243" namespace @ 04/18/23 00:51:35.331
  Apr 18 00:51:35.335: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-4243:e2e" @ 04/18/23 00:51:35.335
  Apr 18 00:51:35.339: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Apr 18 00:51:35.339: INFO: LocalSubjectAccessReview has been verified
  Apr 18 00:51:35.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-4243" for this suite. @ 04/18/23 00:51:35.342
• [0.108 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 04/18/23 00:51:35.348
  Apr 18 00:51:35.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename replication-controller @ 04/18/23 00:51:35.349
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:51:35.362
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:51:35.366
  STEP: creating a ReplicationController @ 04/18/23 00:51:35.372
  STEP: waiting for RC to be added @ 04/18/23 00:51:35.381
  STEP: waiting for available Replicas @ 04/18/23 00:51:35.381
  STEP: patching ReplicationController @ 04/18/23 00:51:37.166
  STEP: waiting for RC to be modified @ 04/18/23 00:51:37.175
  STEP: patching ReplicationController status @ 04/18/23 00:51:37.175
  STEP: waiting for RC to be modified @ 04/18/23 00:51:37.186
  STEP: waiting for available Replicas @ 04/18/23 00:51:37.186
  STEP: fetching ReplicationController status @ 04/18/23 00:51:37.198
  STEP: patching ReplicationController scale @ 04/18/23 00:51:37.203
  STEP: waiting for RC to be modified @ 04/18/23 00:51:37.209
  STEP: waiting for ReplicationController's scale to be the max amount @ 04/18/23 00:51:37.21
  STEP: fetching ReplicationController; ensuring that it's patched @ 04/18/23 00:51:38.807
  STEP: updating ReplicationController status @ 04/18/23 00:51:38.81
  STEP: waiting for RC to be modified @ 04/18/23 00:51:38.818
  STEP: listing all ReplicationControllers @ 04/18/23 00:51:38.819
  STEP: checking that ReplicationController has expected values @ 04/18/23 00:51:38.829
  STEP: deleting ReplicationControllers by collection @ 04/18/23 00:51:38.829
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 04/18/23 00:51:38.835
  Apr 18 00:51:38.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0418 00:51:38.919175      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-5223" for this suite. @ 04/18/23 00:51:38.925
• [3.592 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 04/18/23 00:51:38.942
  Apr 18 00:51:38.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename kubectl @ 04/18/23 00:51:38.942
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:51:38.962
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:51:38.966
  STEP: creating a replication controller @ 04/18/23 00:51:38.969
  Apr 18 00:51:38.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-3262 create -f -'
  Apr 18 00:51:39.628: INFO: stderr: ""
  Apr 18 00:51:39.628: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/18/23 00:51:39.628
  Apr 18 00:51:39.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-3262 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 18 00:51:39.718: INFO: stderr: ""
  Apr 18 00:51:39.718: INFO: stdout: "update-demo-nautilus-26w5x update-demo-nautilus-kgnxh "
  Apr 18 00:51:39.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-3262 get pods update-demo-nautilus-26w5x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 18 00:51:39.786: INFO: stderr: ""
  Apr 18 00:51:39.786: INFO: stdout: ""
  Apr 18 00:51:39.786: INFO: update-demo-nautilus-26w5x is created but not running
  E0418 00:51:39.920343      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:51:40.920933      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:51:41.921274      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:51:42.921462      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:51:43.921536      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 00:51:44.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-3262 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 18 00:51:44.918: INFO: stderr: ""
  Apr 18 00:51:44.918: INFO: stdout: "update-demo-nautilus-26w5x update-demo-nautilus-kgnxh "
  Apr 18 00:51:44.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-3262 get pods update-demo-nautilus-26w5x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E0418 00:51:44.921957      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 00:51:44.992: INFO: stderr: ""
  Apr 18 00:51:44.992: INFO: stdout: "true"
  Apr 18 00:51:44.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-3262 get pods update-demo-nautilus-26w5x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 18 00:51:45.088: INFO: stderr: ""
  Apr 18 00:51:45.088: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 18 00:51:45.088: INFO: validating pod update-demo-nautilus-26w5x
  Apr 18 00:51:45.099: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 18 00:51:45.100: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 18 00:51:45.100: INFO: update-demo-nautilus-26w5x is verified up and running
  Apr 18 00:51:45.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-3262 get pods update-demo-nautilus-kgnxh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 18 00:51:45.184: INFO: stderr: ""
  Apr 18 00:51:45.184: INFO: stdout: "true"
  Apr 18 00:51:45.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-3262 get pods update-demo-nautilus-kgnxh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 18 00:51:45.260: INFO: stderr: ""
  Apr 18 00:51:45.260: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 18 00:51:45.260: INFO: validating pod update-demo-nautilus-kgnxh
  Apr 18 00:51:45.280: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 18 00:51:45.280: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 18 00:51:45.280: INFO: update-demo-nautilus-kgnxh is verified up and running
  STEP: using delete to clean up resources @ 04/18/23 00:51:45.28
  Apr 18 00:51:45.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-3262 delete --grace-period=0 --force -f -'
  Apr 18 00:51:45.372: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 18 00:51:45.372: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Apr 18 00:51:45.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-3262 get rc,svc -l name=update-demo --no-headers'
  Apr 18 00:51:45.500: INFO: stderr: "No resources found in kubectl-3262 namespace.\n"
  Apr 18 00:51:45.500: INFO: stdout: ""
  Apr 18 00:51:45.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-3262 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 18 00:51:45.627: INFO: stderr: ""
  Apr 18 00:51:45.627: INFO: stdout: ""
  Apr 18 00:51:45.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3262" for this suite. @ 04/18/23 00:51:45.632
• [6.700 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 04/18/23 00:51:45.642
  Apr 18 00:51:45.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename configmap @ 04/18/23 00:51:45.643
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:51:45.673
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:51:45.689
  Apr 18 00:51:45.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7513" for this suite. @ 04/18/23 00:51:45.813
• [0.178 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 04/18/23 00:51:45.821
  Apr 18 00:51:45.821: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename namespaces @ 04/18/23 00:51:45.821
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:51:45.846
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:51:45.852
  STEP: Creating a test namespace @ 04/18/23 00:51:45.857
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:51:45.873
  STEP: Creating a service in the namespace @ 04/18/23 00:51:45.877
  STEP: Deleting the namespace @ 04/18/23 00:51:45.899
  STEP: Waiting for the namespace to be removed. @ 04/18/23 00:51:45.915
  E0418 00:51:45.922776      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:51:46.923542      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:51:47.923864      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:51:48.924391      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:51:49.924837      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:51:50.925349      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 04/18/23 00:51:51.918
  E0418 00:51:51.925793      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying there is no service in the namespace @ 04/18/23 00:51:51.935
  Apr 18 00:51:51.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-6116" for this suite. @ 04/18/23 00:51:51.942
  STEP: Destroying namespace "nsdeletetest-8308" for this suite. @ 04/18/23 00:51:51.947
  Apr 18 00:51:51.949: INFO: Namespace nsdeletetest-8308 was already deleted
  STEP: Destroying namespace "nsdeletetest-4372" for this suite. @ 04/18/23 00:51:51.949
• [6.134 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 04/18/23 00:51:51.955
  Apr 18 00:51:51.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename daemonsets @ 04/18/23 00:51:51.956
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:51:51.971
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:51:51.976
  STEP: Creating simple DaemonSet "daemon-set" @ 04/18/23 00:51:51.994
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/18/23 00:51:51.999
  Apr 18 00:51:52.006: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:51:52.010: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 00:51:52.011: INFO: Node ip-10-0-14-154 is running 0 daemon pod, expected 1
  E0418 00:51:52.926133      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 00:51:53.025: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:51:53.030: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 00:51:53.030: INFO: Node ip-10-0-14-154 is running 0 daemon pod, expected 1
  E0418 00:51:53.926634      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 00:51:54.015: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:51:54.018: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 00:51:54.018: INFO: Node ip-10-0-14-154 is running 0 daemon pod, expected 1
  E0418 00:51:54.926772      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 00:51:55.016: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:51:55.019: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 18 00:51:55.019: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 04/18/23 00:51:55.021
  Apr 18 00:51:55.053: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:51:55.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 18 00:51:55.065: INFO: Node ip-10-0-27-81 is running 0 daemon pod, expected 1
  E0418 00:51:55.927384      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 00:51:56.069: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:51:56.071: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 18 00:51:56.071: INFO: Node ip-10-0-27-81 is running 0 daemon pod, expected 1
  E0418 00:51:56.927967      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 00:51:57.070: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:51:57.075: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 18 00:51:57.075: INFO: Node ip-10-0-27-81 is running 0 daemon pod, expected 1
  E0418 00:51:57.928079      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 00:51:58.073: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:51:58.076: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 18 00:51:58.076: INFO: Node ip-10-0-27-81 is running 0 daemon pod, expected 1
  E0418 00:51:58.928412      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 00:51:59.074: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 00:51:59.078: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 18 00:51:59.078: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/18/23 00:51:59.081
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6958, will wait for the garbage collector to delete the pods @ 04/18/23 00:51:59.081
  Apr 18 00:51:59.142: INFO: Deleting DaemonSet.extensions daemon-set took: 6.749105ms
  Apr 18 00:51:59.243: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.793448ms
  E0418 00:51:59.928882      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:00.929942      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:01.930261      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 00:52:02.249: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 00:52:02.249: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 18 00:52:02.254: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"57640"},"items":null}

  Apr 18 00:52:02.258: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"57640"},"items":null}

  Apr 18 00:52:02.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6958" for this suite. @ 04/18/23 00:52:02.377
• [10.457 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 04/18/23 00:52:02.415
  Apr 18 00:52:02.415: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename services @ 04/18/23 00:52:02.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:52:02.461
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:52:02.477
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7483 @ 04/18/23 00:52:02.497
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 04/18/23 00:52:02.544
  STEP: creating service externalsvc in namespace services-7483 @ 04/18/23 00:52:02.544
  STEP: creating replication controller externalsvc in namespace services-7483 @ 04/18/23 00:52:02.594
  I0418 00:52:02.621126      21 runners.go:194] Created replication controller with name: externalsvc, namespace: services-7483, replica count: 2
  E0418 00:52:02.931333      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:03.931964      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:04.932242      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0418 00:52:05.672811      21 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 04/18/23 00:52:05.682
  Apr 18 00:52:05.710: INFO: Creating new exec pod
  E0418 00:52:05.933142      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:06.933244      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 00:52:07.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-7483 exec execpodjg7m2 -- /bin/sh -x -c nslookup clusterip-service.services-7483.svc.cluster.local'
  E0418 00:52:07.935399      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 00:52:08.048: INFO: stderr: "+ nslookup clusterip-service.services-7483.svc.cluster.local\n"
  Apr 18 00:52:08.048: INFO: stdout: "Server:\t\t10.3.0.10\nAddress:\t10.3.0.10#53\n\nclusterip-service.services-7483.svc.cluster.local\tcanonical name = externalsvc.services-7483.svc.cluster.local.\nName:\texternalsvc.services-7483.svc.cluster.local\nAddress: 10.3.143.252\n\n"
  Apr 18 00:52:08.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-7483, will wait for the garbage collector to delete the pods @ 04/18/23 00:52:08.052
  Apr 18 00:52:08.111: INFO: Deleting ReplicationController externalsvc took: 5.798523ms
  Apr 18 00:52:08.212: INFO: Terminating ReplicationController externalsvc pods took: 100.963192ms
  E0418 00:52:08.936371      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:09.936642      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 00:52:10.330: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-7483" for this suite. @ 04/18/23 00:52:10.349
• [7.943 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 04/18/23 00:52:10.358
  Apr 18 00:52:10.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 00:52:10.36
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:52:10.38
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:52:10.384
  STEP: Creating configMap with name projected-configmap-test-volume-map-be950aae-1362-4867-92a3-2dc8515a59b4 @ 04/18/23 00:52:10.388
  STEP: Creating a pod to test consume configMaps @ 04/18/23 00:52:10.392
  E0418 00:52:10.936848      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:11.937004      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:12.937741      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:13.937878      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 00:52:14.411
  Apr 18 00:52:14.414: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-projected-configmaps-aa9e71a0-796a-4b4f-94c9-45f4e8ea6289 container agnhost-container: <nil>
  STEP: delete the pod @ 04/18/23 00:52:14.437
  Apr 18 00:52:14.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4469" for this suite. @ 04/18/23 00:52:14.472
• [4.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 04/18/23 00:52:14.485
  Apr 18 00:52:14.485: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename security-context @ 04/18/23 00:52:14.486
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:52:14.544
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:52:14.559
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 04/18/23 00:52:14.568
  E0418 00:52:14.939382      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:15.939891      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:16.939982      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:17.940007      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 00:52:18.612
  Apr 18 00:52:18.615: INFO: Trying to get logs from node ip-10-0-14-154 pod security-context-57c5f93f-a701-4ae1-9f2c-e626a331fe52 container test-container: <nil>
  STEP: delete the pod @ 04/18/23 00:52:18.622
  Apr 18 00:52:18.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-5843" for this suite. @ 04/18/23 00:52:18.648
• [4.168 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 04/18/23 00:52:18.656
  Apr 18 00:52:18.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename statefulset @ 04/18/23 00:52:18.657
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:52:18.678
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:52:18.687
  STEP: Creating service test in namespace statefulset-4326 @ 04/18/23 00:52:18.693
  STEP: Looking for a node to schedule stateful set and pod @ 04/18/23 00:52:18.699
  STEP: Creating pod with conflicting port in namespace statefulset-4326 @ 04/18/23 00:52:18.721
  STEP: Waiting until pod test-pod will start running in namespace statefulset-4326 @ 04/18/23 00:52:18.73
  E0418 00:52:18.940618      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:19.940955      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating statefulset with conflicting port in namespace statefulset-4326 @ 04/18/23 00:52:20.738
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4326 @ 04/18/23 00:52:20.744
  Apr 18 00:52:20.758: INFO: Observed stateful pod in namespace: statefulset-4326, name: ss-0, uid: 020db295-6d1a-4b77-8e89-b086d88518fa, status phase: Pending. Waiting for statefulset controller to delete.
  Apr 18 00:52:20.781: INFO: Observed stateful pod in namespace: statefulset-4326, name: ss-0, uid: 020db295-6d1a-4b77-8e89-b086d88518fa, status phase: Failed. Waiting for statefulset controller to delete.
  Apr 18 00:52:20.789: INFO: Observed stateful pod in namespace: statefulset-4326, name: ss-0, uid: 020db295-6d1a-4b77-8e89-b086d88518fa, status phase: Failed. Waiting for statefulset controller to delete.
  Apr 18 00:52:20.793: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4326
  STEP: Removing pod with conflicting port in namespace statefulset-4326 @ 04/18/23 00:52:20.793
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4326 and will be in running state @ 04/18/23 00:52:20.811
  E0418 00:52:20.941825      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:21.944142      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 00:52:22.826: INFO: Deleting all statefulset in ns statefulset-4326
  Apr 18 00:52:22.831: INFO: Scaling statefulset ss to 0
  E0418 00:52:22.944398      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:23.944513      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:24.944690      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:25.945173      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:26.945393      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:27.945510      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:28.945770      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:29.945867      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:30.946025      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:31.946253      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 00:52:32.855: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 18 00:52:32.858: INFO: Deleting statefulset ss
  Apr 18 00:52:32.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4326" for this suite. @ 04/18/23 00:52:32.874
• [14.224 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 04/18/23 00:52:32.887
  Apr 18 00:52:32.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename emptydir @ 04/18/23 00:52:32.888
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:52:32.913
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:52:32.919
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 04/18/23 00:52:32.923
  E0418 00:52:32.946573      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:33.947870      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:34.948112      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:35.949196      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 00:52:36.947
  E0418 00:52:36.949494      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 00:52:36.953: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-473aee4f-c10e-4509-a975-8da61b60eac8 container test-container: <nil>
  STEP: delete the pod @ 04/18/23 00:52:36.966
  Apr 18 00:52:37.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2424" for this suite. @ 04/18/23 00:52:37.015
• [4.135 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 04/18/23 00:52:37.024
  Apr 18 00:52:37.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename configmap @ 04/18/23 00:52:37.025
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:52:37.065
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:52:37.086
  STEP: Creating configMap with name configmap-test-volume-map-3529e4ab-9b4b-45d7-8839-4d1b0624cc73 @ 04/18/23 00:52:37.104
  STEP: Creating a pod to test consume configMaps @ 04/18/23 00:52:37.115
  E0418 00:52:37.949888      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:38.950030      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:39.951037      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:40.951069      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 00:52:41.176
  Apr 18 00:52:41.180: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-configmaps-451d9d9c-f41c-4531-a603-9a4284819a69 container agnhost-container: <nil>
  STEP: delete the pod @ 04/18/23 00:52:41.185
  Apr 18 00:52:41.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9474" for this suite. @ 04/18/23 00:52:41.199
• [4.181 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 04/18/23 00:52:41.209
  Apr 18 00:52:41.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename containers @ 04/18/23 00:52:41.21
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:52:41.226
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:52:41.234
  STEP: Creating a pod to test override all @ 04/18/23 00:52:41.239
  E0418 00:52:41.951178      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:42.951272      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:43.951372      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:44.951500      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 00:52:45.258
  Apr 18 00:52:45.261: INFO: Trying to get logs from node ip-10-0-14-154 pod client-containers-b6726299-c99e-4e3a-9428-58c0d4fb1156 container agnhost-container: <nil>
  STEP: delete the pod @ 04/18/23 00:52:45.276
  Apr 18 00:52:45.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-5551" for this suite. @ 04/18/23 00:52:45.317
• [4.114 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 04/18/23 00:52:45.325
  Apr 18 00:52:45.325: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename container-probe @ 04/18/23 00:52:45.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:52:45.348
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:52:45.353
  STEP: Creating pod busybox-3148e6da-f9f4-45fa-a945-8d05b5e9da9c in namespace container-probe-3079 @ 04/18/23 00:52:45.357
  E0418 00:52:45.951692      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:46.952047      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 00:52:47.381: INFO: Started pod busybox-3148e6da-f9f4-45fa-a945-8d05b5e9da9c in namespace container-probe-3079
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/18/23 00:52:47.381
  Apr 18 00:52:47.384: INFO: Initial restart count of pod busybox-3148e6da-f9f4-45fa-a945-8d05b5e9da9c is 0
  E0418 00:52:47.952174      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:48.953564      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:49.953718      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:50.953878      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:51.954776      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:52.954898      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:53.955014      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:54.955735      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:55.955958      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:56.956081      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:57.956231      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:58.956594      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:52:59.956973      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:00.957879      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:01.958810      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:02.958897      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:03.959710      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:04.959971      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:05.961034      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:06.961848      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:07.961924      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:08.962058      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:09.962181      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:10.962491      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:11.963423      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:12.963508      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:13.964221      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:14.964331      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:15.965284      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:16.965406      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:17.965507      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:18.965953      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:19.966868      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:20.967975      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:21.968058      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:22.968234      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:23.968375      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:24.968378      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:25.968561      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:26.968960      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:27.970006      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:28.972216      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:29.973196      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:30.973490      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:31.973604      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:32.973700      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:33.974733      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:34.974902      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:35.975798      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:36.976509      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 00:53:37.487: INFO: Restart count of pod container-probe-3079/busybox-3148e6da-f9f4-45fa-a945-8d05b5e9da9c is now 1 (50.1034139s elapsed)
  Apr 18 00:53:37.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/18/23 00:53:37.495
  STEP: Destroying namespace "container-probe-3079" for this suite. @ 04/18/23 00:53:37.508
• [52.201 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 04/18/23 00:53:37.527
  Apr 18 00:53:37.527: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename emptydir @ 04/18/23 00:53:37.528
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:53:37.548
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:53:37.553
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 04/18/23 00:53:37.56
  E0418 00:53:37.976169      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:38.976272      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:39.976372      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:40.976489      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 00:53:41.592
  Apr 18 00:53:41.595: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-c1645f3a-1a08-4263-b2ec-62c0fac549a2 container test-container: <nil>
  STEP: delete the pod @ 04/18/23 00:53:41.601
  Apr 18 00:53:41.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2657" for this suite. @ 04/18/23 00:53:41.618
• [4.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 04/18/23 00:53:41.627
  Apr 18 00:53:41.627: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename job @ 04/18/23 00:53:41.628
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:53:41.651
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:53:41.657
  STEP: Creating a job @ 04/18/23 00:53:41.662
  STEP: Ensure pods equal to parallelism count is attached to the job @ 04/18/23 00:53:41.67
  E0418 00:53:41.976643      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:42.976959      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 04/18/23 00:53:43.673
  STEP: updating /status @ 04/18/23 00:53:43.681
  STEP: get /status @ 04/18/23 00:53:43.706
  Apr 18 00:53:43.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-4214" for this suite. @ 04/18/23 00:53:43.712
• [2.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 04/18/23 00:53:43.72
  Apr 18 00:53:43.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename taint-single-pod @ 04/18/23 00:53:43.721
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:53:43.737
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:53:43.742
  Apr 18 00:53:43.746: INFO: Waiting up to 1m0s for all nodes to be ready
  E0418 00:53:43.977720      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:44.978528      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:45.978644      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:46.978900      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:47.979382      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:48.979445      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:49.980365      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:50.980638      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:51.980905      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:52.981117      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:53.981832      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:54.981905      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:55.982813      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:56.982934      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:57.984433      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:58.984601      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:53:59.985451      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:00.987131      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:01.988201      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:02.988312      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:03.988416      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:04.988516      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:05.988959      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:06.989106      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:07.989177      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:08.989303      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:09.989848      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:10.989876      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:11.990906      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:12.991005      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:13.992071      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:14.992273      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:15.993332      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:16.993382      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:17.993502      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:18.993902      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:19.994871      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:20.994970      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:21.995052      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:22.995238      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:23.995884      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:24.996008      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:25.997056      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:26.997197      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:27.997360      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:28.998770      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:29.999384      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:31.000064      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:32.000096      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:33.000976      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:34.001980      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:35.002093      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:36.002214      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:37.002342      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:38.003116      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:39.003571      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:40.003729      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:41.003948      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:42.004555      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:43.004794      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 00:54:43.769: INFO: Waiting for terminating namespaces to be deleted...
  Apr 18 00:54:43.777: INFO: Starting informer...
  STEP: Starting pod... @ 04/18/23 00:54:43.777
  Apr 18 00:54:43.994: INFO: Pod is running on ip-10-0-14-154. Tainting Node
  STEP: Trying to apply a taint on the Node @ 04/18/23 00:54:43.994
  E0418 00:54:44.005003      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/18/23 00:54:44.007
  STEP: Waiting short time to make sure Pod is queued for deletion @ 04/18/23 00:54:44.015
  Apr 18 00:54:44.016: INFO: Pod wasn't evicted. Proceeding
  Apr 18 00:54:44.016: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/18/23 00:54:44.046
  STEP: Waiting some time to make sure that toleration time passed. @ 04/18/23 00:54:44.109
  E0418 00:54:45.005154      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:46.007898      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:47.007899      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:48.008053      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:49.008184      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:50.008276      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:51.009197      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:52.009413      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:53.009529      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:54.009635      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:55.010340      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:56.010930      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:57.011125      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:58.011252      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:54:59.011421      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:00.011557      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:01.011663      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:02.011973      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:03.012086      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:04.012320      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:05.012433      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:06.012776      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:07.012979      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:08.013119      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:09.013522      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:10.013938      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:11.014068      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:12.014274      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:13.014494      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:14.014671      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:15.014841      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:16.015418      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:17.015563      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:18.015794      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:19.018884      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:20.018926      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:21.019960      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:22.020079      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:23.020201      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:24.020296      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:25.020957      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:26.021067      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:27.021180      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:28.021963      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:29.022214      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:30.022190      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:31.022304      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:32.022439      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:33.022649      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:34.023098      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:35.024109      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:36.024478      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:37.025238      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:38.025379      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:39.031668      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:40.031791      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:41.032825      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:42.033137      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:43.033267      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:44.034090      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:45.034354      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:46.035025      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:47.035162      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:48.035262      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:49.035381      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:50.035571      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:51.036175      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:52.036478      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:53.036580      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:54.036726      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:55.036952      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:56.037132      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:57.037375      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:58.037619      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:55:59.037712      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 00:55:59.109: INFO: Pod wasn't evicted. Test successful
  Apr 18 00:55:59.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-3435" for this suite. @ 04/18/23 00:55:59.113
• [135.398 seconds]
------------------------------
S
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 04/18/23 00:55:59.119
  Apr 18 00:55:59.119: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename cronjob @ 04/18/23 00:55:59.12
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 00:55:59.139
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 00:55:59.143
  STEP: Creating a ForbidConcurrent cronjob @ 04/18/23 00:55:59.147
  STEP: Ensuring a job is scheduled @ 04/18/23 00:55:59.152
  E0418 00:56:00.038179      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:01.038264      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 04/18/23 00:56:01.156
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 04/18/23 00:56:01.159
  STEP: Ensuring no more jobs are scheduled @ 04/18/23 00:56:01.162
  E0418 00:56:02.039129      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:03.039501      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:04.039632      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:05.039975      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:06.040518      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:07.040614      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:08.040739      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:09.041222      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:10.042129      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:11.042528      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:12.043073      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:13.043103      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:14.043286      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:15.043409      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:16.043703      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:17.043985      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:18.044099      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:19.076385      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:20.076600      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:21.076921      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:22.077386      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:23.077551      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:24.077790      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:25.078078      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:26.078125      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:27.078329      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:28.078589      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:29.078624      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:30.078960      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:31.079950      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:32.080079      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:33.080952      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:34.081080      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:35.081385      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:36.081555      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:37.082281      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:38.082459      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:39.082707      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:40.082927      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:41.083946      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:42.084814      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:43.085010      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:44.085165      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:45.085475      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:46.085579      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:47.085792      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:48.085869      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:49.086045      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:50.086158      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:51.086313      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:52.086427      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:53.086630      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:54.086649      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:55.086732      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:56.086940      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:57.087152      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:58.087261      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:56:59.087483      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:00.087608      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:01.088006      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:02.088070      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:03.088229      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:04.088308      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:05.088472      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:06.089263      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:07.089494      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:08.089585      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:09.089679      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:10.090737      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:11.091178      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:12.091289      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:13.091519      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:14.091624      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:15.091684      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:16.092509      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:17.092591      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:18.092792      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:19.092881      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:20.093012      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:21.093572      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:22.093707      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:23.093904      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:24.094024      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:25.094148      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:26.095131      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:27.095260      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:28.095321      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:29.095434      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:30.095594      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:31.095657      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:32.095808      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:33.095838      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:34.096068      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:35.096075      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:36.097121      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:37.097238      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:38.097412      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:39.097772      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:40.097867      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:41.098560      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:42.098912      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:43.098997      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:44.099084      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:45.099174      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:46.100190      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:47.100272      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:48.100931      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:49.101169      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:50.101896      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:51.102057      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:52.102260      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:53.102862      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:54.103040      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:55.103071      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:56.103291      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:57.103403      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:58.103557      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:57:59.103643      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:00.103781      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:01.104083      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:02.104284      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:03.104470      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:04.104664      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:05.104731      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:06.104905      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:07.105221      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:08.105254      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:09.106064      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:10.106184      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:11.106265      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:12.106391      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:13.106516      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:14.106633      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:15.107047      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:16.107683      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:17.107817      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:18.107897      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:19.108010      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:20.108450      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:21.108286      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:22.109345      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:23.109446      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:24.109941      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:25.110107      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:26.110207      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:27.110326      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:28.110938      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:29.112009      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:30.112316      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:31.112575      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:32.112700      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:33.112834      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:34.112934      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:35.113133      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:36.113267      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:37.113646      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:38.114671      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:39.114798      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:40.114892      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:41.115949      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:42.116068      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:43.116178      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:44.117156      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:45.117363      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:46.117566      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:47.117666      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:48.117877      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:49.118729      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:50.118970      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:51.119954      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:52.120808      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:53.121037      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:54.121137      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:55.121345      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:56.121454      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:57.121718      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:58.121888      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:58:59.122143      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:00.122569      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:01.123271      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:02.124182      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:03.124501      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:04.124943      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:05.125176      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:06.125704      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:07.125894      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:08.126026      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:09.126208      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:10.126334      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:11.126600      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:12.126768      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:13.126879      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:14.127006      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:15.127132      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:16.128163      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:17.129034      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:18.129189      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:19.129478      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:20.130463      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:21.130899      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:22.131609      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:23.131995      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:24.132069      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:25.132285      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:26.133000      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:27.133137      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:28.133170      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:29.133273      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:30.133389      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:31.133820      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:32.133914      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:33.134219      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:34.134677      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:35.134976      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:36.135952      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:37.136041      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:38.136158      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:39.136352      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:40.136471      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:41.136862      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:42.137055      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:43.137242      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:44.137864      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:45.137974      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:46.138948      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:47.139054      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:48.139873      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:49.140133      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:50.140247      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:51.140351      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:52.140896      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:53.141003      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:54.141122      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:55.141229      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:56.142306      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:57.142568      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:58.142706      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 00:59:59.142905      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:00.143043      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:01.143173      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:02.143366      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:03.144017      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:04.144061      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:05.144190      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:06.145209      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:07.145348      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:08.145927      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:09.146115      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:10.146357      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:11.146505      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:12.146870      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:13.147028      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:14.147516      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:15.148197      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:16.148868      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:17.148985      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:18.149875      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:19.149935      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:20.150966      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:21.151041      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:22.151082      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:23.151196      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:24.151318      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:25.151431      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:26.151792      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:27.151996      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:28.153009      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:29.153197      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:30.153373      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:31.153441      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:32.153713      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:33.154101      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:34.154143      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:35.154309      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:36.154432      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:37.154691      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:38.154870      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:39.155907      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:40.156245      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:41.156365      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:42.156470      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:43.156630      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:44.156895      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:45.156913      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:46.157120      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:47.157247      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:48.157392      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:49.157596      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:50.157928      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:51.163362      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:52.163839      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:53.163949      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:54.164086      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:55.164188      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:56.164712      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:57.165017      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:58.165303      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:00:59.166189      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:00.166334      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:01.166428      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 04/18/23 01:01:01.172
  Apr 18 01:01:01.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5616" for this suite. @ 04/18/23 01:01:01.203
• [302.097 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 04/18/23 01:01:01.223
  Apr 18 01:01:01.223: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename field-validation @ 04/18/23 01:01:01.224
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:01:01.3
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:01:01.307
  STEP: apply creating a deployment @ 04/18/23 01:01:01.319
  Apr 18 01:01:01.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-231" for this suite. @ 04/18/23 01:01:01.346
• [0.142 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 04/18/23 01:01:01.386
  Apr 18 01:01:01.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename kubelet-test @ 04/18/23 01:01:01.387
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:01:01.455
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:01:01.48
  E0418 01:01:02.166582      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:03.166892      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:04.167202      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:05.167325      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:01:05.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-5153" for this suite. @ 04/18/23 01:01:05.57
• [4.193 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 04/18/23 01:01:05.582
  Apr 18 01:01:05.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename runtimeclass @ 04/18/23 01:01:05.583
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:01:05.616
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:01:05.621
  Apr 18 01:01:05.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3506" for this suite. @ 04/18/23 01:01:05.644
• [0.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 04/18/23 01:01:05.664
  Apr 18 01:01:05.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename kubectl @ 04/18/23 01:01:05.665
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:01:05.686
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:01:05.691
  STEP: create deployment with httpd image @ 04/18/23 01:01:05.694
  Apr 18 01:01:05.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-4879 create -f -'
  Apr 18 01:01:06.085: INFO: stderr: ""
  Apr 18 01:01:06.085: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 04/18/23 01:01:06.085
  Apr 18 01:01:06.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-4879 diff -f -'
  E0418 01:01:06.168243      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:01:06.463: INFO: rc: 1
  Apr 18 01:01:06.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-4879 delete -f -'
  Apr 18 01:01:06.542: INFO: stderr: ""
  Apr 18 01:01:06.542: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Apr 18 01:01:06.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4879" for this suite. @ 04/18/23 01:01:06.548
• [0.898 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 04/18/23 01:01:06.562
  Apr 18 01:01:06.562: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename kubectl @ 04/18/23 01:01:06.562
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:01:06.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:01:06.64
  STEP: creating Agnhost RC @ 04/18/23 01:01:06.648
  Apr 18 01:01:06.648: INFO: namespace kubectl-3596
  Apr 18 01:01:06.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-3596 create -f -'
  Apr 18 01:01:06.922: INFO: stderr: ""
  Apr 18 01:01:06.922: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/18/23 01:01:06.922
  E0418 01:01:07.169298      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:01:07.927: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 18 01:01:07.927: INFO: Found 0 / 1
  E0418 01:01:08.170027      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:01:08.928: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 18 01:01:08.928: INFO: Found 1 / 1
  Apr 18 01:01:08.928: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Apr 18 01:01:08.932: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 18 01:01:08.932: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 18 01:01:08.932: INFO: wait on agnhost-primary startup in kubectl-3596 
  Apr 18 01:01:08.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-3596 logs agnhost-primary-jj6lf agnhost-primary'
  Apr 18 01:01:09.039: INFO: stderr: ""
  Apr 18 01:01:09.039: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 04/18/23 01:01:09.039
  Apr 18 01:01:09.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-3596 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Apr 18 01:01:09.142: INFO: stderr: ""
  Apr 18 01:01:09.142: INFO: stdout: "service/rm2 exposed\n"
  Apr 18 01:01:09.144: INFO: Service rm2 in namespace kubectl-3596 found.
  E0418 01:01:09.170010      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:10.170980      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: exposing service @ 04/18/23 01:01:11.151
  Apr 18 01:01:11.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-3596 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  E0418 01:01:11.171501      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:01:11.241: INFO: stderr: ""
  Apr 18 01:01:11.241: INFO: stdout: "service/rm3 exposed\n"
  Apr 18 01:01:11.252: INFO: Service rm3 in namespace kubectl-3596 found.
  E0418 01:01:12.171581      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:13.171929      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:01:13.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3596" for this suite. @ 04/18/23 01:01:13.267
• [6.711 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 04/18/23 01:01:13.274
  Apr 18 01:01:13.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename webhook @ 04/18/23 01:01:13.274
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:01:13.296
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:01:13.3
  STEP: Setting up server cert @ 04/18/23 01:01:13.337
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/18/23 01:01:14.017
  STEP: Deploying the webhook pod @ 04/18/23 01:01:14.024
  STEP: Wait for the deployment to be ready @ 04/18/23 01:01:14.04
  Apr 18 01:01:14.053: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0418 01:01:14.172331      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:15.172550      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/18/23 01:01:16.073
  STEP: Verifying the service has paired with the endpoint @ 04/18/23 01:01:16.087
  E0418 01:01:16.173546      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:01:17.089: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 18 01:01:17.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  E0418 01:01:17.174230      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2932-crds.webhook.example.com via the AdmissionRegistration API @ 04/18/23 01:01:17.613
  STEP: Creating a custom resource that should be mutated by the webhook @ 04/18/23 01:01:17.629
  E0418 01:01:18.175200      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:19.175464      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:01:19.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0418 01:01:20.175675      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-2702" for this suite. @ 04/18/23 01:01:20.235
  STEP: Destroying namespace "webhook-markers-5124" for this suite. @ 04/18/23 01:01:20.241
• [6.976 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 04/18/23 01:01:20.251
  Apr 18 01:01:20.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename sched-preemption @ 04/18/23 01:01:20.253
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:01:20.271
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:01:20.274
  Apr 18 01:01:20.291: INFO: Waiting up to 1m0s for all nodes to be ready
  E0418 01:01:21.175705      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:22.177006      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:23.177104      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:24.177227      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:25.177564      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:26.178316      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:27.179326      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:28.179447      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:29.180085      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:30.180193      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:31.180326      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:32.180951      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:33.182149      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:34.182336      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:35.182430      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:36.183332      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:37.183463      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:38.183686      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:39.183894      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:40.184020      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:41.184131      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:42.184265      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:43.184370      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:44.184474      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:45.184652      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:46.185193      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:47.185321      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:48.185556      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:49.186356      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:50.185992      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:51.186123      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:52.186382      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:53.186466      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:54.186921      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:55.187020      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:56.187527      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:57.187861      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:58.188969      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:01:59.189275      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:00.190070      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:01.190127      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:02.190907      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:03.191061      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:04.191276      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:05.191434      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:06.191526      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:07.191652      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:08.191924      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:09.192011      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:10.192532      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:11.192833      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:12.192832      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:13.193884      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:14.194256      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:15.194385      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:16.194694      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:17.194833      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:18.195155      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:19.195273      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:20.195557      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:02:20.313: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 04/18/23 01:02:20.315
  Apr 18 01:02:20.338: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Apr 18 01:02:20.355: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Apr 18 01:02:20.412: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Apr 18 01:02:20.428: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 04/18/23 01:02:20.428
  E0418 01:02:21.196041      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:22.196063      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 04/18/23 01:02:22.487
  E0418 01:02:23.197732      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:24.197127      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:25.197239      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:26.197966      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:27.198092      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:28.198217      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:02:28.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-6298" for this suite. @ 04/18/23 01:02:28.599
• [68.354 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 04/18/23 01:02:28.606
  Apr 18 01:02:28.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename daemonsets @ 04/18/23 01:02:28.607
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:02:28.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:02:28.637
  Apr 18 01:02:28.666: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 04/18/23 01:02:28.672
  Apr 18 01:02:28.676: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 01:02:28.676: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 04/18/23 01:02:28.676
  Apr 18 01:02:28.713: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 01:02:28.713: INFO: Node ip-10-0-14-154 is running 0 daemon pod, expected 1
  E0418 01:02:29.198242      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:02:29.717: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 18 01:02:29.717: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 04/18/23 01:02:29.719
  Apr 18 01:02:29.736: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 18 01:02:29.736: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  E0418 01:02:30.198625      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:02:30.739: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 01:02:30.739: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 04/18/23 01:02:30.739
  Apr 18 01:02:30.750: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 01:02:30.750: INFO: Node ip-10-0-14-154 is running 0 daemon pod, expected 1
  E0418 01:02:31.199433      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:02:31.754: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 01:02:31.754: INFO: Node ip-10-0-14-154 is running 0 daemon pod, expected 1
  E0418 01:02:32.199966      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:02:32.761: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 01:02:32.761: INFO: Node ip-10-0-14-154 is running 0 daemon pod, expected 1
  E0418 01:02:33.200519      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:02:33.754: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 01:02:33.754: INFO: Node ip-10-0-14-154 is running 0 daemon pod, expected 1
  E0418 01:02:34.201110      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:02:34.753: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 18 01:02:34.753: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/18/23 01:02:34.759
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7806, will wait for the garbage collector to delete the pods @ 04/18/23 01:02:34.759
  Apr 18 01:02:34.820: INFO: Deleting DaemonSet.extensions daemon-set took: 6.14784ms
  Apr 18 01:02:34.921: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.339445ms
  E0418 01:02:35.201943      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:36.202850      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:37.202924      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:02:37.826: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 01:02:37.826: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 18 01:02:37.832: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"59725"},"items":null}

  Apr 18 01:02:37.849: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"59725"},"items":null}

  Apr 18 01:02:37.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7806" for this suite. @ 04/18/23 01:02:37.947
• [9.359 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 04/18/23 01:02:37.97
  Apr 18 01:02:37.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename disruption @ 04/18/23 01:02:37.971
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:02:38.02
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:02:38.026
  STEP: Waiting for the pdb to be processed @ 04/18/23 01:02:38.074
  E0418 01:02:38.203673      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 04/18/23 01:02:38.213
  Apr 18 01:02:38.275: INFO: running pods: 0 < 3
  E0418 01:02:39.204546      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:40.204836      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:02:40.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-1006" for this suite. @ 04/18/23 01:02:40.286
• [2.322 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 04/18/23 01:02:40.293
  Apr 18 01:02:40.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename services @ 04/18/23 01:02:40.294
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:02:40.317
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:02:40.326
  STEP: creating service nodeport-test with type=NodePort in namespace services-7189 @ 04/18/23 01:02:40.332
  STEP: creating replication controller nodeport-test in namespace services-7189 @ 04/18/23 01:02:40.358
  I0418 01:02:40.387266      21 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-7189, replica count: 2
  E0418 01:02:41.204909      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:42.205613      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:43.205825      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0418 01:02:43.438114      21 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 18 01:02:43.438: INFO: Creating new exec pod
  E0418 01:02:44.205917      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:45.206125      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:46.206261      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:02:46.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-7189 exec execpod2f5c9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Apr 18 01:02:46.640: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Apr 18 01:02:46.640: INFO: stdout: ""
  E0418 01:02:47.207209      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:02:47.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-7189 exec execpod2f5c9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Apr 18 01:02:47.810: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Apr 18 01:02:47.810: INFO: stdout: ""
  E0418 01:02:48.207957      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:02:48.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-7189 exec execpod2f5c9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Apr 18 01:02:48.864: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Apr 18 01:02:48.864: INFO: stdout: ""
  E0418 01:02:49.208578      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:02:49.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-7189 exec execpod2f5c9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Apr 18 01:02:49.785: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Apr 18 01:02:49.785: INFO: stdout: "nodeport-test-j6t4r"
  Apr 18 01:02:49.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-7189 exec execpod2f5c9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.3.215.191 80'
  Apr 18 01:02:49.940: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.3.215.191 80\nConnection to 10.3.215.191 80 port [tcp/http] succeeded!\n"
  Apr 18 01:02:49.940: INFO: stdout: "nodeport-test-4md2r"
  Apr 18 01:02:49.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-7189 exec execpod2f5c9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.14.154 30987'
  Apr 18 01:02:50.100: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.14.154 30987\nConnection to 10.0.14.154 30987 port [tcp/*] succeeded!\n"
  Apr 18 01:02:50.100: INFO: stdout: ""
  E0418 01:02:50.209489      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:02:51.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-7189 exec execpod2f5c9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.14.154 30987'
  E0418 01:02:51.209823      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:02:51.250: INFO: stderr: "+ + echonc -v -t hostName -w\n 2 10.0.14.154 30987\nConnection to 10.0.14.154 30987 port [tcp/*] succeeded!\n"
  Apr 18 01:02:51.250: INFO: stdout: "nodeport-test-j6t4r"
  Apr 18 01:02:51.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-7189 exec execpod2f5c9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.27.81 30987'
  Apr 18 01:02:51.408: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.27.81 30987\nConnection to 10.0.27.81 30987 port [tcp/*] succeeded!\n"
  Apr 18 01:02:51.408: INFO: stdout: "nodeport-test-4md2r"
  Apr 18 01:02:51.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7189" for this suite. @ 04/18/23 01:02:51.414
• [11.128 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 04/18/23 01:02:51.423
  Apr 18 01:02:51.423: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename security-context-test @ 04/18/23 01:02:51.424
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:02:51.443
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:02:51.447
  E0418 01:02:52.209914      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:53.210010      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:54.210132      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:55.210230      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:02:55.485: INFO: Got logs for pod "busybox-privileged-false-466e871e-a523-4066-a910-d0c01120380b": "ip: RTNETLINK answers: Operation not permitted\n"
  Apr 18 01:02:55.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-8959" for this suite. @ 04/18/23 01:02:55.489
• [4.073 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 04/18/23 01:02:55.497
  Apr 18 01:02:55.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename endpointslicemirroring @ 04/18/23 01:02:55.498
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:02:55.518
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:02:55.523
  STEP: mirroring a new custom Endpoint @ 04/18/23 01:02:55.562
  Apr 18 01:02:55.588: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  E0418 01:02:56.210398      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:57.210471      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring an update to a custom Endpoint @ 04/18/23 01:02:57.593
  Apr 18 01:02:57.604: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  E0418 01:02:58.210968      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:02:59.211171      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring deletion of a custom Endpoint @ 04/18/23 01:02:59.608
  Apr 18 01:02:59.627: INFO: Waiting for 0 EndpointSlices to exist, got 1
  E0418 01:03:00.211966      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:01.212439      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:03:01.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-9322" for this suite. @ 04/18/23 01:03:01.645
• [6.173 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 04/18/23 01:03:01.671
  Apr 18 01:03:01.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename downward-api @ 04/18/23 01:03:01.672
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:03:01.734
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:03:01.744
  STEP: Creating a pod to test downward API volume plugin @ 04/18/23 01:03:01.748
  E0418 01:03:02.212971      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:03.213064      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:04.214078      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:05.214289      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:03:05.789
  Apr 18 01:03:05.797: INFO: Trying to get logs from node ip-10-0-14-154 pod downwardapi-volume-cfaccc2c-3c74-4d77-acd7-6280b2c461d8 container client-container: <nil>
  STEP: delete the pod @ 04/18/23 01:03:05.806
  Apr 18 01:03:05.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-944" for this suite. @ 04/18/23 01:03:05.827
• [4.166 seconds]
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 04/18/23 01:03:05.838
  Apr 18 01:03:05.838: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename container-runtime @ 04/18/23 01:03:05.839
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:03:05.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:03:05.869
  STEP: create the container @ 04/18/23 01:03:05.874
  W0418 01:03:05.887439      21 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/18/23 01:03:05.887
  E0418 01:03:06.215145      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:07.215859      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:08.216289      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:09.217086      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 04/18/23 01:03:09.91
  STEP: the container should be terminated @ 04/18/23 01:03:09.913
  STEP: the termination message should be set @ 04/18/23 01:03:09.913
  Apr 18 01:03:09.913: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 04/18/23 01:03:09.913
  Apr 18 01:03:09.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-4184" for this suite. @ 04/18/23 01:03:09.926
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 04/18/23 01:03:09.939
  Apr 18 01:03:09.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename podtemplate @ 04/18/23 01:03:09.939
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:03:09.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:03:09.959
  STEP: Create a pod template @ 04/18/23 01:03:09.963
  STEP: Replace a pod template @ 04/18/23 01:03:09.969
  Apr 18 01:03:09.976: INFO: Found updated podtemplate annotation: "true"

  Apr 18 01:03:09.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-4903" for this suite. @ 04/18/23 01:03:09.982
• [0.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 04/18/23 01:03:09.994
  Apr 18 01:03:09.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/18/23 01:03:09.995
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:03:10.016
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:03:10.021
  Apr 18 01:03:10.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  E0418 01:03:10.218298      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:11.229441      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/18/23 01:03:11.421
  Apr 18 01:03:11.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-1425 --namespace=crd-publish-openapi-1425 create -f -'
  Apr 18 01:03:12.133: INFO: stderr: ""
  Apr 18 01:03:12.133: INFO: stdout: "e2e-test-crd-publish-openapi-1830-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Apr 18 01:03:12.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-1425 --namespace=crd-publish-openapi-1425 delete e2e-test-crd-publish-openapi-1830-crds test-cr'
  Apr 18 01:03:12.220: INFO: stderr: ""
  Apr 18 01:03:12.220: INFO: stdout: "e2e-test-crd-publish-openapi-1830-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Apr 18 01:03:12.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-1425 --namespace=crd-publish-openapi-1425 apply -f -'
  E0418 01:03:12.226635      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:03:12.482: INFO: stderr: ""
  Apr 18 01:03:12.482: INFO: stdout: "e2e-test-crd-publish-openapi-1830-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Apr 18 01:03:12.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-1425 --namespace=crd-publish-openapi-1425 delete e2e-test-crd-publish-openapi-1830-crds test-cr'
  Apr 18 01:03:12.551: INFO: stderr: ""
  Apr 18 01:03:12.551: INFO: stdout: "e2e-test-crd-publish-openapi-1830-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 04/18/23 01:03:12.551
  Apr 18 01:03:12.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-1425 explain e2e-test-crd-publish-openapi-1830-crds'
  Apr 18 01:03:12.748: INFO: stderr: ""
  Apr 18 01:03:12.748: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-1830-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0418 01:03:13.226742      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:14.227675      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:03:14.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1425" for this suite. @ 04/18/23 01:03:14.707
• [4.733 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 04/18/23 01:03:14.726
  Apr 18 01:03:14.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 01:03:14.727
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:03:14.804
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:03:14.811
  STEP: Creating configMap with name projected-configmap-test-volume-120085a7-9d07-4afe-bf9a-94c45a40dbb7 @ 04/18/23 01:03:14.815
  STEP: Creating a pod to test consume configMaps @ 04/18/23 01:03:14.825
  E0418 01:03:15.228462      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:16.232664      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:17.233849      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:18.234217      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:03:18.872
  Apr 18 01:03:18.876: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-projected-configmaps-fa642a10-0c51-4265-8ca5-4d6998748138 container agnhost-container: <nil>
  STEP: delete the pod @ 04/18/23 01:03:18.883
  Apr 18 01:03:18.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-448" for this suite. @ 04/18/23 01:03:18.902
• [4.183 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 04/18/23 01:03:18.912
  Apr 18 01:03:18.912: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 01:03:18.913
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:03:18.94
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:03:18.945
  STEP: Creating projection with secret that has name projected-secret-test-map-4160a13e-3891-4160-a617-0b660fbaaf9e @ 04/18/23 01:03:18.95
  STEP: Creating a pod to test consume secrets @ 04/18/23 01:03:18.955
  E0418 01:03:19.234955      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:20.235176      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:21.235969      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:22.236008      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:03:22.983
  Apr 18 01:03:22.986: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-projected-secrets-3a0a22fe-6edd-45a4-87b3-d029fbb74cbc container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/18/23 01:03:22.991
  Apr 18 01:03:23.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2687" for this suite. @ 04/18/23 01:03:23.01
• [4.110 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 04/18/23 01:03:23.023
  Apr 18 01:03:23.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename services @ 04/18/23 01:03:23.025
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:03:23.047
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:03:23.052
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-6066 @ 04/18/23 01:03:23.056
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 04/18/23 01:03:23.073
  STEP: creating service externalsvc in namespace services-6066 @ 04/18/23 01:03:23.074
  STEP: creating replication controller externalsvc in namespace services-6066 @ 04/18/23 01:03:23.098
  I0418 01:03:23.109860      21 runners.go:194] Created replication controller with name: externalsvc, namespace: services-6066, replica count: 2
  E0418 01:03:23.236877      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:24.236909      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:25.237128      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0418 01:03:26.167123      21 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 04/18/23 01:03:26.17
  Apr 18 01:03:26.186: INFO: Creating new exec pod
  E0418 01:03:26.238048      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:27.238087      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:03:28.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-6066 exec execpodhk87f -- /bin/sh -x -c nslookup nodeport-service.services-6066.svc.cluster.local'
  E0418 01:03:28.239091      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:03:28.441: INFO: stderr: "+ nslookup nodeport-service.services-6066.svc.cluster.local\n"
  Apr 18 01:03:28.441: INFO: stdout: "Server:\t\t10.3.0.10\nAddress:\t10.3.0.10#53\n\nnodeport-service.services-6066.svc.cluster.local\tcanonical name = externalsvc.services-6066.svc.cluster.local.\nName:\texternalsvc.services-6066.svc.cluster.local\nAddress: 10.3.195.203\n\n"
  Apr 18 01:03:28.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-6066, will wait for the garbage collector to delete the pods @ 04/18/23 01:03:28.447
  Apr 18 01:03:28.509: INFO: Deleting ReplicationController externalsvc took: 6.627063ms
  Apr 18 01:03:28.610: INFO: Terminating ReplicationController externalsvc pods took: 100.847615ms
  E0418 01:03:29.239702      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:30.239936      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:03:30.931: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-6066" for this suite. @ 04/18/23 01:03:30.951
• [7.936 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 04/18/23 01:03:30.964
  Apr 18 01:03:30.964: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename runtimeclass @ 04/18/23 01:03:30.965
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:03:30.997
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:03:31.005
  E0418 01:03:31.240840      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:32.241300      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:03:33.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-1880" for this suite. @ 04/18/23 01:03:33.048
• [2.090 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 04/18/23 01:03:33.054
  Apr 18 01:03:33.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename emptydir-wrapper @ 04/18/23 01:03:33.055
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:03:33.073
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:03:33.078
  E0418 01:03:33.241800      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:34.241963      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:03:35.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 04/18/23 01:03:35.115
  STEP: Cleaning up the configmap @ 04/18/23 01:03:35.121
  STEP: Cleaning up the pod @ 04/18/23 01:03:35.125
  STEP: Destroying namespace "emptydir-wrapper-1353" for this suite. @ 04/18/23 01:03:35.134
• [2.095 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 04/18/23 01:03:35.15
  Apr 18 01:03:35.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename kubectl @ 04/18/23 01:03:35.15
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:03:35.166
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:03:35.172
  STEP: creating the pod @ 04/18/23 01:03:35.175
  Apr 18 01:03:35.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-9991 create -f -'
  E0418 01:03:35.243327      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:03:35.935: INFO: stderr: ""
  Apr 18 01:03:35.935: INFO: stdout: "pod/pause created\n"
  E0418 01:03:36.243364      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:37.243474      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 04/18/23 01:03:37.945
  Apr 18 01:03:37.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-9991 label pods pause testing-label=testing-label-value'
  Apr 18 01:03:38.035: INFO: stderr: ""
  Apr 18 01:03:38.035: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 04/18/23 01:03:38.035
  Apr 18 01:03:38.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-9991 get pod pause -L testing-label'
  Apr 18 01:03:38.110: INFO: stderr: ""
  Apr 18 01:03:38.110: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 04/18/23 01:03:38.11
  Apr 18 01:03:38.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-9991 label pods pause testing-label-'
  Apr 18 01:03:38.216: INFO: stderr: ""
  Apr 18 01:03:38.216: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 04/18/23 01:03:38.216
  Apr 18 01:03:38.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-9991 get pod pause -L testing-label'
  E0418 01:03:38.244047      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:03:38.297: INFO: stderr: ""
  Apr 18 01:03:38.297: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
  STEP: using delete to clean up resources @ 04/18/23 01:03:38.298
  Apr 18 01:03:38.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-9991 delete --grace-period=0 --force -f -'
  Apr 18 01:03:38.392: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 18 01:03:38.392: INFO: stdout: "pod \"pause\" force deleted\n"
  Apr 18 01:03:38.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-9991 get rc,svc -l name=pause --no-headers'
  Apr 18 01:03:38.488: INFO: stderr: "No resources found in kubectl-9991 namespace.\n"
  Apr 18 01:03:38.488: INFO: stdout: ""
  Apr 18 01:03:38.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-9991 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 18 01:03:38.576: INFO: stderr: ""
  Apr 18 01:03:38.576: INFO: stdout: ""
  Apr 18 01:03:38.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9991" for this suite. @ 04/18/23 01:03:38.581
• [3.438 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 04/18/23 01:03:38.588
  Apr 18 01:03:38.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename replicaset @ 04/18/23 01:03:38.589
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:03:38.606
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:03:38.61
  STEP: Create a Replicaset @ 04/18/23 01:03:38.616
  STEP: Verify that the required pods have come up. @ 04/18/23 01:03:38.622
  Apr 18 01:03:38.625: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0418 01:03:39.244199      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:40.244487      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:41.244903      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:42.245967      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:43.246336      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:03:43.628: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/18/23 01:03:43.628
  STEP: Getting /status @ 04/18/23 01:03:43.628
  Apr 18 01:03:43.632: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 04/18/23 01:03:43.632
  Apr 18 01:03:43.642: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 04/18/23 01:03:43.642
  Apr 18 01:03:43.647: INFO: Observed &ReplicaSet event: ADDED
  Apr 18 01:03:43.647: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 18 01:03:43.647: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 18 01:03:43.648: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 18 01:03:43.648: INFO: Found replicaset test-rs in namespace replicaset-1408 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 18 01:03:43.648: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 04/18/23 01:03:43.648
  Apr 18 01:03:43.648: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 18 01:03:43.669: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 04/18/23 01:03:43.669
  Apr 18 01:03:43.673: INFO: Observed &ReplicaSet event: ADDED
  Apr 18 01:03:43.673: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 18 01:03:43.674: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 18 01:03:43.675: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 18 01:03:43.675: INFO: Observed replicaset test-rs in namespace replicaset-1408 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 18 01:03:43.676: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 18 01:03:43.676: INFO: Found replicaset test-rs in namespace replicaset-1408 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Apr 18 01:03:43.676: INFO: Replicaset test-rs has a patched status
  Apr 18 01:03:43.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1408" for this suite. @ 04/18/23 01:03:43.68
• [5.103 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 04/18/23 01:03:43.692
  Apr 18 01:03:43.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename secrets @ 04/18/23 01:03:43.693
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:03:43.722
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:03:43.734
  STEP: Creating secret with name secret-test-93740d3f-c55d-4c3e-8a10-249522b94b1f @ 04/18/23 01:03:43.738
  STEP: Creating a pod to test consume secrets @ 04/18/23 01:03:43.744
  E0418 01:03:44.247445      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:45.247739      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:46.247895      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:47.248054      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:03:47.775
  Apr 18 01:03:47.781: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-secrets-1d8b10fc-2232-4b8e-af43-e867a9cf3005 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/18/23 01:03:47.788
  Apr 18 01:03:47.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6435" for this suite. @ 04/18/23 01:03:47.806
• [4.120 seconds]
------------------------------
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 04/18/23 01:03:47.812
  Apr 18 01:03:47.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename init-container @ 04/18/23 01:03:47.813
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:03:47.83
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:03:47.834
  STEP: creating the pod @ 04/18/23 01:03:47.837
  Apr 18 01:03:47.837: INFO: PodSpec: initContainers in spec.initContainers
  E0418 01:03:48.248681      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:49.248876      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:50.249334      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:51.250653      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:03:52.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-3321" for this suite. @ 04/18/23 01:03:52.034
• [4.236 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 04/18/23 01:03:52.049
  Apr 18 01:03:52.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename secrets @ 04/18/23 01:03:52.049
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:03:52.075
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:03:52.079
  STEP: Creating secret with name secret-test-48825b62-179e-435f-8da0-009cad7677cf @ 04/18/23 01:03:52.082
  STEP: Creating a pod to test consume secrets @ 04/18/23 01:03:52.087
  E0418 01:03:52.251278      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:53.251993      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:54.252952      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:55.253118      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:03:56.118
  Apr 18 01:03:56.121: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-secrets-236e81c0-7e6e-4339-8c0c-a7d700e4345e container secret-volume-test: <nil>
  STEP: delete the pod @ 04/18/23 01:03:56.132
  Apr 18 01:03:56.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8854" for this suite. @ 04/18/23 01:03:56.149
• [4.108 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 04/18/23 01:03:56.158
  Apr 18 01:03:56.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename secrets @ 04/18/23 01:03:56.159
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:03:56.178
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:03:56.183
  STEP: creating a secret @ 04/18/23 01:03:56.188
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 04/18/23 01:03:56.196
  STEP: patching the secret @ 04/18/23 01:03:56.202
  STEP: deleting the secret using a LabelSelector @ 04/18/23 01:03:56.211
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 04/18/23 01:03:56.216
  Apr 18 01:03:56.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-946" for this suite. @ 04/18/23 01:03:56.229
• [0.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 04/18/23 01:03:56.235
  Apr 18 01:03:56.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename webhook @ 04/18/23 01:03:56.236
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:03:56.253
  E0418 01:03:56.254352      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:03:56.258
  STEP: Setting up server cert @ 04/18/23 01:03:56.29
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/18/23 01:03:56.616
  STEP: Deploying the webhook pod @ 04/18/23 01:03:56.622
  STEP: Wait for the deployment to be ready @ 04/18/23 01:03:56.631
  Apr 18 01:03:56.640: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0418 01:03:57.255357      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:03:58.255461      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/18/23 01:03:58.65
  STEP: Verifying the service has paired with the endpoint @ 04/18/23 01:03:58.66
  E0418 01:03:59.256559      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:03:59.660: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 04/18/23 01:03:59.663
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/18/23 01:03:59.679
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 04/18/23 01:03:59.688
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/18/23 01:03:59.697
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 04/18/23 01:03:59.706
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/18/23 01:03:59.715
  Apr 18 01:03:59.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-601" for this suite. @ 04/18/23 01:03:59.767
  STEP: Destroying namespace "webhook-markers-1347" for this suite. @ 04/18/23 01:03:59.775
• [3.548 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 04/18/23 01:03:59.786
  Apr 18 01:03:59.786: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 01:03:59.787
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:03:59.814
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:03:59.823
  STEP: Creating a pod to test downward API volume plugin @ 04/18/23 01:03:59.827
  E0418 01:04:00.257264      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:01.257642      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:02.257808      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:03.257920      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:04:03.874
  Apr 18 01:04:03.877: INFO: Trying to get logs from node ip-10-0-14-154 pod downwardapi-volume-704efb4b-4579-4c1b-ae2c-07fdc1591dd0 container client-container: <nil>
  STEP: delete the pod @ 04/18/23 01:04:03.886
  Apr 18 01:04:03.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4475" for this suite. @ 04/18/23 01:04:03.906
• [4.128 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 04/18/23 01:04:03.915
  Apr 18 01:04:03.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename field-validation @ 04/18/23 01:04:03.916
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:04:03.938
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:04:03.946
  STEP: apply creating a deployment @ 04/18/23 01:04:03.95
  Apr 18 01:04:03.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8115" for this suite. @ 04/18/23 01:04:03.967
• [0.057 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 04/18/23 01:04:03.973
  Apr 18 01:04:03.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename cronjob @ 04/18/23 01:04:03.974
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:04:03.992
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:04:03.997
  STEP: Creating a suspended cronjob @ 04/18/23 01:04:04.003
  STEP: Ensuring no jobs are scheduled @ 04/18/23 01:04:04.009
  E0418 01:04:04.258896      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:05.259741      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:06.260649      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:07.260899      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:08.261037      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:09.261238      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:10.261326      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:11.261402      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:12.261976      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:13.262039      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:14.262978      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:15.263741      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:16.264522      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:17.264579      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:18.264682      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:19.264902      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:20.264979      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:21.265342      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:22.265801      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:23.266170      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:24.267096      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:25.267530      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:26.268347      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:27.268396      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:28.269055      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:29.269122      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:30.269892      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:31.269917      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:32.270494      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:33.270604      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:34.271178      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:35.271166      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:36.271629      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:37.271860      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:38.271949      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:39.272237      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:40.272553      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:41.272810      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:42.273786      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:43.273885      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:44.274872      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:45.275512      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:46.276550      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:47.276651      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:48.277105      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:49.277857      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:50.278776      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:51.279514      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:52.279802      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:53.279972      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:54.280019      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:55.280563      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:56.281034      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:57.281183      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:58.281816      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:04:59.281927      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:00.282423      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:01.282411      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:02.282579      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:03.282881      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:04.283999      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:05.285163      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:06.285556      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:07.285672      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:08.285818      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:09.285923      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:10.286898      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:11.287023      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:12.287118      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:13.287354      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:14.288463      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:15.289430      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:16.289963      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:17.290970      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:18.291221      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:19.291408      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:20.291887      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:21.292215      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:22.293206      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:23.293341      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:24.294328      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:25.295467      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:26.295563      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:27.295736      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:28.296704      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:29.297057      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:30.297595      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:31.297712      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:32.298227      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:33.298424      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:34.299165      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:35.299849      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:36.300298      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:37.300422      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:38.301403      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:39.301507      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:40.302353      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:41.302437      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:42.303402      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:43.303530      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:44.304043      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:45.304866      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:46.305622      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:47.305873      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:48.306605      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:49.306882      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:50.307516      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:51.307593      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:52.308171      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:53.308295      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:54.308485      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:55.308959      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:56.309443      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:57.309558      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:58.309826      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:05:59.309887      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:00.310500      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:01.310970      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:02.311235      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:03.311374      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:04.312156      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:05.312644      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:06.313328      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:07.313513      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:08.314153      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:09.314259      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:10.314696      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:11.314840      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:12.315009      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:13.315366      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:14.315481      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:15.315829      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:16.316736      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:17.316993      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:18.318070      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:19.318134      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:20.318474      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:21.319569      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:22.319790      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:23.320657      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:24.321730      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:25.322348      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:26.322474      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:27.322779      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:28.323423      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:29.323643      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:30.324737      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:31.324774      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:32.325228      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:33.325417      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:34.325883      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:35.326181      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:36.326306      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:37.326510      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:38.326661      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:39.327032      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:40.327638      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:41.327742      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:42.327853      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:43.328056      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:44.328945      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:45.329842      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:46.330504      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:47.330625      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:48.331635      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:49.331774      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:50.331832      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:51.331894      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:52.332868      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:53.333144      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:54.333744      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:55.334339      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:56.334742      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:57.334876      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:58.335947      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:06:59.336924      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:00.337824      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:01.338153      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:02.339143      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:03.339948      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:04.340636      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:05.341259      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:06.342071      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:07.342186      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:08.342774      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:09.342970      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:10.344008      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:11.344154      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:12.344790      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:13.344894      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:14.345047      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:15.345329      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:16.345919      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:17.346070      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:18.346155      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:19.346288      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:20.346881      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:21.347138      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:22.347809      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:23.348453      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:24.348548      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:25.349350      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:26.350469      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:27.350649      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:28.351667      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:29.352213      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:30.352267      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:31.352476      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:32.353281      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:33.353397      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:34.354135      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:35.354676      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:36.354852      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:37.355071      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:38.356086      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:39.356359      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:40.357338      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:41.357640      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:42.357723      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:43.358885      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:44.359500      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:45.359970      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:46.362678      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:47.362823      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:48.362988      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:49.363191      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:50.364042      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:51.364573      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:52.365636      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:53.365779      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:54.366462      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:55.367033      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:56.368033      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:57.368805      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:58.369579      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:07:59.369681      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:00.370202      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:01.370992      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:02.371124      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:03.371343      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:04.371866      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:05.372427      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:06.372987      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:07.373181      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:08.373935      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:09.374056      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:10.374295      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:11.374491      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:12.375262      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:13.375383      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:14.376215      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:15.376858      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:16.376974      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:17.377305      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:18.377415      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:19.378403      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:20.378834      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:21.378885      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:22.379864      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:23.380072      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:24.380351      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:25.381368      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:26.382412      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:27.382714      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:28.382889      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:29.383110      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:30.384119      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:31.384241      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:32.384947      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:33.386021      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:34.386587      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:35.386957      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:36.387553      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:37.388047      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:38.388165      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:39.388367      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:40.388558      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:41.388678      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:42.388901      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:43.388997      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:44.389612      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:45.390024      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:46.390964      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:47.391113      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:48.391189      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:49.391401      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:50.391996      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:51.392125      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:52.392635      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:53.392786      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:54.393860      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:55.394296      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:56.395072      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:57.395304      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:58.395405      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:08:59.395531      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:00.395929      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:01.396099      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:02.396710      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:03.396874      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring no job exists by listing jobs explicitly @ 04/18/23 01:09:04.017
  STEP: Removing cronjob @ 04/18/23 01:09:04.02
  Apr 18 01:09:04.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8043" for this suite. @ 04/18/23 01:09:04.03
• [300.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 04/18/23 01:09:04.042
  Apr 18 01:09:04.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename resourcequota @ 04/18/23 01:09:04.044
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:09:04.065
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:09:04.07
  STEP: Counting existing ResourceQuota @ 04/18/23 01:09:04.074
  E0418 01:09:04.397467      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:05.398368      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:06.399328      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:07.399858      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:08.399919      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/18/23 01:09:09.081
  STEP: Ensuring resource quota status is calculated @ 04/18/23 01:09:09.089
  E0418 01:09:09.401055      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:10.401112      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 04/18/23 01:09:11.093
  STEP: Creating a NodePort Service @ 04/18/23 01:09:11.115
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 04/18/23 01:09:11.138
  STEP: Ensuring resource quota status captures service creation @ 04/18/23 01:09:11.153
  E0418 01:09:11.401267      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:12.401384      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 04/18/23 01:09:13.157
  STEP: Ensuring resource quota status released usage @ 04/18/23 01:09:13.204
  E0418 01:09:13.402320      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:14.402583      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:09:15.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3287" for this suite. @ 04/18/23 01:09:15.214
• [11.177 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 04/18/23 01:09:15.22
  Apr 18 01:09:15.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename pod-network-test @ 04/18/23 01:09:15.221
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:09:15.243
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:09:15.249
  STEP: Performing setup for networking test in namespace pod-network-test-9036 @ 04/18/23 01:09:15.255
  STEP: creating a selector @ 04/18/23 01:09:15.255
  STEP: Creating the service pods in kubernetes @ 04/18/23 01:09:15.255
  Apr 18 01:09:15.255: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0418 01:09:15.403331      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:16.403392      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:17.404220      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:18.404336      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:19.404408      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:20.405399      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:21.405467      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:22.406004      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:23.406860      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:24.406907      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:25.407675      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:26.408149      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:27.408883      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:28.409034      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:29.409587      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:30.410327      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:31.410878      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:32.410980      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:33.411217      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:34.411420      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:35.412032      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:36.412146      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 04/18/23 01:09:37.37
  E0418 01:09:37.413052      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:38.413289      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:09:39.389: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  Apr 18 01:09:39.390: INFO: Breadth first check of 10.2.212.54 on host 10.0.14.154...
  Apr 18 01:09:39.392: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.212.55:9080/dial?request=hostname&protocol=udp&host=10.2.212.54&port=8081&tries=1'] Namespace:pod-network-test-9036 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 01:09:39.392: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 01:09:39.393: INFO: ExecWithOptions: Clientset creation
  Apr 18 01:09:39.393: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-9036/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.212.55%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.2.212.54%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  E0418 01:09:39.414010      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:09:39.467: INFO: Waiting for responses: map[]
  Apr 18 01:09:39.467: INFO: reached 10.2.212.54 after 0/1 tries
  Apr 18 01:09:39.467: INFO: Breadth first check of 10.2.129.161 on host 10.0.27.81...
  Apr 18 01:09:39.471: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.212.55:9080/dial?request=hostname&protocol=udp&host=10.2.129.161&port=8081&tries=1'] Namespace:pod-network-test-9036 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 01:09:39.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 01:09:39.472: INFO: ExecWithOptions: Clientset creation
  Apr 18 01:09:39.472: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-9036/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.212.55%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.2.129.161%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 18 01:09:39.533: INFO: Waiting for responses: map[]
  Apr 18 01:09:39.533: INFO: reached 10.2.129.161 after 0/1 tries
  Apr 18 01:09:39.533: INFO: Going to retry 0 out of 2 pods....
  Apr 18 01:09:39.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-9036" for this suite. @ 04/18/23 01:09:39.537
• [24.323 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 04/18/23 01:09:39.544
  Apr 18 01:09:39.544: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename emptydir @ 04/18/23 01:09:39.545
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:09:39.566
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:09:39.571
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 04/18/23 01:09:39.575
  E0418 01:09:40.414998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:41.415587      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:42.415771      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:43.415899      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:09:43.598
  Apr 18 01:09:43.601: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-cd7739c1-4330-488c-8024-008d0e878152 container test-container: <nil>
  STEP: delete the pod @ 04/18/23 01:09:43.623
  Apr 18 01:09:43.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9831" for this suite. @ 04/18/23 01:09:43.645
• [4.106 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 04/18/23 01:09:43.651
  Apr 18 01:09:43.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename emptydir @ 04/18/23 01:09:43.652
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:09:43.67
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:09:43.673
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 04/18/23 01:09:43.677
  E0418 01:09:44.416041      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:45.417033      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:46.417972      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:47.418089      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:09:47.701
  Apr 18 01:09:47.704: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-140d278f-975a-4af8-8d7b-42ae93537cc0 container test-container: <nil>
  STEP: delete the pod @ 04/18/23 01:09:47.71
  Apr 18 01:09:47.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4907" for this suite. @ 04/18/23 01:09:47.73
• [4.089 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 04/18/23 01:09:47.741
  Apr 18 01:09:47.741: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename sched-pred @ 04/18/23 01:09:47.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:09:47.776
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:09:47.782
  Apr 18 01:09:47.786: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 18 01:09:47.794: INFO: Waiting for terminating namespaces to be deleted...
  Apr 18 01:09:47.805: INFO: 
  Logging pods the apiserver thinks is on node ip-10-0-14-154 before test
  Apr 18 01:09:47.879: INFO: calico-node-pdjm9 from kube-system started at 2023-04-17 16:53:36 +0000 UTC (1 container statuses recorded)
  Apr 18 01:09:47.879: INFO: 	Container calico-node ready: true, restart count 0
  Apr 18 01:09:47.879: INFO: kube-proxy-sf54c from kube-system started at 2023-04-17 16:53:36 +0000 UTC (1 container statuses recorded)
  Apr 18 01:09:47.879: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 18 01:09:47.879: INFO: sonobuoy-systemd-logs-daemon-set-ee374c63bb374e5f-r7gxd from sonobuoy started at 2023-04-18 00:15:14 +0000 UTC (2 container statuses recorded)
  Apr 18 01:09:47.879: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 18 01:09:47.879: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 18 01:09:47.879: INFO: 
  Logging pods the apiserver thinks is on node ip-10-0-27-81 before test
  Apr 18 01:09:47.935: INFO: calico-node-wsfsw from kube-system started at 2023-04-17 16:54:08 +0000 UTC (1 container statuses recorded)
  Apr 18 01:09:47.935: INFO: 	Container calico-node ready: true, restart count 0
  Apr 18 01:09:47.935: INFO: coredns-5cf8b9cff-c4mtg from kube-system started at 2023-04-18 00:54:44 +0000 UTC (1 container statuses recorded)
  Apr 18 01:09:47.935: INFO: 	Container coredns ready: true, restart count 0
  Apr 18 01:09:47.935: INFO: kube-proxy-srh6q from kube-system started at 2023-04-17 16:54:08 +0000 UTC (1 container statuses recorded)
  Apr 18 01:09:47.935: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 18 01:09:47.935: INFO: sonobuoy from sonobuoy started at 2023-04-18 00:15:13 +0000 UTC (1 container statuses recorded)
  Apr 18 01:09:47.935: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 18 01:09:47.935: INFO: sonobuoy-e2e-job-0fa4b43c9b09437b from sonobuoy started at 2023-04-18 00:15:14 +0000 UTC (2 container statuses recorded)
  Apr 18 01:09:47.935: INFO: 	Container e2e ready: true, restart count 0
  Apr 18 01:09:47.935: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 18 01:09:47.935: INFO: sonobuoy-systemd-logs-daemon-set-ee374c63bb374e5f-96krr from sonobuoy started at 2023-04-18 00:15:14 +0000 UTC (2 container statuses recorded)
  Apr 18 01:09:47.935: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 18 01:09:47.935: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node ip-10-0-14-154 @ 04/18/23 01:09:48.092
  STEP: verifying the node has the label node ip-10-0-27-81 @ 04/18/23 01:09:48.135
  Apr 18 01:09:48.149: INFO: Pod calico-node-pdjm9 requesting resource cpu=100m on Node ip-10-0-14-154
  Apr 18 01:09:48.149: INFO: Pod calico-node-wsfsw requesting resource cpu=100m on Node ip-10-0-27-81
  Apr 18 01:09:48.150: INFO: Pod coredns-5cf8b9cff-c4mtg requesting resource cpu=100m on Node ip-10-0-27-81
  Apr 18 01:09:48.150: INFO: Pod kube-proxy-sf54c requesting resource cpu=0m on Node ip-10-0-14-154
  Apr 18 01:09:48.150: INFO: Pod kube-proxy-srh6q requesting resource cpu=0m on Node ip-10-0-27-81
  Apr 18 01:09:48.150: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-27-81
  Apr 18 01:09:48.151: INFO: Pod sonobuoy-e2e-job-0fa4b43c9b09437b requesting resource cpu=0m on Node ip-10-0-27-81
  Apr 18 01:09:48.151: INFO: Pod sonobuoy-systemd-logs-daemon-set-ee374c63bb374e5f-96krr requesting resource cpu=0m on Node ip-10-0-27-81
  Apr 18 01:09:48.151: INFO: Pod sonobuoy-systemd-logs-daemon-set-ee374c63bb374e5f-r7gxd requesting resource cpu=0m on Node ip-10-0-14-154
  STEP: Starting Pods to consume most of the cluster CPU. @ 04/18/23 01:09:48.152
  Apr 18 01:09:48.152: INFO: Creating a pod which consumes cpu=1260m on Node ip-10-0-27-81
  Apr 18 01:09:48.161: INFO: Creating a pod which consumes cpu=1330m on Node ip-10-0-14-154
  E0418 01:09:48.419195      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:49.419327      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating another pod that requires unavailable amount of CPU. @ 04/18/23 01:09:50.2
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-ac75680b-6d30-4000-9203-0d1e53e905e1.1756e22bb36c13c9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7571/filler-pod-ac75680b-6d30-4000-9203-0d1e53e905e1 to ip-10-0-14-154] @ 04/18/23 01:09:50.204
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-ac75680b-6d30-4000-9203-0d1e53e905e1.1756e22bde2dc0ee], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/18/23 01:09:50.204
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-ac75680b-6d30-4000-9203-0d1e53e905e1.1756e22bdf5e5626], Reason = [Created], Message = [Created container filler-pod-ac75680b-6d30-4000-9203-0d1e53e905e1] @ 04/18/23 01:09:50.204
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-ac75680b-6d30-4000-9203-0d1e53e905e1.1756e22be5155e06], Reason = [Started], Message = [Started container filler-pod-ac75680b-6d30-4000-9203-0d1e53e905e1] @ 04/18/23 01:09:50.204
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-daa1c008-8054-44af-b78d-0e854c1ef0ba.1756e22bb2478559], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7571/filler-pod-daa1c008-8054-44af-b78d-0e854c1ef0ba to ip-10-0-27-81] @ 04/18/23 01:09:50.204
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-daa1c008-8054-44af-b78d-0e854c1ef0ba.1756e22be0e163be], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/18/23 01:09:50.204
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-daa1c008-8054-44af-b78d-0e854c1ef0ba.1756e22be22236e9], Reason = [Created], Message = [Created container filler-pod-daa1c008-8054-44af-b78d-0e854c1ef0ba] @ 04/18/23 01:09:50.204
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-daa1c008-8054-44af-b78d-0e854c1ef0ba.1756e22bea49589a], Reason = [Started], Message = [Started container filler-pod-daa1c008-8054-44af-b78d-0e854c1ef0ba] @ 04/18/23 01:09:50.204
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.1756e22c2bf731a8], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/controller: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod..] @ 04/18/23 01:09:50.221
  E0418 01:09:50.420396      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label node off the node ip-10-0-27-81 @ 04/18/23 01:09:51.218
  STEP: verifying the node doesn't have the label node @ 04/18/23 01:09:51.24
  STEP: removing the label node off the node ip-10-0-14-154 @ 04/18/23 01:09:51.252
  STEP: verifying the node doesn't have the label node @ 04/18/23 01:09:51.283
  Apr 18 01:09:51.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-7571" for this suite. @ 04/18/23 01:09:51.305
• [3.591 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 04/18/23 01:09:51.367
  Apr 18 01:09:51.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/18/23 01:09:51.368
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:09:51.403
  E0418 01:09:51.421180      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:09:51.421
  Apr 18 01:09:51.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  E0418 01:09:52.421857      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:53.421939      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/18/23 01:09:53.798
  Apr 18 01:09:53.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-9829 --namespace=crd-publish-openapi-9829 create -f -'
  E0418 01:09:54.422060      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:09:54.702: INFO: stderr: ""
  Apr 18 01:09:54.702: INFO: stdout: "e2e-test-crd-publish-openapi-6050-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Apr 18 01:09:54.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-9829 --namespace=crd-publish-openapi-9829 delete e2e-test-crd-publish-openapi-6050-crds test-cr'
  Apr 18 01:09:54.778: INFO: stderr: ""
  Apr 18 01:09:54.778: INFO: stdout: "e2e-test-crd-publish-openapi-6050-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Apr 18 01:09:54.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-9829 --namespace=crd-publish-openapi-9829 apply -f -'
  Apr 18 01:09:54.997: INFO: stderr: ""
  Apr 18 01:09:54.997: INFO: stdout: "e2e-test-crd-publish-openapi-6050-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Apr 18 01:09:54.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-9829 --namespace=crd-publish-openapi-9829 delete e2e-test-crd-publish-openapi-6050-crds test-cr'
  Apr 18 01:09:55.071: INFO: stderr: ""
  Apr 18 01:09:55.071: INFO: stdout: "e2e-test-crd-publish-openapi-6050-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 04/18/23 01:09:55.071
  Apr 18 01:09:55.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-9829 explain e2e-test-crd-publish-openapi-6050-crds'
  Apr 18 01:09:55.326: INFO: stderr: ""
  Apr 18 01:09:55.326: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-6050-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0418 01:09:55.422624      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:56.423597      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:09:57.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9829" for this suite. @ 04/18/23 01:09:57.26
• [5.899 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 04/18/23 01:09:57.267
  Apr 18 01:09:57.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename emptydir @ 04/18/23 01:09:57.269
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:09:57.292
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:09:57.3
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 04/18/23 01:09:57.337
  E0418 01:09:57.424941      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:58.424921      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:09:59.425117      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:00.425365      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:10:01.401
  Apr 18 01:10:01.409: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-fbd1ea54-81e4-49e9-99d3-8fa82cf96a9c container test-container: <nil>
  E0418 01:10:01.425737      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod @ 04/18/23 01:10:01.43
  Apr 18 01:10:01.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6773" for this suite. @ 04/18/23 01:10:01.475
• [4.216 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 04/18/23 01:10:01.484
  Apr 18 01:10:01.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename field-validation @ 04/18/23 01:10:01.528
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:10:01.553
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:10:01.561
  Apr 18 01:10:01.570: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  E0418 01:10:02.426742      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:03.426906      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:04.427192      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0418 01:10:04.568103      21 warnings.go:70] unknown field "alpha"
  W0418 01:10:04.568127      21 warnings.go:70] unknown field "beta"
  W0418 01:10:04.568131      21 warnings.go:70] unknown field "delta"
  W0418 01:10:04.568136      21 warnings.go:70] unknown field "epsilon"
  W0418 01:10:04.568140      21 warnings.go:70] unknown field "gamma"
  Apr 18 01:10:04.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4952" for this suite. @ 04/18/23 01:10:04.669
• [3.194 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 04/18/23 01:10:04.679
  Apr 18 01:10:04.679: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 01:10:04.68
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:10:04.704
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:10:04.711
  STEP: Creating configMap with name projected-configmap-test-volume-map-fa1aa2b8-5df0-4d26-9f43-721b74433c0a @ 04/18/23 01:10:04.716
  STEP: Creating a pod to test consume configMaps @ 04/18/23 01:10:04.721
  E0418 01:10:05.427357      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:06.427605      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:07.427722      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:08.427820      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:10:08.765
  Apr 18 01:10:08.774: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-projected-configmaps-53026f97-1940-471d-a9ee-1e288e90039f container agnhost-container: <nil>
  STEP: delete the pod @ 04/18/23 01:10:08.781
  Apr 18 01:10:08.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3027" for this suite. @ 04/18/23 01:10:08.828
• [4.173 seconds]
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 04/18/23 01:10:08.853
  Apr 18 01:10:08.853: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename kubectl @ 04/18/23 01:10:08.856
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:10:08.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:10:08.885
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/18/23 01:10:08.889
  Apr 18 01:10:08.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-6589 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Apr 18 01:10:08.976: INFO: stderr: ""
  Apr 18 01:10:08.976: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 04/18/23 01:10:08.976
  E0418 01:10:09.428627      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:10.428994      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:11.429105      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:12.429974      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:13.430955      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 04/18/23 01:10:14.026
  Apr 18 01:10:14.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-6589 get pod e2e-test-httpd-pod -o json'
  Apr 18 01:10:14.131: INFO: stderr: ""
  Apr 18 01:10:14.131: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"6659351f135aa3dcadd4984058c68a0f755232b294e19925e83805a1006b1776\",\n            \"cni.projectcalico.org/podIP\": \"10.2.212.15/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.2.212.15/32\"\n        },\n        \"creationTimestamp\": \"2023-04-18T01:10:08Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6589\",\n        \"resourceVersion\": \"61657\",\n        \"uid\": \"13341c7c-073c-4141-a557-332ac1895269\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-tz5n7\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-14-154\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-tz5n7\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-18T01:10:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-18T01:10:09Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-18T01:10:09Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-18T01:10:08Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://0a4dc06ca53955e6e633aade9070e4a19fb3ff611832aae44fe24585cf3b0f0a\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-04-18T01:10:09Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.14.154\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.2.212.15\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.2.212.15\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-04-18T01:10:08Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 04/18/23 01:10:14.131
  Apr 18 01:10:14.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-6589 replace -f -'
  E0418 01:10:14.431597      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:10:15.107: INFO: stderr: ""
  Apr 18 01:10:15.107: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 04/18/23 01:10:15.107
  Apr 18 01:10:15.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-6589 delete pods e2e-test-httpd-pod'
  E0418 01:10:15.432006      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:16.432347      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:10:16.878: INFO: stderr: ""
  Apr 18 01:10:16.878: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 18 01:10:16.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6589" for this suite. @ 04/18/23 01:10:16.885
• [8.047 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 04/18/23 01:10:16.9
  Apr 18 01:10:16.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/18/23 01:10:16.903
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:10:16.932
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:10:16.937
  Apr 18 01:10:16.943: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  E0418 01:10:17.432311      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:10:17.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-5682" for this suite. @ 04/18/23 01:10:17.502
• [0.621 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 04/18/23 01:10:17.524
  Apr 18 01:10:17.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename hostport @ 04/18/23 01:10:17.525
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:10:17.543
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:10:17.548
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 04/18/23 01:10:17.556
  E0418 01:10:18.432967      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:19.433057      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.14.154 on the node which pod1 resides and expect scheduled @ 04/18/23 01:10:19.583
  E0418 01:10:20.433281      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:21.433313      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.14.154 but use UDP protocol on the node which pod2 resides @ 04/18/23 01:10:21.602
  E0418 01:10:22.433359      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:23.433580      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:24.433697      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:25.434600      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 04/18/23 01:10:25.642
  Apr 18 01:10:25.642: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.14.154 http://127.0.0.1:54323/hostname] Namespace:hostport-1861 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 01:10:25.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 01:10:25.643: INFO: ExecWithOptions: Clientset creation
  Apr 18 01:10:25.643: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/hostport-1861/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.14.154+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.14.154, port: 54323 @ 04/18/23 01:10:25.733
  Apr 18 01:10:25.733: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.14.154:54323/hostname] Namespace:hostport-1861 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 01:10:25.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 01:10:25.733: INFO: ExecWithOptions: Clientset creation
  Apr 18 01:10:25.733: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/hostport-1861/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.14.154%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.14.154, port: 54323 UDP @ 04/18/23 01:10:25.796
  Apr 18 01:10:25.796: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.14.154 54323] Namespace:hostport-1861 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 01:10:25.796: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 01:10:25.797: INFO: ExecWithOptions: Clientset creation
  Apr 18 01:10:25.797: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/hostport-1861/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.14.154+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0418 01:10:26.435570      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:27.435936      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:28.436540      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:29.436794      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:30.437168      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:10:30.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-1861" for this suite. @ 04/18/23 01:10:30.867
• [13.351 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 04/18/23 01:10:30.876
  Apr 18 01:10:30.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename kubectl @ 04/18/23 01:10:30.877
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:10:30.92
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:10:30.929
  STEP: creating Agnhost RC @ 04/18/23 01:10:30.936
  Apr 18 01:10:30.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-4380 create -f -'
  Apr 18 01:10:31.356: INFO: stderr: ""
  Apr 18 01:10:31.356: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/18/23 01:10:31.356
  E0418 01:10:31.437557      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:10:32.383: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 18 01:10:32.383: INFO: Found 0 / 1
  E0418 01:10:32.438552      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:10:33.359: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 18 01:10:33.359: INFO: Found 1 / 1
  Apr 18 01:10:33.359: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 04/18/23 01:10:33.359
  Apr 18 01:10:33.363: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 18 01:10:33.363: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 18 01:10:33.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-4380 patch pod agnhost-primary-vnvx2 -p {"metadata":{"annotations":{"x":"y"}}}'
  E0418 01:10:33.439087      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:10:33.455: INFO: stderr: ""
  Apr 18 01:10:33.455: INFO: stdout: "pod/agnhost-primary-vnvx2 patched\n"
  STEP: checking annotations @ 04/18/23 01:10:33.455
  Apr 18 01:10:33.459: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 18 01:10:33.459: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 18 01:10:33.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4380" for this suite. @ 04/18/23 01:10:33.462
• [2.592 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 04/18/23 01:10:33.507
  Apr 18 01:10:33.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename container-probe @ 04/18/23 01:10:33.509
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:10:33.529
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:10:33.532
  STEP: Creating pod liveness-5bc8dedb-f514-4009-9ff2-fd1915949988 in namespace container-probe-4311 @ 04/18/23 01:10:33.536
  E0418 01:10:34.439892      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:35.440375      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:10:35.553: INFO: Started pod liveness-5bc8dedb-f514-4009-9ff2-fd1915949988 in namespace container-probe-4311
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/18/23 01:10:35.553
  Apr 18 01:10:35.558: INFO: Initial restart count of pod liveness-5bc8dedb-f514-4009-9ff2-fd1915949988 is 0
  E0418 01:10:36.440548      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:37.440618      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:38.440912      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:39.441089      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:40.441143      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:41.441229      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:42.441362      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:43.441627      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:44.441778      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:45.442022      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:46.442162      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:47.442302      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:48.442403      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:49.442531      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:50.443272      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:51.443368      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:52.443473      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:53.443571      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:54.443714      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:55.446794      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:10:55.608: INFO: Restart count of pod container-probe-4311/liveness-5bc8dedb-f514-4009-9ff2-fd1915949988 is now 1 (20.049559087s elapsed)
  E0418 01:10:56.445819      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:57.445923      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:58.446060      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:10:59.446176      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:00.446981      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:01.447428      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:02.447556      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:03.447945      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:04.448044      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:05.448402      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:06.448535      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:07.448772      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:08.448895      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:09.448993      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:10.449773      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:11.449985      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:12.450066      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:13.450183      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:14.450285      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:15.450931      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:11:15.662: INFO: Restart count of pod container-probe-4311/liveness-5bc8dedb-f514-4009-9ff2-fd1915949988 is now 2 (40.103517322s elapsed)
  E0418 01:11:16.451055      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:17.451143      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:18.451283      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:19.451404      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:20.452321      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:21.452496      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:22.452825      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:23.452786      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:24.452905      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:25.453536      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:26.453668      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:27.453853      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:28.454783      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:29.454995      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:30.455885      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:31.456023      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:32.456461      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:33.456541      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:34.457510      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:35.457948      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:11:35.707: INFO: Restart count of pod container-probe-4311/liveness-5bc8dedb-f514-4009-9ff2-fd1915949988 is now 3 (1m0.148931413s elapsed)
  E0418 01:11:36.458464      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:37.459081      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:38.459163      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:39.460621      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:40.461624      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:41.462045      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:42.463074      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:43.463176      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:44.463281      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:45.463649      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:46.463912      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:47.464040      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:48.464171      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:49.464460      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:50.465353      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:51.465511      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:52.465688      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:53.465929      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:54.466047      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:55.466495      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:11:55.748: INFO: Restart count of pod container-probe-4311/liveness-5bc8dedb-f514-4009-9ff2-fd1915949988 is now 4 (1m20.189328871s elapsed)
  E0418 01:11:56.466900      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:57.467035      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:58.467223      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:11:59.467450      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:00.468022      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:01.468119      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:02.468233      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:03.468353      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:04.468571      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:05.469575      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:06.470576      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:07.470589      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:08.470722      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:09.470902      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:10.471975      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:11.472186      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:12.472245      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:13.472298      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:14.472401      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:15.472385      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:16.472575      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:17.472591      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:18.472819      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:19.473733      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:20.474097      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:21.474202      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:22.474886      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:23.475036      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:24.475882      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:25.476359      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:26.477130      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:27.477376      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:28.477739      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:29.477896      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:30.478495      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:31.478894      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:32.479043      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:33.479269      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:34.480269      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:35.480946      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:36.481786      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:37.481890      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:38.482959      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:39.483142      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:40.483943      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:41.484991      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:42.485114      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:43.485332      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:44.485623      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:45.486217      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:46.487079      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:47.487325      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:48.487531      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:49.488431      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:50.489332      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:51.489415      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:52.489900      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:53.489964      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:54.490953      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:55.491005      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:56.491327      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:57.491948      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:58.492038      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:12:59.492339      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:00.495971      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:01.496490      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:02.496575      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:03.496791      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:04.496885      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:05.497055      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:06.498979      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:07.499075      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:13:07.902: INFO: Restart count of pod container-probe-4311/liveness-5bc8dedb-f514-4009-9ff2-fd1915949988 is now 5 (2m32.343743296s elapsed)
  Apr 18 01:13:07.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/18/23 01:13:07.907
  STEP: Destroying namespace "container-probe-4311" for this suite. @ 04/18/23 01:13:07.918
• [154.419 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 04/18/23 01:13:07.926
  Apr 18 01:13:07.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename replication-controller @ 04/18/23 01:13:07.927
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:13:07.966
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:13:07.973
  Apr 18 01:13:07.978: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  E0418 01:13:08.499243      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 04/18/23 01:13:08.993
  STEP: Checking rc "condition-test" has the desired failure condition set @ 04/18/23 01:13:08.997
  E0418 01:13:09.499296      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 04/18/23 01:13:10.01
  Apr 18 01:13:10.022: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 04/18/23 01:13:10.022
  E0418 01:13:10.500302      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:13:11.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-5843" for this suite. @ 04/18/23 01:13:11.033
• [3.113 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 04/18/23 01:13:11.04
  Apr 18 01:13:11.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename sched-pred @ 04/18/23 01:13:11.04
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:13:11.059
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:13:11.065
  Apr 18 01:13:11.070: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 18 01:13:11.076: INFO: Waiting for terminating namespaces to be deleted...
  Apr 18 01:13:11.079: INFO: 
  Logging pods the apiserver thinks is on node ip-10-0-14-154 before test
  Apr 18 01:13:11.083: INFO: calico-node-pdjm9 from kube-system started at 2023-04-17 16:53:36 +0000 UTC (1 container statuses recorded)
  Apr 18 01:13:11.083: INFO: 	Container calico-node ready: true, restart count 0
  Apr 18 01:13:11.084: INFO: kube-proxy-sf54c from kube-system started at 2023-04-17 16:53:36 +0000 UTC (1 container statuses recorded)
  Apr 18 01:13:11.084: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 18 01:13:11.084: INFO: condition-test-rslfl from replication-controller-5843 started at 2023-04-18 01:13:09 +0000 UTC (1 container statuses recorded)
  Apr 18 01:13:11.084: INFO: 	Container httpd ready: true, restart count 0
  Apr 18 01:13:11.084: INFO: sonobuoy-systemd-logs-daemon-set-ee374c63bb374e5f-r7gxd from sonobuoy started at 2023-04-18 00:15:14 +0000 UTC (2 container statuses recorded)
  Apr 18 01:13:11.084: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 18 01:13:11.084: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 18 01:13:11.084: INFO: 
  Logging pods the apiserver thinks is on node ip-10-0-27-81 before test
  Apr 18 01:13:11.090: INFO: calico-node-wsfsw from kube-system started at 2023-04-17 16:54:08 +0000 UTC (1 container statuses recorded)
  Apr 18 01:13:11.090: INFO: 	Container calico-node ready: true, restart count 0
  Apr 18 01:13:11.090: INFO: coredns-5cf8b9cff-c4mtg from kube-system started at 2023-04-18 00:54:44 +0000 UTC (1 container statuses recorded)
  Apr 18 01:13:11.090: INFO: 	Container coredns ready: true, restart count 0
  Apr 18 01:13:11.090: INFO: kube-proxy-srh6q from kube-system started at 2023-04-17 16:54:08 +0000 UTC (1 container statuses recorded)
  Apr 18 01:13:11.090: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 18 01:13:11.090: INFO: condition-test-w97hl from replication-controller-5843 started at 2023-04-18 01:13:09 +0000 UTC (1 container statuses recorded)
  Apr 18 01:13:11.090: INFO: 	Container httpd ready: true, restart count 0
  Apr 18 01:13:11.090: INFO: sonobuoy from sonobuoy started at 2023-04-18 00:15:13 +0000 UTC (1 container statuses recorded)
  Apr 18 01:13:11.090: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 18 01:13:11.090: INFO: sonobuoy-e2e-job-0fa4b43c9b09437b from sonobuoy started at 2023-04-18 00:15:14 +0000 UTC (2 container statuses recorded)
  Apr 18 01:13:11.090: INFO: 	Container e2e ready: true, restart count 0
  Apr 18 01:13:11.090: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 18 01:13:11.090: INFO: sonobuoy-systemd-logs-daemon-set-ee374c63bb374e5f-96krr from sonobuoy started at 2023-04-18 00:15:14 +0000 UTC (2 container statuses recorded)
  Apr 18 01:13:11.090: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 18 01:13:11.090: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/18/23 01:13:11.09
  E0418 01:13:11.500724      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:12.500948      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/18/23 01:13:13.11
  STEP: Trying to apply a random label on the found node. @ 04/18/23 01:13:13.126
  STEP: verifying the node has the label kubernetes.io/e2e-c0bf82c2-db93-4c2e-9874-80f7dfda489f 95 @ 04/18/23 01:13:13.138
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 04/18/23 01:13:13.142
  E0418 01:13:13.501060      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:14.501516      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.14.154 on the node which pod4 resides and expect not scheduled @ 04/18/23 01:13:15.159
  E0418 01:13:15.501430      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:16.501706      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:17.502569      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:18.502685      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:19.503315      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:20.504331      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:21.505297      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:22.505628      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:23.506576      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:24.506769      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:25.507352      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:26.507953      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:27.508987      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:28.509096      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:29.509693      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:30.510251      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:31.511298      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:32.511966      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:33.512887      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:34.513011      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:35.513418      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:36.513549      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:37.514496      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:38.514629      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:39.514786      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:40.514989      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:41.515469      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:42.515630      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:43.515686      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:44.515808      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:45.516615      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:46.516717      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:47.517379      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:48.517484      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:49.518342      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:50.519278      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:51.519492      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:52.519626      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:53.520199      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:54.520285      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:55.520934      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:56.521976      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:57.522097      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:58.522215      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:13:59.522316      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:00.522868      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:01.523831      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:02.524144      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:03.524200      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:04.524360      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:05.524607      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:06.524963      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:07.525513      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:08.525725      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:09.525918      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:10.526343      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:11.527187      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:12.527310      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:13.528139      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:14.528332      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:15.529440      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:16.529883      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:17.530502      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:18.530885      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:19.531364      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:20.532275      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:21.532387      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:22.532521      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:23.532929      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:24.532968      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:25.533956      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:26.534263      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:27.535046      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:28.535380      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:29.535965      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:30.536203      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:31.537270      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:32.537465      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:33.537955      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:34.538447      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:35.539533      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:36.539872      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:37.540933      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:38.541054      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:39.541797      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:40.542305      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:41.542413      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:42.542608      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:43.543652      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:44.543876      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:45.544783      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:46.544996      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:47.545362      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:48.545489      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:49.545547      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:50.546496      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:51.547314      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:52.548070      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:53.548361      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:54.548621      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:55.549345      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:56.549462      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:57.549510      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:58.550364      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:14:59.551379      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:00.551823      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:01.552492      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:02.552638      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:03.553723      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:04.553858      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:05.554982      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:06.555991      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:07.556432      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:08.556699      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:09.556940      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:10.557942      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:11.558127      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:12.558139      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:13.558941      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:14.560131      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:15.563228      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:16.563332      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:17.564169      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:18.564288      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:19.564512      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:20.565539      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:21.565722      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:22.565953      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:23.566036      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:24.566153      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:25.566708      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:26.566995      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:27.567891      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:28.568086      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:29.568827      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:30.569898      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:31.570329      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:32.570467      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:33.571423      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:34.571550      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:35.571670      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:36.571906      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:37.572513      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:38.572563      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:39.573256      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:40.573293      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:41.574166      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:42.574278      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:43.574490      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:44.574701      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:45.575860      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:46.575871      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:47.576045      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:48.576355      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:49.576853      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:50.576967      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:51.577162      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:52.577294      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:53.577892      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:54.578012      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:55.578873      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:56.578968      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:57.579684      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:58.579897      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:15:59.580640      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:00.581429      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:01.581576      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:02.582254      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:03.584045      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:04.584298      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:05.585145      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:06.585425      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:07.586263      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:08.586354      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:09.586483      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:10.587440      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:11.587695      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:12.588603      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:13.588814      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:14.589634      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:15.590488      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:16.590892      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:17.590990      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:18.591731      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:19.591958      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:20.592321      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:21.592421      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:22.592517      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:23.592720      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:24.597737      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:25.598380      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:26.598551      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:27.599598      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:28.599675      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:29.599796      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:30.600076      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:31.600975      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:32.601102      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:33.601211      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:34.601632      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:35.601415      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:36.601494      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:37.601947      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:38.602229      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:39.602532      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:40.603540      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:41.603645      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:42.603794      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:43.603985      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:44.604167      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:45.605098      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:46.605248      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:47.605365      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:48.605470      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:49.605588      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:50.605913      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:51.606151      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:52.606279      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:53.606480      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:54.606585      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:55.607275      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:56.607434      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:57.607647      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:58.608644      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:16:59.608906      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:00.609526      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:01.609648      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:02.609944      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:03.610031      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:04.611055      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:05.611855      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:06.611995      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:07.612268      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:08.612899      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:09.613016      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:10.613988      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:11.614205      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:12.614331      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:13.614449      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:14.615466      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:15.615576      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:16.615706      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:17.615859      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:18.616012      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:19.616154      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:20.616355      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:21.616547      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:22.616615      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:23.616864      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:24.617396      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:25.618523      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:26.618646      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:27.618881      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:28.618993      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:29.619954      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:30.620104      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:31.620196      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:32.620306      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:33.620395      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:34.620536      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:35.621634      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:36.621765      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:37.621906      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:38.622877      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:39.623070      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:40.623133      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:41.623275      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:42.623382      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:43.623491      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:44.624473      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:45.625178      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:46.625199      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:47.625482      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:48.626459      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:49.626578      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:50.627195      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:51.628022      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:52.628282      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:53.628463      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:54.628916      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:55.630010      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:56.630964      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:57.631172      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:58.631637      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:17:59.631775      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:00.632097      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:01.632247      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:02.632378      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:03.632483      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:04.632598      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:05.633126      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:06.633196      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:07.633400      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:08.633977      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:09.634404      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:10.634420      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:11.634561      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:12.634691      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:13.635647      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:14.635806      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-c0bf82c2-db93-4c2e-9874-80f7dfda489f off the node ip-10-0-14-154 @ 04/18/23 01:18:15.166
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-c0bf82c2-db93-4c2e-9874-80f7dfda489f @ 04/18/23 01:18:15.189
  Apr 18 01:18:15.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-4671" for this suite. @ 04/18/23 01:18:15.201
• [304.174 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 04/18/23 01:18:15.214
  Apr 18 01:18:15.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename job @ 04/18/23 01:18:15.215
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:18:15.245
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:18:15.248
  STEP: Creating a job @ 04/18/23 01:18:15.257
  STEP: Ensuring active pods == parallelism @ 04/18/23 01:18:15.273
  E0418 01:18:15.636081      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:16.636432      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete a job @ 04/18/23 01:18:17.277
  STEP: deleting Job.batch foo in namespace job-8638, will wait for the garbage collector to delete the pods @ 04/18/23 01:18:17.277
  Apr 18 01:18:17.341: INFO: Deleting Job.batch foo took: 5.653521ms
  Apr 18 01:18:17.442: INFO: Terminating Job.batch foo pods took: 100.149477ms
  E0418 01:18:17.636519      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:18.637602      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:19.638907      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:20.639565      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:21.640460      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:22.641066      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:23.641698      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:24.642150      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:25.643007      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:26.643292      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:27.644027      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:28.644192      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:29.645229      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:30.646234      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:31.647048      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:32.647426      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:33.647861      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:34.649889      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:35.650161      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:36.650795      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:37.651735      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:38.652344      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:39.653156      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:40.653836      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:41.654513      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:42.655269      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:43.655892      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:44.656671      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:45.657632      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:46.658111      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:47.658612      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:48.659081      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:49.659393      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring job was deleted @ 04/18/23 01:18:50.143
  Apr 18 01:18:50.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8638" for this suite. @ 04/18/23 01:18:50.149
• [34.941 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 04/18/23 01:18:50.16
  Apr 18 01:18:50.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/18/23 01:18:50.161
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:18:50.178
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:18:50.181
  STEP: create the container to handle the HTTPGet hook request. @ 04/18/23 01:18:50.188
  E0418 01:18:50.659873      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:51.659893      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 04/18/23 01:18:52.21
  E0418 01:18:52.660686      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:53.660924      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 04/18/23 01:18:54.232
  E0418 01:18:54.661960      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:55.662983      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:56.663079      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:18:57.663190      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 04/18/23 01:18:58.256
  Apr 18 01:18:58.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-8721" for this suite. @ 04/18/23 01:18:58.28
• [8.129 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 04/18/23 01:18:58.289
  Apr 18 01:18:58.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename sched-pred @ 04/18/23 01:18:58.29
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:18:58.307
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:18:58.311
  Apr 18 01:18:58.315: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 18 01:18:58.322: INFO: Waiting for terminating namespaces to be deleted...
  Apr 18 01:18:58.326: INFO: 
  Logging pods the apiserver thinks is on node ip-10-0-14-154 before test
  Apr 18 01:18:58.332: INFO: calico-node-pdjm9 from kube-system started at 2023-04-17 16:53:36 +0000 UTC (1 container statuses recorded)
  Apr 18 01:18:58.332: INFO: 	Container calico-node ready: true, restart count 0
  Apr 18 01:18:58.332: INFO: kube-proxy-sf54c from kube-system started at 2023-04-17 16:53:36 +0000 UTC (1 container statuses recorded)
  Apr 18 01:18:58.332: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 18 01:18:58.332: INFO: sonobuoy-systemd-logs-daemon-set-ee374c63bb374e5f-r7gxd from sonobuoy started at 2023-04-18 00:15:14 +0000 UTC (2 container statuses recorded)
  Apr 18 01:18:58.332: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 18 01:18:58.332: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 18 01:18:58.332: INFO: 
  Logging pods the apiserver thinks is on node ip-10-0-27-81 before test
  Apr 18 01:18:58.339: INFO: pod-handle-http-request from container-lifecycle-hook-8721 started at 2023-04-18 01:18:50 +0000 UTC (2 container statuses recorded)
  Apr 18 01:18:58.339: INFO: 	Container container-handle-http-request ready: true, restart count 0
  Apr 18 01:18:58.339: INFO: 	Container container-handle-https-request ready: true, restart count 0
  Apr 18 01:18:58.339: INFO: calico-node-wsfsw from kube-system started at 2023-04-17 16:54:08 +0000 UTC (1 container statuses recorded)
  Apr 18 01:18:58.339: INFO: 	Container calico-node ready: true, restart count 0
  Apr 18 01:18:58.339: INFO: coredns-5cf8b9cff-c4mtg from kube-system started at 2023-04-18 00:54:44 +0000 UTC (1 container statuses recorded)
  Apr 18 01:18:58.339: INFO: 	Container coredns ready: true, restart count 0
  Apr 18 01:18:58.339: INFO: kube-proxy-srh6q from kube-system started at 2023-04-17 16:54:08 +0000 UTC (1 container statuses recorded)
  Apr 18 01:18:58.339: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 18 01:18:58.339: INFO: sonobuoy from sonobuoy started at 2023-04-18 00:15:13 +0000 UTC (1 container statuses recorded)
  Apr 18 01:18:58.339: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 18 01:18:58.339: INFO: sonobuoy-e2e-job-0fa4b43c9b09437b from sonobuoy started at 2023-04-18 00:15:14 +0000 UTC (2 container statuses recorded)
  Apr 18 01:18:58.339: INFO: 	Container e2e ready: true, restart count 0
  Apr 18 01:18:58.339: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 18 01:18:58.339: INFO: sonobuoy-systemd-logs-daemon-set-ee374c63bb374e5f-96krr from sonobuoy started at 2023-04-18 00:15:14 +0000 UTC (2 container statuses recorded)
  Apr 18 01:18:58.339: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 18 01:18:58.339: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 04/18/23 01:18:58.339
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.1756e2abcc015af2], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/controller: }, 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] @ 04/18/23 01:18:58.369
  E0418 01:18:58.663786      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:18:59.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-1636" for this suite. @ 04/18/23 01:18:59.364
• [1.079 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 04/18/23 01:18:59.369
  Apr 18 01:18:59.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename limitrange @ 04/18/23 01:18:59.37
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:18:59.387
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:18:59.391
  STEP: Creating a LimitRange @ 04/18/23 01:18:59.398
  STEP: Setting up watch @ 04/18/23 01:18:59.399
  STEP: Submitting a LimitRange @ 04/18/23 01:18:59.505
  STEP: Verifying LimitRange creation was observed @ 04/18/23 01:18:59.519
  STEP: Fetching the LimitRange to ensure it has proper values @ 04/18/23 01:18:59.519
  Apr 18 01:18:59.521: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Apr 18 01:18:59.522: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 04/18/23 01:18:59.522
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 04/18/23 01:18:59.526
  Apr 18 01:18:59.535: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Apr 18 01:18:59.535: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 04/18/23 01:18:59.535
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 04/18/23 01:18:59.545
  Apr 18 01:18:59.557: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Apr 18 01:18:59.557: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 04/18/23 01:18:59.557
  STEP: Failing to create a Pod with more than max resources @ 04/18/23 01:18:59.562
  STEP: Updating a LimitRange @ 04/18/23 01:18:59.564
  STEP: Verifying LimitRange updating is effective @ 04/18/23 01:18:59.57
  E0418 01:18:59.664094      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:00.664344      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod with less than former min resources @ 04/18/23 01:19:01.574
  STEP: Failing to create a Pod with more than max resources @ 04/18/23 01:19:01.592
  STEP: Deleting a LimitRange @ 04/18/23 01:19:01.63
  E0418 01:19:01.665346      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying the LimitRange was deleted @ 04/18/23 01:19:01.676
  E0418 01:19:02.665614      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:03.666046      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:04.666462      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:05.666581      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:06.666683      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:19:06.679: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 04/18/23 01:19:06.679
  Apr 18 01:19:06.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-6034" for this suite. @ 04/18/23 01:19:06.717
• [7.374 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 04/18/23 01:19:06.746
  Apr 18 01:19:06.746: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename endpointslice @ 04/18/23 01:19:06.747
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:19:06.806
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:19:06.823
  E0418 01:19:07.666814      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:08.666980      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:09.667287      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:10.667405      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:11.667809      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 04/18/23 01:19:12.042
  E0418 01:19:12.667931      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:13.668945      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:14.668981      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:15.669282      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:16.669362      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 04/18/23 01:19:17.053
  E0418 01:19:17.670242      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:18.670366      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:19.670449      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:20.670509      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:21.670806      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 04/18/23 01:19:22.06
  E0418 01:19:22.670929      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:23.671386      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:24.671464      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:25.672169      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:26.672273      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 04/18/23 01:19:27.07
  Apr 18 01:19:27.114: INFO: EndpointSlice for Service endpointslice-3548/example-named-port not found
  E0418 01:19:27.672353      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:28.672385      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:29.672470      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:30.672613      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:31.672702      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:32.673034      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:33.673003      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:34.673128      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:35.674044      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:36.674334      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:19:37.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-3548" for this suite. @ 04/18/23 01:19:37.126
• [30.387 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 04/18/23 01:19:37.134
  Apr 18 01:19:37.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename emptydir @ 04/18/23 01:19:37.135
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:19:37.153
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:19:37.157
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 04/18/23 01:19:37.161
  E0418 01:19:37.674502      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:38.674608      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:39.675652      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:40.676590      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:19:41.186
  Apr 18 01:19:41.189: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-8dd82ce3-8222-44d8-aefd-3d230e7756b1 container test-container: <nil>
  STEP: delete the pod @ 04/18/23 01:19:41.206
  Apr 18 01:19:41.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6605" for this suite. @ 04/18/23 01:19:41.221
• [4.092 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 04/18/23 01:19:41.227
  Apr 18 01:19:41.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename pods @ 04/18/23 01:19:41.228
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:19:41.258
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:19:41.267
  STEP: creating pod @ 04/18/23 01:19:41.273
  E0418 01:19:41.677532      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:42.677666      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:19:43.299: INFO: Pod pod-hostip-aef79098-d0a0-4c21-9d00-8ad6bb0678f9 has hostIP: 10.0.14.154
  Apr 18 01:19:43.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4203" for this suite. @ 04/18/23 01:19:43.307
• [2.094 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 04/18/23 01:19:43.322
  Apr 18 01:19:43.322: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename services @ 04/18/23 01:19:43.323
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:19:43.356
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:19:43.359
  STEP: creating an Endpoint @ 04/18/23 01:19:43.365
  STEP: waiting for available Endpoint @ 04/18/23 01:19:43.383
  STEP: listing all Endpoints @ 04/18/23 01:19:43.396
  STEP: updating the Endpoint @ 04/18/23 01:19:43.399
  STEP: fetching the Endpoint @ 04/18/23 01:19:43.418
  STEP: patching the Endpoint @ 04/18/23 01:19:43.42
  STEP: fetching the Endpoint @ 04/18/23 01:19:43.448
  STEP: deleting the Endpoint by Collection @ 04/18/23 01:19:43.45
  STEP: waiting for Endpoint deletion @ 04/18/23 01:19:43.468
  STEP: fetching the Endpoint @ 04/18/23 01:19:43.471
  Apr 18 01:19:43.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4835" for this suite. @ 04/18/23 01:19:43.481
• [0.179 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 04/18/23 01:19:43.501
  Apr 18 01:19:43.501: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 01:19:43.504
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:19:43.559
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:19:43.567
  STEP: Creating a pod to test downward API volume plugin @ 04/18/23 01:19:43.576
  E0418 01:19:43.677993      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:44.678137      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:45.678865      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:46.678889      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:19:47.593
  Apr 18 01:19:47.596: INFO: Trying to get logs from node ip-10-0-14-154 pod downwardapi-volume-78bd76a5-b01a-459c-bfb8-635e522d5449 container client-container: <nil>
  STEP: delete the pod @ 04/18/23 01:19:47.601
  Apr 18 01:19:47.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9603" for this suite. @ 04/18/23 01:19:47.619
• [4.124 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 04/18/23 01:19:47.628
  Apr 18 01:19:47.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/18/23 01:19:47.63
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:19:47.647
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:19:47.652
  STEP: create the container to handle the HTTPGet hook request. @ 04/18/23 01:19:47.659
  E0418 01:19:47.679647      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:48.680304      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:49.680641      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 04/18/23 01:19:49.685
  E0418 01:19:50.681242      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:51.681587      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 04/18/23 01:19:51.712
  STEP: delete the pod with lifecycle hook @ 04/18/23 01:19:51.726
  E0418 01:19:52.681882      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:53.681998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:19:53.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-834" for this suite. @ 04/18/23 01:19:53.755
• [6.133 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 04/18/23 01:19:53.763
  Apr 18 01:19:53.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename container-probe @ 04/18/23 01:19:53.764
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:19:53.782
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:19:53.786
  STEP: Creating pod liveness-8fda1d82-997c-4e02-ab4e-231816c1469a in namespace container-probe-9036 @ 04/18/23 01:19:53.789
  E0418 01:19:54.682634      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:55.682632      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:19:55.807: INFO: Started pod liveness-8fda1d82-997c-4e02-ab4e-231816c1469a in namespace container-probe-9036
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/18/23 01:19:55.807
  Apr 18 01:19:55.809: INFO: Initial restart count of pod liveness-8fda1d82-997c-4e02-ab4e-231816c1469a is 0
  E0418 01:19:56.682790      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:57.682915      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:58.683060      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:19:59.683088      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:00.683433      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:01.683842      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:02.683961      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:03.684090      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:04.684316      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:05.684464      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:06.684610      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:07.684710      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:08.684912      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:09.685998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:10.686190      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:11.686310      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:12.686649      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:13.686631      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:14.686990      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:15.687252      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:20:15.982: INFO: Restart count of pod container-probe-9036/liveness-8fda1d82-997c-4e02-ab4e-231816c1469a is now 1 (20.172590547s elapsed)
  Apr 18 01:20:15.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/18/23 01:20:15.987
  STEP: Destroying namespace "container-probe-9036" for this suite. @ 04/18/23 01:20:16.003
• [22.247 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 04/18/23 01:20:16.013
  Apr 18 01:20:16.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename pods @ 04/18/23 01:20:16.015
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:20:16.037
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:20:16.041
  Apr 18 01:20:16.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: creating the pod @ 04/18/23 01:20:16.053
  STEP: submitting the pod to kubernetes @ 04/18/23 01:20:16.053
  E0418 01:20:16.687478      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:17.687683      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:20:18.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2016" for this suite. @ 04/18/23 01:20:18.118
• [2.113 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 04/18/23 01:20:18.143
  Apr 18 01:20:18.144: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename downward-api @ 04/18/23 01:20:18.148
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:20:18.171
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:20:18.177
  STEP: Creating a pod to test downward API volume plugin @ 04/18/23 01:20:18.185
  E0418 01:20:18.688219      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:19.688394      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:20.688717      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:21.688903      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:20:22.224
  Apr 18 01:20:22.228: INFO: Trying to get logs from node ip-10-0-14-154 pod downwardapi-volume-f49ee9b1-46ed-41f9-b8f5-032fa03925c2 container client-container: <nil>
  STEP: delete the pod @ 04/18/23 01:20:22.234
  Apr 18 01:20:22.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6447" for this suite. @ 04/18/23 01:20:22.271
• [4.149 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 04/18/23 01:20:22.293
  Apr 18 01:20:22.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename crd-watch @ 04/18/23 01:20:22.294
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:20:22.339
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:20:22.359
  Apr 18 01:20:22.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  E0418 01:20:22.688999      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:23.689510      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:24.689904      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 04/18/23 01:20:25.017
  Apr 18 01:20:25.032: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-18T01:20:25Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-18T01:20:25Z]] name:name1 resourceVersion:63502 uid:e08ed9f1-1b53-4607-ba2a-e5f16e747bd8] num:map[num1:9223372036854775807 num2:1000000]]}
  E0418 01:20:25.689959      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:26.691361      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:27.691088      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:28.691188      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:29.691384      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:30.691504      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:31.691674      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:32.691950      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:33.692287      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:34.692870      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 04/18/23 01:20:35.032
  Apr 18 01:20:35.038: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-18T01:20:35Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-18T01:20:35Z]] name:name2 resourceVersion:63527 uid:fa55fb5c-d669-4585-8093-3689397eaf5f] num:map[num1:9223372036854775807 num2:1000000]]}
  E0418 01:20:35.693223      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:36.693308      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:37.693509      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:38.693839      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:39.694005      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:40.694334      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:41.694606      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:42.694847      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:43.695021      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:44.695316      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 04/18/23 01:20:45.038
  Apr 18 01:20:45.045: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-18T01:20:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-18T01:20:45Z]] name:name1 resourceVersion:63543 uid:e08ed9f1-1b53-4607-ba2a-e5f16e747bd8] num:map[num1:9223372036854775807 num2:1000000]]}
  E0418 01:20:45.695459      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:46.695666      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:47.696048      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:48.696182      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:49.696306      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:50.696423      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:51.696905      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:52.697162      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:53.697950      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:54.699028      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 04/18/23 01:20:55.046
  Apr 18 01:20:55.055: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-18T01:20:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-18T01:20:55Z]] name:name2 resourceVersion:63567 uid:fa55fb5c-d669-4585-8093-3689397eaf5f] num:map[num1:9223372036854775807 num2:1000000]]}
  E0418 01:20:55.699124      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:56.699249      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:57.700043      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:58.700150      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:20:59.701040      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:00.701155      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:01.702009      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:02.702138      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:03.702339      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:04.702485      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 04/18/23 01:21:05.056
  Apr 18 01:21:05.066: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-18T01:20:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-18T01:20:45Z]] name:name1 resourceVersion:63585 uid:e08ed9f1-1b53-4607-ba2a-e5f16e747bd8] num:map[num1:9223372036854775807 num2:1000000]]}
  E0418 01:21:05.702616      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:06.702718      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:07.703137      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:08.703402      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:09.703462      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:10.703535      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:11.703843      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:12.704011      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:13.704211      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:14.704512      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 04/18/23 01:21:15.067
  Apr 18 01:21:15.078: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-18T01:20:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-18T01:20:55Z]] name:name2 resourceVersion:63601 uid:fa55fb5c-d669-4585-8093-3689397eaf5f] num:map[num1:9223372036854775807 num2:1000000]]}
  E0418 01:21:15.704647      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:16.704968      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:17.705245      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:18.706176      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:19.706208      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:20.706346      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:21.706969      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:22.707262      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:23.707441      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:24.707725      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:21:25.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-9797" for this suite. @ 04/18/23 01:21:25.606
• [63.323 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 04/18/23 01:21:25.616
  Apr 18 01:21:25.616: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename taint-multiple-pods @ 04/18/23 01:21:25.617
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:21:25.705
  E0418 01:21:25.707797      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:21:25.71
  Apr 18 01:21:25.715: INFO: Waiting up to 1m0s for all nodes to be ready
  E0418 01:21:26.708101      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:27.708186      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:28.708896      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:29.708982      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:30.709083      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:31.709187      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:32.709948      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:33.711032      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:34.711072      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:35.711180      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:36.711388      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:37.711599      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:38.712318      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:39.712585      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:40.713044      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:41.714065      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:42.714190      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:43.714314      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:44.714892      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:45.715183      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:46.715321      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:47.715529      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:48.715715      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:49.715901      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:50.716903      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:51.717343      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:52.717236      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:53.717774      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:54.719180      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:55.719681      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:56.719959      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:57.721214      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:58.721961      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:21:59.722939      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:00.723210      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:01.724000      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:02.724127      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:03.724283      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:04.724405      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:05.724677      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:06.724823      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:07.724943      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:08.727113      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:09.727308      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:10.727523      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:11.727640      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:12.728357      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:13.728728      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:14.728856      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:15.729581      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:16.729628      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:17.730073      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:18.732277      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:19.732455      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:20.732915      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:21.733220      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:22.733791      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:23.733808      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:24.733993      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:25.734364      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:22:25.741: INFO: Waiting for terminating namespaces to be deleted...
  Apr 18 01:22:25.744: INFO: Starting informer...
  STEP: Starting pods... @ 04/18/23 01:22:25.744
  Apr 18 01:22:25.966: INFO: Pod1 is running on ip-10-0-14-154. Tainting Node
  E0418 01:22:26.734506      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:27.735234      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:22:28.202: INFO: Pod2 is running on ip-10-0-14-154. Tainting Node
  STEP: Trying to apply a taint on the Node @ 04/18/23 01:22:28.202
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/18/23 01:22:28.217
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 04/18/23 01:22:28.22
  E0418 01:22:28.737150      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:29.737135      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:30.737968      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:31.738240      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:32.738326      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:33.739155      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:22:34.589: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  E0418 01:22:34.739621      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:35.739963      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:36.740053      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:37.740974      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:38.743912      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:39.744013      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:40.744280      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:41.744630      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:42.744869      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:43.745140      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:44.745360      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:45.745672      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:46.745980      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:47.746383      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:48.747292      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:49.747657      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:50.748067      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:51.748374      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:52.748491      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:53.748531      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:22:53.925: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Apr 18 01:22:53.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/18/23 01:22:53.943
  STEP: Destroying namespace "taint-multiple-pods-5868" for this suite. @ 04/18/23 01:22:53.95
• [88.341 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 04/18/23 01:22:53.96
  Apr 18 01:22:53.960: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename webhook @ 04/18/23 01:22:53.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:22:53.98
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:22:53.984
  STEP: Setting up server cert @ 04/18/23 01:22:54.014
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/18/23 01:22:54.414
  STEP: Deploying the webhook pod @ 04/18/23 01:22:54.427
  STEP: Wait for the deployment to be ready @ 04/18/23 01:22:54.437
  Apr 18 01:22:54.447: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0418 01:22:54.749238      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:55.749324      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/18/23 01:22:56.466
  STEP: Verifying the service has paired with the endpoint @ 04/18/23 01:22:56.494
  E0418 01:22:56.749959      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:22:57.494: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 18 01:22:57.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  E0418 01:22:57.750117      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6406-crds.webhook.example.com via the AdmissionRegistration API @ 04/18/23 01:22:58.014
  STEP: Creating a custom resource that should be mutated by the webhook @ 04/18/23 01:22:58.031
  E0418 01:22:58.750889      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:22:59.751067      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:23:00.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0418 01:23:00.751952      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-2273" for this suite. @ 04/18/23 01:23:00.829
  STEP: Destroying namespace "webhook-markers-2463" for this suite. @ 04/18/23 01:23:00.852
• [6.909 seconds]
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 04/18/23 01:23:00.867
  Apr 18 01:23:00.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename container-runtime @ 04/18/23 01:23:00.868
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:23:00.912
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:23:00.917
  STEP: create the container @ 04/18/23 01:23:00.921
  W0418 01:23:00.929146      21 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/18/23 01:23:00.929
  E0418 01:23:01.752951      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:02.753977      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:03.754082      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:04.754328      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 04/18/23 01:23:04.966
  STEP: the container should be terminated @ 04/18/23 01:23:04.97
  STEP: the termination message should be set @ 04/18/23 01:23:04.97
  Apr 18 01:23:04.970: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 04/18/23 01:23:04.97
  Apr 18 01:23:04.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6490" for this suite. @ 04/18/23 01:23:04.99
• [4.131 seconds]
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 04/18/23 01:23:04.999
  Apr 18 01:23:04.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/18/23 01:23:05
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:23:05.067
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:23:05.081
  STEP: create the container to handle the HTTPGet hook request. @ 04/18/23 01:23:05.092
  E0418 01:23:05.755353      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:06.755214      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 04/18/23 01:23:07.136
  E0418 01:23:07.755354      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:08.755806      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 04/18/23 01:23:09.156
  E0418 01:23:09.755560      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:10.755602      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:11.755878      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:12.755932      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 04/18/23 01:23:13.18
  Apr 18 01:23:13.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-1811" for this suite. @ 04/18/23 01:23:13.209
• [8.217 seconds]
------------------------------
S
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 04/18/23 01:23:13.215
  Apr 18 01:23:13.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename endpointslice @ 04/18/23 01:23:13.217
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:23:13.237
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:23:13.241
  STEP: getting /apis @ 04/18/23 01:23:13.245
  STEP: getting /apis/discovery.k8s.io @ 04/18/23 01:23:13.251
  STEP: getting /apis/discovery.k8s.iov1 @ 04/18/23 01:23:13.253
  STEP: creating @ 04/18/23 01:23:13.255
  STEP: getting @ 04/18/23 01:23:13.271
  STEP: listing @ 04/18/23 01:23:13.273
  STEP: watching @ 04/18/23 01:23:13.276
  Apr 18 01:23:13.276: INFO: starting watch
  STEP: cluster-wide listing @ 04/18/23 01:23:13.278
  STEP: cluster-wide watching @ 04/18/23 01:23:13.28
  Apr 18 01:23:13.281: INFO: starting watch
  STEP: patching @ 04/18/23 01:23:13.282
  STEP: updating @ 04/18/23 01:23:13.292
  Apr 18 01:23:13.307: INFO: waiting for watch events with expected annotations
  Apr 18 01:23:13.307: INFO: saw patched and updated annotations
  STEP: deleting @ 04/18/23 01:23:13.307
  STEP: deleting a collection @ 04/18/23 01:23:13.323
  Apr 18 01:23:13.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-4432" for this suite. @ 04/18/23 01:23:13.348
• [0.139 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 04/18/23 01:23:13.36
  Apr 18 01:23:13.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename kubectl @ 04/18/23 01:23:13.36
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:23:13.385
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:23:13.393
  STEP: creating all guestbook components @ 04/18/23 01:23:13.4
  Apr 18 01:23:13.400: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Apr 18 01:23:13.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-2726 create -f -'
  E0418 01:23:13.756969      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:23:13.942: INFO: stderr: ""
  Apr 18 01:23:13.942: INFO: stdout: "service/agnhost-replica created\n"
  Apr 18 01:23:13.942: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Apr 18 01:23:13.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-2726 create -f -'
  Apr 18 01:23:14.566: INFO: stderr: ""
  Apr 18 01:23:14.566: INFO: stdout: "service/agnhost-primary created\n"
  Apr 18 01:23:14.566: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Apr 18 01:23:14.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-2726 create -f -'
  E0418 01:23:14.757086      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:23:14.923: INFO: stderr: ""
  Apr 18 01:23:14.923: INFO: stdout: "service/frontend created\n"
  Apr 18 01:23:14.923: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Apr 18 01:23:14.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-2726 create -f -'
  Apr 18 01:23:15.268: INFO: stderr: ""
  Apr 18 01:23:15.268: INFO: stdout: "deployment.apps/frontend created\n"
  Apr 18 01:23:15.268: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Apr 18 01:23:15.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-2726 create -f -'
  Apr 18 01:23:15.534: INFO: stderr: ""
  Apr 18 01:23:15.534: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Apr 18 01:23:15.534: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Apr 18 01:23:15.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-2726 create -f -'
  E0418 01:23:15.757576      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:23:15.843: INFO: stderr: ""
  Apr 18 01:23:15.843: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 04/18/23 01:23:15.843
  Apr 18 01:23:15.843: INFO: Waiting for all frontend pods to be Running.
  E0418 01:23:16.758295      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:17.758508      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:18.763645      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:19.763844      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:20.763902      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:23:20.894: INFO: Waiting for frontend to serve content.
  Apr 18 01:23:20.902: INFO: Trying to add a new entry to the guestbook.
  Apr 18 01:23:20.913: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 04/18/23 01:23:20.922
  Apr 18 01:23:20.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-2726 delete --grace-period=0 --force -f -'
  Apr 18 01:23:21.055: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 18 01:23:21.055: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 04/18/23 01:23:21.055
  Apr 18 01:23:21.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-2726 delete --grace-period=0 --force -f -'
  Apr 18 01:23:21.230: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 18 01:23:21.230: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 04/18/23 01:23:21.23
  Apr 18 01:23:21.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-2726 delete --grace-period=0 --force -f -'
  Apr 18 01:23:21.377: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 18 01:23:21.377: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 04/18/23 01:23:21.377
  Apr 18 01:23:21.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-2726 delete --grace-period=0 --force -f -'
  Apr 18 01:23:21.549: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 18 01:23:21.549: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 04/18/23 01:23:21.549
  Apr 18 01:23:21.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-2726 delete --grace-period=0 --force -f -'
  Apr 18 01:23:21.694: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 18 01:23:21.694: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 04/18/23 01:23:21.694
  Apr 18 01:23:21.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-2726 delete --grace-period=0 --force -f -'
  E0418 01:23:21.764259      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:23:21.812: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 18 01:23:21.812: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Apr 18 01:23:21.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2726" for this suite. @ 04/18/23 01:23:21.818
• [8.468 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 04/18/23 01:23:21.828
  Apr 18 01:23:21.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename subpath @ 04/18/23 01:23:21.829
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:23:21.885
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:23:21.891
  STEP: Setting up data @ 04/18/23 01:23:21.897
  STEP: Creating pod pod-subpath-test-projected-z2xp @ 04/18/23 01:23:21.912
  STEP: Creating a pod to test atomic-volume-subpath @ 04/18/23 01:23:21.912
  E0418 01:23:22.764966      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:23.765158      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:24.765315      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:25.765667      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:26.765782      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:27.766069      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:28.769877      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:29.769902      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:30.770958      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:31.771156      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:32.771291      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:33.771401      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:34.771476      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:35.771660      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:36.772639      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:37.772801      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:38.788836      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:39.777661      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:40.777900      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:41.777967      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:42.778783      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:43.778959      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:44.779126      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:45.779626      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:23:45.98
  Apr 18 01:23:45.983: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-subpath-test-projected-z2xp container test-container-subpath-projected-z2xp: <nil>
  STEP: delete the pod @ 04/18/23 01:23:46.009
  STEP: Deleting pod pod-subpath-test-projected-z2xp @ 04/18/23 01:23:46.037
  Apr 18 01:23:46.037: INFO: Deleting pod "pod-subpath-test-projected-z2xp" in namespace "subpath-2363"
  Apr 18 01:23:46.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-2363" for this suite. @ 04/18/23 01:23:46.045
• [24.227 seconds]
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 04/18/23 01:23:46.055
  Apr 18 01:23:46.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename disruption @ 04/18/23 01:23:46.055
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:23:46.093
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:23:46.098
  STEP: Creating a pdb that targets all three pods in a test replica set @ 04/18/23 01:23:46.101
  STEP: Waiting for the pdb to be processed @ 04/18/23 01:23:46.107
  E0418 01:23:46.779685      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:47.780000      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: First trying to evict a pod which shouldn't be evictable @ 04/18/23 01:23:48.136
  STEP: Waiting for all pods to be running @ 04/18/23 01:23:48.136
  Apr 18 01:23:48.155: INFO: pods: 0 < 3
  E0418 01:23:48.782995      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:49.783328      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 04/18/23 01:23:50.159
  STEP: Updating the pdb to allow a pod to be evicted @ 04/18/23 01:23:50.169
  STEP: Waiting for the pdb to be processed @ 04/18/23 01:23:50.177
  E0418 01:23:50.784048      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:51.784205      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 04/18/23 01:23:52.185
  STEP: Waiting for all pods to be running @ 04/18/23 01:23:52.185
  STEP: Waiting for the pdb to observed all healthy pods @ 04/18/23 01:23:52.19
  STEP: Patching the pdb to disallow a pod to be evicted @ 04/18/23 01:23:52.228
  STEP: Waiting for the pdb to be processed @ 04/18/23 01:23:52.327
  E0418 01:23:52.784248      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:53.784408      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 04/18/23 01:23:54.35
  STEP: locating a running pod @ 04/18/23 01:23:54.353
  STEP: Deleting the pdb to allow a pod to be evicted @ 04/18/23 01:23:54.361
  STEP: Waiting for the pdb to be deleted @ 04/18/23 01:23:54.369
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 04/18/23 01:23:54.373
  STEP: Waiting for all pods to be running @ 04/18/23 01:23:54.373
  Apr 18 01:23:54.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-5198" for this suite. @ 04/18/23 01:23:54.395
• [8.355 seconds]
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 04/18/23 01:23:54.41
  Apr 18 01:23:54.410: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename runtimeclass @ 04/18/23 01:23:54.411
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:23:54.459
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:23:54.476
  E0418 01:23:54.784898      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:55.785227      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:23:56.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-6842" for this suite. @ 04/18/23 01:23:56.528
• [2.124 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 04/18/23 01:23:56.535
  Apr 18 01:23:56.535: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename watch @ 04/18/23 01:23:56.536
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:23:56.555
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:23:56.563
  STEP: creating a watch on configmaps with label A @ 04/18/23 01:23:56.576
  STEP: creating a watch on configmaps with label B @ 04/18/23 01:23:56.578
  STEP: creating a watch on configmaps with label A or B @ 04/18/23 01:23:56.58
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 04/18/23 01:23:56.581
  Apr 18 01:23:56.586: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2374  fb486346-736c-4d6a-8f3b-037961bdf319 64513 0 2023-04-18 01:23:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-18 01:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 18 01:23:56.586: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2374  fb486346-736c-4d6a-8f3b-037961bdf319 64513 0 2023-04-18 01:23:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-18 01:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 04/18/23 01:23:56.586
  Apr 18 01:23:56.593: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2374  fb486346-736c-4d6a-8f3b-037961bdf319 64514 0 2023-04-18 01:23:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-18 01:23:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 18 01:23:56.593: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2374  fb486346-736c-4d6a-8f3b-037961bdf319 64514 0 2023-04-18 01:23:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-18 01:23:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 04/18/23 01:23:56.594
  Apr 18 01:23:56.601: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2374  fb486346-736c-4d6a-8f3b-037961bdf319 64515 0 2023-04-18 01:23:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-18 01:23:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 18 01:23:56.601: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2374  fb486346-736c-4d6a-8f3b-037961bdf319 64515 0 2023-04-18 01:23:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-18 01:23:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 04/18/23 01:23:56.601
  Apr 18 01:23:56.609: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2374  fb486346-736c-4d6a-8f3b-037961bdf319 64516 0 2023-04-18 01:23:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-18 01:23:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 18 01:23:56.609: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2374  fb486346-736c-4d6a-8f3b-037961bdf319 64516 0 2023-04-18 01:23:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-18 01:23:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 04/18/23 01:23:56.609
  Apr 18 01:23:56.614: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2374  6e20efbc-74b4-474b-9859-5ee532b8c2bb 64517 0 2023-04-18 01:23:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-18 01:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 18 01:23:56.614: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2374  6e20efbc-74b4-474b-9859-5ee532b8c2bb 64517 0 2023-04-18 01:23:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-18 01:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0418 01:23:56.786184      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:57.786369      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:58.792109      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:23:59.792131      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:00.792758      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:01.792890      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:02.793017      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:03.794079      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:04.794296      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:05.795414      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 04/18/23 01:24:06.615
  Apr 18 01:24:06.623: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2374  6e20efbc-74b4-474b-9859-5ee532b8c2bb 64595 0 2023-04-18 01:23:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-18 01:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 18 01:24:06.623: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2374  6e20efbc-74b4-474b-9859-5ee532b8c2bb 64595 0 2023-04-18 01:23:56 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-18 01:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0418 01:24:06.796417      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:07.796684      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:08.797918      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:09.798028      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:10.798342      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:11.798582      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:12.798927      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:13.798949      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:14.799015      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:15.799932      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:24:16.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2374" for this suite. @ 04/18/23 01:24:16.629
• [20.101 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 04/18/23 01:24:16.636
  Apr 18 01:24:16.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename kubectl @ 04/18/23 01:24:16.637
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:24:16.683
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:24:16.695
  STEP: validating cluster-info @ 04/18/23 01:24:16.704
  Apr 18 01:24:16.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-4822 cluster-info'
  Apr 18 01:24:16.795: INFO: stderr: ""
  Apr 18 01:24:16.795: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.3.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Apr 18 01:24:16.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0418 01:24:16.800857      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "kubectl-4822" for this suite. @ 04/18/23 01:24:16.801
• [0.173 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 04/18/23 01:24:16.81
  Apr 18 01:24:16.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename gc @ 04/18/23 01:24:16.811
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:24:16.828
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:24:16.832
  STEP: create the rc @ 04/18/23 01:24:16.836
  W0418 01:24:16.841163      21 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0418 01:24:17.801208      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:18.808896      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:19.809249      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:20.809600      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:21.809636      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 04/18/23 01:24:21.845
  STEP: wait for all pods to be garbage collected @ 04/18/23 01:24:21.854
  E0418 01:24:22.810081      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:23.809885      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:24.809991      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:25.810956      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:26.811999      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 04/18/23 01:24:26.862
  Apr 18 01:24:27.048: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 18 01:24:27.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3437" for this suite. @ 04/18/23 01:24:27.054
• [10.252 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 04/18/23 01:24:27.063
  Apr 18 01:24:27.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename pod-network-test @ 04/18/23 01:24:27.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:24:27.085
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:24:27.09
  STEP: Performing setup for networking test in namespace pod-network-test-7766 @ 04/18/23 01:24:27.094
  STEP: creating a selector @ 04/18/23 01:24:27.094
  STEP: Creating the service pods in kubernetes @ 04/18/23 01:24:27.094
  Apr 18 01:24:27.094: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0418 01:24:27.812985      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:28.814952      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:29.815028      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:30.815949      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:31.817270      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:32.817081      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:33.817373      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:34.817541      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:35.817957      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:36.817895      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:37.818683      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:38.822963      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 04/18/23 01:24:39.18
  E0418 01:24:39.823773      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:40.823866      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:24:41.209: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  Apr 18 01:24:41.209: INFO: Breadth first check of 10.2.212.61 on host 10.0.14.154...
  Apr 18 01:24:41.212: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.212.60:9080/dial?request=hostname&protocol=http&host=10.2.212.61&port=8083&tries=1'] Namespace:pod-network-test-7766 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 01:24:41.212: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 01:24:41.214: INFO: ExecWithOptions: Clientset creation
  Apr 18 01:24:41.214: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-7766/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.212.60%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.2.212.61%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 18 01:24:41.288: INFO: Waiting for responses: map[]
  Apr 18 01:24:41.288: INFO: reached 10.2.212.61 after 0/1 tries
  Apr 18 01:24:41.288: INFO: Breadth first check of 10.2.129.174 on host 10.0.27.81...
  Apr 18 01:24:41.291: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.212.60:9080/dial?request=hostname&protocol=http&host=10.2.129.174&port=8083&tries=1'] Namespace:pod-network-test-7766 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 01:24:41.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 01:24:41.292: INFO: ExecWithOptions: Clientset creation
  Apr 18 01:24:41.292: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-7766/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.2.212.60%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.2.129.174%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 18 01:24:41.359: INFO: Waiting for responses: map[]
  Apr 18 01:24:41.359: INFO: reached 10.2.129.174 after 0/1 tries
  Apr 18 01:24:41.359: INFO: Going to retry 0 out of 2 pods....
  Apr 18 01:24:41.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-7766" for this suite. @ 04/18/23 01:24:41.363
• [14.307 seconds]
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 04/18/23 01:24:41.373
  Apr 18 01:24:41.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename dns @ 04/18/23 01:24:41.379
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:24:41.411
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:24:41.416
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 04/18/23 01:24:41.423
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 04/18/23 01:24:41.423
  STEP: creating a pod to probe DNS @ 04/18/23 01:24:41.423
  STEP: submitting the pod to kubernetes @ 04/18/23 01:24:41.423
  E0418 01:24:41.824867      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:42.824993      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/18/23 01:24:43.444
  STEP: looking for the results for each expected name from probers @ 04/18/23 01:24:43.447
  Apr 18 01:24:43.459: INFO: DNS probes using dns-4006/dns-test-91c10942-c08c-473e-80b8-d76f0c4edc0f succeeded

  Apr 18 01:24:43.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/18/23 01:24:43.463
  STEP: Destroying namespace "dns-4006" for this suite. @ 04/18/23 01:24:43.474
• [2.106 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 04/18/23 01:24:43.48
  Apr 18 01:24:43.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename namespaces @ 04/18/23 01:24:43.481
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:24:43.5
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:24:43.504
  STEP: Creating namespace "e2e-ns-cg97h" @ 04/18/23 01:24:43.507
  Apr 18 01:24:43.524: INFO: Namespace "e2e-ns-cg97h-5827" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-cg97h-5827" @ 04/18/23 01:24:43.524
  Apr 18 01:24:43.532: INFO: Namespace "e2e-ns-cg97h-5827" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-cg97h-5827" @ 04/18/23 01:24:43.532
  Apr 18 01:24:43.540: INFO: Namespace "e2e-ns-cg97h-5827" has []v1.FinalizerName{"kubernetes"}
  Apr 18 01:24:43.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7307" for this suite. @ 04/18/23 01:24:43.547
  STEP: Destroying namespace "e2e-ns-cg97h-5827" for this suite. @ 04/18/23 01:24:43.562
• [0.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 04/18/23 01:24:43.571
  Apr 18 01:24:43.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 01:24:43.572
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:24:43.591
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:24:43.595
  STEP: Creating the pod @ 04/18/23 01:24:43.599
  E0418 01:24:43.826345      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:44.825448      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:45.825595      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:24:46.147: INFO: Successfully updated pod "annotationupdatec05e5be4-6b68-4464-b665-63ed990702b8"
  E0418 01:24:46.826281      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:47.827104      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:48.827660      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:49.827969      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:24:50.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-817" for this suite. @ 04/18/23 01:24:50.179
• [6.614 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 04/18/23 01:24:50.186
  Apr 18 01:24:50.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename secrets @ 04/18/23 01:24:50.187
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:24:50.202
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:24:50.206
  STEP: Creating secret with name secret-test-73e9cd94-3efc-44a3-a64e-9d7d6da0ab7a @ 04/18/23 01:24:50.21
  STEP: Creating a pod to test consume secrets @ 04/18/23 01:24:50.214
  E0418 01:24:50.828010      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:51.828080      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:52.828964      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:53.829064      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:24:54.234
  Apr 18 01:24:54.238: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-secrets-ee8df759-a501-4ba5-b08b-e8e12bffa6fa container secret-volume-test: <nil>
  STEP: delete the pod @ 04/18/23 01:24:54.244
  Apr 18 01:24:54.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5795" for this suite. @ 04/18/23 01:24:54.275
• [4.096 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 04/18/23 01:24:54.283
  Apr 18 01:24:54.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename security-context @ 04/18/23 01:24:54.284
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:24:54.34
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:24:54.347
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 04/18/23 01:24:54.351
  E0418 01:24:54.829207      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:55.829584      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:56.829975      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:57.830058      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:24:58.388
  Apr 18 01:24:58.391: INFO: Trying to get logs from node ip-10-0-14-154 pod security-context-b4af7e31-4d22-43ce-b3ec-29eaa88528a4 container test-container: <nil>
  STEP: delete the pod @ 04/18/23 01:24:58.397
  Apr 18 01:24:58.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-8987" for this suite. @ 04/18/23 01:24:58.414
• [4.141 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 04/18/23 01:24:58.429
  Apr 18 01:24:58.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename server-version @ 04/18/23 01:24:58.43
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:24:58.447
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:24:58.452
  STEP: Request ServerVersion @ 04/18/23 01:24:58.456
  STEP: Confirm major version @ 04/18/23 01:24:58.458
  Apr 18 01:24:58.458: INFO: Major version: 1
  STEP: Confirm minor version @ 04/18/23 01:24:58.459
  Apr 18 01:24:58.459: INFO: cleanMinorVersion: 27
  Apr 18 01:24:58.459: INFO: Minor version: 27
  Apr 18 01:24:58.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-7361" for this suite. @ 04/18/23 01:24:58.462
• [0.039 seconds]
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 04/18/23 01:24:58.467
  Apr 18 01:24:58.467: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename downward-api @ 04/18/23 01:24:58.468
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:24:58.491
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:24:58.495
  STEP: Creating the pod @ 04/18/23 01:24:58.499
  E0418 01:24:58.830992      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:24:59.831978      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:00.832568      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:25:01.058: INFO: Successfully updated pod "annotationupdate0b313a82-03a4-4d29-9bbc-d63f12172448"
  E0418 01:25:01.833412      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:02.833561      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:25:03.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8645" for this suite. @ 04/18/23 01:25:03.089
• [4.628 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 04/18/23 01:25:03.101
  Apr 18 01:25:03.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename var-expansion @ 04/18/23 01:25:03.102
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:25:03.121
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:25:03.127
  STEP: Creating a pod to test substitution in container's command @ 04/18/23 01:25:03.132
  E0418 01:25:03.833676      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:04.836344      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:05.837093      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:06.837217      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:25:07.158
  Apr 18 01:25:07.162: INFO: Trying to get logs from node ip-10-0-14-154 pod var-expansion-26dc3aff-3664-468d-be92-e79237b71c7a container dapi-container: <nil>
  STEP: delete the pod @ 04/18/23 01:25:07.169
  Apr 18 01:25:07.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5031" for this suite. @ 04/18/23 01:25:07.189
• [4.095 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 04/18/23 01:25:07.197
  Apr 18 01:25:07.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename secrets @ 04/18/23 01:25:07.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:25:07.23
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:25:07.234
  STEP: Creating secret with name secret-test-f3e218e6-d609-4a95-a1cc-72cf1479b42e @ 04/18/23 01:25:07.254
  STEP: Creating a pod to test consume secrets @ 04/18/23 01:25:07.262
  E0418 01:25:07.837977      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:08.841711      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:09.841820      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:10.842145      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:25:11.299
  Apr 18 01:25:11.302: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-secrets-24d49757-4bf9-4f69-bdb1-6d169ed9a273 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/18/23 01:25:11.307
  Apr 18 01:25:11.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9953" for this suite. @ 04/18/23 01:25:11.323
• [4.133 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 04/18/23 01:25:11.333
  Apr 18 01:25:11.333: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename init-container @ 04/18/23 01:25:11.334
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:25:11.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:25:11.357
  STEP: creating the pod @ 04/18/23 01:25:11.362
  Apr 18 01:25:11.362: INFO: PodSpec: initContainers in spec.initContainers
  E0418 01:25:11.842780      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:12.842855      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:13.843923      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:14.844617      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:15.844769      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:16.844876      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:17.845255      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:18.846129      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:19.846120      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:20.846341      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:21.846479      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:22.846613      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:23.846688      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:24.847010      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:25.847516      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:26.847795      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:27.847884      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:28.848080      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:29.848490      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:30.848600      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:31.848714      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:32.848889      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:33.849027      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:34.849106      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:35.849956      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:36.850950      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:37.851120      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:38.864385      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:39.864539      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:40.864639      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:41.864807      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:42.864900      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:43.865112      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:44.865345      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:45.865937      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:46.865892      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:47.866046      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:48.866591      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:49.866889      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:50.867215      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:51.867341      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:52.867824      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:53.867963      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:54.868257      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:25:55.201: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-05c451ce-de8e-4430-b3e7-8e9e8b8cc5ec", GenerateName:"", Namespace:"init-container-2093", SelfLink:"", UID:"709705d7-556f-42a7-813e-71b3d720b3f8", ResourceVersion:"65214", Generation:0, CreationTimestamp:time.Date(2023, time.April, 18, 1, 25, 11, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"362072653"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"125b35d8827fc8a85729384eb49766b7359f2e4890fd4fba10530b04bab4d77b", "cni.projectcalico.org/podIP":"10.2.212.69/32", "cni.projectcalico.org/podIPs":"10.2.212.69/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 18, 1, 25, 11, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0019ef2d8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 18, 1, 25, 11, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0019ef338), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 18, 1, 25, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0019ef398), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-sxg58", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0039b8c20), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-sxg58", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-sxg58", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-sxg58", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004a373e8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-14-154", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000b0a230), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004a37480)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004a374a0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004a374a8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004a374ac), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0017b9ad0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 18, 1, 25, 11, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 18, 1, 25, 11, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 18, 1, 25, 11, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 18, 1, 25, 11, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.14.154", PodIP:"10.2.212.69", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.2.212.69"}}, StartTime:time.Date(2023, time.April, 18, 1, 25, 11, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000b0a310)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000b0a380)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://b3714245d63acbd5cdf485be2c28ec014be0f96dee0368a027710e85cfb9d845", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0039b8ca0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0039b8c80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004a3752f), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Apr 18 01:25:55.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-2093" for this suite. @ 04/18/23 01:25:55.209
• [43.883 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 04/18/23 01:25:55.216
  Apr 18 01:25:55.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename replicaset @ 04/18/23 01:25:55.217
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:25:55.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:25:55.241
  Apr 18 01:25:55.244: INFO: Creating ReplicaSet my-hostname-basic-489b4914-bbc3-4648-a867-69cbbe9c7e04
  Apr 18 01:25:55.253: INFO: Pod name my-hostname-basic-489b4914-bbc3-4648-a867-69cbbe9c7e04: Found 0 pods out of 1
  E0418 01:25:55.868514      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:56.869507      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:57.869662      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:58.869781      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:25:59.870615      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:26:00.258: INFO: Pod name my-hostname-basic-489b4914-bbc3-4648-a867-69cbbe9c7e04: Found 1 pods out of 1
  Apr 18 01:26:00.258: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-489b4914-bbc3-4648-a867-69cbbe9c7e04" is running
  Apr 18 01:26:00.271: INFO: Pod "my-hostname-basic-489b4914-bbc3-4648-a867-69cbbe9c7e04-s8ghx" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-18 01:25:55 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-18 01:25:56 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-18 01:25:56 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-18 01:25:55 +0000 UTC Reason: Message:}])
  Apr 18 01:26:00.271: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 04/18/23 01:26:00.271
  Apr 18 01:26:00.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4945" for this suite. @ 04/18/23 01:26:00.294
• [5.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 04/18/23 01:26:00.306
  Apr 18 01:26:00.306: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename resourcequota @ 04/18/23 01:26:00.307
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:26:00.336
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:26:00.345
  STEP: Creating a ResourceQuota with terminating scope @ 04/18/23 01:26:00.351
  STEP: Ensuring ResourceQuota status is calculated @ 04/18/23 01:26:00.359
  E0418 01:26:00.870893      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:01.870851      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not terminating scope @ 04/18/23 01:26:02.378
  STEP: Ensuring ResourceQuota status is calculated @ 04/18/23 01:26:02.42
  E0418 01:26:02.871061      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:03.871185      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a long running pod @ 04/18/23 01:26:04.424
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 04/18/23 01:26:04.441
  E0418 01:26:04.871335      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:05.871695      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 04/18/23 01:26:06.443
  E0418 01:26:06.872578      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:07.872697      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 04/18/23 01:26:08.449
  STEP: Ensuring resource quota status released the pod usage @ 04/18/23 01:26:08.482
  E0418 01:26:08.873192      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:09.873646      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a terminating pod @ 04/18/23 01:26:10.487
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 04/18/23 01:26:10.497
  E0418 01:26:10.873880      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:11.874075      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 04/18/23 01:26:12.501
  E0418 01:26:12.875065      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:13.875258      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 04/18/23 01:26:14.507
  STEP: Ensuring resource quota status released the pod usage @ 04/18/23 01:26:14.519
  E0418 01:26:14.875820      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:15.876327      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:26:16.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1320" for this suite. @ 04/18/23 01:26:16.526
• [16.226 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 04/18/23 01:26:16.536
  Apr 18 01:26:16.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename daemonsets @ 04/18/23 01:26:16.537
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:26:16.574
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:26:16.58
  Apr 18 01:26:16.614: INFO: Create a RollingUpdate DaemonSet
  Apr 18 01:26:16.621: INFO: Check that daemon pods launch on every node of the cluster
  Apr 18 01:26:16.630: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 01:26:16.641: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 01:26:16.641: INFO: Node ip-10-0-14-154 is running 0 daemon pod, expected 1
  E0418 01:26:16.876802      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:26:17.645: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 01:26:17.647: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 01:26:17.647: INFO: Node ip-10-0-14-154 is running 0 daemon pod, expected 1
  E0418 01:26:17.877208      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:26:18.648: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 01:26:18.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 18 01:26:18.652: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  Apr 18 01:26:18.652: INFO: Update the DaemonSet to trigger a rollout
  Apr 18 01:26:18.660: INFO: Updating DaemonSet daemon-set
  E0418 01:26:18.877896      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:19.878380      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:20.879403      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:26:21.701: INFO: Roll back the DaemonSet before rollout is complete
  Apr 18 01:26:21.739: INFO: Updating DaemonSet daemon-set
  Apr 18 01:26:21.739: INFO: Make sure DaemonSet rollback is complete
  Apr 18 01:26:21.751: INFO: Wrong image for pod: daemon-set-lsx42. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Apr 18 01:26:21.751: INFO: Pod daemon-set-lsx42 is not available
  Apr 18 01:26:21.765: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0418 01:26:21.880181      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:26:22.775: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0418 01:26:22.880296      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:26:23.772: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0418 01:26:23.880330      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:26:24.770: INFO: Pod daemon-set-mh6l8 is not available
  Apr 18 01:26:24.774: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Deleting DaemonSet "daemon-set" @ 04/18/23 01:26:24.785
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2782, will wait for the garbage collector to delete the pods @ 04/18/23 01:26:24.785
  Apr 18 01:26:24.850: INFO: Deleting DaemonSet.extensions daemon-set took: 8.805857ms
  E0418 01:26:24.880447      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:26:24.951: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.947153ms
  E0418 01:26:25.881303      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:26:26.555: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 01:26:26.555: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 18 01:26:26.558: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"65464"},"items":null}

  Apr 18 01:26:26.561: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"65464"},"items":null}

  Apr 18 01:26:26.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-2782" for this suite. @ 04/18/23 01:26:26.573
• [10.044 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 04/18/23 01:26:26.58
  Apr 18 01:26:26.580: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename downward-api @ 04/18/23 01:26:26.581
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:26:26.606
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:26:26.611
  STEP: Creating the pod @ 04/18/23 01:26:26.615
  E0418 01:26:26.882094      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:27.882316      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:28.885559      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:26:29.150: INFO: Successfully updated pod "labelsupdateb76121b0-a85a-4be6-9ee0-4d15a5b7878c"
  E0418 01:26:29.885701      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:30.886679      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:31.886885      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:32.887248      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:26:33.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2826" for this suite. @ 04/18/23 01:26:33.18
• [6.607 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 04/18/23 01:26:33.189
  Apr 18 01:26:33.189: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename secrets @ 04/18/23 01:26:33.19
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:26:33.215
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:26:33.227
  STEP: Creating projection with secret that has name secret-emptykey-test-57925159-23d4-43f2-8f9d-8d162f5a959c @ 04/18/23 01:26:33.232
  Apr 18 01:26:33.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-952" for this suite. @ 04/18/23 01:26:33.245
• [0.062 seconds]
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 04/18/23 01:26:33.252
  Apr 18 01:26:33.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename emptydir @ 04/18/23 01:26:33.253
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:26:33.273
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:26:33.278
  STEP: Creating a pod to test emptydir volume type on node default medium @ 04/18/23 01:26:33.283
  E0418 01:26:33.887352      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:34.888168      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:35.888506      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:36.889425      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:26:37.309
  Apr 18 01:26:37.314: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-5d12bd53-604d-4f1f-b63c-26182407c2d2 container test-container: <nil>
  STEP: delete the pod @ 04/18/23 01:26:37.321
  Apr 18 01:26:37.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8460" for this suite. @ 04/18/23 01:26:37.343
• [4.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 04/18/23 01:26:37.353
  Apr 18 01:26:37.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename deployment @ 04/18/23 01:26:37.354
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:26:37.383
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:26:37.388
  Apr 18 01:26:37.404: INFO: Pod name rollover-pod: Found 0 pods out of 1
  E0418 01:26:37.890077      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:38.890182      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:39.890304      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:40.894538      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:41.895048      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:26:42.412: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/18/23 01:26:42.412
  Apr 18 01:26:42.412: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  E0418 01:26:42.896015      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:43.896115      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:26:44.416: INFO: Creating deployment "test-rollover-deployment"
  Apr 18 01:26:44.425: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  E0418 01:26:44.896205      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:45.897165      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:26:46.431: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Apr 18 01:26:46.436: INFO: Ensure that both replica sets have 1 created replica
  Apr 18 01:26:46.443: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Apr 18 01:26:46.456: INFO: Updating deployment test-rollover-deployment
  Apr 18 01:26:46.456: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  E0418 01:26:46.897787      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:47.897905      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:26:48.471: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Apr 18 01:26:48.481: INFO: Make sure deployment "test-rollover-deployment" is complete
  Apr 18 01:26:48.488: INFO: all replica sets need to contain the pod-template-hash label
  Apr 18 01:26:48.488: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 1, 26, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 1, 26, 44, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 1, 26, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 1, 26, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0418 01:26:48.899603      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:49.898801      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:26:50.497: INFO: all replica sets need to contain the pod-template-hash label
  Apr 18 01:26:50.497: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 1, 26, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 1, 26, 44, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 1, 26, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 1, 26, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0418 01:26:50.899826      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:51.899912      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:26:52.500: INFO: all replica sets need to contain the pod-template-hash label
  Apr 18 01:26:52.500: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 1, 26, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 1, 26, 44, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 1, 26, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 1, 26, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0418 01:26:52.900021      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:53.900148      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:26:54.494: INFO: all replica sets need to contain the pod-template-hash label
  Apr 18 01:26:54.494: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 1, 26, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 1, 26, 44, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 1, 26, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 1, 26, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0418 01:26:54.900886      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:55.900983      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:26:56.495: INFO: all replica sets need to contain the pod-template-hash label
  Apr 18 01:26:56.496: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 1, 26, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 1, 26, 44, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 1, 26, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 1, 26, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0418 01:26:56.901994      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:57.902976      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:26:58.496: INFO: 
  Apr 18 01:26:58.496: INFO: Ensure that both old replica sets have no replicas
  Apr 18 01:26:58.506: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-2697  7f0161af-77d4-4b1f-a055-6268c3c07e49 65704 2 2023-04-18 01:26:44 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-18 01:26:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-18 01:26:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d5c4d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-18 01:26:44 +0000 UTC,LastTransitionTime:2023-04-18 01:26:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-04-18 01:26:58 +0000 UTC,LastTransitionTime:2023-04-18 01:26:44 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 18 01:26:58.509: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-2697  de97fc6b-bf3a-4d8e-b6d3-33ac4848a3db 65694 2 2023-04-18 01:26:46 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 7f0161af-77d4-4b1f-a055-6268c3c07e49 0xc0046708a7 0xc0046708a8}] [] [{kube-controller-manager Update apps/v1 2023-04-18 01:26:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f0161af-77d4-4b1f-a055-6268c3c07e49\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-18 01:26:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004670c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 18 01:26:58.510: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Apr 18 01:26:58.510: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2697  cbccd0dd-fd73-4ac0-aa35-ee83459e45cf 65703 2 2023-04-18 01:26:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 7f0161af-77d4-4b1f-a055-6268c3c07e49 0xc004670407 0xc004670408}] [] [{e2e.test Update apps/v1 2023-04-18 01:26:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-18 01:26:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f0161af-77d4-4b1f-a055-6268c3c07e49\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-18 01:26:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004670638 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 18 01:26:58.510: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-2697  6f062904-f8d2-47bd-805e-de7ee4bb3e6a 65661 2 2023-04-18 01:26:44 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 7f0161af-77d4-4b1f-a055-6268c3c07e49 0xc004670d17 0xc004670d18}] [] [{kube-controller-manager Update apps/v1 2023-04-18 01:26:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f0161af-77d4-4b1f-a055-6268c3c07e49\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-18 01:26:46 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004670f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 18 01:26:58.513: INFO: Pod "test-rollover-deployment-57777854c9-kw4kk" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-kw4kk test-rollover-deployment-57777854c9- deployment-2697  211eb96c-a5e9-48c7-8718-a04ec8bf003d 65676 0 2023-04-18 01:26:46 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[cni.projectcalico.org/containerID:e8054e3a7e2a0fb5669dcca3459285e9f95e39c028230ffafe97f384c3e0c933 cni.projectcalico.org/podIP:10.2.212.76/32 cni.projectcalico.org/podIPs:10.2.212.76/32] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 de97fc6b-bf3a-4d8e-b6d3-33ac4848a3db 0xc00531a187 0xc00531a188}] [] [{kube-controller-manager Update v1 2023-04-18 01:26:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de97fc6b-bf3a-4d8e-b6d3-33ac4848a3db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-18 01:26:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-18 01:26:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.212.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9sphf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9sphf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-14-154,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:26:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:26:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:26:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:26:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.14.154,PodIP:10.2.212.76,StartTime:2023-04-18 01:26:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-18 01:26:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://a66cf5d58762f62247988a180b537e22a02795e16f1a99644444f1caba071ff4,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.212.76,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 01:26:58.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2697" for this suite. @ 04/18/23 01:26:58.517
• [21.170 seconds]
------------------------------
SSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 04/18/23 01:26:58.524
  Apr 18 01:26:58.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename ingress @ 04/18/23 01:26:58.525
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:26:58.546
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:26:58.552
  STEP: getting /apis @ 04/18/23 01:26:58.556
  STEP: getting /apis/networking.k8s.io @ 04/18/23 01:26:58.561
  STEP: getting /apis/networking.k8s.iov1 @ 04/18/23 01:26:58.563
  STEP: creating @ 04/18/23 01:26:58.564
  STEP: getting @ 04/18/23 01:26:58.59
  STEP: listing @ 04/18/23 01:26:58.593
  STEP: watching @ 04/18/23 01:26:58.596
  Apr 18 01:26:58.596: INFO: starting watch
  STEP: cluster-wide listing @ 04/18/23 01:26:58.598
  STEP: cluster-wide watching @ 04/18/23 01:26:58.601
  Apr 18 01:26:58.601: INFO: starting watch
  STEP: patching @ 04/18/23 01:26:58.603
  STEP: updating @ 04/18/23 01:26:58.667
  Apr 18 01:26:58.675: INFO: waiting for watch events with expected annotations
  Apr 18 01:26:58.675: INFO: saw patched and updated annotations
  STEP: patching /status @ 04/18/23 01:26:58.675
  STEP: updating /status @ 04/18/23 01:26:58.681
  STEP: get /status @ 04/18/23 01:26:58.689
  STEP: deleting @ 04/18/23 01:26:58.693
  STEP: deleting a collection @ 04/18/23 01:26:58.704
  Apr 18 01:26:58.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-9567" for this suite. @ 04/18/23 01:26:58.74
• [0.223 seconds]
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 04/18/23 01:26:58.748
  Apr 18 01:26:58.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename security-context-test @ 04/18/23 01:26:58.751
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:26:58.802
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:26:58.825
  E0418 01:26:58.903889      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:26:59.904000      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:00.904327      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:01.904504      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:02.904612      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:03.904730      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:04.905083      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:27:04.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-8687" for this suite. @ 04/18/23 01:27:04.947
• [6.208 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 04/18/23 01:27:04.956
  Apr 18 01:27:04.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename prestop @ 04/18/23 01:27:04.957
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:27:04.983
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:27:04.987
  STEP: Creating server pod server in namespace prestop-562 @ 04/18/23 01:27:04.992
  STEP: Waiting for pods to come up. @ 04/18/23 01:27:05.01
  E0418 01:27:05.905945      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:06.906367      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating tester pod tester in namespace prestop-562 @ 04/18/23 01:27:07.041
  E0418 01:27:07.906345      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:08.906480      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pre-stop pod @ 04/18/23 01:27:09.057
  E0418 01:27:09.907048      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:10.907185      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:11.907350      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:12.907461      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:13.907701      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:27:14.068: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Apr 18 01:27:14.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 04/18/23 01:27:14.073
  STEP: Destroying namespace "prestop-562" for this suite. @ 04/18/23 01:27:14.084
• [9.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 04/18/23 01:27:14.097
  Apr 18 01:27:14.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename pods @ 04/18/23 01:27:14.098
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:27:14.117
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:27:14.127
  Apr 18 01:27:14.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: creating the pod @ 04/18/23 01:27:14.134
  STEP: submitting the pod to kubernetes @ 04/18/23 01:27:14.134
  E0418 01:27:14.907826      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:15.907899      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:27:16.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5214" for this suite. @ 04/18/23 01:27:16.296
• [2.209 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 04/18/23 01:27:16.308
  Apr 18 01:27:16.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename kubelet-test @ 04/18/23 01:27:16.309
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:27:16.385
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:27:16.399
  STEP: Waiting for pod completion @ 04/18/23 01:27:16.428
  E0418 01:27:16.908021      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:17.908155      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:18.908796      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:19.908903      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:27:20.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-2972" for this suite. @ 04/18/23 01:27:20.456
• [4.153 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 04/18/23 01:27:20.462
  Apr 18 01:27:20.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename subpath @ 04/18/23 01:27:20.464
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:27:20.479
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:27:20.486
  STEP: Setting up data @ 04/18/23 01:27:20.491
  STEP: Creating pod pod-subpath-test-secret-v272 @ 04/18/23 01:27:20.499
  STEP: Creating a pod to test atomic-volume-subpath @ 04/18/23 01:27:20.499
  E0418 01:27:20.909445      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:21.909574      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:22.910545      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:23.910651      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:24.913004      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:25.911344      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:26.912248      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:27.912385      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:28.912840      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:29.913384      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:30.914195      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:31.914312      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:32.914990      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:33.915157      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:34.916104      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:35.916242      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:36.916394      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:37.916514      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:38.916704      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:39.916843      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:40.917875      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:41.918070      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:42.918095      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:43.918201      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:27:44.572
  Apr 18 01:27:44.575: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-subpath-test-secret-v272 container test-container-subpath-secret-v272: <nil>
  STEP: delete the pod @ 04/18/23 01:27:44.581
  STEP: Deleting pod pod-subpath-test-secret-v272 @ 04/18/23 01:27:44.594
  Apr 18 01:27:44.594: INFO: Deleting pod "pod-subpath-test-secret-v272" in namespace "subpath-8947"
  Apr 18 01:27:44.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8947" for this suite. @ 04/18/23 01:27:44.601
• [24.154 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 04/18/23 01:27:44.62
  Apr 18 01:27:44.621: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename svcaccounts @ 04/18/23 01:27:44.622
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:27:44.664
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:27:44.67
  STEP: Creating a pod to test service account token:  @ 04/18/23 01:27:44.676
  E0418 01:27:44.918720      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:45.918894      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:46.919985      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:47.920083      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:27:48.713
  Apr 18 01:27:48.716: INFO: Trying to get logs from node ip-10-0-14-154 pod test-pod-c33774c3-b0d9-47c4-85fc-d39eaa9069b2 container agnhost-container: <nil>
  STEP: delete the pod @ 04/18/23 01:27:48.727
  Apr 18 01:27:48.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4541" for this suite. @ 04/18/23 01:27:48.764
• [4.166 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 04/18/23 01:27:48.8
  Apr 18 01:27:48.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename services @ 04/18/23 01:27:48.806
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:27:48.836
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:27:48.844
  STEP: creating service in namespace services-9880 @ 04/18/23 01:27:48.848
  STEP: creating service affinity-clusterip in namespace services-9880 @ 04/18/23 01:27:48.849
  STEP: creating replication controller affinity-clusterip in namespace services-9880 @ 04/18/23 01:27:48.884
  I0418 01:27:48.896491      21 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-9880, replica count: 3
  E0418 01:27:48.941401      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:49.941494      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:50.941813      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:51.941884      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0418 01:27:52.022177      21 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 18 01:27:52.029: INFO: Creating new exec pod
  E0418 01:27:52.942003      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:53.942234      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:54.942250      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:27:55.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-9880 exec execpod-affinitycwc8q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Apr 18 01:27:55.194: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Apr 18 01:27:55.194: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 18 01:27:55.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-9880 exec execpod-affinitycwc8q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.3.45.113 80'
  Apr 18 01:27:55.370: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.3.45.113 80\nConnection to 10.3.45.113 80 port [tcp/http] succeeded!\n"
  Apr 18 01:27:55.370: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 18 01:27:55.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-9880 exec execpod-affinitycwc8q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.3.45.113:80/ ; done'
  Apr 18 01:27:55.687: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.45.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.45.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.45.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.45.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.45.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.45.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.45.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.45.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.45.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.45.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.45.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.45.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.45.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.45.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.45.113:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.3.45.113:80/\n"
  Apr 18 01:27:55.687: INFO: stdout: "\naffinity-clusterip-dfp9h\naffinity-clusterip-dfp9h\naffinity-clusterip-dfp9h\naffinity-clusterip-dfp9h\naffinity-clusterip-dfp9h\naffinity-clusterip-dfp9h\naffinity-clusterip-dfp9h\naffinity-clusterip-dfp9h\naffinity-clusterip-dfp9h\naffinity-clusterip-dfp9h\naffinity-clusterip-dfp9h\naffinity-clusterip-dfp9h\naffinity-clusterip-dfp9h\naffinity-clusterip-dfp9h\naffinity-clusterip-dfp9h\naffinity-clusterip-dfp9h"
  Apr 18 01:27:55.687: INFO: Received response from host: affinity-clusterip-dfp9h
  Apr 18 01:27:55.687: INFO: Received response from host: affinity-clusterip-dfp9h
  Apr 18 01:27:55.687: INFO: Received response from host: affinity-clusterip-dfp9h
  Apr 18 01:27:55.687: INFO: Received response from host: affinity-clusterip-dfp9h
  Apr 18 01:27:55.687: INFO: Received response from host: affinity-clusterip-dfp9h
  Apr 18 01:27:55.687: INFO: Received response from host: affinity-clusterip-dfp9h
  Apr 18 01:27:55.687: INFO: Received response from host: affinity-clusterip-dfp9h
  Apr 18 01:27:55.687: INFO: Received response from host: affinity-clusterip-dfp9h
  Apr 18 01:27:55.687: INFO: Received response from host: affinity-clusterip-dfp9h
  Apr 18 01:27:55.687: INFO: Received response from host: affinity-clusterip-dfp9h
  Apr 18 01:27:55.687: INFO: Received response from host: affinity-clusterip-dfp9h
  Apr 18 01:27:55.687: INFO: Received response from host: affinity-clusterip-dfp9h
  Apr 18 01:27:55.687: INFO: Received response from host: affinity-clusterip-dfp9h
  Apr 18 01:27:55.688: INFO: Received response from host: affinity-clusterip-dfp9h
  Apr 18 01:27:55.688: INFO: Received response from host: affinity-clusterip-dfp9h
  Apr 18 01:27:55.688: INFO: Received response from host: affinity-clusterip-dfp9h
  Apr 18 01:27:55.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 18 01:27:55.691: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-9880, will wait for the garbage collector to delete the pods @ 04/18/23 01:27:55.705
  Apr 18 01:27:55.770: INFO: Deleting ReplicationController affinity-clusterip took: 6.927522ms
  Apr 18 01:27:55.871: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.632202ms
  E0418 01:27:55.942817      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:56.943765      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-9880" for this suite. @ 04/18/23 01:27:57.79
• [9.001 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 04/18/23 01:27:57.803
  Apr 18 01:27:57.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename sched-preemption @ 04/18/23 01:27:57.804
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:27:57.826
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:27:57.831
  Apr 18 01:27:57.847: INFO: Waiting up to 1m0s for all nodes to be ready
  E0418 01:27:57.944038      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:58.944259      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:27:59.944960      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:00.945777      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:01.946244      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:02.946990      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:03.947526      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:04.947862      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:05.947940      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:06.948052      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:07.948673      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:08.948771      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:09.949726      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:10.949889      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:11.950310      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:12.950977      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:13.951974      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:14.952119      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:15.952511      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:16.952889      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:17.953470      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:18.954017      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:19.954530      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:20.954710      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:21.955893      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:22.956293      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:23.956769      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:24.956899      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:25.956952      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:26.957165      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:27.957487      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:28.957902      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:29.958637      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:30.958781      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:31.959698      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:32.959817      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:33.960654      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:34.961495      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:35.961957      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:36.962073      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:37.962937      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:38.963148      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:39.963398      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:40.963428      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:41.964440      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:42.964613      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:43.965021      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:44.965311      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:45.965401      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:46.965686      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:47.966831      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:48.967110      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:49.967299      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:50.967442      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:51.968332      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:52.968461      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:53.969238      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:54.970561      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:55.970585      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:56.970886      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:28:57.876: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 04/18/23 01:28:57.879
  Apr 18 01:28:57.879: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename sched-preemption-path @ 04/18/23 01:28:57.881
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:28:57.909
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:28:57.925
  Apr 18 01:28:57.963: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  E0418 01:28:57.971166      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:28:57.971: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Apr 18 01:28:58.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 18 01:28:58.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-8504" for this suite. @ 04/18/23 01:28:58.105
  STEP: Destroying namespace "sched-preemption-7469" for this suite. @ 04/18/23 01:28:58.129
• [60.342 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 04/18/23 01:28:58.145
  Apr 18 01:28:58.145: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename container-runtime @ 04/18/23 01:28:58.146
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:28:58.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:28:58.177
  STEP: create the container @ 04/18/23 01:28:58.18
  W0418 01:28:58.186563      21 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/18/23 01:28:58.186
  E0418 01:28:58.971337      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:28:59.971468      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:00.971909      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 04/18/23 01:29:01.209
  STEP: the container should be terminated @ 04/18/23 01:29:01.212
  STEP: the termination message should be set @ 04/18/23 01:29:01.212
  Apr 18 01:29:01.213: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 04/18/23 01:29:01.213
  Apr 18 01:29:01.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-2124" for this suite. @ 04/18/23 01:29:01.232
• [3.094 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 04/18/23 01:29:01.24
  Apr 18 01:29:01.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename secrets @ 04/18/23 01:29:01.241
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:29:01.275
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:29:01.281
  STEP: Creating secret with name secret-test-map-d59bdc24-1039-4605-9919-0eaa5d00b425 @ 04/18/23 01:29:01.286
  STEP: Creating a pod to test consume secrets @ 04/18/23 01:29:01.291
  E0418 01:29:01.972000      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:02.972136      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:03.972271      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:04.972782      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:29:05.357
  Apr 18 01:29:05.361: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-secrets-564fd9de-4e2a-413b-9d65-b5e252511d6e container secret-volume-test: <nil>
  STEP: delete the pod @ 04/18/23 01:29:05.366
  Apr 18 01:29:05.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9058" for this suite. @ 04/18/23 01:29:05.385
• [4.152 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 04/18/23 01:29:05.393
  Apr 18 01:29:05.394: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/18/23 01:29:05.394
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:29:05.419
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:29:05.424
  Apr 18 01:29:05.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  E0418 01:29:05.972814      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:06.972905      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:07.973782      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:08.974217      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:09.974781      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:10.975360      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:29:11.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-1554" for this suite. @ 04/18/23 01:29:11.468
• [6.079 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 04/18/23 01:29:11.478
  Apr 18 01:29:11.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename downward-api @ 04/18/23 01:29:11.479
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:29:11.496
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:29:11.5
  STEP: Creating a pod to test downward api env vars @ 04/18/23 01:29:11.504
  E0418 01:29:11.977248      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:12.977973      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:13.979281      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:14.979352      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:29:15.524
  Apr 18 01:29:15.527: INFO: Trying to get logs from node ip-10-0-14-154 pod downward-api-debfde69-c70d-4c16-ab86-bf922544a773 container dapi-container: <nil>
  STEP: delete the pod @ 04/18/23 01:29:15.533
  Apr 18 01:29:15.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1361" for this suite. @ 04/18/23 01:29:15.551
• [4.084 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 04/18/23 01:29:15.563
  Apr 18 01:29:15.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename webhook @ 04/18/23 01:29:15.565
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:29:15.584
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:29:15.59
  STEP: Setting up server cert @ 04/18/23 01:29:15.63
  E0418 01:29:15.980174      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/18/23 01:29:16.343
  STEP: Deploying the webhook pod @ 04/18/23 01:29:16.355
  STEP: Wait for the deployment to be ready @ 04/18/23 01:29:16.368
  Apr 18 01:29:16.380: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0418 01:29:16.980944      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:17.981192      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/18/23 01:29:18.393
  STEP: Verifying the service has paired with the endpoint @ 04/18/23 01:29:18.413
  E0418 01:29:18.981989      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:29:19.413: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 04/18/23 01:29:19.417
  STEP: create a pod that should be denied by the webhook @ 04/18/23 01:29:19.432
  STEP: create a pod that causes the webhook to hang @ 04/18/23 01:29:19.444
  E0418 01:29:19.982724      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:20.982898      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:21.983133      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:22.983255      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:23.983398      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:24.983492      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:25.983662      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:26.984828      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:27.984866      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:28.985863      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 04/18/23 01:29:29.452
  STEP: create a configmap that should be admitted by the webhook @ 04/18/23 01:29:29.483
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 04/18/23 01:29:29.5
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 04/18/23 01:29:29.521
  STEP: create a namespace that bypass the webhook @ 04/18/23 01:29:29.553
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 04/18/23 01:29:29.62
  Apr 18 01:29:29.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8151" for this suite. @ 04/18/23 01:29:29.852
  STEP: Destroying namespace "webhook-markers-9406" for this suite. @ 04/18/23 01:29:29.926
  STEP: Destroying namespace "exempted-namespace-5713" for this suite. @ 04/18/23 01:29:29.951
• [14.405 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 04/18/23 01:29:29.969
  Apr 18 01:29:29.969: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 01:29:29.97
  E0418 01:29:29.986930      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:29:30.031
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:29:30.045
  STEP: Creating the pod @ 04/18/23 01:29:30.067
  E0418 01:29:30.987617      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:31.987863      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:29:32.669: INFO: Successfully updated pod "labelsupdate748cabba-1351-463b-9404-e438d5144141"
  E0418 01:29:32.988178      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:33.988255      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:29:34.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8345" for this suite. @ 04/18/23 01:29:34.693
• [4.735 seconds]
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 04/18/23 01:29:34.704
  Apr 18 01:29:34.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename pod-network-test @ 04/18/23 01:29:34.705
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:29:34.74
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:29:34.746
  STEP: Performing setup for networking test in namespace pod-network-test-3559 @ 04/18/23 01:29:34.75
  STEP: creating a selector @ 04/18/23 01:29:34.75
  STEP: Creating the service pods in kubernetes @ 04/18/23 01:29:34.75
  Apr 18 01:29:34.750: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0418 01:29:34.988631      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:35.988883      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:36.989959      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:37.990971      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:38.991628      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:39.991972      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:40.992416      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:41.992626      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:42.992876      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:43.992912      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:44.993008      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:45.994013      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 04/18/23 01:29:46.836
  E0418 01:29:46.994826      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:47.994933      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:29:48.881: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  Apr 18 01:29:48.881: INFO: Going to poll 10.2.212.91 on port 8083 at least 0 times, with a maximum of 34 tries before failing
  Apr 18 01:29:48.884: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.2.212.91:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3559 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 01:29:48.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 01:29:48.886: INFO: ExecWithOptions: Clientset creation
  Apr 18 01:29:48.886: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-3559/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.2.212.91%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 18 01:29:48.963: INFO: Found all 1 expected endpoints: [netserver-0]
  Apr 18 01:29:48.963: INFO: Going to poll 10.2.129.177 on port 8083 at least 0 times, with a maximum of 34 tries before failing
  Apr 18 01:29:48.966: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.2.129.177:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3559 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 01:29:48.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 01:29:48.967: INFO: ExecWithOptions: Clientset creation
  Apr 18 01:29:48.967: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/pod-network-test-3559/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.2.129.177%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0418 01:29:48.995251      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:29:49.029: INFO: Found all 1 expected endpoints: [netserver-1]
  Apr 18 01:29:49.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-3559" for this suite. @ 04/18/23 01:29:49.033
• [14.335 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 04/18/23 01:29:49.04
  Apr 18 01:29:49.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename webhook @ 04/18/23 01:29:49.041
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:29:49.057
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:29:49.065
  STEP: Setting up server cert @ 04/18/23 01:29:49.093
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/18/23 01:29:49.708
  STEP: Deploying the webhook pod @ 04/18/23 01:29:49.715
  STEP: Wait for the deployment to be ready @ 04/18/23 01:29:49.73
  Apr 18 01:29:49.738: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0418 01:29:49.996324      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:50.996888      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/18/23 01:29:51.748
  STEP: Verifying the service has paired with the endpoint @ 04/18/23 01:29:51.758
  E0418 01:29:51.997149      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:29:52.758: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 04/18/23 01:29:52.762
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 04/18/23 01:29:52.776
  STEP: Creating a dummy validating-webhook-configuration object @ 04/18/23 01:29:52.791
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 04/18/23 01:29:52.799
  STEP: Creating a dummy mutating-webhook-configuration object @ 04/18/23 01:29:52.817
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 04/18/23 01:29:52.854
  Apr 18 01:29:52.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4935" for this suite. @ 04/18/23 01:29:52.949
  STEP: Destroying namespace "webhook-markers-2703" for this suite. @ 04/18/23 01:29:52.986
• [3.952 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 04/18/23 01:29:52.994
  Apr 18 01:29:52.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/18/23 01:29:52.996
  E0418 01:29:52.997185      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:29:53.023
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:29:53.034
  Apr 18 01:29:53.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  E0418 01:29:53.998198      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:29:54.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-3774" for this suite. @ 04/18/23 01:29:54.085
• [1.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 04/18/23 01:29:54.131
  Apr 18 01:29:54.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename crd-webhook @ 04/18/23 01:29:54.15
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:29:54.187
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:29:54.196
  STEP: Setting up server cert @ 04/18/23 01:29:54.221
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 04/18/23 01:29:54.836
  STEP: Deploying the custom resource conversion webhook pod @ 04/18/23 01:29:54.842
  STEP: Wait for the deployment to be ready @ 04/18/23 01:29:54.855
  Apr 18 01:29:54.865: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0418 01:29:54.999169      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:55.999960      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/18/23 01:29:56.874
  STEP: Verifying the service has paired with the endpoint @ 04/18/23 01:29:56.888
  E0418 01:29:57.000507      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:29:57.888: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Apr 18 01:29:57.892: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  E0418 01:29:58.001142      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:29:59.001118      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:00.001217      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 04/18/23 01:30:00.446
  STEP: v2 custom resource should be converted @ 04/18/23 01:30:00.453
  Apr 18 01:30:00.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0418 01:30:01.001916      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-6426" for this suite. @ 04/18/23 01:30:01.305
• [7.262 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 04/18/23 01:30:01.39
  Apr 18 01:30:01.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename emptydir-wrapper @ 04/18/23 01:30:01.391
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:30:01.457
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:30:01.472
  STEP: Creating 50 configmaps @ 04/18/23 01:30:01.485
  E0418 01:30:02.002740      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating RC which spawns configmap-volume pods @ 04/18/23 01:30:02.863
  Apr 18 01:30:02.892: INFO: Pod name wrapped-volume-race-f4a68e27-02d0-4291-a246-20d9c0aba551: Found 0 pods out of 5
  E0418 01:30:03.003558      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:04.004485      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:05.004634      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:06.005141      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:07.005270      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:30:07.899: INFO: Pod name wrapped-volume-race-f4a68e27-02d0-4291-a246-20d9c0aba551: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/18/23 01:30:07.899
  STEP: Creating RC which spawns configmap-volume pods @ 04/18/23 01:30:07.92
  Apr 18 01:30:07.934: INFO: Pod name wrapped-volume-race-a830be86-45e3-48d0-a06d-88b508bf6282: Found 0 pods out of 5
  E0418 01:30:08.005999      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:09.006445      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:10.009376      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:11.009977      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:12.010074      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:30:12.942: INFO: Pod name wrapped-volume-race-a830be86-45e3-48d0-a06d-88b508bf6282: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/18/23 01:30:12.942
  STEP: Creating RC which spawns configmap-volume pods @ 04/18/23 01:30:12.971
  Apr 18 01:30:12.998: INFO: Pod name wrapped-volume-race-f67c5f08-76e3-49ae-b9b0-9a3bf3abec3e: Found 0 pods out of 5
  E0418 01:30:13.010443      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:14.010553      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:15.010784      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:16.010898      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:17.011096      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:30:18.009: INFO: Pod name wrapped-volume-race-f67c5f08-76e3-49ae-b9b0-9a3bf3abec3e: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/18/23 01:30:18.009
  E0418 01:30:18.011626      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:30:18.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-f67c5f08-76e3-49ae-b9b0-9a3bf3abec3e in namespace emptydir-wrapper-157, will wait for the garbage collector to delete the pods @ 04/18/23 01:30:18.033
  Apr 18 01:30:18.095: INFO: Deleting ReplicationController wrapped-volume-race-f67c5f08-76e3-49ae-b9b0-9a3bf3abec3e took: 6.329701ms
  Apr 18 01:30:18.196: INFO: Terminating ReplicationController wrapped-volume-race-f67c5f08-76e3-49ae-b9b0-9a3bf3abec3e pods took: 100.675775ms
  E0418 01:30:19.012812      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:20.013137      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-a830be86-45e3-48d0-a06d-88b508bf6282 in namespace emptydir-wrapper-157, will wait for the garbage collector to delete the pods @ 04/18/23 01:30:20.497
  Apr 18 01:30:20.561: INFO: Deleting ReplicationController wrapped-volume-race-a830be86-45e3-48d0-a06d-88b508bf6282 took: 7.483324ms
  Apr 18 01:30:20.665: INFO: Terminating ReplicationController wrapped-volume-race-a830be86-45e3-48d0-a06d-88b508bf6282 pods took: 103.936717ms
  E0418 01:30:21.014168      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:22.017268      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-f4a68e27-02d0-4291-a246-20d9c0aba551 in namespace emptydir-wrapper-157, will wait for the garbage collector to delete the pods @ 04/18/23 01:30:22.665
  Apr 18 01:30:22.760: INFO: Deleting ReplicationController wrapped-volume-race-f4a68e27-02d0-4291-a246-20d9c0aba551 took: 22.782566ms
  Apr 18 01:30:22.861: INFO: Terminating ReplicationController wrapped-volume-race-f4a68e27-02d0-4291-a246-20d9c0aba551 pods took: 100.861922ms
  E0418 01:30:23.017819      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:24.018539      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:25.019435      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 04/18/23 01:30:25.362
  STEP: Destroying namespace "emptydir-wrapper-157" for this suite. @ 04/18/23 01:30:25.573
• [24.187 seconds]
------------------------------
S
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 04/18/23 01:30:25.578
  Apr 18 01:30:25.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename services @ 04/18/23 01:30:25.579
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:30:25.598
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:30:25.603
  STEP: fetching services @ 04/18/23 01:30:25.606
  Apr 18 01:30:25.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8301" for this suite. @ 04/18/23 01:30:25.613
• [0.040 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 04/18/23 01:30:25.619
  Apr 18 01:30:25.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename gc @ 04/18/23 01:30:25.62
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:30:25.643
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:30:25.647
  STEP: create the rc @ 04/18/23 01:30:25.661
  W0418 01:30:25.666316      21 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0418 01:30:26.020193      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:27.020345      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:28.020452      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:29.020603      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 04/18/23 01:30:29.699
  STEP: wait for the rc to be deleted @ 04/18/23 01:30:29.746
  E0418 01:30:30.021236      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:31.022300      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:32.023041      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:33.026389      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:34.035312      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 04/18/23 01:30:34.766
  E0418 01:30:35.037644      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:36.046202      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:37.050304      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:38.053438      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:39.080073      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:40.079322      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:41.082508      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:42.082992      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:43.083980      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:44.084442      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:45.084486      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:46.084888      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:47.085962      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:48.086318      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:49.087347      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:50.087564      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:51.087887      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:52.088600      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:53.089095      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:54.089178      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:55.089266      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:56.089394      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:57.090413      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:58.090578      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:30:59.090683      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:00.090917      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:01.091500      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:02.093969      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:03.094815      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:04.094899      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 04/18/23 01:31:04.784
  Apr 18 01:31:04.985: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 18 01:31:04.985: INFO: Deleting pod "simpletest.rc-26x7r" in namespace "gc-610"
  Apr 18 01:31:05.002: INFO: Deleting pod "simpletest.rc-294sr" in namespace "gc-610"
  Apr 18 01:31:05.042: INFO: Deleting pod "simpletest.rc-2dmhs" in namespace "gc-610"
  Apr 18 01:31:05.059: INFO: Deleting pod "simpletest.rc-2nzpc" in namespace "gc-610"
  Apr 18 01:31:05.080: INFO: Deleting pod "simpletest.rc-2xp6x" in namespace "gc-610"
  E0418 01:31:05.100316      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:31:05.147: INFO: Deleting pod "simpletest.rc-4d28r" in namespace "gc-610"
  Apr 18 01:31:05.170: INFO: Deleting pod "simpletest.rc-4h8t2" in namespace "gc-610"
  Apr 18 01:31:05.189: INFO: Deleting pod "simpletest.rc-4hfdx" in namespace "gc-610"
  Apr 18 01:31:05.222: INFO: Deleting pod "simpletest.rc-4mpm9" in namespace "gc-610"
  Apr 18 01:31:05.247: INFO: Deleting pod "simpletest.rc-4sh2z" in namespace "gc-610"
  Apr 18 01:31:05.278: INFO: Deleting pod "simpletest.rc-54v9l" in namespace "gc-610"
  Apr 18 01:31:05.310: INFO: Deleting pod "simpletest.rc-5fgpv" in namespace "gc-610"
  Apr 18 01:31:05.361: INFO: Deleting pod "simpletest.rc-5hvkk" in namespace "gc-610"
  Apr 18 01:31:05.401: INFO: Deleting pod "simpletest.rc-5p9gh" in namespace "gc-610"
  Apr 18 01:31:05.435: INFO: Deleting pod "simpletest.rc-5rmxj" in namespace "gc-610"
  Apr 18 01:31:05.481: INFO: Deleting pod "simpletest.rc-5w9wd" in namespace "gc-610"
  Apr 18 01:31:05.524: INFO: Deleting pod "simpletest.rc-62fs6" in namespace "gc-610"
  Apr 18 01:31:05.559: INFO: Deleting pod "simpletest.rc-64rd4" in namespace "gc-610"
  Apr 18 01:31:05.581: INFO: Deleting pod "simpletest.rc-65thq" in namespace "gc-610"
  Apr 18 01:31:05.608: INFO: Deleting pod "simpletest.rc-6q8qc" in namespace "gc-610"
  Apr 18 01:31:05.659: INFO: Deleting pod "simpletest.rc-6qjl2" in namespace "gc-610"
  Apr 18 01:31:05.700: INFO: Deleting pod "simpletest.rc-6vjcx" in namespace "gc-610"
  Apr 18 01:31:05.742: INFO: Deleting pod "simpletest.rc-6vkvd" in namespace "gc-610"
  Apr 18 01:31:05.781: INFO: Deleting pod "simpletest.rc-7gp6t" in namespace "gc-610"
  Apr 18 01:31:05.821: INFO: Deleting pod "simpletest.rc-7vzjw" in namespace "gc-610"
  Apr 18 01:31:05.846: INFO: Deleting pod "simpletest.rc-8npz8" in namespace "gc-610"
  Apr 18 01:31:05.878: INFO: Deleting pod "simpletest.rc-8xqb5" in namespace "gc-610"
  Apr 18 01:31:05.896: INFO: Deleting pod "simpletest.rc-98f9j" in namespace "gc-610"
  Apr 18 01:31:05.937: INFO: Deleting pod "simpletest.rc-9hdpp" in namespace "gc-610"
  Apr 18 01:31:05.971: INFO: Deleting pod "simpletest.rc-9mv5l" in namespace "gc-610"
  Apr 18 01:31:05.984: INFO: Deleting pod "simpletest.rc-9w9j8" in namespace "gc-610"
  Apr 18 01:31:06.066: INFO: Deleting pod "simpletest.rc-c62zm" in namespace "gc-610"
  Apr 18 01:31:06.084: INFO: Deleting pod "simpletest.rc-c7dlf" in namespace "gc-610"
  E0418 01:31:06.100494      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:31:06.118: INFO: Deleting pod "simpletest.rc-ctrc8" in namespace "gc-610"
  Apr 18 01:31:06.133: INFO: Deleting pod "simpletest.rc-d69m6" in namespace "gc-610"
  Apr 18 01:31:06.173: INFO: Deleting pod "simpletest.rc-d7qkw" in namespace "gc-610"
  Apr 18 01:31:06.216: INFO: Deleting pod "simpletest.rc-dqlmr" in namespace "gc-610"
  Apr 18 01:31:06.272: INFO: Deleting pod "simpletest.rc-drkkh" in namespace "gc-610"
  Apr 18 01:31:06.297: INFO: Deleting pod "simpletest.rc-fqg7j" in namespace "gc-610"
  Apr 18 01:31:06.318: INFO: Deleting pod "simpletest.rc-fvlfq" in namespace "gc-610"
  Apr 18 01:31:06.331: INFO: Deleting pod "simpletest.rc-gn2hk" in namespace "gc-610"
  Apr 18 01:31:06.364: INFO: Deleting pod "simpletest.rc-gv566" in namespace "gc-610"
  Apr 18 01:31:06.382: INFO: Deleting pod "simpletest.rc-h42xv" in namespace "gc-610"
  Apr 18 01:31:06.438: INFO: Deleting pod "simpletest.rc-hfnfb" in namespace "gc-610"
  Apr 18 01:31:06.479: INFO: Deleting pod "simpletest.rc-hfxfn" in namespace "gc-610"
  Apr 18 01:31:06.507: INFO: Deleting pod "simpletest.rc-hgfj9" in namespace "gc-610"
  Apr 18 01:31:06.543: INFO: Deleting pod "simpletest.rc-hhm92" in namespace "gc-610"
  Apr 18 01:31:06.574: INFO: Deleting pod "simpletest.rc-hhvkr" in namespace "gc-610"
  Apr 18 01:31:06.592: INFO: Deleting pod "simpletest.rc-hntg9" in namespace "gc-610"
  Apr 18 01:31:06.607: INFO: Deleting pod "simpletest.rc-hp8wn" in namespace "gc-610"
  Apr 18 01:31:06.626: INFO: Deleting pod "simpletest.rc-hpq2v" in namespace "gc-610"
  Apr 18 01:31:06.654: INFO: Deleting pod "simpletest.rc-hw9b4" in namespace "gc-610"
  Apr 18 01:31:06.663: INFO: Deleting pod "simpletest.rc-j7smh" in namespace "gc-610"
  Apr 18 01:31:06.680: INFO: Deleting pod "simpletest.rc-jg5z7" in namespace "gc-610"
  Apr 18 01:31:06.696: INFO: Deleting pod "simpletest.rc-jj5qb" in namespace "gc-610"
  Apr 18 01:31:06.713: INFO: Deleting pod "simpletest.rc-jtg7l" in namespace "gc-610"
  Apr 18 01:31:06.797: INFO: Deleting pod "simpletest.rc-jvkdw" in namespace "gc-610"
  Apr 18 01:31:06.811: INFO: Deleting pod "simpletest.rc-jwhl4" in namespace "gc-610"
  Apr 18 01:31:06.828: INFO: Deleting pod "simpletest.rc-k6mms" in namespace "gc-610"
  Apr 18 01:31:06.841: INFO: Deleting pod "simpletest.rc-k9mbb" in namespace "gc-610"
  Apr 18 01:31:06.857: INFO: Deleting pod "simpletest.rc-klbfx" in namespace "gc-610"
  Apr 18 01:31:06.873: INFO: Deleting pod "simpletest.rc-ldmv4" in namespace "gc-610"
  Apr 18 01:31:06.908: INFO: Deleting pod "simpletest.rc-ls5pm" in namespace "gc-610"
  Apr 18 01:31:06.948: INFO: Deleting pod "simpletest.rc-lx97w" in namespace "gc-610"
  Apr 18 01:31:06.969: INFO: Deleting pod "simpletest.rc-m99hl" in namespace "gc-610"
  Apr 18 01:31:06.988: INFO: Deleting pod "simpletest.rc-mcknb" in namespace "gc-610"
  Apr 18 01:31:07.006: INFO: Deleting pod "simpletest.rc-njzkn" in namespace "gc-610"
  Apr 18 01:31:07.035: INFO: Deleting pod "simpletest.rc-nkfhc" in namespace "gc-610"
  Apr 18 01:31:07.065: INFO: Deleting pod "simpletest.rc-nqttb" in namespace "gc-610"
  Apr 18 01:31:07.090: INFO: Deleting pod "simpletest.rc-nrdmj" in namespace "gc-610"
  E0418 01:31:07.100977      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:31:07.108: INFO: Deleting pod "simpletest.rc-nzqsc" in namespace "gc-610"
  Apr 18 01:31:07.151: INFO: Deleting pod "simpletest.rc-pcngg" in namespace "gc-610"
  Apr 18 01:31:07.178: INFO: Deleting pod "simpletest.rc-pxrtr" in namespace "gc-610"
  Apr 18 01:31:07.193: INFO: Deleting pod "simpletest.rc-qrqc5" in namespace "gc-610"
  Apr 18 01:31:07.223: INFO: Deleting pod "simpletest.rc-r2l2c" in namespace "gc-610"
  Apr 18 01:31:07.254: INFO: Deleting pod "simpletest.rc-r8h8k" in namespace "gc-610"
  Apr 18 01:31:07.276: INFO: Deleting pod "simpletest.rc-r98tq" in namespace "gc-610"
  Apr 18 01:31:07.291: INFO: Deleting pod "simpletest.rc-rpzqv" in namespace "gc-610"
  Apr 18 01:31:07.311: INFO: Deleting pod "simpletest.rc-rxncf" in namespace "gc-610"
  Apr 18 01:31:07.329: INFO: Deleting pod "simpletest.rc-rzxxg" in namespace "gc-610"
  Apr 18 01:31:07.362: INFO: Deleting pod "simpletest.rc-sg8x2" in namespace "gc-610"
  Apr 18 01:31:07.388: INFO: Deleting pod "simpletest.rc-spxz6" in namespace "gc-610"
  Apr 18 01:31:07.416: INFO: Deleting pod "simpletest.rc-sqtlh" in namespace "gc-610"
  Apr 18 01:31:07.426: INFO: Deleting pod "simpletest.rc-tjx8k" in namespace "gc-610"
  Apr 18 01:31:07.469: INFO: Deleting pod "simpletest.rc-v2zzq" in namespace "gc-610"
  Apr 18 01:31:07.524: INFO: Deleting pod "simpletest.rc-vhq58" in namespace "gc-610"
  Apr 18 01:31:07.546: INFO: Deleting pod "simpletest.rc-vjw89" in namespace "gc-610"
  Apr 18 01:31:07.563: INFO: Deleting pod "simpletest.rc-vlxvw" in namespace "gc-610"
  Apr 18 01:31:07.585: INFO: Deleting pod "simpletest.rc-vlxzz" in namespace "gc-610"
  Apr 18 01:31:07.600: INFO: Deleting pod "simpletest.rc-w4p65" in namespace "gc-610"
  Apr 18 01:31:07.656: INFO: Deleting pod "simpletest.rc-w972t" in namespace "gc-610"
  Apr 18 01:31:07.673: INFO: Deleting pod "simpletest.rc-wpv2m" in namespace "gc-610"
  Apr 18 01:31:07.688: INFO: Deleting pod "simpletest.rc-x2j4t" in namespace "gc-610"
  Apr 18 01:31:07.723: INFO: Deleting pod "simpletest.rc-xhzc7" in namespace "gc-610"
  Apr 18 01:31:07.768: INFO: Deleting pod "simpletest.rc-xjs7h" in namespace "gc-610"
  Apr 18 01:31:07.795: INFO: Deleting pod "simpletest.rc-xws45" in namespace "gc-610"
  Apr 18 01:31:07.838: INFO: Deleting pod "simpletest.rc-z827p" in namespace "gc-610"
  Apr 18 01:31:07.868: INFO: Deleting pod "simpletest.rc-zk7vq" in namespace "gc-610"
  Apr 18 01:31:07.886: INFO: Deleting pod "simpletest.rc-zmpv9" in namespace "gc-610"
  Apr 18 01:31:07.918: INFO: Deleting pod "simpletest.rc-zz5sz" in namespace "gc-610"
  Apr 18 01:31:07.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-610" for this suite. @ 04/18/23 01:31:07.98
• [42.380 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 04/18/23 01:31:08
  Apr 18 01:31:08.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename runtimeclass @ 04/18/23 01:31:08.002
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:31:08.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:31:08.043
  STEP: getting /apis @ 04/18/23 01:31:08.048
  STEP: getting /apis/node.k8s.io @ 04/18/23 01:31:08.054
  STEP: getting /apis/node.k8s.io/v1 @ 04/18/23 01:31:08.055
  STEP: creating @ 04/18/23 01:31:08.057
  STEP: watching @ 04/18/23 01:31:08.078
  Apr 18 01:31:08.079: INFO: starting watch
  STEP: getting @ 04/18/23 01:31:08.085
  STEP: listing @ 04/18/23 01:31:08.088
  E0418 01:31:08.117917      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching @ 04/18/23 01:31:08.12
  STEP: updating @ 04/18/23 01:31:08.13
  Apr 18 01:31:08.137: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 04/18/23 01:31:08.137
  STEP: deleting a collection @ 04/18/23 01:31:08.161
  Apr 18 01:31:08.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3463" for this suite. @ 04/18/23 01:31:08.183
• [0.189 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 04/18/23 01:31:08.19
  Apr 18 01:31:08.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename podtemplate @ 04/18/23 01:31:08.191
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:31:08.212
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:31:08.226
  STEP: Create set of pod templates @ 04/18/23 01:31:08.233
  Apr 18 01:31:08.242: INFO: created test-podtemplate-1
  Apr 18 01:31:08.247: INFO: created test-podtemplate-2
  Apr 18 01:31:08.270: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 04/18/23 01:31:08.27
  STEP: delete collection of pod templates @ 04/18/23 01:31:08.273
  Apr 18 01:31:08.273: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 04/18/23 01:31:08.303
  Apr 18 01:31:08.303: INFO: requesting list of pod templates to confirm quantity
  Apr 18 01:31:08.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-34" for this suite. @ 04/18/23 01:31:08.309
• [0.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 04/18/23 01:31:08.319
  Apr 18 01:31:08.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename svcaccounts @ 04/18/23 01:31:08.322
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:31:08.343
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:31:08.346
  E0418 01:31:09.118141      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:10.118558      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:11.118921      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:12.120298      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:13.121165      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:14.121340      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:15.121987      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:16.122069      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:17.122174      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:18.123007      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 04/18/23 01:31:18.442
  Apr 18 01:31:18.442: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6876 pod-service-account-10bf3e34-61c3-427f-9fdb-27bd5bfaaea3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 04/18/23 01:31:19.119
  Apr 18 01:31:19.119: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6876 pod-service-account-10bf3e34-61c3-427f-9fdb-27bd5bfaaea3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  E0418 01:31:19.123330      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 04/18/23 01:31:19.344
  Apr 18 01:31:19.344: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6876 pod-service-account-10bf3e34-61c3-427f-9fdb-27bd5bfaaea3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Apr 18 01:31:19.578: INFO: Got root ca configmap in namespace "svcaccounts-6876"
  Apr 18 01:31:19.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6876" for this suite. @ 04/18/23 01:31:19.606
• [11.304 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 04/18/23 01:31:19.625
  Apr 18 01:31:19.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename daemonsets @ 04/18/23 01:31:19.625
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:31:19.666
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:31:19.683
  Apr 18 01:31:19.739: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/18/23 01:31:19.752
  Apr 18 01:31:19.790: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 01:31:19.810: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 01:31:19.810: INFO: Node ip-10-0-14-154 is running 0 daemon pod, expected 1
  E0418 01:31:20.124208      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:31:20.840: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 01:31:20.843: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 01:31:20.844: INFO: Node ip-10-0-14-154 is running 0 daemon pod, expected 1
  E0418 01:31:21.124791      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:31:21.814: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 01:31:21.816: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 18 01:31:21.816: INFO: Node ip-10-0-14-154 is running 0 daemon pod, expected 1
  E0418 01:31:22.125282      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:31:22.815: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 01:31:22.818: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 18 01:31:22.818: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Update daemon pods image. @ 04/18/23 01:31:22.828
  STEP: Check that daemon pods images are updated. @ 04/18/23 01:31:22.838
  Apr 18 01:31:22.845: INFO: Wrong image for pod: daemon-set-f9dj5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 18 01:31:22.845: INFO: Wrong image for pod: daemon-set-g7fgn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 18 01:31:22.858: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0418 01:31:23.125531      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:31:23.863: INFO: Wrong image for pod: daemon-set-f9dj5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 18 01:31:23.865: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0418 01:31:24.126178      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:31:24.863: INFO: Wrong image for pod: daemon-set-f9dj5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 18 01:31:24.873: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0418 01:31:25.127248      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:31:25.862: INFO: Pod daemon-set-9phsj is not available
  Apr 18 01:31:25.862: INFO: Wrong image for pod: daemon-set-f9dj5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 18 01:31:25.865: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0418 01:31:26.127897      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:31:26.865: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0418 01:31:27.128155      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:31:27.862: INFO: Pod daemon-set-mhgqc is not available
  Apr 18 01:31:27.865: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Check that daemon pods are still running on every node of the cluster. @ 04/18/23 01:31:27.865
  Apr 18 01:31:27.867: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 01:31:27.870: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 18 01:31:27.870: INFO: Node ip-10-0-27-81 is running 0 daemon pod, expected 1
  E0418 01:31:28.129022      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:31:28.874: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 01:31:28.878: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 18 01:31:28.878: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/18/23 01:31:28.903
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-784, will wait for the garbage collector to delete the pods @ 04/18/23 01:31:28.903
  Apr 18 01:31:28.967: INFO: Deleting DaemonSet.extensions daemon-set took: 10.436915ms
  Apr 18 01:31:29.067: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.259919ms
  E0418 01:31:29.129587      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:30.130010      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:31.130898      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:31:31.472: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 01:31:31.472: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 18 01:31:31.475: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"70181"},"items":null}

  Apr 18 01:31:31.478: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"70181"},"items":null}

  Apr 18 01:31:31.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-784" for this suite. @ 04/18/23 01:31:31.499
• [11.883 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 04/18/23 01:31:31.508
  Apr 18 01:31:31.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename pods @ 04/18/23 01:31:31.509
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:31:31.532
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:31:31.537
  STEP: creating a Pod with a static label @ 04/18/23 01:31:31.546
  STEP: watching for Pod to be ready @ 04/18/23 01:31:31.555
  Apr 18 01:31:31.563: INFO: observed Pod pod-test in namespace pods-3813 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Apr 18 01:31:31.564: INFO: observed Pod pod-test in namespace pods-3813 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:31:31 +0000 UTC  }]
  Apr 18 01:31:31.583: INFO: observed Pod pod-test in namespace pods-3813 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:31:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:31:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:31:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:31:31 +0000 UTC  }]
  E0418 01:31:32.131843      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:31:32.191: INFO: observed Pod pod-test in namespace pods-3813 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:31:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:31:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:31:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:31:31 +0000 UTC  }]
  Apr 18 01:31:32.641: INFO: Found Pod pod-test in namespace pods-3813 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:31:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:31:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:31:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:31:31 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 04/18/23 01:31:32.646
  STEP: getting the Pod and ensuring that it's patched @ 04/18/23 01:31:32.659
  STEP: replacing the Pod's status Ready condition to False @ 04/18/23 01:31:32.662
  STEP: check the Pod again to ensure its Ready conditions are False @ 04/18/23 01:31:32.675
  STEP: deleting the Pod via a Collection with a LabelSelector @ 04/18/23 01:31:32.675
  STEP: watching for the Pod to be deleted @ 04/18/23 01:31:32.683
  Apr 18 01:31:32.686: INFO: observed event type MODIFIED
  E0418 01:31:33.132182      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:34.132628      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:31:34.649: INFO: observed event type MODIFIED
  Apr 18 01:31:34.974: INFO: observed event type MODIFIED
  E0418 01:31:35.132953      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:31:35.160: INFO: observed event type MODIFIED
  Apr 18 01:31:35.666: INFO: observed event type MODIFIED
  Apr 18 01:31:35.678: INFO: observed event type MODIFIED
  Apr 18 01:31:35.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3813" for this suite. @ 04/18/23 01:31:35.689
• [4.190 seconds]
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 04/18/23 01:31:35.699
  Apr 18 01:31:35.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename statefulset @ 04/18/23 01:31:35.7
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:31:35.726
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:31:35.734
  STEP: Creating service test in namespace statefulset-5944 @ 04/18/23 01:31:35.737
  STEP: Creating statefulset ss in namespace statefulset-5944 @ 04/18/23 01:31:35.745
  Apr 18 01:31:35.775: INFO: Found 0 stateful pods, waiting for 1
  E0418 01:31:36.133098      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:37.133214      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:38.133311      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:39.133823      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:40.133924      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:41.134619      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:42.134986      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:43.135029      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:44.135980      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:45.136937      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:31:45.779: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 04/18/23 01:31:45.785
  STEP: updating a scale subresource @ 04/18/23 01:31:45.79
  STEP: verifying the statefulset Spec.Replicas was modified @ 04/18/23 01:31:45.798
  STEP: Patch a scale subresource @ 04/18/23 01:31:45.819
  STEP: verifying the statefulset Spec.Replicas was modified @ 04/18/23 01:31:45.829
  Apr 18 01:31:45.841: INFO: Deleting all statefulset in ns statefulset-5944
  Apr 18 01:31:45.851: INFO: Scaling statefulset ss to 0
  E0418 01:31:46.137767      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:47.138048      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:48.138061      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:49.138110      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:50.139109      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:51.139147      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:52.140029      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:53.140106      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:54.140159      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:55.140976      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:31:55.885: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 18 01:31:55.888: INFO: Deleting statefulset ss
  Apr 18 01:31:55.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5944" for this suite. @ 04/18/23 01:31:55.902
• [20.209 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 04/18/23 01:31:55.91
  Apr 18 01:31:55.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename webhook @ 04/18/23 01:31:55.911
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:31:55.933
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:31:55.942
  STEP: Setting up server cert @ 04/18/23 01:31:55.964
  E0418 01:31:56.141156      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/18/23 01:31:56.21
  STEP: Deploying the webhook pod @ 04/18/23 01:31:56.217
  STEP: Wait for the deployment to be ready @ 04/18/23 01:31:56.229
  Apr 18 01:31:56.240: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0418 01:31:57.142092      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:31:58.142189      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/18/23 01:31:58.25
  STEP: Verifying the service has paired with the endpoint @ 04/18/23 01:31:58.264
  E0418 01:31:59.142873      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:31:59.264: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 04/18/23 01:31:59.325
  STEP: Creating a configMap that should be mutated @ 04/18/23 01:31:59.343
  STEP: Deleting the collection of validation webhooks @ 04/18/23 01:31:59.369
  STEP: Creating a configMap that should not be mutated @ 04/18/23 01:31:59.401
  Apr 18 01:31:59.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1099" for this suite. @ 04/18/23 01:31:59.458
  STEP: Destroying namespace "webhook-markers-5283" for this suite. @ 04/18/23 01:31:59.467
• [3.565 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 04/18/23 01:31:59.476
  Apr 18 01:31:59.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename webhook @ 04/18/23 01:31:59.478
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:31:59.502
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:31:59.511
  STEP: Setting up server cert @ 04/18/23 01:31:59.543
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/18/23 01:31:59.74
  STEP: Deploying the webhook pod @ 04/18/23 01:31:59.747
  STEP: Wait for the deployment to be ready @ 04/18/23 01:31:59.758
  Apr 18 01:31:59.766: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0418 01:32:00.143259      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:01.143570      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/18/23 01:32:01.786
  STEP: Verifying the service has paired with the endpoint @ 04/18/23 01:32:01.813
  E0418 01:32:02.146684      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:32:02.815: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 18 01:32:02.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  E0418 01:32:03.146965      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3641-crds.webhook.example.com via the AdmissionRegistration API @ 04/18/23 01:32:03.353
  STEP: Creating a custom resource while v1 is storage version @ 04/18/23 01:32:03.377
  E0418 01:32:04.147189      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:05.148173      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 04/18/23 01:32:05.431
  STEP: Patching the custom resource while v2 is storage version @ 04/18/23 01:32:05.438
  Apr 18 01:32:05.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0418 01:32:06.148387      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-4704" for this suite. @ 04/18/23 01:32:06.275
  STEP: Destroying namespace "webhook-markers-6930" for this suite. @ 04/18/23 01:32:06.28
• [6.811 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 04/18/23 01:32:06.287
  Apr 18 01:32:06.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename deployment @ 04/18/23 01:32:06.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:32:06.31
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:32:06.316
  STEP: creating a Deployment @ 04/18/23 01:32:06.326
  STEP: waiting for Deployment to be created @ 04/18/23 01:32:06.337
  STEP: waiting for all Replicas to be Ready @ 04/18/23 01:32:06.339
  Apr 18 01:32:06.342: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 18 01:32:06.342: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 18 01:32:06.356: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 18 01:32:06.356: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 18 01:32:06.384: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 18 01:32:06.384: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 18 01:32:06.433: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 18 01:32:06.433: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E0418 01:32:07.148670      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:32:07.510: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Apr 18 01:32:07.510: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Apr 18 01:32:07.834: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 04/18/23 01:32:07.834
  W0418 01:32:07.841313      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 18 01:32:07.843: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 04/18/23 01:32:07.843
  Apr 18 01:32:07.847: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 0
  Apr 18 01:32:07.847: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 0
  Apr 18 01:32:07.847: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 0
  Apr 18 01:32:07.847: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 0
  Apr 18 01:32:07.847: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 0
  Apr 18 01:32:07.847: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 0
  Apr 18 01:32:07.847: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 0
  Apr 18 01:32:07.847: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 0
  Apr 18 01:32:07.848: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 1
  Apr 18 01:32:07.848: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 1
  Apr 18 01:32:07.848: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 2
  Apr 18 01:32:07.848: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 2
  Apr 18 01:32:07.848: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 2
  Apr 18 01:32:07.848: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 2
  Apr 18 01:32:07.861: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 2
  Apr 18 01:32:07.861: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 2
  Apr 18 01:32:07.891: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 2
  Apr 18 01:32:07.891: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 2
  Apr 18 01:32:07.922: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 1
  Apr 18 01:32:07.922: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 1
  Apr 18 01:32:07.932: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 1
  Apr 18 01:32:07.932: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 1
  E0418 01:32:08.148768      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:32:08.864: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 2
  Apr 18 01:32:08.864: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 2
  Apr 18 01:32:08.889: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 1
  STEP: listing Deployments @ 04/18/23 01:32:08.889
  Apr 18 01:32:08.892: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 04/18/23 01:32:08.893
  Apr 18 01:32:08.981: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 04/18/23 01:32:08.982
  Apr 18 01:32:09.015: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 18 01:32:09.015: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 18 01:32:09.038: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 18 01:32:09.078: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 18 01:32:09.093: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 18 01:32:09.104: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  E0418 01:32:09.149529      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:32:09.857: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 18 01:32:09.963: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 18 01:32:09.972: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0418 01:32:10.149965      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:11.150053      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:32:11.630: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 04/18/23 01:32:11.695
  STEP: fetching the DeploymentStatus @ 04/18/23 01:32:11.729
  Apr 18 01:32:11.759: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 1
  Apr 18 01:32:11.759: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 1
  Apr 18 01:32:11.759: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 1
  Apr 18 01:32:11.759: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 1
  Apr 18 01:32:11.759: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 1
  Apr 18 01:32:11.759: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 1
  Apr 18 01:32:11.759: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 2
  Apr 18 01:32:11.759: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 2
  Apr 18 01:32:11.759: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 2
  Apr 18 01:32:11.759: INFO: observed Deployment test-deployment in namespace deployment-9151 with ReadyReplicas 3
  STEP: deleting the Deployment @ 04/18/23 01:32:11.759
  Apr 18 01:32:11.814: INFO: observed event type MODIFIED
  Apr 18 01:32:11.814: INFO: observed event type MODIFIED
  Apr 18 01:32:11.814: INFO: observed event type MODIFIED
  Apr 18 01:32:11.814: INFO: observed event type MODIFIED
  Apr 18 01:32:11.814: INFO: observed event type MODIFIED
  Apr 18 01:32:11.814: INFO: observed event type MODIFIED
  Apr 18 01:32:11.814: INFO: observed event type MODIFIED
  Apr 18 01:32:11.814: INFO: observed event type MODIFIED
  Apr 18 01:32:11.814: INFO: observed event type MODIFIED
  Apr 18 01:32:11.814: INFO: observed event type MODIFIED
  Apr 18 01:32:11.814: INFO: observed event type MODIFIED
  Apr 18 01:32:11.850: INFO: Log out all the ReplicaSets if there is no deployment created
  Apr 18 01:32:11.864: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-9151  83e4ed04-ba38-4a44-ad51-7ce438e37b1f 70639 3 2023-04-18 01:32:06 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 002a618f-6f15-4d1c-9b67-25e4ff12b1d1 0xc004a0ffa7 0xc004a0ffa8}] [] [{kube-controller-manager Update apps/v1 2023-04-18 01:32:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"002a618f-6f15-4d1c-9b67-25e4ff12b1d1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-18 01:32:08 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ef2030 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Apr 18 01:32:11.916: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-9151  b03f13b9-5778-4d20-9f12-d79a6b89f408 70737 4 2023-04-18 01:32:07 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 002a618f-6f15-4d1c-9b67-25e4ff12b1d1 0xc004ef2097 0xc004ef2098}] [] [{kube-controller-manager Update apps/v1 2023-04-18 01:32:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"002a618f-6f15-4d1c-9b67-25e4ff12b1d1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-18 01:32:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ef2120 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Apr 18 01:32:11.936: INFO: pod: "test-deployment-5b5dcbcd95-cgh7r":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-cgh7r test-deployment-5b5dcbcd95- deployment-9151  8979a14d-bb8e-4c7a-8fed-83dda937859f 70739 0 2023-04-18 01:32:08 +0000 UTC 2023-04-18 01:32:10 +0000 UTC 0xc004ef26e8 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[cni.projectcalico.org/containerID:679d5c7683e0238689ddb0b5fcdad834818643f6c026c6e6436d6379a41c1374 cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs:] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 b03f13b9-5778-4d20-9f12-d79a6b89f408 0xc004ef2717 0xc004ef2718}] [] [{kube-controller-manager Update v1 2023-04-18 01:32:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b03f13b9-5778-4d20-9f12-d79a6b89f408\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-18 01:32:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.129.237\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2023-04-18 01:32:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-llz4q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-llz4q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-27-81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:32:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:32:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:32:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:32:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.27.81,PodIP:10.2.129.237,StartTime:2023-04-18 01:32:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-18 01:32:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://06b54d97e04b794449c06077b67d3f3f1cdbb892f64892e4d99fe291b06386ca,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.129.237,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 18 01:32:11.937: INFO: pod: "test-deployment-5b5dcbcd95-fgqr9":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-fgqr9 test-deployment-5b5dcbcd95- deployment-9151  fc5bc28f-3e0b-4fe8-8c1a-1792a9df4ac1 70733 0 2023-04-18 01:32:07 +0000 UTC 2023-04-18 01:32:12 +0000 UTC 0xc004ef2900 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[cni.projectcalico.org/containerID:c96d8a0ec848e0d89556397de68a917f62681781ab30fcd4b7c9dfab56539ce5 cni.projectcalico.org/podIP:10.2.212.166/32 cni.projectcalico.org/podIPs:10.2.212.166/32] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 b03f13b9-5778-4d20-9f12-d79a6b89f408 0xc004ef2957 0xc004ef2958}] [] [{kube-controller-manager Update v1 2023-04-18 01:32:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b03f13b9-5778-4d20-9f12-d79a6b89f408\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-18 01:32:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-18 01:32:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.212.166\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tpkvk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tpkvk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-14-154,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:32:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:32:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:32:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:32:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.14.154,PodIP:10.2.212.166,StartTime:2023-04-18 01:32:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-18 01:32:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://9ee4f29e134456533146835b9fac6fd0ae278606379d57b0439a188665f73a20,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.212.166,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 18 01:32:11.937: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-9151  20197d54-ed36-476d-af3c-c470425cbff4 70729 2 2023-04-18 01:32:09 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 002a618f-6f15-4d1c-9b67-25e4ff12b1d1 0xc004ef2187 0xc004ef2188}] [] [{kube-controller-manager Update apps/v1 2023-04-18 01:32:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"002a618f-6f15-4d1c-9b67-25e4ff12b1d1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-18 01:32:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ef2210 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  Apr 18 01:32:11.954: INFO: pod: "test-deployment-6fc78d85c6-26zxr":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-26zxr test-deployment-6fc78d85c6- deployment-9151  d7f476ac-997e-40fb-b5d6-4eccb6c44e5f 70681 0 2023-04-18 01:32:09 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[cni.projectcalico.org/containerID:b48126f61b82c045c61ebfc61dbefc8f48c90359bbfbde155ab2472c5bc7caae cni.projectcalico.org/podIP:10.2.212.167/32 cni.projectcalico.org/podIPs:10.2.212.167/32] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 20197d54-ed36-476d-af3c-c470425cbff4 0xc004ef3da7 0xc004ef3da8}] [] [{calico Update v1 2023-04-18 01:32:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-18 01:32:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"20197d54-ed36-476d-af3c-c470425cbff4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-18 01:32:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.212.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2pvfd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2pvfd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-14-154,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:32:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:32:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:32:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:32:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.14.154,PodIP:10.2.212.167,StartTime:2023-04-18 01:32:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-18 01:32:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://320e73234cb3afa371b5c4be4a6c927c7e1eca3750b8a66c86c62ccfb606f62e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.212.167,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 18 01:32:11.955: INFO: pod: "test-deployment-6fc78d85c6-6btln":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-6btln test-deployment-6fc78d85c6- deployment-9151  8a4d5ec4-470e-4f00-bc9f-df750846a830 70728 0 2023-04-18 01:32:09 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[cni.projectcalico.org/containerID:d79760cb7b38a0079be9e637ab72bd08f8d87200a4173317f82499fcc99c75f1 cni.projectcalico.org/podIP:10.2.129.238/32 cni.projectcalico.org/podIPs:10.2.129.238/32] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 20197d54-ed36-476d-af3c-c470425cbff4 0xc004ef3ff7 0xc004ef3ff8}] [] [{kube-controller-manager Update v1 2023-04-18 01:32:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"20197d54-ed36-476d-af3c-c470425cbff4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-18 01:32:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-18 01:32:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.129.238\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bghtc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bghtc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-27-81,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:32:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:32:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:32:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:32:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.27.81,PodIP:10.2.129.238,StartTime:2023-04-18 01:32:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-18 01:32:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://039a728410a4d094afc2ad39eec8b34cd1312affa04ce1037cffe575e7985c5f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.129.238,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 18 01:32:11.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9151" for this suite. @ 04/18/23 01:32:11.977
• [5.702 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 04/18/23 01:32:11.994
  Apr 18 01:32:11.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename emptydir @ 04/18/23 01:32:11.995
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:32:12.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:32:12.042
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 04/18/23 01:32:12.046
  E0418 01:32:12.150226      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:13.150616      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:14.150881      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:15.151454      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:32:16.089
  Apr 18 01:32:16.092: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-cec07a0e-94bb-4916-9d0c-7177dacbbcff container test-container: <nil>
  STEP: delete the pod @ 04/18/23 01:32:16.114
  Apr 18 01:32:16.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9843" for this suite. @ 04/18/23 01:32:16.13
• [4.145 seconds]
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 04/18/23 01:32:16.14
  Apr 18 01:32:16.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename field-validation @ 04/18/23 01:32:16.142
  E0418 01:32:16.151947      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:32:16.158
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:32:16.162
  Apr 18 01:32:16.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  W0418 01:32:16.167341      21 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc0012f9740 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E0418 01:32:17.152435      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:18.152658      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0418 01:32:18.749481      21 warnings.go:70] unknown field "alpha"
  W0418 01:32:18.749610      21 warnings.go:70] unknown field "beta"
  W0418 01:32:18.749660      21 warnings.go:70] unknown field "delta"
  W0418 01:32:18.749695      21 warnings.go:70] unknown field "epsilon"
  W0418 01:32:18.749738      21 warnings.go:70] unknown field "gamma"
  Apr 18 01:32:18.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1433" for this suite. @ 04/18/23 01:32:18.796
• [2.666 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 04/18/23 01:32:18.837
  Apr 18 01:32:18.842: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename replicaset @ 04/18/23 01:32:18.848
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:32:18.982
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:32:19.011
  STEP: Create a ReplicaSet @ 04/18/23 01:32:19.03
  STEP: Verify that the required pods have come up @ 04/18/23 01:32:19.039
  Apr 18 01:32:19.044: INFO: Pod name sample-pod: Found 0 pods out of 3
  E0418 01:32:19.153796      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:20.154547      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:21.155011      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:22.155392      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:23.155506      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:32:24.056: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 04/18/23 01:32:24.056
  Apr 18 01:32:24.072: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 04/18/23 01:32:24.072
  STEP: DeleteCollection of the ReplicaSets @ 04/18/23 01:32:24.083
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 04/18/23 01:32:24.109
  Apr 18 01:32:24.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3487" for this suite. @ 04/18/23 01:32:24.122
• [5.294 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 04/18/23 01:32:24.132
  Apr 18 01:32:24.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename kubectl @ 04/18/23 01:32:24.133
  E0418 01:32:24.157313      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:32:24.178
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:32:24.206
  STEP: Starting the proxy @ 04/18/23 01:32:24.213
  Apr 18 01:32:24.214: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-6739 proxy --unix-socket=/tmp/kubectl-proxy-unix3007713669/test'
  STEP: retrieving proxy /api/ output @ 04/18/23 01:32:24.276
  Apr 18 01:32:24.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6739" for this suite. @ 04/18/23 01:32:24.282
• [0.155 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 04/18/23 01:32:24.287
  Apr 18 01:32:24.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename csistoragecapacity @ 04/18/23 01:32:24.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:32:24.316
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:32:24.321
  STEP: getting /apis @ 04/18/23 01:32:24.327
  STEP: getting /apis/storage.k8s.io @ 04/18/23 01:32:24.335
  STEP: getting /apis/storage.k8s.io/v1 @ 04/18/23 01:32:24.336
  STEP: creating @ 04/18/23 01:32:24.339
  STEP: watching @ 04/18/23 01:32:24.369
  Apr 18 01:32:24.369: INFO: starting watch
  STEP: getting @ 04/18/23 01:32:24.382
  STEP: listing in namespace @ 04/18/23 01:32:24.384
  STEP: listing across namespaces @ 04/18/23 01:32:24.387
  STEP: patching @ 04/18/23 01:32:24.389
  STEP: updating @ 04/18/23 01:32:24.394
  Apr 18 01:32:24.399: INFO: waiting for watch events with expected annotations in namespace
  Apr 18 01:32:24.399: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 04/18/23 01:32:24.4
  STEP: deleting a collection @ 04/18/23 01:32:24.409
  Apr 18 01:32:24.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-3820" for this suite. @ 04/18/23 01:32:24.423
• [0.139 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 04/18/23 01:32:24.443
  Apr 18 01:32:24.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename deployment @ 04/18/23 01:32:24.444
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:32:24.461
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:32:24.465
  Apr 18 01:32:24.476: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Apr 18 01:32:24.484: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0418 01:32:25.157161      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:26.157736      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:27.157913      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:28.157912      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:29.158141      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:32:29.492: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/18/23 01:32:29.492
  Apr 18 01:32:29.493: INFO: Creating deployment "test-rolling-update-deployment"
  Apr 18 01:32:29.499: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Apr 18 01:32:29.516: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0418 01:32:30.158252      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:31.158459      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:32:31.525: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Apr 18 01:32:31.528: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Apr 18 01:32:31.542: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3894  9c6a8989-a2a3-4b56-8ecd-d347ea350eb7 71074 1 2023-04-18 01:32:29 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-04-18 01:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-18 01:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004626cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-18 01:32:29 +0000 UTC,LastTransitionTime:2023-04-18 01:32:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-04-18 01:32:31 +0000 UTC,LastTransitionTime:2023-04-18 01:32:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 18 01:32:31.545: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-3894  467dab71-fa72-4a86-8796-79924a6daf31 71064 1 2023-04-18 01:32:29 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 9c6a8989-a2a3-4b56-8ecd-d347ea350eb7 0xc0046271e7 0xc0046271e8}] [] [{kube-controller-manager Update apps/v1 2023-04-18 01:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c6a8989-a2a3-4b56-8ecd-d347ea350eb7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-18 01:32:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004627298 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 18 01:32:31.545: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Apr 18 01:32:31.546: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3894  c0f0bc98-2018-41cf-91ce-5aecac59b809 71073 2 2023-04-18 01:32:24 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 9c6a8989-a2a3-4b56-8ecd-d347ea350eb7 0xc0046270b7 0xc0046270b8}] [] [{e2e.test Update apps/v1 2023-04-18 01:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-18 01:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c6a8989-a2a3-4b56-8ecd-d347ea350eb7\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-18 01:32:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004627178 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 18 01:32:31.550: INFO: Pod "test-rolling-update-deployment-656d657cd8-nnzkn" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-nnzkn test-rolling-update-deployment-656d657cd8- deployment-3894  9ff3e3fd-096f-4b2d-abc1-989b5732ef09 71063 0 2023-04-18 01:32:29 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[cni.projectcalico.org/containerID:43f51ab5d0764d3382485cb9356288cc56eb8492133207d444f53d1f81d87664 cni.projectcalico.org/podIP:10.2.212.172/32 cni.projectcalico.org/podIPs:10.2.212.172/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 467dab71-fa72-4a86-8796-79924a6daf31 0xc003fca7a7 0xc003fca7a8}] [] [{kube-controller-manager Update v1 2023-04-18 01:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"467dab71-fa72-4a86-8796-79924a6daf31\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-18 01:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-18 01:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.212.172\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4lx7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4lx7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-14-154,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:32:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.14.154,PodIP:10.2.212.172,StartTime:2023-04-18 01:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-18 01:32:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://19d3f2caa620f7b19e8c71cff0ed7c6d182f5967e413bb2f6afd3fbe8586b9b9,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.212.172,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 01:32:31.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3894" for this suite. @ 04/18/23 01:32:31.553
• [7.124 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 04/18/23 01:32:31.576
  Apr 18 01:32:31.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename webhook @ 04/18/23 01:32:31.576
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:32:31.597
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:32:31.602
  STEP: Setting up server cert @ 04/18/23 01:32:31.639
  E0418 01:32:32.160557      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/18/23 01:32:32.404
  STEP: Deploying the webhook pod @ 04/18/23 01:32:32.413
  STEP: Wait for the deployment to be ready @ 04/18/23 01:32:32.427
  Apr 18 01:32:32.461: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0418 01:32:33.160594      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:34.160788      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/18/23 01:32:34.473
  STEP: Verifying the service has paired with the endpoint @ 04/18/23 01:32:34.483
  E0418 01:32:35.161606      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:32:35.484: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 04/18/23 01:32:35.488
  STEP: create a namespace for the webhook @ 04/18/23 01:32:35.504
  STEP: create a configmap should be unconditionally rejected by the webhook @ 04/18/23 01:32:35.519
  Apr 18 01:32:35.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4420" for this suite. @ 04/18/23 01:32:35.604
  STEP: Destroying namespace "webhook-markers-5536" for this suite. @ 04/18/23 01:32:35.618
  STEP: Destroying namespace "fail-closed-namespace-6758" for this suite. @ 04/18/23 01:32:35.625
• [4.058 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 04/18/23 01:32:35.638
  Apr 18 01:32:35.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename sysctl @ 04/18/23 01:32:35.639
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:32:35.667
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:32:35.672
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 04/18/23 01:32:35.677
  STEP: Watching for error events or started pod @ 04/18/23 01:32:35.685
  E0418 01:32:36.162401      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:37.162522      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 04/18/23 01:32:37.69
  E0418 01:32:38.162993      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:39.163093      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 04/18/23 01:32:39.699
  STEP: Getting logs from the pod @ 04/18/23 01:32:39.699
  STEP: Checking that the sysctl is actually updated @ 04/18/23 01:32:39.704
  Apr 18 01:32:39.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-4998" for this suite. @ 04/18/23 01:32:39.709
• [4.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 04/18/23 01:32:39.718
  Apr 18 01:32:39.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename webhook @ 04/18/23 01:32:39.72
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:32:39.754
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:32:39.766
  STEP: Setting up server cert @ 04/18/23 01:32:39.814
  E0418 01:32:40.163951      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/18/23 01:32:40.425
  STEP: Deploying the webhook pod @ 04/18/23 01:32:40.432
  STEP: Wait for the deployment to be ready @ 04/18/23 01:32:40.45
  Apr 18 01:32:40.465: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0418 01:32:41.164983      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:42.166117      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/18/23 01:32:42.478
  STEP: Verifying the service has paired with the endpoint @ 04/18/23 01:32:42.492
  E0418 01:32:43.166142      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:32:43.492: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 04/18/23 01:32:43.496
  STEP: create a pod @ 04/18/23 01:32:43.511
  E0418 01:32:44.166190      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:45.166472      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 04/18/23 01:32:45.525
  Apr 18 01:32:45.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=webhook-2719 attach --namespace=webhook-2719 to-be-attached-pod -i -c=container1'
  Apr 18 01:32:45.641: INFO: rc: 1
  Apr 18 01:32:45.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2719" for this suite. @ 04/18/23 01:32:45.699
  STEP: Destroying namespace "webhook-markers-6758" for this suite. @ 04/18/23 01:32:45.704
• [5.999 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 04/18/23 01:32:45.718
  Apr 18 01:32:45.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename var-expansion @ 04/18/23 01:32:45.72
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:32:45.739
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:32:45.744
  STEP: Creating a pod to test substitution in volume subpath @ 04/18/23 01:32:45.749
  E0418 01:32:46.167462      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:47.167600      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:48.167671      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:49.167793      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:32:49.771
  Apr 18 01:32:49.775: INFO: Trying to get logs from node ip-10-0-14-154 pod var-expansion-b5423f79-5b71-4109-b712-891ebaf44465 container dapi-container: <nil>
  STEP: delete the pod @ 04/18/23 01:32:49.782
  Apr 18 01:32:49.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5309" for this suite. @ 04/18/23 01:32:49.801
• [4.090 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 04/18/23 01:32:49.809
  Apr 18 01:32:49.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename kubectl @ 04/18/23 01:32:49.81
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:32:49.851
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:32:49.862
  Apr 18 01:32:49.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-8332 version'
  Apr 18 01:32:49.927: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Apr 18 01:32:49.927: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:21:19Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:14:42Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Apr 18 01:32:49.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8332" for this suite. @ 04/18/23 01:32:49.932
• [0.132 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 04/18/23 01:32:49.943
  Apr 18 01:32:49.943: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename cronjob @ 04/18/23 01:32:49.944
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:32:49.975
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:32:49.988
  STEP: Creating a ReplaceConcurrent cronjob @ 04/18/23 01:32:49.992
  STEP: Ensuring a job is scheduled @ 04/18/23 01:32:50.002
  E0418 01:32:50.167971      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:51.168100      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:52.169141      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:53.169232      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:54.169470      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:55.169883      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:56.171276      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:57.171353      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:58.171889      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:32:59.172251      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:00.173129      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:01.173439      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 04/18/23 01:33:02.006
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 04/18/23 01:33:02.013
  STEP: Ensuring the job is replaced with a new one @ 04/18/23 01:33:02.019
  E0418 01:33:02.173669      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:03.173917      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:04.174429      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:05.174567      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:06.174950      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:07.175192      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:08.175197      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:09.175290      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:10.175957      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:11.176063      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:12.176732      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:13.176992      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:14.177571      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:15.178005      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:16.178718      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:17.179169      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:18.179269      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:19.179605      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:20.180195      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:21.180268      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:22.180583      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:23.180906      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:24.181951      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:25.182956      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:26.183827      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:27.183881      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:28.183956      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:29.184050      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:30.184417      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:31.184515      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:32.184873      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:33.185141      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:34.185679      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:35.185910      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:36.186518      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:37.187046      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:38.187777      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:39.187912      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:40.188498      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:41.188877      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:42.189491      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:43.189579      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:44.189874      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:45.189917      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:46.190781      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:47.190875      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:48.191197      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:49.191979      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:50.192868      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:51.193204      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:52.193697      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:53.193834      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:54.194781      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:55.194876      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:56.195278      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:57.195646      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:58.196525      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:33:59.196681      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:00.196891      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:01.197204      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 04/18/23 01:34:02.022
  Apr 18 01:34:02.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8753" for this suite. @ 04/18/23 01:34:02.061
• [72.160 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 04/18/23 01:34:02.103
  Apr 18 01:34:02.103: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename secrets @ 04/18/23 01:34:02.104
  E0418 01:34:02.198084      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:34:02.355
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:34:02.368
  STEP: Creating secret with name s-test-opt-del-601208f2-a0d3-43ea-aea8-c32f942fde84 @ 04/18/23 01:34:02.389
  STEP: Creating secret with name s-test-opt-upd-6e66248f-c5e5-40ea-acfe-cf3c32d1ae5e @ 04/18/23 01:34:02.433
  STEP: Creating the pod @ 04/18/23 01:34:02.531
  E0418 01:34:03.198512      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:04.199212      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:05.199301      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:06.199987      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-601208f2-a0d3-43ea-aea8-c32f942fde84 @ 04/18/23 01:34:06.632
  STEP: Updating secret s-test-opt-upd-6e66248f-c5e5-40ea-acfe-cf3c32d1ae5e @ 04/18/23 01:34:06.638
  STEP: Creating secret with name s-test-opt-create-a4ddbd4a-db51-4fd8-a8b0-7cef90a775b4 @ 04/18/23 01:34:06.643
  STEP: waiting to observe update in volume @ 04/18/23 01:34:06.647
  E0418 01:34:07.200144      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:08.200219      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:09.200972      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:10.201095      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:34:10.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6505" for this suite. @ 04/18/23 01:34:10.685
• [8.588 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 04/18/23 01:34:10.697
  Apr 18 01:34:10.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 01:34:10.698
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:34:10.731
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:34:10.738
  STEP: Creating a pod to test downward API volume plugin @ 04/18/23 01:34:10.743
  E0418 01:34:11.201978      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:12.202057      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:13.202958      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:14.203227      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:34:14.769
  Apr 18 01:34:14.772: INFO: Trying to get logs from node ip-10-0-27-81 pod downwardapi-volume-313bcf46-40e3-466b-a97a-fa1e97ef45f7 container client-container: <nil>
  STEP: delete the pod @ 04/18/23 01:34:14.827
  Apr 18 01:34:14.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8699" for this suite. @ 04/18/23 01:34:14.861
• [4.179 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 04/18/23 01:34:14.877
  Apr 18 01:34:14.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename resourcequota @ 04/18/23 01:34:14.878
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:34:15.003
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:34:15.022
  STEP: Creating a ResourceQuota @ 04/18/23 01:34:15.026
  STEP: Getting a ResourceQuota @ 04/18/23 01:34:15.042
  STEP: Listing all ResourceQuotas with LabelSelector @ 04/18/23 01:34:15.047
  STEP: Patching the ResourceQuota @ 04/18/23 01:34:15.054
  STEP: Deleting a Collection of ResourceQuotas @ 04/18/23 01:34:15.066
  STEP: Verifying the deleted ResourceQuota @ 04/18/23 01:34:15.094
  Apr 18 01:34:15.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3446" for this suite. @ 04/18/23 01:34:15.103
• [0.237 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 04/18/23 01:34:15.114
  Apr 18 01:34:15.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename namespaces @ 04/18/23 01:34:15.115
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:34:15.158
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:34:15.162
  STEP: Updating Namespace "namespaces-6661" @ 04/18/23 01:34:15.165
  Apr 18 01:34:15.174: INFO: Namespace "namespaces-6661" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"3f27e2f9-bd1e-4da4-8323-1cbff9cd81a4", "kubernetes.io/metadata.name":"namespaces-6661", "namespaces-6661":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Apr 18 01:34:15.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-6661" for this suite. @ 04/18/23 01:34:15.179
• [0.072 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 04/18/23 01:34:15.187
  Apr 18 01:34:15.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename sysctl @ 04/18/23 01:34:15.188
  E0418 01:34:15.203684      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:34:15.226
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:34:15.233
  STEP: Creating a pod with one valid and two invalid sysctls @ 04/18/23 01:34:15.269
  Apr 18 01:34:15.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-2014" for this suite. @ 04/18/23 01:34:15.284
• [0.105 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 04/18/23 01:34:15.293
  Apr 18 01:34:15.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename dns @ 04/18/23 01:34:15.294
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:34:15.334
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:34:15.339
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 04/18/23 01:34:15.343
  Apr 18 01:34:15.357: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9356  9ddbb857-4aca-4bab-9f09-4f206ec29c30 71631 0 2023-04-18 01:34:15 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-04-18 01:34:15 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4t6p4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4t6p4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0418 01:34:16.204264      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:17.204379      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS suffix list is configured on pod... @ 04/18/23 01:34:17.371
  Apr 18 01:34:17.372: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9356 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 01:34:17.372: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 01:34:17.372: INFO: ExecWithOptions: Clientset creation
  Apr 18 01:34:17.372: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/dns-9356/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 04/18/23 01:34:17.466
  Apr 18 01:34:17.466: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9356 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 01:34:17.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 01:34:17.466: INFO: ExecWithOptions: Clientset creation
  Apr 18 01:34:17.466: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/dns-9356/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 18 01:34:17.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 18 01:34:17.555: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-9356" for this suite. @ 04/18/23 01:34:17.566
• [2.278 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 04/18/23 01:34:17.572
  Apr 18 01:34:17.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename disruption @ 04/18/23 01:34:17.574
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:34:17.591
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:34:17.597
  STEP: Creating a kubernetes client @ 04/18/23 01:34:17.601
  Apr 18 01:34:17.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename disruption-2 @ 04/18/23 01:34:17.603
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:34:17.618
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:34:17.622
  STEP: Waiting for the pdb to be processed @ 04/18/23 01:34:17.631
  E0418 01:34:18.204921      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:19.205045      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 04/18/23 01:34:19.644
  E0418 01:34:20.205120      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:21.205212      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 04/18/23 01:34:21.656
  E0418 01:34:22.205999      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:23.206143      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: listing a collection of PDBs across all namespaces @ 04/18/23 01:34:23.663
  STEP: listing a collection of PDBs in namespace disruption-2855 @ 04/18/23 01:34:23.669
  STEP: deleting a collection of PDBs @ 04/18/23 01:34:23.672
  STEP: Waiting for the PDB collection to be deleted @ 04/18/23 01:34:23.678
  Apr 18 01:34:23.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 18 01:34:23.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-1463" for this suite. @ 04/18/23 01:34:23.686
  STEP: Destroying namespace "disruption-2855" for this suite. @ 04/18/23 01:34:23.691
• [6.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 04/18/23 01:34:23.7
  Apr 18 01:34:23.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename services @ 04/18/23 01:34:23.701
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:34:23.717
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:34:23.721
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-126 @ 04/18/23 01:34:23.725
  STEP: changing the ExternalName service to type=ClusterIP @ 04/18/23 01:34:23.734
  STEP: creating replication controller externalname-service in namespace services-126 @ 04/18/23 01:34:23.751
  I0418 01:34:23.764879      21 runners.go:194] Created replication controller with name: externalname-service, namespace: services-126, replica count: 2
  E0418 01:34:24.207453      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:25.208420      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:26.208978      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0418 01:34:26.817077      21 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 18 01:34:26.817: INFO: Creating new exec pod
  E0418 01:34:27.209580      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:28.210331      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:29.210440      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:34:29.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-126 exec execpod4jhvs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 18 01:34:29.990: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 18 01:34:29.990: INFO: stdout: "externalname-service-k8txc"
  Apr 18 01:34:29.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-126 exec execpod4jhvs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.3.9.236 80'
  Apr 18 01:34:30.129: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.3.9.236 80\nConnection to 10.3.9.236 80 port [tcp/http] succeeded!\n"
  Apr 18 01:34:30.129: INFO: stdout: ""
  E0418 01:34:30.211094      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:34:31.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-126 exec execpod4jhvs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.3.9.236 80'
  E0418 01:34:31.211532      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:34:31.287: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.3.9.236 80\nConnection to 10.3.9.236 80 port [tcp/http] succeeded!\n"
  Apr 18 01:34:31.287: INFO: stdout: ""
  Apr 18 01:34:32.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-126 exec execpod4jhvs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.3.9.236 80'
  E0418 01:34:32.212551      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:34:32.298: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.3.9.236 80\nConnection to 10.3.9.236 80 port [tcp/http] succeeded!\n"
  Apr 18 01:34:32.298: INFO: stdout: "externalname-service-8sq2m"
  Apr 18 01:34:32.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 18 01:34:32.314: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-126" for this suite. @ 04/18/23 01:34:32.353
• [8.680 seconds]
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 04/18/23 01:34:32.38
  Apr 18 01:34:32.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename services @ 04/18/23 01:34:32.382
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:34:32.456
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:34:32.462
  Apr 18 01:34:32.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8554" for this suite. @ 04/18/23 01:34:32.493
• [0.130 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 04/18/23 01:34:32.513
  Apr 18 01:34:32.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename proxy @ 04/18/23 01:34:32.516
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:34:32.561
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:34:32.568
  STEP: starting an echo server on multiple ports @ 04/18/23 01:34:32.596
  STEP: creating replication controller proxy-service-p44dn in namespace proxy-5671 @ 04/18/23 01:34:32.596
  I0418 01:34:32.611973      21 runners.go:194] Created replication controller with name: proxy-service-p44dn, namespace: proxy-5671, replica count: 1
  E0418 01:34:33.213613      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0418 01:34:33.663363      21 runners.go:194] proxy-service-p44dn Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0418 01:34:34.213889      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0418 01:34:34.663520      21 runners.go:194] proxy-service-p44dn Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 18 01:34:34.671: INFO: setup took 2.090910583s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 04/18/23 01:34:34.671
  Apr 18 01:34:34.732: INFO: (0) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname1/proxy/: tls baz (200; 60.054626ms)
  Apr 18 01:34:34.732: INFO: (0) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">... (200; 59.784382ms)
  Apr 18 01:34:34.732: INFO: (0) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname2/proxy/: tls qux (200; 60.026922ms)
  Apr 18 01:34:34.735: INFO: (0) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname2/proxy/: bar (200; 62.85268ms)
  Apr 18 01:34:34.735: INFO: (0) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 62.801716ms)
  Apr 18 01:34:34.735: INFO: (0) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/rewriteme">test</a> (200; 63.261437ms)
  Apr 18 01:34:34.736: INFO: (0) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname1/proxy/: foo (200; 63.714573ms)
  Apr 18 01:34:34.736: INFO: (0) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 63.565289ms)
  Apr 18 01:34:34.736: INFO: (0) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/tlsrewritem... (200; 64.441308ms)
  Apr 18 01:34:34.736: INFO: (0) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 63.557455ms)
  Apr 18 01:34:34.736: INFO: (0) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:462/proxy/: tls qux (200; 64.172561ms)
  Apr 18 01:34:34.736: INFO: (0) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 63.802992ms)
  Apr 18 01:34:34.736: INFO: (0) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname2/proxy/: bar (200; 63.957563ms)
  Apr 18 01:34:34.742: INFO: (0) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:460/proxy/: tls baz (200; 71.063716ms)
  Apr 18 01:34:34.742: INFO: (0) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">test<... (200; 69.933123ms)
  Apr 18 01:34:34.750: INFO: (0) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname1/proxy/: foo (200; 77.053097ms)
  Apr 18 01:34:34.787: INFO: (1) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 37.059792ms)
  Apr 18 01:34:34.788: INFO: (1) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 37.950916ms)
  Apr 18 01:34:34.788: INFO: (1) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:462/proxy/: tls qux (200; 38.114479ms)
  Apr 18 01:34:34.788: INFO: (1) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 37.8404ms)
  Apr 18 01:34:34.788: INFO: (1) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/rewriteme">test</a> (200; 38.038199ms)
  Apr 18 01:34:34.791: INFO: (1) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">... (200; 40.550842ms)
  Apr 18 01:34:34.791: INFO: (1) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">test<... (200; 40.907872ms)
  Apr 18 01:34:34.791: INFO: (1) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/tlsrewritem... (200; 41.386709ms)
  Apr 18 01:34:34.791: INFO: (1) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 41.311021ms)
  Apr 18 01:34:34.791: INFO: (1) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:460/proxy/: tls baz (200; 41.538175ms)
  Apr 18 01:34:34.794: INFO: (1) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname1/proxy/: foo (200; 44.450221ms)
  Apr 18 01:34:34.795: INFO: (1) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname2/proxy/: bar (200; 44.528919ms)
  Apr 18 01:34:34.795: INFO: (1) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname1/proxy/: foo (200; 44.639342ms)
  Apr 18 01:34:34.796: INFO: (1) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname2/proxy/: bar (200; 46.418693ms)
  Apr 18 01:34:34.797: INFO: (1) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname1/proxy/: tls baz (200; 46.281484ms)
  Apr 18 01:34:34.797: INFO: (1) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname2/proxy/: tls qux (200; 46.34973ms)
  Apr 18 01:34:34.830: INFO: (2) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">test<... (200; 33.254788ms)
  Apr 18 01:34:34.830: INFO: (2) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname2/proxy/: bar (200; 33.196672ms)
  Apr 18 01:34:34.830: INFO: (2) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname1/proxy/: tls baz (200; 33.138728ms)
  Apr 18 01:34:34.830: INFO: (2) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:460/proxy/: tls baz (200; 33.192241ms)
  Apr 18 01:34:34.830: INFO: (2) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/tlsrewritem... (200; 33.326735ms)
  Apr 18 01:34:34.831: INFO: (2) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 34.317709ms)
  Apr 18 01:34:34.830: INFO: (2) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:462/proxy/: tls qux (200; 33.446677ms)
  Apr 18 01:34:34.830: INFO: (2) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/rewriteme">test</a> (200; 33.533886ms)
  Apr 18 01:34:34.830: INFO: (2) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 33.67107ms)
  Apr 18 01:34:34.831: INFO: (2) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname2/proxy/: tls qux (200; 33.894202ms)
  Apr 18 01:34:34.831: INFO: (2) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">... (200; 33.846028ms)
  Apr 18 01:34:34.831: INFO: (2) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 34.145494ms)
  Apr 18 01:34:34.832: INFO: (2) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname2/proxy/: bar (200; 34.621718ms)
  Apr 18 01:34:34.832: INFO: (2) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname1/proxy/: foo (200; 34.812805ms)
  Apr 18 01:34:34.832: INFO: (2) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 35.19692ms)
  Apr 18 01:34:34.832: INFO: (2) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname1/proxy/: foo (200; 35.031345ms)
  Apr 18 01:34:34.879: INFO: (3) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 46.014861ms)
  Apr 18 01:34:34.879: INFO: (3) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 45.642414ms)
  Apr 18 01:34:34.879: INFO: (3) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/rewriteme">test</a> (200; 45.672806ms)
  Apr 18 01:34:34.881: INFO: (3) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/tlsrewritem... (200; 47.967618ms)
  Apr 18 01:34:34.881: INFO: (3) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">... (200; 47.853205ms)
  Apr 18 01:34:34.881: INFO: (3) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:460/proxy/: tls baz (200; 47.665963ms)
  Apr 18 01:34:34.881: INFO: (3) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:462/proxy/: tls qux (200; 48.284613ms)
  Apr 18 01:34:34.884: INFO: (3) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">test<... (200; 51.454512ms)
  Apr 18 01:34:34.884: INFO: (3) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 50.960178ms)
  Apr 18 01:34:34.884: INFO: (3) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 50.860702ms)
  Apr 18 01:34:34.884: INFO: (3) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname1/proxy/: foo (200; 51.539334ms)
  Apr 18 01:34:34.884: INFO: (3) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname1/proxy/: foo (200; 51.915738ms)
  Apr 18 01:34:34.885: INFO: (3) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname2/proxy/: bar (200; 52.423651ms)
  Apr 18 01:34:34.885: INFO: (3) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname2/proxy/: bar (200; 52.297084ms)
  Apr 18 01:34:34.885: INFO: (3) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname2/proxy/: tls qux (200; 52.010177ms)
  Apr 18 01:34:34.885: INFO: (3) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname1/proxy/: tls baz (200; 52.48615ms)
  Apr 18 01:34:34.918: INFO: (4) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 32.102679ms)
  Apr 18 01:34:34.918: INFO: (4) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 32.399856ms)
  Apr 18 01:34:34.918: INFO: (4) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 32.366276ms)
  Apr 18 01:34:34.918: INFO: (4) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">... (200; 32.729602ms)
  Apr 18 01:34:34.918: INFO: (4) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/rewriteme">test</a> (200; 32.829929ms)
  Apr 18 01:34:34.918: INFO: (4) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 33.048877ms)
  Apr 18 01:34:34.918: INFO: (4) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">test<... (200; 32.873206ms)
  Apr 18 01:34:34.918: INFO: (4) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:462/proxy/: tls qux (200; 32.886605ms)
  Apr 18 01:34:34.918: INFO: (4) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/tlsrewritem... (200; 32.844749ms)
  Apr 18 01:34:34.919: INFO: (4) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:460/proxy/: tls baz (200; 32.685331ms)
  Apr 18 01:34:34.919: INFO: (4) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname2/proxy/: tls qux (200; 33.362494ms)
  Apr 18 01:34:34.919: INFO: (4) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname1/proxy/: foo (200; 33.248336ms)
  Apr 18 01:34:34.919: INFO: (4) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname2/proxy/: bar (200; 33.426751ms)
  Apr 18 01:34:34.919: INFO: (4) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname1/proxy/: tls baz (200; 33.524658ms)
  Apr 18 01:34:34.919: INFO: (4) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname1/proxy/: foo (200; 33.582495ms)
  Apr 18 01:34:34.919: INFO: (4) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname2/proxy/: bar (200; 33.713531ms)
  Apr 18 01:34:34.963: INFO: (5) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">test<... (200; 43.426155ms)
  Apr 18 01:34:34.963: INFO: (5) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 43.48783ms)
  Apr 18 01:34:34.964: INFO: (5) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 43.870668ms)
  Apr 18 01:34:34.964: INFO: (5) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">... (200; 43.992806ms)
  Apr 18 01:34:34.964: INFO: (5) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 43.919266ms)
  Apr 18 01:34:34.964: INFO: (5) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 44.667186ms)
  Apr 18 01:34:34.964: INFO: (5) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/rewriteme">test</a> (200; 44.41491ms)
  Apr 18 01:34:34.965: INFO: (5) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/tlsrewritem... (200; 44.377437ms)
  Apr 18 01:34:34.965: INFO: (5) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:460/proxy/: tls baz (200; 44.523871ms)
  Apr 18 01:34:34.965: INFO: (5) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:462/proxy/: tls qux (200; 44.865515ms)
  Apr 18 01:34:34.965: INFO: (5) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname1/proxy/: foo (200; 45.446751ms)
  Apr 18 01:34:34.965: INFO: (5) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname2/proxy/: bar (200; 45.213425ms)
  Apr 18 01:34:34.966: INFO: (5) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname2/proxy/: tls qux (200; 45.665373ms)
  Apr 18 01:34:34.966: INFO: (5) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname1/proxy/: tls baz (200; 45.717586ms)
  Apr 18 01:34:34.966: INFO: (5) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname1/proxy/: foo (200; 45.621124ms)
  Apr 18 01:34:34.966: INFO: (5) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname2/proxy/: bar (200; 45.584226ms)
  Apr 18 01:34:35.004: INFO: (6) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 38.434495ms)
  Apr 18 01:34:35.004: INFO: (6) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/rewriteme">test</a> (200; 38.107064ms)
  Apr 18 01:34:35.005: INFO: (6) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">test<... (200; 38.712416ms)
  Apr 18 01:34:35.008: INFO: (6) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:460/proxy/: tls baz (200; 42.302698ms)
  Apr 18 01:34:35.008: INFO: (6) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 42.428432ms)
  Apr 18 01:34:35.008: INFO: (6) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/tlsrewritem... (200; 42.531638ms)
  Apr 18 01:34:35.008: INFO: (6) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:462/proxy/: tls qux (200; 42.672579ms)
  Apr 18 01:34:35.012: INFO: (6) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname2/proxy/: bar (200; 45.864275ms)
  Apr 18 01:34:35.013: INFO: (6) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 47.334334ms)
  Apr 18 01:34:35.013: INFO: (6) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname1/proxy/: foo (200; 47.230676ms)
  Apr 18 01:34:35.013: INFO: (6) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">... (200; 47.375889ms)
  Apr 18 01:34:35.013: INFO: (6) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 47.317356ms)
  Apr 18 01:34:35.013: INFO: (6) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname2/proxy/: tls qux (200; 47.212221ms)
  Apr 18 01:34:35.013: INFO: (6) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname1/proxy/: tls baz (200; 47.359393ms)
  Apr 18 01:34:35.014: INFO: (6) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname2/proxy/: bar (200; 47.879214ms)
  Apr 18 01:34:35.014: INFO: (6) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname1/proxy/: foo (200; 48.030587ms)
  Apr 18 01:34:35.045: INFO: (7) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:460/proxy/: tls baz (200; 30.974741ms)
  Apr 18 01:34:35.045: INFO: (7) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 30.731349ms)
  Apr 18 01:34:35.052: INFO: (7) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 38.009678ms)
  Apr 18 01:34:35.052: INFO: (7) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 38.027926ms)
  Apr 18 01:34:35.053: INFO: (7) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/rewriteme">test</a> (200; 38.016612ms)
  Apr 18 01:34:35.053: INFO: (7) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">test<... (200; 38.01625ms)
  Apr 18 01:34:35.053: INFO: (7) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 38.332513ms)
  Apr 18 01:34:35.053: INFO: (7) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">... (200; 38.349543ms)
  Apr 18 01:34:35.053: INFO: (7) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/tlsrewritem... (200; 38.977679ms)
  Apr 18 01:34:35.053: INFO: (7) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:462/proxy/: tls qux (200; 38.511088ms)
  Apr 18 01:34:35.054: INFO: (7) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname1/proxy/: tls baz (200; 39.310424ms)
  Apr 18 01:34:35.054: INFO: (7) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname1/proxy/: foo (200; 39.66126ms)
  Apr 18 01:34:35.054: INFO: (7) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname2/proxy/: bar (200; 40.04102ms)
  Apr 18 01:34:35.054: INFO: (7) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname2/proxy/: tls qux (200; 39.834548ms)
  Apr 18 01:34:35.054: INFO: (7) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname1/proxy/: foo (200; 39.911112ms)
  Apr 18 01:34:35.054: INFO: (7) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname2/proxy/: bar (200; 40.112956ms)
  Apr 18 01:34:35.090: INFO: (8) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">... (200; 34.443617ms)
  Apr 18 01:34:35.090: INFO: (8) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 34.271351ms)
  Apr 18 01:34:35.090: INFO: (8) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 34.265236ms)
  Apr 18 01:34:35.090: INFO: (8) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/rewriteme">test</a> (200; 35.103912ms)
  Apr 18 01:34:35.090: INFO: (8) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 34.291873ms)
  Apr 18 01:34:35.091: INFO: (8) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">test<... (200; 35.852701ms)
  Apr 18 01:34:35.091: INFO: (8) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:462/proxy/: tls qux (200; 36.026217ms)
  Apr 18 01:34:35.091: INFO: (8) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:460/proxy/: tls baz (200; 36.086253ms)
  Apr 18 01:34:35.099: INFO: (8) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 44.149273ms)
  Apr 18 01:34:35.099: INFO: (8) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/tlsrewritem... (200; 44.690612ms)
  Apr 18 01:34:35.103: INFO: (8) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname2/proxy/: bar (200; 47.948231ms)
  Apr 18 01:34:35.104: INFO: (8) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname1/proxy/: foo (200; 49.231466ms)
  Apr 18 01:34:35.104: INFO: (8) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname1/proxy/: foo (200; 49.140616ms)
  Apr 18 01:34:35.104: INFO: (8) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname1/proxy/: tls baz (200; 49.527178ms)
  Apr 18 01:34:35.104: INFO: (8) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname2/proxy/: tls qux (200; 49.659488ms)
  Apr 18 01:34:35.104: INFO: (8) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname2/proxy/: bar (200; 48.971771ms)
  Apr 18 01:34:35.117: INFO: (9) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 12.794452ms)
  Apr 18 01:34:35.118: INFO: (9) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:460/proxy/: tls baz (200; 13.602182ms)
  Apr 18 01:34:35.126: INFO: (9) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 21.373102ms)
  Apr 18 01:34:35.127: INFO: (9) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/tlsrewritem... (200; 21.48727ms)
  Apr 18 01:34:35.127: INFO: (9) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:462/proxy/: tls qux (200; 21.942695ms)
  Apr 18 01:34:35.127: INFO: (9) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname2/proxy/: tls qux (200; 21.874825ms)
  Apr 18 01:34:35.127: INFO: (9) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">test<... (200; 22.23132ms)
  Apr 18 01:34:35.127: INFO: (9) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 22.330454ms)
  Apr 18 01:34:35.127: INFO: (9) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 22.304476ms)
  Apr 18 01:34:35.127: INFO: (9) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/rewriteme">test</a> (200; 22.36392ms)
  Apr 18 01:34:35.128: INFO: (9) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">... (200; 22.919914ms)
  Apr 18 01:34:35.128: INFO: (9) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname1/proxy/: foo (200; 22.849981ms)
  Apr 18 01:34:35.128: INFO: (9) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname1/proxy/: tls baz (200; 22.816777ms)
  Apr 18 01:34:35.128: INFO: (9) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname1/proxy/: foo (200; 22.672141ms)
  Apr 18 01:34:35.128: INFO: (9) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname2/proxy/: bar (200; 22.603744ms)
  Apr 18 01:34:35.128: INFO: (9) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname2/proxy/: bar (200; 22.734811ms)
  Apr 18 01:34:35.146: INFO: (10) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/rewriteme">test</a> (200; 17.280249ms)
  Apr 18 01:34:35.146: INFO: (10) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 17.099836ms)
  Apr 18 01:34:35.146: INFO: (10) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 17.182274ms)
  Apr 18 01:34:35.151: INFO: (10) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">... (200; 21.767053ms)
  Apr 18 01:34:35.151: INFO: (10) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">test<... (200; 21.584872ms)
  Apr 18 01:34:35.151: INFO: (10) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 22.283806ms)
  Apr 18 01:34:35.151: INFO: (10) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 21.765247ms)
  Apr 18 01:34:35.151: INFO: (10) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/tlsrewritem... (200; 22.054662ms)
  Apr 18 01:34:35.151: INFO: (10) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:462/proxy/: tls qux (200; 21.977985ms)
  Apr 18 01:34:35.151: INFO: (10) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:460/proxy/: tls baz (200; 22.267449ms)
  Apr 18 01:34:35.151: INFO: (10) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname2/proxy/: bar (200; 23.108542ms)
  Apr 18 01:34:35.152: INFO: (10) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname2/proxy/: bar (200; 22.489568ms)
  Apr 18 01:34:35.152: INFO: (10) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname1/proxy/: foo (200; 22.611022ms)
  Apr 18 01:34:35.152: INFO: (10) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname2/proxy/: tls qux (200; 22.880894ms)
  Apr 18 01:34:35.152: INFO: (10) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname1/proxy/: tls baz (200; 22.971604ms)
  Apr 18 01:34:35.152: INFO: (10) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname1/proxy/: foo (200; 23.081431ms)
  Apr 18 01:34:35.168: INFO: (11) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 15.240055ms)
  Apr 18 01:34:35.168: INFO: (11) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 15.30278ms)
  Apr 18 01:34:35.168: INFO: (11) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/rewriteme">test</a> (200; 15.389719ms)
  Apr 18 01:34:35.168: INFO: (11) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">test<... (200; 15.51899ms)
  Apr 18 01:34:35.168: INFO: (11) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 15.593472ms)
  Apr 18 01:34:35.168: INFO: (11) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 15.877654ms)
  Apr 18 01:34:35.168: INFO: (11) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:462/proxy/: tls qux (200; 15.702387ms)
  Apr 18 01:34:35.168: INFO: (11) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">... (200; 15.916877ms)
  Apr 18 01:34:35.168: INFO: (11) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/tlsrewritem... (200; 16.147565ms)
  Apr 18 01:34:35.168: INFO: (11) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:460/proxy/: tls baz (200; 16.311896ms)
  Apr 18 01:34:35.169: INFO: (11) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname1/proxy/: foo (200; 16.623757ms)
  Apr 18 01:34:35.169: INFO: (11) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname1/proxy/: foo (200; 16.501072ms)
  Apr 18 01:34:35.169: INFO: (11) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname2/proxy/: bar (200; 17.049512ms)
  Apr 18 01:34:35.169: INFO: (11) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname1/proxy/: tls baz (200; 16.997013ms)
  Apr 18 01:34:35.169: INFO: (11) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname2/proxy/: tls qux (200; 16.746066ms)
  Apr 18 01:34:35.169: INFO: (11) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname2/proxy/: bar (200; 16.97942ms)
  Apr 18 01:34:35.186: INFO: (12) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 15.98896ms)
  Apr 18 01:34:35.187: INFO: (12) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 16.300863ms)
  Apr 18 01:34:35.187: INFO: (12) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 16.292213ms)
  Apr 18 01:34:35.187: INFO: (12) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/rewriteme">test</a> (200; 16.858156ms)
  Apr 18 01:34:35.187: INFO: (12) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 16.400836ms)
  Apr 18 01:34:35.187: INFO: (12) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">test<... (200; 17.051523ms)
  Apr 18 01:34:35.187: INFO: (12) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:460/proxy/: tls baz (200; 16.661416ms)
  Apr 18 01:34:35.187: INFO: (12) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">... (200; 17.165503ms)
  Apr 18 01:34:35.187: INFO: (12) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/tlsrewritem... (200; 17.028318ms)
  Apr 18 01:34:35.187: INFO: (12) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname2/proxy/: tls qux (200; 17.677743ms)
  Apr 18 01:34:35.187: INFO: (12) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:462/proxy/: tls qux (200; 17.380046ms)
  Apr 18 01:34:35.188: INFO: (12) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname2/proxy/: bar (200; 18.124802ms)
  Apr 18 01:34:35.188: INFO: (12) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname2/proxy/: bar (200; 17.960343ms)
  Apr 18 01:34:35.189: INFO: (12) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname1/proxy/: foo (200; 18.537514ms)
  Apr 18 01:34:35.189: INFO: (12) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname1/proxy/: tls baz (200; 19.011932ms)
  Apr 18 01:34:35.189: INFO: (12) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname1/proxy/: foo (200; 18.502341ms)
  Apr 18 01:34:35.200: INFO: (13) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:460/proxy/: tls baz (200; 11.29096ms)
  Apr 18 01:34:35.203: INFO: (13) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 13.32142ms)
  Apr 18 01:34:35.203: INFO: (13) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">... (200; 13.288523ms)
  Apr 18 01:34:35.203: INFO: (13) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 13.489492ms)
  Apr 18 01:34:35.203: INFO: (13) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 13.874236ms)
  Apr 18 01:34:35.204: INFO: (13) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 14.158374ms)
  Apr 18 01:34:35.204: INFO: (13) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">test<... (200; 14.703887ms)
  Apr 18 01:34:35.204: INFO: (13) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/rewriteme">test</a> (200; 14.866026ms)
  Apr 18 01:34:35.204: INFO: (13) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:462/proxy/: tls qux (200; 14.524829ms)
  Apr 18 01:34:35.204: INFO: (13) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname2/proxy/: tls qux (200; 15.102403ms)
  Apr 18 01:34:35.204: INFO: (13) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/tlsrewritem... (200; 14.90743ms)
  Apr 18 01:34:35.204: INFO: (13) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname2/proxy/: bar (200; 14.840961ms)
  Apr 18 01:34:35.205: INFO: (13) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname1/proxy/: foo (200; 15.393495ms)
  Apr 18 01:34:35.205: INFO: (13) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname1/proxy/: foo (200; 15.079707ms)
  Apr 18 01:34:35.205: INFO: (13) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname2/proxy/: bar (200; 15.535509ms)
  Apr 18 01:34:35.205: INFO: (13) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname1/proxy/: tls baz (200; 16.070982ms)
  E0418 01:34:35.214831      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:34:35.216: INFO: (14) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 11.210734ms)
  Apr 18 01:34:35.219: INFO: (14) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/rewriteme">test</a> (200; 13.583445ms)
  Apr 18 01:34:35.220: INFO: (14) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 14.255503ms)
  Apr 18 01:34:35.220: INFO: (14) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 14.161362ms)
  Apr 18 01:34:35.220: INFO: (14) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 14.614782ms)
  Apr 18 01:34:35.220: INFO: (14) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">... (200; 14.859883ms)
  Apr 18 01:34:35.220: INFO: (14) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">test<... (200; 14.495738ms)
  Apr 18 01:34:35.220: INFO: (14) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/tlsrewritem... (200; 14.498111ms)
  Apr 18 01:34:35.221: INFO: (14) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:460/proxy/: tls baz (200; 15.052076ms)
  Apr 18 01:34:35.221: INFO: (14) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:462/proxy/: tls qux (200; 14.772818ms)
  Apr 18 01:34:35.221: INFO: (14) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname2/proxy/: bar (200; 14.946301ms)
  Apr 18 01:34:35.221: INFO: (14) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname2/proxy/: tls qux (200; 15.223341ms)
  Apr 18 01:34:35.221: INFO: (14) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname1/proxy/: tls baz (200; 15.295544ms)
  Apr 18 01:34:35.221: INFO: (14) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname1/proxy/: foo (200; 15.389696ms)
  Apr 18 01:34:35.221: INFO: (14) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname2/proxy/: bar (200; 15.019047ms)
  Apr 18 01:34:35.221: INFO: (14) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname1/proxy/: foo (200; 15.177824ms)
  Apr 18 01:34:35.233: INFO: (15) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/tlsrewritem... (200; 11.897021ms)
  Apr 18 01:34:35.236: INFO: (15) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 13.124293ms)
  Apr 18 01:34:35.236: INFO: (15) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 13.215282ms)
  Apr 18 01:34:35.236: INFO: (15) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 13.324515ms)
  Apr 18 01:34:35.236: INFO: (15) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 13.18956ms)
  Apr 18 01:34:35.236: INFO: (15) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">test<... (200; 13.791069ms)
  Apr 18 01:34:35.236: INFO: (15) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/rewriteme">test</a> (200; 14.012951ms)
  Apr 18 01:34:35.236: INFO: (15) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">... (200; 14.366294ms)
  Apr 18 01:34:35.237: INFO: (15) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:462/proxy/: tls qux (200; 15.293656ms)
  Apr 18 01:34:35.237: INFO: (15) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:460/proxy/: tls baz (200; 15.751454ms)
  Apr 18 01:34:35.239: INFO: (15) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname1/proxy/: foo (200; 16.955582ms)
  Apr 18 01:34:35.239: INFO: (15) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname2/proxy/: bar (200; 16.872432ms)
  Apr 18 01:34:35.240: INFO: (15) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname1/proxy/: foo (200; 17.832405ms)
  Apr 18 01:34:35.240: INFO: (15) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname2/proxy/: tls qux (200; 17.754358ms)
  Apr 18 01:34:35.240: INFO: (15) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname2/proxy/: bar (200; 17.484183ms)
  Apr 18 01:34:35.240: INFO: (15) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname1/proxy/: tls baz (200; 17.891486ms)
  Apr 18 01:34:35.256: INFO: (16) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">test<... (200; 15.167493ms)
  Apr 18 01:34:35.256: INFO: (16) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/rewriteme">test</a> (200; 15.218647ms)
  Apr 18 01:34:35.256: INFO: (16) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 16.040108ms)
  Apr 18 01:34:35.256: INFO: (16) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">... (200; 16.215171ms)
  Apr 18 01:34:35.256: INFO: (16) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 16.486759ms)
  Apr 18 01:34:35.256: INFO: (16) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 16.142437ms)
  Apr 18 01:34:35.256: INFO: (16) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:460/proxy/: tls baz (200; 15.944946ms)
  Apr 18 01:34:35.257: INFO: (16) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:462/proxy/: tls qux (200; 16.437886ms)
  Apr 18 01:34:35.257: INFO: (16) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname1/proxy/: foo (200; 16.478548ms)
  Apr 18 01:34:35.257: INFO: (16) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname2/proxy/: bar (200; 16.291883ms)
  Apr 18 01:34:35.257: INFO: (16) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname2/proxy/: bar (200; 16.265641ms)
  Apr 18 01:34:35.257: INFO: (16) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname2/proxy/: tls qux (200; 16.557089ms)
  Apr 18 01:34:35.257: INFO: (16) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname1/proxy/: tls baz (200; 17.162402ms)
  Apr 18 01:34:35.257: INFO: (16) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname1/proxy/: foo (200; 16.676602ms)
  Apr 18 01:34:35.257: INFO: (16) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/tlsrewritem... (200; 16.555202ms)
  Apr 18 01:34:35.257: INFO: (16) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 17.087916ms)
  Apr 18 01:34:35.276: INFO: (17) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">test<... (200; 17.191471ms)
  Apr 18 01:34:35.276: INFO: (17) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 17.18879ms)
  Apr 18 01:34:35.276: INFO: (17) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 17.502066ms)
  Apr 18 01:34:35.276: INFO: (17) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 17.628213ms)
  Apr 18 01:34:35.276: INFO: (17) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">... (200; 18.211511ms)
  Apr 18 01:34:35.276: INFO: (17) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/rewriteme">test</a> (200; 17.62239ms)
  Apr 18 01:34:35.276: INFO: (17) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:462/proxy/: tls qux (200; 17.576522ms)
  Apr 18 01:34:35.276: INFO: (17) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/tlsrewritem... (200; 17.558494ms)
  Apr 18 01:34:35.276: INFO: (17) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 17.501598ms)
  Apr 18 01:34:35.276: INFO: (17) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:460/proxy/: tls baz (200; 17.471982ms)
  Apr 18 01:34:35.277: INFO: (17) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname2/proxy/: bar (200; 18.382819ms)
  Apr 18 01:34:35.277: INFO: (17) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname2/proxy/: bar (200; 18.408033ms)
  Apr 18 01:34:35.277: INFO: (17) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname1/proxy/: foo (200; 18.946066ms)
  Apr 18 01:34:35.277: INFO: (17) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname1/proxy/: tls baz (200; 18.855916ms)
  Apr 18 01:34:35.277: INFO: (17) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname2/proxy/: tls qux (200; 18.506109ms)
  Apr 18 01:34:35.277: INFO: (17) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname1/proxy/: foo (200; 18.404744ms)
  Apr 18 01:34:35.290: INFO: (18) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/tlsrewritem... (200; 12.261559ms)
  Apr 18 01:34:35.292: INFO: (18) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 13.288958ms)
  Apr 18 01:34:35.292: INFO: (18) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">... (200; 13.130683ms)
  Apr 18 01:34:35.292: INFO: (18) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">test<... (200; 13.789914ms)
  Apr 18 01:34:35.293: INFO: (18) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname1/proxy/: foo (200; 14.279455ms)
  Apr 18 01:34:35.292: INFO: (18) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 13.516482ms)
  Apr 18 01:34:35.292: INFO: (18) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/rewriteme">test</a> (200; 14.330031ms)
  Apr 18 01:34:35.292: INFO: (18) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 14.72378ms)
  Apr 18 01:34:35.293: INFO: (18) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 13.909515ms)
  Apr 18 01:34:35.293: INFO: (18) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:460/proxy/: tls baz (200; 13.771634ms)
  Apr 18 01:34:35.293: INFO: (18) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:462/proxy/: tls qux (200; 14.138119ms)
  Apr 18 01:34:35.293: INFO: (18) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname1/proxy/: tls baz (200; 14.986612ms)
  Apr 18 01:34:35.294: INFO: (18) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname2/proxy/: tls qux (200; 15.747505ms)
  Apr 18 01:34:35.294: INFO: (18) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname2/proxy/: bar (200; 15.065753ms)
  Apr 18 01:34:35.294: INFO: (18) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname1/proxy/: foo (200; 15.331903ms)
  Apr 18 01:34:35.294: INFO: (18) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname2/proxy/: bar (200; 15.33777ms)
  Apr 18 01:34:35.307: INFO: (19) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 12.098613ms)
  Apr 18 01:34:35.310: INFO: (19) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">... (200; 15.384238ms)
  Apr 18 01:34:35.310: INFO: (19) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd/proxy/rewriteme">test</a> (200; 15.311265ms)
  Apr 18 01:34:35.310: INFO: (19) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:1080/proxy/rewriteme">test<... (200; 15.293183ms)
  Apr 18 01:34:35.311: INFO: (19) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:160/proxy/: foo (200; 16.140501ms)
  Apr 18 01:34:35.311: INFO: (19) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname2/proxy/: bar (200; 16.361681ms)
  Apr 18 01:34:35.311: INFO: (19) /api/v1/namespaces/proxy-5671/pods/http:proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 15.71236ms)
  Apr 18 01:34:35.311: INFO: (19) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:460/proxy/: tls baz (200; 16.175493ms)
  Apr 18 01:34:35.311: INFO: (19) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:462/proxy/: tls qux (200; 15.975481ms)
  Apr 18 01:34:35.311: INFO: (19) /api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/: <a href="/api/v1/namespaces/proxy-5671/pods/https:proxy-service-p44dn-dpvfd:443/proxy/tlsrewritem... (200; 15.917554ms)
  Apr 18 01:34:35.312: INFO: (19) /api/v1/namespaces/proxy-5671/pods/proxy-service-p44dn-dpvfd:162/proxy/: bar (200; 17.161026ms)
  Apr 18 01:34:35.312: INFO: (19) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname1/proxy/: foo (200; 16.899741ms)
  Apr 18 01:34:35.312: INFO: (19) /api/v1/namespaces/proxy-5671/services/proxy-service-p44dn:portname1/proxy/: foo (200; 17.135761ms)
  Apr 18 01:34:35.312: INFO: (19) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname2/proxy/: tls qux (200; 17.389946ms)
  Apr 18 01:34:35.312: INFO: (19) /api/v1/namespaces/proxy-5671/services/http:proxy-service-p44dn:portname2/proxy/: bar (200; 17.163367ms)
  Apr 18 01:34:35.312: INFO: (19) /api/v1/namespaces/proxy-5671/services/https:proxy-service-p44dn:tlsportname1/proxy/: tls baz (200; 17.488875ms)
  Apr 18 01:34:35.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-p44dn in namespace proxy-5671, will wait for the garbage collector to delete the pods @ 04/18/23 01:34:35.316
  Apr 18 01:34:35.373: INFO: Deleting ReplicationController proxy-service-p44dn took: 4.699139ms
  Apr 18 01:34:35.474: INFO: Terminating ReplicationController proxy-service-p44dn pods took: 100.715796ms
  E0418 01:34:36.215387      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:37.216128      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:38.216694      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-5671" for this suite. @ 04/18/23 01:34:38.478
• [5.977 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 04/18/23 01:34:38.49
  Apr 18 01:34:38.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename deployment @ 04/18/23 01:34:38.491
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:34:38.511
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:34:38.516
  STEP: creating a Deployment @ 04/18/23 01:34:38.528
  Apr 18 01:34:38.528: INFO: Creating simple deployment test-deployment-7qxkn
  Apr 18 01:34:38.545: INFO: deployment "test-deployment-7qxkn" doesn't have the required revision set
  E0418 01:34:39.217649      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:40.217904      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Getting /status @ 04/18/23 01:34:40.567
  Apr 18 01:34:40.576: INFO: Deployment test-deployment-7qxkn has Conditions: [{Available True 2023-04-18 01:34:39 +0000 UTC 2023-04-18 01:34:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-04-18 01:34:39 +0000 UTC 2023-04-18 01:34:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7qxkn-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 04/18/23 01:34:40.576
  Apr 18 01:34:40.586: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 1, 34, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 1, 34, 39, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 1, 34, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 1, 34, 38, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-7qxkn-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 04/18/23 01:34:40.587
  Apr 18 01:34:40.589: INFO: Observed &Deployment event: ADDED
  Apr 18 01:34:40.589: INFO: Observed Deployment test-deployment-7qxkn in namespace deployment-930 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-18 01:34:38 +0000 UTC 2023-04-18 01:34:38 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7qxkn-5994cf9475"}
  Apr 18 01:34:40.590: INFO: Observed &Deployment event: MODIFIED
  Apr 18 01:34:40.590: INFO: Observed Deployment test-deployment-7qxkn in namespace deployment-930 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-18 01:34:38 +0000 UTC 2023-04-18 01:34:38 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7qxkn-5994cf9475"}
  Apr 18 01:34:40.594: INFO: Observed Deployment test-deployment-7qxkn in namespace deployment-930 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-18 01:34:38 +0000 UTC 2023-04-18 01:34:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 18 01:34:40.595: INFO: Observed &Deployment event: MODIFIED
  Apr 18 01:34:40.601: INFO: Observed Deployment test-deployment-7qxkn in namespace deployment-930 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-18 01:34:38 +0000 UTC 2023-04-18 01:34:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 18 01:34:40.601: INFO: Observed Deployment test-deployment-7qxkn in namespace deployment-930 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-18 01:34:38 +0000 UTC 2023-04-18 01:34:38 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-7qxkn-5994cf9475" is progressing.}
  Apr 18 01:34:40.601: INFO: Observed &Deployment event: MODIFIED
  Apr 18 01:34:40.605: INFO: Observed Deployment test-deployment-7qxkn in namespace deployment-930 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-18 01:34:39 +0000 UTC 2023-04-18 01:34:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 18 01:34:40.606: INFO: Observed Deployment test-deployment-7qxkn in namespace deployment-930 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-18 01:34:39 +0000 UTC 2023-04-18 01:34:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7qxkn-5994cf9475" has successfully progressed.}
  Apr 18 01:34:40.606: INFO: Observed &Deployment event: MODIFIED
  Apr 18 01:34:40.606: INFO: Observed Deployment test-deployment-7qxkn in namespace deployment-930 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-18 01:34:39 +0000 UTC 2023-04-18 01:34:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 18 01:34:40.607: INFO: Observed Deployment test-deployment-7qxkn in namespace deployment-930 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-18 01:34:39 +0000 UTC 2023-04-18 01:34:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7qxkn-5994cf9475" has successfully progressed.}
  Apr 18 01:34:40.607: INFO: Found Deployment test-deployment-7qxkn in namespace deployment-930 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 18 01:34:40.608: INFO: Deployment test-deployment-7qxkn has an updated status
  STEP: patching the Statefulset Status @ 04/18/23 01:34:40.608
  Apr 18 01:34:40.609: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 18 01:34:40.616: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 04/18/23 01:34:40.616
  Apr 18 01:34:40.620: INFO: Observed &Deployment event: ADDED
  Apr 18 01:34:40.620: INFO: Observed deployment test-deployment-7qxkn in namespace deployment-930 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-18 01:34:38 +0000 UTC 2023-04-18 01:34:38 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7qxkn-5994cf9475"}
  Apr 18 01:34:40.620: INFO: Observed &Deployment event: MODIFIED
  Apr 18 01:34:40.621: INFO: Observed deployment test-deployment-7qxkn in namespace deployment-930 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-18 01:34:38 +0000 UTC 2023-04-18 01:34:38 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7qxkn-5994cf9475"}
  Apr 18 01:34:40.621: INFO: Observed deployment test-deployment-7qxkn in namespace deployment-930 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-18 01:34:38 +0000 UTC 2023-04-18 01:34:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 18 01:34:40.621: INFO: Observed &Deployment event: MODIFIED
  Apr 18 01:34:40.621: INFO: Observed deployment test-deployment-7qxkn in namespace deployment-930 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-18 01:34:38 +0000 UTC 2023-04-18 01:34:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 18 01:34:40.621: INFO: Observed deployment test-deployment-7qxkn in namespace deployment-930 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-18 01:34:38 +0000 UTC 2023-04-18 01:34:38 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-7qxkn-5994cf9475" is progressing.}
  Apr 18 01:34:40.621: INFO: Observed &Deployment event: MODIFIED
  Apr 18 01:34:40.621: INFO: Observed deployment test-deployment-7qxkn in namespace deployment-930 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-18 01:34:39 +0000 UTC 2023-04-18 01:34:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 18 01:34:40.621: INFO: Observed deployment test-deployment-7qxkn in namespace deployment-930 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-18 01:34:39 +0000 UTC 2023-04-18 01:34:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7qxkn-5994cf9475" has successfully progressed.}
  Apr 18 01:34:40.621: INFO: Observed &Deployment event: MODIFIED
  Apr 18 01:34:40.621: INFO: Observed deployment test-deployment-7qxkn in namespace deployment-930 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-18 01:34:39 +0000 UTC 2023-04-18 01:34:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 18 01:34:40.621: INFO: Observed deployment test-deployment-7qxkn in namespace deployment-930 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-18 01:34:39 +0000 UTC 2023-04-18 01:34:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7qxkn-5994cf9475" has successfully progressed.}
  Apr 18 01:34:40.621: INFO: Observed deployment test-deployment-7qxkn in namespace deployment-930 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 18 01:34:40.622: INFO: Observed &Deployment event: MODIFIED
  Apr 18 01:34:40.622: INFO: Found deployment test-deployment-7qxkn in namespace deployment-930 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Apr 18 01:34:40.622: INFO: Deployment test-deployment-7qxkn has a patched status
  Apr 18 01:34:40.629: INFO: Deployment "test-deployment-7qxkn":
  &Deployment{ObjectMeta:{test-deployment-7qxkn  deployment-930  a7c0d3b2-42de-4520-aab7-cc5a4e28d032 71970 1 2023-04-18 01:34:38 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-04-18 01:34:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-04-18 01:34:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-04-18 01:34:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052a5dd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-7qxkn-5994cf9475",LastUpdateTime:2023-04-18 01:34:40 +0000 UTC,LastTransitionTime:2023-04-18 01:34:40 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 18 01:34:40.635: INFO: New ReplicaSet "test-deployment-7qxkn-5994cf9475" of Deployment "test-deployment-7qxkn":
  &ReplicaSet{ObjectMeta:{test-deployment-7qxkn-5994cf9475  deployment-930  3ff70099-2d89-45ad-b1ff-253cbf2d86e5 71955 1 2023-04-18 01:34:38 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-7qxkn a7c0d3b2-42de-4520-aab7-cc5a4e28d032 0xc00502e1b0 0xc00502e1b1}] [] [{kube-controller-manager Update apps/v1 2023-04-18 01:34:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a7c0d3b2-42de-4520-aab7-cc5a4e28d032\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-18 01:34:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00502e258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 18 01:34:40.648: INFO: Pod "test-deployment-7qxkn-5994cf9475-sgbsl" is available:
  &Pod{ObjectMeta:{test-deployment-7qxkn-5994cf9475-sgbsl test-deployment-7qxkn-5994cf9475- deployment-930  04744c0e-201d-4f7c-bfac-02efb9697aeb 71954 0 2023-04-18 01:34:38 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[cni.projectcalico.org/containerID:1f2dc6220677f0dc9d4c680f069b1a651daa4b59e27264f30e9f6054e941d9b9 cni.projectcalico.org/podIP:10.2.212.183/32 cni.projectcalico.org/podIPs:10.2.212.183/32] [{apps/v1 ReplicaSet test-deployment-7qxkn-5994cf9475 3ff70099-2d89-45ad-b1ff-253cbf2d86e5 0xc00502e630 0xc00502e631}] [] [{kube-controller-manager Update v1 2023-04-18 01:34:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ff70099-2d89-45ad-b1ff-253cbf2d86e5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-18 01:34:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-18 01:34:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.212.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rrfhz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rrfhz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-14-154,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:34:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:34:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:34:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:34:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.14.154,PodIP:10.2.212.183,StartTime:2023-04-18 01:34:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-18 01:34:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0c83cf83bb0b249a707acf7e618a8f96f9263551ffc6dc853d761012dfa3f48b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.212.183,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 01:34:40.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-930" for this suite. @ 04/18/23 01:34:40.654
• [2.173 seconds]
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 04/18/23 01:34:40.664
  Apr 18 01:34:40.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename security-context-test @ 04/18/23 01:34:40.665
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:34:40.681
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:34:40.696
  E0418 01:34:41.219584      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:42.220303      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:43.221263      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:44.221369      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:34:44.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-2804" for this suite. @ 04/18/23 01:34:44.725
• [4.067 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 04/18/23 01:34:44.732
  Apr 18 01:34:44.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename webhook @ 04/18/23 01:34:44.732
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:34:44.752
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:34:44.761
  STEP: Setting up server cert @ 04/18/23 01:34:44.787
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/18/23 01:34:45.067
  STEP: Deploying the webhook pod @ 04/18/23 01:34:45.074
  STEP: Wait for the deployment to be ready @ 04/18/23 01:34:45.084
  Apr 18 01:34:45.093: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0418 01:34:45.221644      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:46.222175      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/18/23 01:34:47.102
  STEP: Verifying the service has paired with the endpoint @ 04/18/23 01:34:47.111
  E0418 01:34:47.224106      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:34:48.112: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 04/18/23 01:34:48.115
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/18/23 01:34:48.115
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 04/18/23 01:34:48.13
  E0418 01:34:48.224183      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 04/18/23 01:34:49.138
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/18/23 01:34:49.138
  E0418 01:34:49.224640      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is longer than webhook latency @ 04/18/23 01:34:50.165
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/18/23 01:34:50.165
  E0418 01:34:50.225374      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:51.225586      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:52.225814      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:53.225907      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:54.226032      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 04/18/23 01:34:55.196
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/18/23 01:34:55.196
  E0418 01:34:55.226406      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:56.226987      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:57.227197      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:58.227501      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:34:59.227607      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:35:00.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0418 01:35:00.227642      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-8769" for this suite. @ 04/18/23 01:35:00.336
  STEP: Destroying namespace "webhook-markers-9899" for this suite. @ 04/18/23 01:35:00.349
• [15.625 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 04/18/23 01:35:00.363
  Apr 18 01:35:00.363: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename container-probe @ 04/18/23 01:35:00.365
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:35:00.387
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:35:00.393
  E0418 01:35:01.227741      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:02.227902      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:03.228982      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:04.229090      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:05.230113      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:06.230190      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:07.230557      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:08.231000      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:09.231971      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:10.232168      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:11.232929      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:12.233041      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:13.233836      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:14.234039      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:15.234154      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:16.234839      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:17.235013      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:18.235239      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:19.235351      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:20.235665      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:21.235803      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:22.235888      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:35:22.470: INFO: Container started at 2023-04-18 01:35:01 +0000 UTC, pod became ready at 2023-04-18 01:35:20 +0000 UTC
  Apr 18 01:35:22.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-5164" for this suite. @ 04/18/23 01:35:22.476
• [22.119 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 04/18/23 01:35:22.486
  Apr 18 01:35:22.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename kubectl @ 04/18/23 01:35:22.488
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:35:22.507
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:35:22.512
  Apr 18 01:35:22.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-781 create -f -'
  E0418 01:35:23.236019      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:35:23.609: INFO: stderr: ""
  Apr 18 01:35:23.609: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Apr 18 01:35:23.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-781 create -f -'
  Apr 18 01:35:24.053: INFO: stderr: ""
  Apr 18 01:35:24.054: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/18/23 01:35:24.054
  E0418 01:35:24.237046      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:35:25.058: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 18 01:35:25.058: INFO: Found 0 / 1
  E0418 01:35:25.238005      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:35:26.057: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 18 01:35:26.057: INFO: Found 1 / 1
  Apr 18 01:35:26.057: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Apr 18 01:35:26.061: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 18 01:35:26.061: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 18 01:35:26.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-781 describe pod agnhost-primary-7j7hv'
  Apr 18 01:35:26.157: INFO: stderr: ""
  Apr 18 01:35:26.157: INFO: stdout: "Name:             agnhost-primary-7j7hv\nNamespace:        kubectl-781\nPriority:         0\nService Account:  default\nNode:             ip-10-0-14-154/10.0.14.154\nStart Time:       Tue, 18 Apr 2023 01:35:23 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: c2cb7b0c3e7c899685acab97ae51cd0ced69b6ebdd20e95b084d9cbdab7b31bb\n                  cni.projectcalico.org/podIP: 10.2.212.188/32\n                  cni.projectcalico.org/podIPs: 10.2.212.188/32\nStatus:           Running\nIP:               10.2.212.188\nIPs:\n  IP:           10.2.212.188\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://e8fa10bb4acde43117faae3e863ad1ffc6860c004ed6621907b61f3a8025ae09\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 18 Apr 2023 01:35:24 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dqwcc (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-dqwcc:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-781/agnhost-primary-7j7hv to ip-10-0-14-154\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
  Apr 18 01:35:26.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-781 describe rc agnhost-primary'
  E0418 01:35:26.238397      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:35:26.261: INFO: stderr: ""
  Apr 18 01:35:26.261: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-781\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-7j7hv\n"
  Apr 18 01:35:26.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-781 describe service agnhost-primary'
  Apr 18 01:35:26.345: INFO: stderr: ""
  Apr 18 01:35:26.345: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-781\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.3.161.45\nIPs:               10.3.161.45\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.2.212.188:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Apr 18 01:35:26.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-781 describe node ip-10-0-14-154'
  Apr 18 01:35:26.489: INFO: stderr: ""
  Apr 18 01:35:26.489: INFO: stdout: "Name:               ip-10-0-14-154\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-14-154\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/node=\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.14.154/20\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.2.212.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 17 Apr 2023 16:53:30 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-14-154\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 18 Apr 2023 01:35:18 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 17 Apr 2023 16:54:31 +0000   Mon, 17 Apr 2023 16:54:31 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 18 Apr 2023 01:30:19 +0000   Mon, 17 Apr 2023 16:53:30 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 18 Apr 2023 01:30:19 +0000   Mon, 17 Apr 2023 16:53:30 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 18 Apr 2023 01:30:19 +0000   Mon, 17 Apr 2023 16:53:30 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 18 Apr 2023 01:30:19 +0000   Mon, 17 Apr 2023 16:54:31 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.0.14.154\n  Hostname:    ip-10-0-14-154\nCapacity:\n  cpu:                  2\n  ephemeral-storage:    30866412Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               1961688Ki\n  pods:                 110\nAllocatable:\n  cpu:                  2\n  ephemeral-storage:    28446485253\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               1859288Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 ec2200eda51f2d232dfdda883b406bc6\n  System UUID:                ec2200ed-a51f-2d23-2dfd-da883b406bc6\n  Boot ID:                    f0345fee-a5ea-4117-a11a-9522aa1ad466\n  Kernel Version:             6.2.9-300.fc38.x86_64\n  OS Image:                   Fedora CoreOS 38.20230414.2.0\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.19\n  Kubelet Version:            v1.27.1\n  Kube-Proxy Version:         v1.27.1\nPodCIDR:                      10.2.0.0/24\nPodCIDRs:                     10.2.0.0/24\nProviderID:                   aws:///us-east-2a/i-0fce90a6431ac58d4\nNon-terminated Pods:          (5 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  container-probe-5164        test-webserver-fd66ef41-7fac-4c09-baf7-4648f3e380e5        0 (0%)        0 (0%)      0 (0%)           0 (0%)         26s\n  kube-system                 calico-node-pdjm9                                          100m (5%)     0 (0%)      0 (0%)           0 (0%)         8h\n  kube-system                 kube-proxy-sf54c                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         8h\n  kubectl-781                 agnhost-primary-7j7hv                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-ee374c63bb374e5f-r7gxd    0 (0%)        0 (0%)      0 (0%)           0 (0%)         80m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests   Limits\n  --------             --------   ------\n  cpu                  100m (5%)  0 (0%)\n  memory               0 (0%)     0 (0%)\n  ephemeral-storage    0 (0%)     0 (0%)\n  hugepages-1Gi        0 (0%)     0 (0%)\n  hugepages-2Mi        0 (0%)     0 (0%)\n  example.com/fakecpu  0          0\nEvents:                <none>\n"
  Apr 18 01:35:26.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-781 describe namespace kubectl-781'
  Apr 18 01:35:26.604: INFO: stderr: ""
  Apr 18 01:35:26.604: INFO: stdout: "Name:         kubectl-781\nLabels:       e2e-framework=kubectl\n              e2e-run=3f27e2f9-bd1e-4da4-8323-1cbff9cd81a4\n              kubernetes.io/metadata.name=kubectl-781\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Apr 18 01:35:26.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-781" for this suite. @ 04/18/23 01:35:26.607
• [4.129 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 04/18/23 01:35:26.617
  Apr 18 01:35:26.617: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename container-probe @ 04/18/23 01:35:26.617
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:35:26.64
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:35:26.644
  STEP: Creating pod busybox-32a8fca1-d445-4727-ae1a-a62a5774bbd6 in namespace container-probe-1755 @ 04/18/23 01:35:26.648
  E0418 01:35:27.238943      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:28.241132      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:35:28.665: INFO: Started pod busybox-32a8fca1-d445-4727-ae1a-a62a5774bbd6 in namespace container-probe-1755
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/18/23 01:35:28.665
  Apr 18 01:35:28.668: INFO: Initial restart count of pod busybox-32a8fca1-d445-4727-ae1a-a62a5774bbd6 is 0
  E0418 01:35:29.242043      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:30.242342      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:31.242449      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:32.242681      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:33.243291      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:34.242930      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:35.242975      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:36.243558      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:37.244339      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:38.245355      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:39.261783      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:40.262064      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:41.262441      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:42.262939      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:43.263929      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:44.264040      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:45.264811      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:46.265282      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:47.265381      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:48.265502      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:49.265904      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:50.266940      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:51.267990      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:52.268092      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:53.268218      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:54.268544      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:55.269048      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:56.269008      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:57.269088      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:58.269973      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:35:59.270882      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:00.271053      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:01.271996      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:02.272232      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:03.272385      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:04.272626      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:05.273407      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:06.273634      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:07.273691      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:08.274847      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:09.275486      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:10.276004      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:11.276153      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:12.276351      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:13.276546      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:14.276920      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:15.277833      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:16.277913      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:17.278636      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:18.278940      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:19.279692      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:20.280063      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:21.280904      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:22.281065      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:23.281035      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:24.281127      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:25.281530      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:26.282040      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:27.282163      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:28.282413      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:29.282975      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:30.283263      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:31.283860      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:32.284084      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:33.284387      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:34.284677      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:35.285280      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:36.285214      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:37.285349      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:38.285586      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:39.286450      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:40.287152      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:41.287287      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:42.287325      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:43.287473      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:44.287677      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:45.288319      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:46.288561      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:47.288881      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:48.288969      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:49.289942      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:50.290309      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:51.290964      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:52.291080      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:53.291385      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:54.291578      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:55.292038      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:56.292289      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:57.292413      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:58.292649      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:36:59.293641      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:00.294096      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:01.294221      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:02.295762      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:03.296883      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:04.297109      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:05.297258      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:06.297380      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:07.297986      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:08.298139      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:09.300210      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:10.301099      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:11.301249      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:12.301229      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:13.301246      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:14.301966      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:15.303462      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:16.303671      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:17.303731      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:18.303957      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:19.304090      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:20.304343      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:21.305213      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:22.305428      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:23.306385      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:24.306502      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:25.307039      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:26.307300      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:27.307578      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:28.307871      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:29.307816      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:30.308046      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:31.308780      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:32.309219      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:33.309724      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:34.309870      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:35.310735      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:36.310878      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:37.311614      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:38.311867      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:39.312614      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:40.312876      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:41.313802      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:42.313893      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:43.313961      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:44.314293      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:45.314851      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:46.315189      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:47.315313      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:48.315628      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:49.315898      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:50.316951      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:51.318020      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:52.318222      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:53.318459      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:54.318673      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:55.318693      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:56.318866      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:57.319578      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:58.319682      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:37:59.320814      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:00.321142      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:01.321721      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:02.322132      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:03.322551      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:04.322887      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:05.322993      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:06.323186      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:07.323290      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:08.323599      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:09.323633      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:10.324015      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:11.324968      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:12.325098      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:13.325366      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:14.325502      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:15.325850      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:16.326204      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:17.326740      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:18.326874      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:19.327322      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:20.328258      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:21.328710      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:22.328887      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:23.329194      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:24.329301      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:25.330258      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:26.330391      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:27.331266      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:28.331383      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:29.331856      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:30.332093      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:31.332190      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:32.332386      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:33.332720      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:34.332918      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:35.333708      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:36.333861      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:37.334232      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:38.334427      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:39.335107      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:40.335155      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:41.335369      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:42.335731      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:43.336392      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:44.336507      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:45.337538      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:46.337848      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:47.338224      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:48.338338      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:49.338879      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:50.339203      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:51.340111      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:52.340384      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:53.340936      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:54.341050      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:55.341952      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:56.342074      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:57.342523      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:58.342865      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:38:59.343797      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:00.344091      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:01.344641      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:02.344821      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:03.345686      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:04.345963      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:05.347036      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:06.346884      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:07.347194      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:08.347313      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:09.347586      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:10.347872      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:11.348962      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:12.349888      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:13.350308      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:14.350435      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:15.351263      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:16.351699      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:17.352298      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:18.352449      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:19.352852      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:20.353191      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:21.353693      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:22.353905      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:23.354328      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:24.354598      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:25.354630      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:26.354814      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:27.355302      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:28.355417      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:39:29.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/18/23 01:39:29.285
  STEP: Destroying namespace "container-probe-1755" for this suite. @ 04/18/23 01:39:29.318
• [242.721 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 04/18/23 01:39:29.338
  Apr 18 01:39:29.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename services @ 04/18/23 01:39:29.339
  E0418 01:39:29.356393      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:39:29.36
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:39:29.366
  STEP: creating service in namespace services-2492 @ 04/18/23 01:39:29.372
  STEP: creating service affinity-nodeport in namespace services-2492 @ 04/18/23 01:39:29.372
  STEP: creating replication controller affinity-nodeport in namespace services-2492 @ 04/18/23 01:39:29.385
  I0418 01:39:29.399210      21 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-2492, replica count: 3
  E0418 01:39:30.357218      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:31.358343      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:32.358318      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0418 01:39:32.452739      21 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 18 01:39:32.463: INFO: Creating new exec pod
  E0418 01:39:33.358450      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:34.358736      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:35.359194      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:39:35.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-2492 exec execpod-affinity8t5bj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Apr 18 01:39:35.715: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Apr 18 01:39:35.715: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 18 01:39:35.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-2492 exec execpod-affinity8t5bj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.3.202.38 80'
  Apr 18 01:39:35.963: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.3.202.38 80\nConnection to 10.3.202.38 80 port [tcp/http] succeeded!\n"
  Apr 18 01:39:35.963: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 18 01:39:35.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-2492 exec execpod-affinity8t5bj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.14.154 32404'
  Apr 18 01:39:36.178: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.14.154 32404\nConnection to 10.0.14.154 32404 port [tcp/*] succeeded!\n"
  Apr 18 01:39:36.178: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 18 01:39:36.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-2492 exec execpod-affinity8t5bj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.27.81 32404'
  Apr 18 01:39:36.321: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.27.81 32404\nConnection to 10.0.27.81 32404 port [tcp/*] succeeded!\n"
  Apr 18 01:39:36.321: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 18 01:39:36.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-2492 exec execpod-affinity8t5bj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.14.154:32404/ ; done'
  E0418 01:39:36.359440      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:39:36.569: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:32404/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:32404/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:32404/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:32404/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:32404/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:32404/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:32404/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:32404/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:32404/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:32404/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:32404/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:32404/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:32404/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:32404/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:32404/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.14.154:32404/\n"
  Apr 18 01:39:36.569: INFO: stdout: "\naffinity-nodeport-clzdn\naffinity-nodeport-clzdn\naffinity-nodeport-clzdn\naffinity-nodeport-clzdn\naffinity-nodeport-clzdn\naffinity-nodeport-clzdn\naffinity-nodeport-clzdn\naffinity-nodeport-clzdn\naffinity-nodeport-clzdn\naffinity-nodeport-clzdn\naffinity-nodeport-clzdn\naffinity-nodeport-clzdn\naffinity-nodeport-clzdn\naffinity-nodeport-clzdn\naffinity-nodeport-clzdn\naffinity-nodeport-clzdn"
  Apr 18 01:39:36.569: INFO: Received response from host: affinity-nodeport-clzdn
  Apr 18 01:39:36.569: INFO: Received response from host: affinity-nodeport-clzdn
  Apr 18 01:39:36.569: INFO: Received response from host: affinity-nodeport-clzdn
  Apr 18 01:39:36.569: INFO: Received response from host: affinity-nodeport-clzdn
  Apr 18 01:39:36.569: INFO: Received response from host: affinity-nodeport-clzdn
  Apr 18 01:39:36.569: INFO: Received response from host: affinity-nodeport-clzdn
  Apr 18 01:39:36.569: INFO: Received response from host: affinity-nodeport-clzdn
  Apr 18 01:39:36.569: INFO: Received response from host: affinity-nodeport-clzdn
  Apr 18 01:39:36.569: INFO: Received response from host: affinity-nodeport-clzdn
  Apr 18 01:39:36.569: INFO: Received response from host: affinity-nodeport-clzdn
  Apr 18 01:39:36.569: INFO: Received response from host: affinity-nodeport-clzdn
  Apr 18 01:39:36.569: INFO: Received response from host: affinity-nodeport-clzdn
  Apr 18 01:39:36.569: INFO: Received response from host: affinity-nodeport-clzdn
  Apr 18 01:39:36.569: INFO: Received response from host: affinity-nodeport-clzdn
  Apr 18 01:39:36.569: INFO: Received response from host: affinity-nodeport-clzdn
  Apr 18 01:39:36.569: INFO: Received response from host: affinity-nodeport-clzdn
  Apr 18 01:39:36.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 18 01:39:36.589: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-2492, will wait for the garbage collector to delete the pods @ 04/18/23 01:39:36.623
  Apr 18 01:39:36.700: INFO: Deleting ReplicationController affinity-nodeport took: 6.911143ms
  Apr 18 01:39:36.800: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.233889ms
  E0418 01:39:37.360158      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:38.360436      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-2492" for this suite. @ 04/18/23 01:39:39.123
• [9.793 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 04/18/23 01:39:39.135
  Apr 18 01:39:39.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 01:39:39.136
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:39:39.154
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:39:39.161
  STEP: Creating configMap with name projected-configmap-test-volume-map-4b7385c0-7e55-4eee-bb22-67c9cf599c4f @ 04/18/23 01:39:39.165
  STEP: Creating a pod to test consume configMaps @ 04/18/23 01:39:39.171
  E0418 01:39:39.361072      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:40.361063      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:41.361741      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:42.361905      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:39:43.191
  Apr 18 01:39:43.194: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-projected-configmaps-45e43f1b-3138-49e1-b0ed-2443149663ef container agnhost-container: <nil>
  STEP: delete the pod @ 04/18/23 01:39:43.21
  Apr 18 01:39:43.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9202" for this suite. @ 04/18/23 01:39:43.228
• [4.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 04/18/23 01:39:43.247
  Apr 18 01:39:43.247: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename tables @ 04/18/23 01:39:43.25
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:39:43.282
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:39:43.288
  Apr 18 01:39:43.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-8891" for this suite. @ 04/18/23 01:39:43.302
• [0.060 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 04/18/23 01:39:43.308
  Apr 18 01:39:43.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename svcaccounts @ 04/18/23 01:39:43.313
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:39:43.339
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:39:43.344
  STEP: Creating ServiceAccount "e2e-sa-j8dhl"  @ 04/18/23 01:39:43.35
  Apr 18 01:39:43.355: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-j8dhl"  @ 04/18/23 01:39:43.355
  E0418 01:39:43.362709      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:39:43.363: INFO: AutomountServiceAccountToken: true
  Apr 18 01:39:43.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5059" for this suite. @ 04/18/23 01:39:43.374
• [0.078 seconds]
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 04/18/23 01:39:43.386
  Apr 18 01:39:43.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename pods @ 04/18/23 01:39:43.388
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:39:43.41
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:39:43.414
  STEP: creating the pod @ 04/18/23 01:39:43.422
  STEP: submitting the pod to kubernetes @ 04/18/23 01:39:43.422
  E0418 01:39:44.363007      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:45.363839      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 04/18/23 01:39:45.444
  STEP: updating the pod @ 04/18/23 01:39:45.447
  Apr 18 01:39:45.962: INFO: Successfully updated pod "pod-update-4050fef1-a51a-4178-8dda-da6c9d234d95"
  STEP: verifying the updated pod is in kubernetes @ 04/18/23 01:39:45.968
  Apr 18 01:39:45.971: INFO: Pod update OK
  Apr 18 01:39:45.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5723" for this suite. @ 04/18/23 01:39:45.974
• [2.598 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 04/18/23 01:39:45.987
  Apr 18 01:39:45.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename emptydir @ 04/18/23 01:39:45.988
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:39:46.015
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:39:46.019
  STEP: Creating Pod @ 04/18/23 01:39:46.023
  E0418 01:39:46.363895      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:47.364351      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 04/18/23 01:39:48.042
  Apr 18 01:39:48.042: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8033 PodName:pod-sharedvolume-e66d59b0-0290-4214-ab39-22189677ffa8 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 01:39:48.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 01:39:48.043: INFO: ExecWithOptions: Clientset creation
  Apr 18 01:39:48.043: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/emptydir-8033/pods/pod-sharedvolume-e66d59b0-0290-4214-ab39-22189677ffa8/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Apr 18 01:39:48.139: INFO: Exec stderr: ""
  Apr 18 01:39:48.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8033" for this suite. @ 04/18/23 01:39:48.143
• [2.162 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 04/18/23 01:39:48.152
  Apr 18 01:39:48.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename configmap @ 04/18/23 01:39:48.152
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:39:48.172
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:39:48.178
  STEP: Creating configMap with name cm-test-opt-del-94503c50-83e5-4a2a-a488-283353fa4822 @ 04/18/23 01:39:48.186
  STEP: Creating configMap with name cm-test-opt-upd-5b736d93-5b44-4448-8ac6-57f81c34cc71 @ 04/18/23 01:39:48.19
  STEP: Creating the pod @ 04/18/23 01:39:48.201
  E0418 01:39:48.364853      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:49.365862      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-94503c50-83e5-4a2a-a488-283353fa4822 @ 04/18/23 01:39:50.253
  STEP: Updating configmap cm-test-opt-upd-5b736d93-5b44-4448-8ac6-57f81c34cc71 @ 04/18/23 01:39:50.258
  STEP: Creating configMap with name cm-test-opt-create-d7e896c8-911e-4d1d-a3de-df6b802f062f @ 04/18/23 01:39:50.263
  STEP: waiting to observe update in volume @ 04/18/23 01:39:50.268
  E0418 01:39:50.365927      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:51.366249      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:52.367308      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:53.367490      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:39:54.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6370" for this suite. @ 04/18/23 01:39:54.317
• [6.171 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 04/18/23 01:39:54.327
  Apr 18 01:39:54.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename emptydir @ 04/18/23 01:39:54.327
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:39:54.344
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:39:54.349
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 04/18/23 01:39:54.352
  E0418 01:39:54.368438      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:55.369180      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:56.370222      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:57.370476      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:39:58.370867      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:39:58.371
  Apr 18 01:39:58.374: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-6f9dfc39-0dc3-4867-9ae3-dd79a225977f container test-container: <nil>
  STEP: delete the pod @ 04/18/23 01:39:58.379
  Apr 18 01:39:58.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2242" for this suite. @ 04/18/23 01:39:58.396
• [4.074 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 04/18/23 01:39:58.401
  Apr 18 01:39:58.401: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename sched-preemption @ 04/18/23 01:39:58.402
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:39:58.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:39:58.423
  Apr 18 01:39:58.439: INFO: Waiting up to 1m0s for all nodes to be ready
  E0418 01:39:59.370978      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:00.371090      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:01.372144      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:02.372299      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:03.372419      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:04.372505      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:05.373449      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:06.373582      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:07.373777      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:08.374135      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:09.375041      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:10.375984      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:11.376051      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:12.376262      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:13.376405      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:14.376529      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:15.377186      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:16.377295      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:17.378220      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:18.378975      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:19.379442      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:20.380206      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:21.380939      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:22.382006      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:23.382950      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:24.383141      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:25.383258      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:26.383476      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:27.383584      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:28.383799      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:29.383890      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:30.384176      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:31.384366      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:32.384563      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:33.385266      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:34.385370      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:35.385936      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:36.386103      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:37.387004      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:38.387320      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:39.388222      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:40.388712      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:41.389072      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:42.389087      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:43.389957      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:44.390062      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:45.390171      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:46.390389      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:47.390943      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:48.391299      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:49.391482      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:50.392269      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:51.392620      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:52.392847      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:53.393024      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:54.393064      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:55.393126      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:56.393296      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:57.393459      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:40:58.393664      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:40:58.471: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 04/18/23 01:40:58.475
  Apr 18 01:40:58.519: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Apr 18 01:40:58.539: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Apr 18 01:40:58.614: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Apr 18 01:40:58.640: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 04/18/23 01:40:58.64
  E0418 01:40:59.395786      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:00.395989      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:01.396161      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:02.396277      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 04/18/23 01:41:02.693
  E0418 01:41:03.397219      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:04.397639      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:05.397851      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:06.398798      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:07.398958      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:08.399265      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:09.399534      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:10.400207      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:41:10.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-5120" for this suite. @ 04/18/23 01:41:10.77
• [72.374 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 04/18/23 01:41:10.775
  Apr 18 01:41:10.775: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename deployment @ 04/18/23 01:41:10.777
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:41:10.803
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:41:10.809
  Apr 18 01:41:10.828: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0418 01:41:11.400585      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:12.400950      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:13.401035      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:14.401136      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:15.402022      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:41:15.833: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/18/23 01:41:15.833
  Apr 18 01:41:15.833: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 04/18/23 01:41:15.847
  Apr 18 01:41:15.873: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3547  d36ef6de-0284-40bc-8442-4139e248bf18 73284 1 2023-04-18 01:41:15 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-04-18 01:41:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0020c6498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Apr 18 01:41:15.879: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
  Apr 18 01:41:15.879: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Apr 18 01:41:15.879: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-3547  497948ee-17fb-4447-9d4e-fb62af036655 73287 1 2023-04-18 01:41:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment d36ef6de-0284-40bc-8442-4139e248bf18 0xc006175fc7 0xc006175fc8}] [] [{e2e.test Update apps/v1 2023-04-18 01:41:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-18 01:41:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-18 01:41:15 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"d36ef6de-0284-40bc-8442-4139e248bf18\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0048ba088 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 18 01:41:15.893: INFO: Pod "test-cleanup-controller-gf56r" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-gf56r test-cleanup-controller- deployment-3547  cfd65207-c489-4f40-97a4-a274aefbf95c 73274 0 2023-04-18 01:41:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:0535bb57a87a030b8cdfcbcaf6112230db668cbdfefe6bbfeaa18697b9a501fe cni.projectcalico.org/podIP:10.2.212.201/32 cni.projectcalico.org/podIPs:10.2.212.201/32] [{apps/v1 ReplicaSet test-cleanup-controller 497948ee-17fb-4447-9d4e-fb62af036655 0xc0048ba3d7 0xc0048ba3d8}] [] [{kube-controller-manager Update v1 2023-04-18 01:41:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"497948ee-17fb-4447-9d4e-fb62af036655\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-18 01:41:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-18 01:41:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.212.201\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rxj7g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rxj7g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-14-154,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:41:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:41:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:41:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-18 01:41:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.14.154,PodIP:10.2.212.201,StartTime:2023-04-18 01:41:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-18 01:41:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8c8d7d4621d2f445ca7fcf428f572ce03779f75f526e91fdb5aad29cb64af76c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.2.212.201,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 18 01:41:15.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3547" for this suite. @ 04/18/23 01:41:15.913
• [5.159 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 04/18/23 01:41:15.936
  Apr 18 01:41:15.936: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename container-probe @ 04/18/23 01:41:15.937
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:41:15.979
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:41:15.985
  E0418 01:41:16.402146      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:17.402231      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:18.402735      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:19.402928      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:20.403682      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:21.404135      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:22.404283      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:23.404320      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:24.405070      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:25.405572      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:26.405597      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:27.405796      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:28.406432      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:29.406450      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:30.407160      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:31.407307      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:32.407361      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:33.407529      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:34.407720      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:35.408155      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:36.408921      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:37.409787      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:38.410336      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:39.410956      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:40.411389      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:41.411512      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:42.412021      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:43.412444      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:44.412611      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:45.413156      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:46.413207      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:47.413832      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:48.414677      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:49.415785      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:50.416504      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:51.416966      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:52.417085      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:53.417789      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:54.418668      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:55.419543      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:56.419615      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:57.419958      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:58.420537      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:41:59.420823      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:00.420906      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:01.421076      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:02.421706      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:03.422224      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:04.422858      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:05.423511      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:06.423562      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:07.423727      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:08.424046      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:09.424274      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:10.424394      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:11.425081      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:12.425831      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:13.426234      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:14.427002      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:15.427135      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:42:16.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-7746" for this suite. @ 04/18/23 01:42:16.008
• [60.079 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 04/18/23 01:42:16.017
  Apr 18 01:42:16.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/18/23 01:42:16.018
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:42:16.052
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:42:16.056
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 04/18/23 01:42:16.062
  Apr 18 01:42:16.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  E0418 01:42:16.428132      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:17.428495      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:42:17.487: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  E0418 01:42:18.429115      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:19.429796      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:20.430819      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:21.431741      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:22.431931      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:42:23.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5872" for this suite. @ 04/18/23 01:42:23.392
• [7.381 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 04/18/23 01:42:23.398
  Apr 18 01:42:23.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename services @ 04/18/23 01:42:23.399
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:42:23.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:42:23.42
  STEP: creating a collection of services @ 04/18/23 01:42:23.424
  Apr 18 01:42:23.424: INFO: Creating e2e-svc-a-6cqj6
  E0418 01:42:23.432085      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:42:23.435: INFO: Creating e2e-svc-b-xjqn4
  Apr 18 01:42:23.445: INFO: Creating e2e-svc-c-8f6cs
  STEP: deleting service collection @ 04/18/23 01:42:23.462
  Apr 18 01:42:23.490: INFO: Collection of services has been deleted
  Apr 18 01:42:23.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4298" for this suite. @ 04/18/23 01:42:23.495
• [0.104 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 04/18/23 01:42:23.505
  Apr 18 01:42:23.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename field-validation @ 04/18/23 01:42:23.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:42:23.526
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:42:23.53
  Apr 18 01:42:23.534: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  E0418 01:42:24.432243      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:25.432460      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:42:26.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2055" for this suite. @ 04/18/23 01:42:26.104
• [2.604 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 04/18/23 01:42:26.111
  Apr 18 01:42:26.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename container-runtime @ 04/18/23 01:42:26.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:42:26.141
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:42:26.145
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 04/18/23 01:42:26.157
  E0418 01:42:26.432964      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:27.433388      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:28.434311      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:29.434940      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:30.435854      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:31.436764      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:32.437674      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:33.437941      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:34.438441      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:35.439443      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:36.439888      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:37.440676      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:38.441467      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:39.442081      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:40.443054      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:41.443152      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:42.443822      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:43.444382      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 04/18/23 01:42:44.254
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 04/18/23 01:42:44.257
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 04/18/23 01:42:44.265
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 04/18/23 01:42:44.265
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 04/18/23 01:42:44.306
  E0418 01:42:44.445059      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:45.445891      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:46.446003      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 04/18/23 01:42:47.341
  E0418 01:42:47.446235      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 04/18/23 01:42:48.347
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 04/18/23 01:42:48.36
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 04/18/23 01:42:48.36
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 04/18/23 01:42:48.381
  E0418 01:42:48.446558      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 04/18/23 01:42:49.395
  E0418 01:42:49.447228      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:50.448202      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 04/18/23 01:42:51.408
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 04/18/23 01:42:51.413
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 04/18/23 01:42:51.413
  Apr 18 01:42:51.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0418 01:42:51.448651      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "container-runtime-8411" for this suite. @ 04/18/23 01:42:51.449
• [25.344 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 04/18/23 01:42:51.456
  Apr 18 01:42:51.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename disruption @ 04/18/23 01:42:51.457
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:42:51.481
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:42:51.486
  STEP: creating the pdb @ 04/18/23 01:42:51.49
  STEP: Waiting for the pdb to be processed @ 04/18/23 01:42:51.497
  E0418 01:42:52.448891      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:53.449279      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pdb @ 04/18/23 01:42:53.503
  STEP: Waiting for the pdb to be processed @ 04/18/23 01:42:53.509
  E0418 01:42:54.449908      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:55.450971      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching the pdb @ 04/18/23 01:42:55.517
  STEP: Waiting for the pdb to be processed @ 04/18/23 01:42:55.534
  E0418 01:42:56.451094      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:57.451193      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be deleted @ 04/18/23 01:42:57.548
  Apr 18 01:42:57.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-4068" for this suite. @ 04/18/23 01:42:57.556
• [6.106 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 04/18/23 01:42:57.565
  Apr 18 01:42:57.565: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename configmap @ 04/18/23 01:42:57.567
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:42:57.59
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:42:57.594
  STEP: Creating configMap configmap-9483/configmap-test-5c7be711-0c06-44ac-b6b6-1669602862b6 @ 04/18/23 01:42:57.598
  STEP: Creating a pod to test consume configMaps @ 04/18/23 01:42:57.602
  E0418 01:42:58.451222      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:42:59.451351      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:00.452259      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:01.452410      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:43:01.626
  Apr 18 01:43:01.634: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-configmaps-117e03b4-184e-4226-a26f-cb2a2cf41574 container env-test: <nil>
  STEP: delete the pod @ 04/18/23 01:43:01.711
  Apr 18 01:43:01.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9483" for this suite. @ 04/18/23 01:43:01.83
• [4.306 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 04/18/23 01:43:01.885
  Apr 18 01:43:01.885: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename downward-api @ 04/18/23 01:43:01.886
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:43:02.032
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:43:02.059
  STEP: Creating a pod to test downward API volume plugin @ 04/18/23 01:43:02.067
  E0418 01:43:02.452571      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:03.452944      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:04.453873      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:05.454683      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:43:06.138
  Apr 18 01:43:06.140: INFO: Trying to get logs from node ip-10-0-14-154 pod downwardapi-volume-4938fb5f-9c40-4efd-b20b-63d655ea4c1e container client-container: <nil>
  STEP: delete the pod @ 04/18/23 01:43:06.155
  Apr 18 01:43:06.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-790" for this suite. @ 04/18/23 01:43:06.188
• [4.318 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 04/18/23 01:43:06.203
  Apr 18 01:43:06.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename webhook @ 04/18/23 01:43:06.204
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:43:06.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:43:06.241
  STEP: Setting up server cert @ 04/18/23 01:43:06.298
  E0418 01:43:06.454816      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/18/23 01:43:06.727
  STEP: Deploying the webhook pod @ 04/18/23 01:43:06.736
  STEP: Wait for the deployment to be ready @ 04/18/23 01:43:06.748
  Apr 18 01:43:06.765: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0418 01:43:07.454942      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:08.455013      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:43:08.804: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 18, 1, 43, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 1, 43, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 18, 1, 43, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 18, 1, 43, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0418 01:43:09.456066      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:10.456953      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/18/23 01:43:10.807
  STEP: Verifying the service has paired with the endpoint @ 04/18/23 01:43:10.824
  E0418 01:43:11.457052      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:43:11.825: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 04/18/23 01:43:11.829
  STEP: create a configmap that should be updated by the webhook @ 04/18/23 01:43:11.844
  Apr 18 01:43:11.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9036" for this suite. @ 04/18/23 01:43:11.92
  STEP: Destroying namespace "webhook-markers-7345" for this suite. @ 04/18/23 01:43:11.935
• [5.752 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 04/18/23 01:43:11.962
  Apr 18 01:43:11.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename pods @ 04/18/23 01:43:11.964
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:43:11.983
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:43:11.988
  STEP: Create set of pods @ 04/18/23 01:43:11.993
  Apr 18 01:43:12.001: INFO: created test-pod-1
  Apr 18 01:43:12.011: INFO: created test-pod-2
  Apr 18 01:43:12.026: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 04/18/23 01:43:12.026
  E0418 01:43:12.457238      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:13.457955      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 04/18/23 01:43:14.091
  Apr 18 01:43:14.097: INFO: Pod quantity 3 is different from expected quantity 0
  E0418 01:43:14.458433      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:43:15.105: INFO: Pod quantity 3 is different from expected quantity 0
  E0418 01:43:15.458548      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:43:16.102: INFO: Pod quantity 3 is different from expected quantity 0
  E0418 01:43:16.459553      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:43:17.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2419" for this suite. @ 04/18/23 01:43:17.108
• [5.152 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 04/18/23 01:43:17.115
  Apr 18 01:43:17.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename resourcequota @ 04/18/23 01:43:17.116
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:43:17.138
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:43:17.145
  STEP: Counting existing ResourceQuota @ 04/18/23 01:43:17.149
  E0418 01:43:17.459877      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:18.460887      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:19.463875      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:20.461842      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:21.461939      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/18/23 01:43:22.156
  STEP: Ensuring resource quota status is calculated @ 04/18/23 01:43:22.164
  E0418 01:43:22.462435      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:23.462557      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:43:24.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2816" for this suite. @ 04/18/23 01:43:24.172
• [7.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 04/18/23 01:43:24.181
  Apr 18 01:43:24.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename secrets @ 04/18/23 01:43:24.182
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:43:24.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:43:24.214
  Apr 18 01:43:24.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5071" for this suite. @ 04/18/23 01:43:24.279
• [0.104 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 04/18/23 01:43:24.286
  Apr 18 01:43:24.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename sched-pred @ 04/18/23 01:43:24.287
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:43:24.307
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:43:24.312
  Apr 18 01:43:24.316: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 18 01:43:24.322: INFO: Waiting for terminating namespaces to be deleted...
  Apr 18 01:43:24.325: INFO: 
  Logging pods the apiserver thinks is on node ip-10-0-14-154 before test
  Apr 18 01:43:24.331: INFO: calico-node-pdjm9 from kube-system started at 2023-04-17 16:53:36 +0000 UTC (1 container statuses recorded)
  Apr 18 01:43:24.331: INFO: 	Container calico-node ready: true, restart count 0
  Apr 18 01:43:24.331: INFO: kube-proxy-sf54c from kube-system started at 2023-04-17 16:53:36 +0000 UTC (1 container statuses recorded)
  Apr 18 01:43:24.331: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 18 01:43:24.331: INFO: sonobuoy-systemd-logs-daemon-set-ee374c63bb374e5f-r7gxd from sonobuoy started at 2023-04-18 00:15:14 +0000 UTC (2 container statuses recorded)
  Apr 18 01:43:24.331: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 18 01:43:24.331: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 18 01:43:24.331: INFO: 
  Logging pods the apiserver thinks is on node ip-10-0-27-81 before test
  Apr 18 01:43:24.341: INFO: calico-node-wsfsw from kube-system started at 2023-04-17 16:54:08 +0000 UTC (1 container statuses recorded)
  Apr 18 01:43:24.342: INFO: 	Container calico-node ready: true, restart count 0
  Apr 18 01:43:24.342: INFO: coredns-5cf8b9cff-c4mtg from kube-system started at 2023-04-18 00:54:44 +0000 UTC (1 container statuses recorded)
  Apr 18 01:43:24.342: INFO: 	Container coredns ready: true, restart count 0
  Apr 18 01:43:24.342: INFO: kube-proxy-srh6q from kube-system started at 2023-04-17 16:54:08 +0000 UTC (1 container statuses recorded)
  Apr 18 01:43:24.342: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 18 01:43:24.342: INFO: sonobuoy from sonobuoy started at 2023-04-18 00:15:13 +0000 UTC (1 container statuses recorded)
  Apr 18 01:43:24.342: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 18 01:43:24.342: INFO: sonobuoy-e2e-job-0fa4b43c9b09437b from sonobuoy started at 2023-04-18 00:15:14 +0000 UTC (2 container statuses recorded)
  Apr 18 01:43:24.342: INFO: 	Container e2e ready: true, restart count 0
  Apr 18 01:43:24.342: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 18 01:43:24.342: INFO: sonobuoy-systemd-logs-daemon-set-ee374c63bb374e5f-96krr from sonobuoy started at 2023-04-18 00:15:14 +0000 UTC (2 container statuses recorded)
  Apr 18 01:43:24.342: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 18 01:43:24.342: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/18/23 01:43:24.342
  E0418 01:43:24.463794      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:25.464384      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/18/23 01:43:26.366
  STEP: Trying to apply a random label on the found node. @ 04/18/23 01:43:26.377
  STEP: verifying the node has the label kubernetes.io/e2e-8c89194a-ae94-4122-9d4b-4556c35c114e 42 @ 04/18/23 01:43:26.395
  STEP: Trying to relaunch the pod, now with labels. @ 04/18/23 01:43:26.403
  E0418 01:43:26.465460      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:27.465769      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-8c89194a-ae94-4122-9d4b-4556c35c114e off the node ip-10-0-14-154 @ 04/18/23 01:43:28.424
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-8c89194a-ae94-4122-9d4b-4556c35c114e @ 04/18/23 01:43:28.437
  Apr 18 01:43:28.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-293" for this suite. @ 04/18/23 01:43:28.446
• [4.165 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 04/18/23 01:43:28.454
  Apr 18 01:43:28.454: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename resourcequota @ 04/18/23 01:43:28.454
  E0418 01:43:28.466607      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:43:28.472
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:43:28.476
  STEP: Creating a ResourceQuota @ 04/18/23 01:43:28.482
  STEP: Getting a ResourceQuota @ 04/18/23 01:43:28.486
  STEP: Updating a ResourceQuota @ 04/18/23 01:43:28.489
  STEP: Verifying a ResourceQuota was modified @ 04/18/23 01:43:28.497
  STEP: Deleting a ResourceQuota @ 04/18/23 01:43:28.501
  STEP: Verifying the deleted ResourceQuota @ 04/18/23 01:43:28.511
  Apr 18 01:43:28.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8459" for this suite. @ 04/18/23 01:43:28.519
• [0.071 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 04/18/23 01:43:28.526
  Apr 18 01:43:28.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename field-validation @ 04/18/23 01:43:28.527
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:43:28.552
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:43:28.556
  Apr 18 01:43:28.560: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  E0418 01:43:29.466800      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:30.467058      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0418 01:43:31.113685      21 warnings.go:70] unknown field "alpha"
  W0418 01:43:31.113709      21 warnings.go:70] unknown field "beta"
  W0418 01:43:31.113714      21 warnings.go:70] unknown field "delta"
  W0418 01:43:31.113718      21 warnings.go:70] unknown field "epsilon"
  W0418 01:43:31.113725      21 warnings.go:70] unknown field "gamma"
  Apr 18 01:43:31.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8650" for this suite. @ 04/18/23 01:43:31.139
• [2.621 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 04/18/23 01:43:31.148
  Apr 18 01:43:31.148: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename limitrange @ 04/18/23 01:43:31.149
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:43:31.17
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:43:31.177
  STEP: Creating LimitRange "e2e-limitrange-fksgp" in namespace "limitrange-1982" @ 04/18/23 01:43:31.182
  STEP: Creating another limitRange in another namespace @ 04/18/23 01:43:31.189
  Apr 18 01:43:31.210: INFO: Namespace "e2e-limitrange-fksgp-1713" created
  Apr 18 01:43:31.210: INFO: Creating LimitRange "e2e-limitrange-fksgp" in namespace "e2e-limitrange-fksgp-1713"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-fksgp" @ 04/18/23 01:43:31.215
  Apr 18 01:43:31.218: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-fksgp" in "limitrange-1982" namespace @ 04/18/23 01:43:31.218
  Apr 18 01:43:31.230: INFO: LimitRange "e2e-limitrange-fksgp" has been patched
  STEP: Delete LimitRange "e2e-limitrange-fksgp" by Collection with labelSelector: "e2e-limitrange-fksgp=patched" @ 04/18/23 01:43:31.23
  STEP: Confirm that the limitRange "e2e-limitrange-fksgp" has been deleted @ 04/18/23 01:43:31.236
  Apr 18 01:43:31.236: INFO: Requesting list of LimitRange to confirm quantity
  Apr 18 01:43:31.240: INFO: Found 0 LimitRange with label "e2e-limitrange-fksgp=patched"
  Apr 18 01:43:31.240: INFO: LimitRange "e2e-limitrange-fksgp" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-fksgp" @ 04/18/23 01:43:31.24
  Apr 18 01:43:31.242: INFO: Found 1 limitRange
  Apr 18 01:43:31.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-1982" for this suite. @ 04/18/23 01:43:31.246
  STEP: Destroying namespace "e2e-limitrange-fksgp-1713" for this suite. @ 04/18/23 01:43:31.253
• [0.112 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 04/18/23 01:43:31.261
  Apr 18 01:43:31.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename configmap @ 04/18/23 01:43:31.262
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:43:31.283
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:43:31.288
  STEP: Creating configMap with name configmap-test-upd-08a46e37-bbc1-43be-9687-5312af54d73c @ 04/18/23 01:43:31.296
  STEP: Creating the pod @ 04/18/23 01:43:31.3
  E0418 01:43:31.467803      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:32.468606      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod with text data @ 04/18/23 01:43:33.32
  STEP: Waiting for pod with binary data @ 04/18/23 01:43:33.327
  Apr 18 01:43:33.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8902" for this suite. @ 04/18/23 01:43:33.336
• [2.083 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 04/18/23 01:43:33.344
  Apr 18 01:43:33.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename containers @ 04/18/23 01:43:33.346
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:43:33.368
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:43:33.373
  STEP: Creating a pod to test override command @ 04/18/23 01:43:33.376
  E0418 01:43:33.468905      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:34.469000      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:35.469488      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:36.470022      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:43:37.397
  Apr 18 01:43:37.400: INFO: Trying to get logs from node ip-10-0-14-154 pod client-containers-93ae7ec8-f50a-43e5-afd5-a76238f94e85 container agnhost-container: <nil>
  STEP: delete the pod @ 04/18/23 01:43:37.405
  Apr 18 01:43:37.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-4818" for this suite. @ 04/18/23 01:43:37.42
• [4.082 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 04/18/23 01:43:37.427
  Apr 18 01:43:37.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename statefulset @ 04/18/23 01:43:37.428
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:43:37.453
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:43:37.462
  E0418 01:43:37.470024      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating service test in namespace statefulset-7825 @ 04/18/23 01:43:37.478
  STEP: Creating stateful set ss in namespace statefulset-7825 @ 04/18/23 01:43:37.489
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7825 @ 04/18/23 01:43:37.504
  Apr 18 01:43:37.527: INFO: Found 0 stateful pods, waiting for 1
  E0418 01:43:38.471030      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:39.471085      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:40.471234      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:41.471580      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:42.471686      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:43.471977      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:44.472085      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:45.472884      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:46.474473      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:47.475017      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:43:47.538: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 04/18/23 01:43:47.538
  Apr 18 01:43:47.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=statefulset-7825 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 18 01:43:47.721: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 18 01:43:47.721: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 18 01:43:47.721: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 18 01:43:47.725: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0418 01:43:48.475219      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:49.475329      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:50.476910      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:51.476342      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:52.476850      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:53.476965      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:54.477979      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:55.478985      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:56.480005      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:43:57.480962      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:43:57.730: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 18 01:43:57.730: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 18 01:43:57.744: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
  Apr 18 01:43:57.744: INFO: ss-0  ip-10-0-14-154  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:43:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:43:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:43:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:43:37 +0000 UTC  }]
  Apr 18 01:43:57.744: INFO: 
  Apr 18 01:43:57.744: INFO: StatefulSet ss has not reached scale 3, at 1
  E0418 01:43:58.481082      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:43:58.752: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995225746s
  E0418 01:43:59.481941      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:43:59.759: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987523444s
  E0418 01:44:00.482308      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:44:00.763: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.980352652s
  E0418 01:44:01.483315      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:44:01.767: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976411948s
  E0418 01:44:02.483437      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:44:02.776: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.97193247s
  E0418 01:44:03.483975      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:44:03.780: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963593159s
  E0418 01:44:04.484140      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:44:04.784: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.959106304s
  E0418 01:44:05.484299      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:44:05.790: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.955148081s
  E0418 01:44:06.485323      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:44:06.800: INFO: Verifying statefulset ss doesn't scale past 3 for another 948.375468ms
  E0418 01:44:07.485519      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7825 @ 04/18/23 01:44:07.801
  Apr 18 01:44:07.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=statefulset-7825 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 18 01:44:07.961: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 18 01:44:07.961: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 18 01:44:07.961: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 18 01:44:07.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=statefulset-7825 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 18 01:44:08.157: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Apr 18 01:44:08.157: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 18 01:44:08.157: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 18 01:44:08.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=statefulset-7825 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 18 01:44:08.393: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Apr 18 01:44:08.393: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 18 01:44:08.393: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 18 01:44:08.399: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
  E0418 01:44:08.486277      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:09.486401      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:10.487343      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:11.487452      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:12.487578      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:13.487967      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:14.487984      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:15.488576      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:16.488869      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:17.489271      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:44:18.403: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 18 01:44:18.403: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 18 01:44:18.403: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 04/18/23 01:44:18.403
  Apr 18 01:44:18.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=statefulset-7825 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0418 01:44:18.489615      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:44:18.557: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 18 01:44:18.557: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 18 01:44:18.557: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 18 01:44:18.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=statefulset-7825 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 18 01:44:18.732: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 18 01:44:18.732: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 18 01:44:18.732: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 18 01:44:18.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=statefulset-7825 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 18 01:44:19.062: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 18 01:44:19.062: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 18 01:44:19.062: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 18 01:44:19.062: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 18 01:44:19.065: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
  E0418 01:44:19.489880      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:20.490194      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:21.490292      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:22.490515      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:23.490728      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:24.490981      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:25.491540      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:26.491639      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:27.491923      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:28.492949      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:44:29.075: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 18 01:44:29.075: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Apr 18 01:44:29.075: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Apr 18 01:44:29.089: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
  Apr 18 01:44:29.089: INFO: ss-0  ip-10-0-14-154  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:43:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:44:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:44:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:43:37 +0000 UTC  }]
  Apr 18 01:44:29.089: INFO: ss-1  ip-10-0-27-81   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:43:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:44:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:44:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:43:57 +0000 UTC  }]
  Apr 18 01:44:29.089: INFO: ss-2  ip-10-0-14-154  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:43:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:44:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:44:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:43:57 +0000 UTC  }]
  Apr 18 01:44:29.089: INFO: 
  Apr 18 01:44:29.089: INFO: StatefulSet ss has not reached scale 0, at 3
  E0418 01:44:29.493632      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:44:30.093: INFO: POD   NODE           PHASE      GRACE  CONDITIONS
  Apr 18 01:44:30.093: INFO: ss-1  ip-10-0-27-81  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:43:57 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:44:19 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:44:19 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-18 01:43:57 +0000 UTC  }]
  Apr 18 01:44:30.093: INFO: 
  Apr 18 01:44:30.093: INFO: StatefulSet ss has not reached scale 0, at 1
  E0418 01:44:30.493978      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:44:31.097: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.989829891s
  E0418 01:44:31.494824      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:44:32.101: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.986781224s
  E0418 01:44:32.494942      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:44:33.105: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.982936296s
  E0418 01:44:33.495001      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:44:34.109: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.978594914s
  E0418 01:44:34.495673      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:44:35.112: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.975235888s
  E0418 01:44:35.496169      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:44:36.115: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.971865701s
  E0418 01:44:36.497184      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:44:37.118: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.968557329s
  E0418 01:44:37.497428      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:44:38.122: INFO: Verifying statefulset ss doesn't scale past 0 for another 965.332943ms
  E0418 01:44:38.498232      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7825 @ 04/18/23 01:44:39.123
  Apr 18 01:44:39.126: INFO: Scaling statefulset ss to 0
  Apr 18 01:44:39.137: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 18 01:44:39.140: INFO: Deleting all statefulset in ns statefulset-7825
  Apr 18 01:44:39.143: INFO: Scaling statefulset ss to 0
  Apr 18 01:44:39.152: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 18 01:44:39.155: INFO: Deleting statefulset ss
  Apr 18 01:44:39.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7825" for this suite. @ 04/18/23 01:44:39.172
• [61.752 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 04/18/23 01:44:39.18
  Apr 18 01:44:39.180: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename resourcequota @ 04/18/23 01:44:39.181
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:44:39.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:44:39.214
  STEP: Counting existing ResourceQuota @ 04/18/23 01:44:39.218
  E0418 01:44:39.498901      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:40.499604      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:41.499959      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:42.500581      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:43.501078      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/18/23 01:44:44.226
  STEP: Ensuring resource quota status is calculated @ 04/18/23 01:44:44.231
  E0418 01:44:44.501146      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:45.501336      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod that fits quota @ 04/18/23 01:44:46.236
  STEP: Ensuring ResourceQuota status captures the pod usage @ 04/18/23 01:44:46.251
  E0418 01:44:46.501798      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:47.502033      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 04/18/23 01:44:48.255
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 04/18/23 01:44:48.26
  STEP: Ensuring a pod cannot update its resource requirements @ 04/18/23 01:44:48.263
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 04/18/23 01:44:48.267
  E0418 01:44:48.502918      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:49.502784      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 04/18/23 01:44:50.271
  STEP: Ensuring resource quota status released the pod usage @ 04/18/23 01:44:50.291
  E0418 01:44:50.503114      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:51.503301      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:44:52.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6409" for this suite. @ 04/18/23 01:44:52.329
• [13.229 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 04/18/23 01:44:52.409
  Apr 18 01:44:52.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 04/18/23 01:44:52.41
  E0418 01:44:52.504378      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:44:52.611
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:44:52.64
  STEP: creating a target pod @ 04/18/23 01:44:52.688
  E0418 01:44:53.505187      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:54.506217      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding an ephemeral container @ 04/18/23 01:44:54.826
  E0418 01:44:55.506409      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:56.506547      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:57.506723      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:44:58.506907      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking pod container endpoints @ 04/18/23 01:44:58.856
  Apr 18 01:44:58.856: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-7814 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 18 01:44:58.856: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  Apr 18 01:44:58.856: INFO: ExecWithOptions: Clientset creation
  Apr 18 01:44:58.857: INFO: ExecWithOptions: execute(POST https://10.3.0.1:443/api/v1/namespaces/ephemeral-containers-test-7814/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Apr 18 01:44:58.931: INFO: Exec stderr: ""
  Apr 18 01:44:58.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-7814" for this suite. @ 04/18/23 01:44:58.946
• [6.543 seconds]
------------------------------
SS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 04/18/23 01:44:58.953
  Apr 18 01:44:58.953: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename cronjob @ 04/18/23 01:44:58.955
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:44:58.971
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:44:58.975
  STEP: Creating a cronjob @ 04/18/23 01:44:58.978
  STEP: Ensuring more than one job is running at a time @ 04/18/23 01:44:58.982
  E0418 01:44:59.507156      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:00.507486      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:01.507609      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:02.507991      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:03.508885      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:04.510012      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:05.510775      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:06.511177      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:07.511869      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:08.512079      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:09.512204      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:10.512231      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:11.512357      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:12.512555      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:13.512916      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:14.513133      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:15.514199      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:16.514517      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:17.514596      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:18.514705      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:19.514770      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:20.517525      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:21.517968      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:22.518052      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:23.518160      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:24.518575      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:25.518683      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:26.518795      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:27.519572      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:28.519893      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:29.520660      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:30.521776      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:31.521908      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:32.522082      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:33.522774      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:34.522839      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:35.522979      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:36.523960      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:37.524998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:38.525096      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:39.525978      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:40.526472      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:41.526586      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:42.526724      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:43.530908      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:44.531058      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:45.531507      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:46.531615      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:47.531809      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:48.532049      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:49.532112      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:50.532203      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:51.532320      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:52.532580      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:53.533601      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:54.533883      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:55.534162      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:56.534284      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:57.535232      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:58.535542      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:45:59.535665      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:00.536439      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 04/18/23 01:46:00.988
  STEP: Removing cronjob @ 04/18/23 01:46:00.992
  Apr 18 01:46:00.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-3040" for this suite. @ 04/18/23 01:46:01.003
• [62.080 seconds]
------------------------------
S
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 04/18/23 01:46:01.034
  Apr 18 01:46:01.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename csiinlinevolumes @ 04/18/23 01:46:01.035
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:46:01.102
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:46:01.123
  STEP: creating @ 04/18/23 01:46:01.16
  STEP: getting @ 04/18/23 01:46:01.183
  STEP: listing @ 04/18/23 01:46:01.188
  STEP: deleting @ 04/18/23 01:46:01.191
  Apr 18 01:46:01.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-9694" for this suite. @ 04/18/23 01:46:01.218
• [0.191 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 04/18/23 01:46:01.236
  Apr 18 01:46:01.236: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename services @ 04/18/23 01:46:01.238
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:46:01.256
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:46:01.264
  STEP: creating service multi-endpoint-test in namespace services-281 @ 04/18/23 01:46:01.269
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-281 to expose endpoints map[] @ 04/18/23 01:46:01.281
  Apr 18 01:46:01.314: INFO: successfully validated that service multi-endpoint-test in namespace services-281 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-281 @ 04/18/23 01:46:01.318
  E0418 01:46:01.545553      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:02.545712      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-281 to expose endpoints map[pod1:[100]] @ 04/18/23 01:46:03.343
  Apr 18 01:46:03.363: INFO: successfully validated that service multi-endpoint-test in namespace services-281 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-281 @ 04/18/23 01:46:03.363
  E0418 01:46:03.546604      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:04.546998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-281 to expose endpoints map[pod1:[100] pod2:[101]] @ 04/18/23 01:46:05.394
  Apr 18 01:46:05.409: INFO: successfully validated that service multi-endpoint-test in namespace services-281 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 04/18/23 01:46:05.409
  Apr 18 01:46:05.409: INFO: Creating new exec pod
  E0418 01:46:05.547641      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:06.547964      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:07.548224      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:46:08.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-281 exec execpodltm97 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  E0418 01:46:08.548496      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:46:08.606: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Apr 18 01:46:08.606: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 18 01:46:08.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-281 exec execpodltm97 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.3.163.114 80'
  Apr 18 01:46:08.776: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.3.163.114 80\nConnection to 10.3.163.114 80 port [tcp/http] succeeded!\n"
  Apr 18 01:46:08.776: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 18 01:46:08.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-281 exec execpodltm97 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Apr 18 01:46:09.016: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Apr 18 01:46:09.016: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 18 01:46:09.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-281 exec execpodltm97 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.3.163.114 81'
  Apr 18 01:46:09.177: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.3.163.114 81\nConnection to 10.3.163.114 81 port [tcp/*] succeeded!\n"
  Apr 18 01:46:09.177: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-281 @ 04/18/23 01:46:09.177
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-281 to expose endpoints map[pod2:[101]] @ 04/18/23 01:46:09.191
  Apr 18 01:46:09.233: INFO: successfully validated that service multi-endpoint-test in namespace services-281 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-281 @ 04/18/23 01:46:09.233
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-281 to expose endpoints map[] @ 04/18/23 01:46:09.256
  Apr 18 01:46:09.304: INFO: successfully validated that service multi-endpoint-test in namespace services-281 exposes endpoints map[]
  Apr 18 01:46:09.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-281" for this suite. @ 04/18/23 01:46:09.437
  E0418 01:46:09.549308      21 retrywatcher.go:130] "Watch failed" err="context canceled"
• [8.354 seconds]
------------------------------
S
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 04/18/23 01:46:09.591
  Apr 18 01:46:09.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename job @ 04/18/23 01:46:09.592
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:46:09.676
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:46:09.69
  STEP: Creating a suspended job @ 04/18/23 01:46:09.738
  STEP: Patching the Job @ 04/18/23 01:46:09.785
  STEP: Watching for Job to be patched @ 04/18/23 01:46:09.977
  Apr 18 01:46:09.996: INFO: Event ADDED observed for Job e2e-s997r in namespace job-5493 with labels: map[e2e-job-label:e2e-s997r] and annotations: map[batch.kubernetes.io/job-tracking:]
  Apr 18 01:46:09.996: INFO: Event MODIFIED observed for Job e2e-s997r in namespace job-5493 with labels: map[e2e-job-label:e2e-s997r] and annotations: map[batch.kubernetes.io/job-tracking:]
  Apr 18 01:46:09.996: INFO: Event MODIFIED found for Job e2e-s997r in namespace job-5493 with labels: map[e2e-job-label:e2e-s997r e2e-s997r:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 04/18/23 01:46:09.996
  STEP: Watching for Job to be updated @ 04/18/23 01:46:10.077
  Apr 18 01:46:10.090: INFO: Event MODIFIED found for Job e2e-s997r in namespace job-5493 with labels: map[e2e-job-label:e2e-s997r e2e-s997r:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 18 01:46:10.090: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 04/18/23 01:46:10.091
  Apr 18 01:46:10.120: INFO: Job: e2e-s997r as labels: map[e2e-job-label:e2e-s997r e2e-s997r:patched]
  STEP: Waiting for job to complete @ 04/18/23 01:46:10.12
  E0418 01:46:10.550023      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:11.550970      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:12.551259      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:13.551544      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:14.553485      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:15.554469      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:16.554717      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:17.554986      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:18.555702      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:19.557217      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Delete a job collection with a labelselector @ 04/18/23 01:46:20.127
  STEP: Watching for Job to be deleted @ 04/18/23 01:46:20.136
  Apr 18 01:46:20.139: INFO: Event MODIFIED observed for Job e2e-s997r in namespace job-5493 with labels: map[e2e-job-label:e2e-s997r e2e-s997r:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 18 01:46:20.139: INFO: Event MODIFIED observed for Job e2e-s997r in namespace job-5493 with labels: map[e2e-job-label:e2e-s997r e2e-s997r:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 18 01:46:20.140: INFO: Event MODIFIED observed for Job e2e-s997r in namespace job-5493 with labels: map[e2e-job-label:e2e-s997r e2e-s997r:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 18 01:46:20.140: INFO: Event MODIFIED observed for Job e2e-s997r in namespace job-5493 with labels: map[e2e-job-label:e2e-s997r e2e-s997r:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 18 01:46:20.140: INFO: Event MODIFIED observed for Job e2e-s997r in namespace job-5493 with labels: map[e2e-job-label:e2e-s997r e2e-s997r:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 18 01:46:20.140: INFO: Event MODIFIED observed for Job e2e-s997r in namespace job-5493 with labels: map[e2e-job-label:e2e-s997r e2e-s997r:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 18 01:46:20.141: INFO: Event MODIFIED observed for Job e2e-s997r in namespace job-5493 with labels: map[e2e-job-label:e2e-s997r e2e-s997r:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 18 01:46:20.141: INFO: Event MODIFIED observed for Job e2e-s997r in namespace job-5493 with labels: map[e2e-job-label:e2e-s997r e2e-s997r:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 18 01:46:20.141: INFO: Event MODIFIED observed for Job e2e-s997r in namespace job-5493 with labels: map[e2e-job-label:e2e-s997r e2e-s997r:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 18 01:46:20.141: INFO: Event MODIFIED observed for Job e2e-s997r in namespace job-5493 with labels: map[e2e-job-label:e2e-s997r e2e-s997r:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 18 01:46:20.141: INFO: Event DELETED found for Job e2e-s997r in namespace job-5493 with labels: map[e2e-job-label:e2e-s997r e2e-s997r:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 04/18/23 01:46:20.141
  Apr 18 01:46:20.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-5493" for this suite. @ 04/18/23 01:46:20.157
• [10.587 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 04/18/23 01:46:20.179
  Apr 18 01:46:20.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename pods @ 04/18/23 01:46:20.18
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:46:20.207
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:46:20.212
  STEP: creating the pod @ 04/18/23 01:46:20.217
  STEP: submitting the pod to kubernetes @ 04/18/23 01:46:20.217
  STEP: verifying QOS class is set on the pod @ 04/18/23 01:46:20.225
  Apr 18 01:46:20.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9396" for this suite. @ 04/18/23 01:46:20.236
• [0.078 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 04/18/23 01:46:20.258
  Apr 18 01:46:20.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename downward-api @ 04/18/23 01:46:20.259
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:46:20.281
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:46:20.285
  STEP: Creating a pod to test downward API volume plugin @ 04/18/23 01:46:20.288
  E0418 01:46:20.556282      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:21.556977      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:22.557962      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:23.558109      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:46:24.311
  Apr 18 01:46:24.314: INFO: Trying to get logs from node ip-10-0-14-154 pod downwardapi-volume-f261b9a1-aaf1-4823-948b-a55534be5b0a container client-container: <nil>
  STEP: delete the pod @ 04/18/23 01:46:24.319
  Apr 18 01:46:24.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-480" for this suite. @ 04/18/23 01:46:24.34
• [4.091 seconds]
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 04/18/23 01:46:24.354
  Apr 18 01:46:24.354: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename secrets @ 04/18/23 01:46:24.355
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:46:24.379
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:46:24.383
  STEP: Creating secret with name secret-test-map-0325dc1b-73f5-44ca-8722-3bfadfcea305 @ 04/18/23 01:46:24.387
  STEP: Creating a pod to test consume secrets @ 04/18/23 01:46:24.392
  E0418 01:46:24.558133      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:25.558291      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:26.558935      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:27.559054      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:46:28.415
  Apr 18 01:46:28.419: INFO: Trying to get logs from node ip-10-0-14-154 pod pod-secrets-cca0d99c-a642-4e95-b819-1cab9e297f06 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/18/23 01:46:28.425
  Apr 18 01:46:28.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-784" for this suite. @ 04/18/23 01:46:28.448
• [4.100 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 04/18/23 01:46:28.458
  Apr 18 01:46:28.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename downward-api @ 04/18/23 01:46:28.46
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:46:28.478
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:46:28.482
  STEP: Creating a pod to test downward API volume plugin @ 04/18/23 01:46:28.485
  E0418 01:46:28.559631      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:29.559891      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:30.560161      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:31.560983      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:46:32.507
  Apr 18 01:46:32.515: INFO: Trying to get logs from node ip-10-0-14-154 pod downwardapi-volume-44ee19ee-8955-499b-a73d-618ebaa498b8 container client-container: <nil>
  STEP: delete the pod @ 04/18/23 01:46:32.531
  E0418 01:46:32.562079      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:46:32.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8523" for this suite. @ 04/18/23 01:46:32.579
• [4.141 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 04/18/23 01:46:32.6
  Apr 18 01:46:32.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename resourcequota @ 04/18/23 01:46:32.601
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:46:32.639
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:46:32.642
  STEP: Counting existing ResourceQuota @ 04/18/23 01:46:32.646
  E0418 01:46:33.562950      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:34.564040      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:35.564595      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:36.565048      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:37.566052      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/18/23 01:46:37.651
  STEP: Ensuring resource quota status is calculated @ 04/18/23 01:46:37.657
  E0418 01:46:38.566726      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:39.566916      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicaSet @ 04/18/23 01:46:39.666
  STEP: Ensuring resource quota status captures replicaset creation @ 04/18/23 01:46:39.686
  E0418 01:46:40.567414      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:41.567606      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicaSet @ 04/18/23 01:46:41.69
  STEP: Ensuring resource quota status released usage @ 04/18/23 01:46:41.703
  E0418 01:46:42.567891      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:43.568015      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:46:43.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4812" for this suite. @ 04/18/23 01:46:43.71
• [11.117 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 04/18/23 01:46:43.719
  Apr 18 01:46:43.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename init-container @ 04/18/23 01:46:43.72
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:46:43.737
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:46:43.741
  STEP: creating the pod @ 04/18/23 01:46:43.744
  Apr 18 01:46:43.744: INFO: PodSpec: initContainers in spec.initContainers
  E0418 01:46:44.568873      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:45.569730      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:46.570711      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:47.570983      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:48.571418      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:46:48.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-3196" for this suite. @ 04/18/23 01:46:48.669
• [4.957 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 04/18/23 01:46:48.676
  Apr 18 01:46:48.676: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename projected @ 04/18/23 01:46:48.679
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:46:48.696
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:46:48.701
  STEP: Creating secret with name s-test-opt-del-c951da9f-b4f5-42d4-a4dd-c2e169de96f5 @ 04/18/23 01:46:48.71
  STEP: Creating secret with name s-test-opt-upd-c35ead08-ff88-4a82-b2ec-6f1da3b68476 @ 04/18/23 01:46:48.715
  STEP: Creating the pod @ 04/18/23 01:46:48.72
  E0418 01:46:49.571841      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:50.572512      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-c951da9f-b4f5-42d4-a4dd-c2e169de96f5 @ 04/18/23 01:46:50.758
  STEP: Updating secret s-test-opt-upd-c35ead08-ff88-4a82-b2ec-6f1da3b68476 @ 04/18/23 01:46:50.762
  STEP: Creating secret with name s-test-opt-create-392928bc-48f2-4427-ba78-dd0ec38db0cb @ 04/18/23 01:46:50.767
  STEP: waiting to observe update in volume @ 04/18/23 01:46:50.771
  E0418 01:46:51.572655      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:52.572911      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:46:52.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2691" for this suite. @ 04/18/23 01:46:52.798
• [4.130 seconds]
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 04/18/23 01:46:52.806
  Apr 18 01:46:52.806: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename statefulset @ 04/18/23 01:46:52.808
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:46:52.827
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:46:52.834
  STEP: Creating service test in namespace statefulset-7740 @ 04/18/23 01:46:52.839
  Apr 18 01:46:52.863: INFO: Found 0 stateful pods, waiting for 1
  E0418 01:46:53.573481      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:54.573870      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:55.574397      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:56.574522      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:57.575057      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:58.575744      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:46:59.575680      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:00.575739      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:01.575911      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:02.576081      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:47:02.868: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 04/18/23 01:47:02.88
  W0418 01:47:02.896001      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 18 01:47:02.915: INFO: Found 1 stateful pods, waiting for 2
  E0418 01:47:03.576627      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:04.576738      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:05.577102      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:06.577968      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:07.578200      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:08.578337      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:09.579059      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:10.580060      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:11.580385      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:12.580941      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:47:12.920: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 18 01:47:12.920: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 04/18/23 01:47:12.926
  STEP: Delete all of the StatefulSets @ 04/18/23 01:47:12.929
  STEP: Verify that StatefulSets have been deleted @ 04/18/23 01:47:12.935
  Apr 18 01:47:12.940: INFO: Deleting all statefulset in ns statefulset-7740
  Apr 18 01:47:12.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7740" for this suite. @ 04/18/23 01:47:12.971
• [20.207 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 04/18/23 01:47:13.02
  Apr 18 01:47:13.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/18/23 01:47:13.034
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:47:13.09
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:47:13.094
  Apr 18 01:47:13.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  E0418 01:47:13.581735      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:14.581894      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 04/18/23 01:47:15.458
  Apr 18 01:47:15.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-6066 --namespace=crd-publish-openapi-6066 create -f -'
  E0418 01:47:15.582281      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:47:16.356: INFO: stderr: ""
  Apr 18 01:47:16.356: INFO: stdout: "e2e-test-crd-publish-openapi-3641-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Apr 18 01:47:16.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-6066 --namespace=crd-publish-openapi-6066 delete e2e-test-crd-publish-openapi-3641-crds test-foo'
  Apr 18 01:47:16.423: INFO: stderr: ""
  Apr 18 01:47:16.423: INFO: stdout: "e2e-test-crd-publish-openapi-3641-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Apr 18 01:47:16.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-6066 --namespace=crd-publish-openapi-6066 apply -f -'
  E0418 01:47:16.582850      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:47:16.640: INFO: stderr: ""
  Apr 18 01:47:16.640: INFO: stdout: "e2e-test-crd-publish-openapi-3641-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Apr 18 01:47:16.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-6066 --namespace=crd-publish-openapi-6066 delete e2e-test-crd-publish-openapi-3641-crds test-foo'
  Apr 18 01:47:16.709: INFO: stderr: ""
  Apr 18 01:47:16.709: INFO: stdout: "e2e-test-crd-publish-openapi-3641-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 04/18/23 01:47:16.709
  Apr 18 01:47:16.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-6066 --namespace=crd-publish-openapi-6066 create -f -'
  Apr 18 01:47:16.920: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 04/18/23 01:47:16.92
  Apr 18 01:47:16.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-6066 --namespace=crd-publish-openapi-6066 create -f -'
  Apr 18 01:47:17.154: INFO: rc: 1
  Apr 18 01:47:17.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-6066 --namespace=crd-publish-openapi-6066 apply -f -'
  Apr 18 01:47:17.372: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 04/18/23 01:47:17.372
  Apr 18 01:47:17.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-6066 --namespace=crd-publish-openapi-6066 create -f -'
  E0418 01:47:17.582884      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:47:17.582: INFO: rc: 1
  Apr 18 01:47:17.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-6066 --namespace=crd-publish-openapi-6066 apply -f -'
  Apr 18 01:47:17.830: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 04/18/23 01:47:17.83
  Apr 18 01:47:17.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-6066 explain e2e-test-crd-publish-openapi-3641-crds'
  Apr 18 01:47:18.047: INFO: stderr: ""
  Apr 18 01:47:18.047: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3641-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 04/18/23 01:47:18.047
  Apr 18 01:47:18.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-6066 explain e2e-test-crd-publish-openapi-3641-crds.metadata'
  Apr 18 01:47:18.261: INFO: stderr: ""
  Apr 18 01:47:18.261: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3641-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Apr 18 01:47:18.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-6066 explain e2e-test-crd-publish-openapi-3641-crds.spec'
  Apr 18 01:47:18.502: INFO: stderr: ""
  Apr 18 01:47:18.502: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3641-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Apr 18 01:47:18.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-6066 explain e2e-test-crd-publish-openapi-3641-crds.spec.bars'
  E0418 01:47:18.583138      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:47:18.729: INFO: stderr: ""
  Apr 18 01:47:18.729: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3641-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 04/18/23 01:47:18.73
  Apr 18 01:47:18.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=crd-publish-openapi-6066 explain e2e-test-crd-publish-openapi-3641-crds.spec.bars2'
  Apr 18 01:47:19.043: INFO: rc: 1
  E0418 01:47:19.583391      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:47:20.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6066" for this suite. @ 04/18/23 01:47:20.448
• [7.442 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 04/18/23 01:47:20.465
  Apr 18 01:47:20.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename kubectl-logs @ 04/18/23 01:47:20.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:47:20.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:47:20.508
  STEP: creating an pod @ 04/18/23 01:47:20.513
  Apr 18 01:47:20.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-logs-4254 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  E0418 01:47:20.584058      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:47:20.595: INFO: stderr: ""
  Apr 18 01:47:20.595: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 04/18/23 01:47:20.595
  Apr 18 01:47:20.596: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E0418 01:47:21.584112      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:22.584458      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:47:22.685: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 04/18/23 01:47:22.685
  Apr 18 01:47:22.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-logs-4254 logs logs-generator logs-generator'
  Apr 18 01:47:22.795: INFO: stderr: ""
  Apr 18 01:47:22.795: INFO: stdout: "I0418 01:47:22.330446       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/95b 294\nI0418 01:47:22.530998       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/qdnd 345\nI0418 01:47:22.731050       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/bghr 209\n"
  STEP: limiting log lines @ 04/18/23 01:47:22.795
  Apr 18 01:47:22.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-logs-4254 logs logs-generator logs-generator --tail=1'
  Apr 18 01:47:22.898: INFO: stderr: ""
  Apr 18 01:47:22.898: INFO: stdout: "I0418 01:47:22.731050       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/bghr 209\n"
  Apr 18 01:47:22.898: INFO: got output "I0418 01:47:22.731050       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/bghr 209\n"
  STEP: limiting log bytes @ 04/18/23 01:47:22.898
  Apr 18 01:47:22.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-logs-4254 logs logs-generator logs-generator --limit-bytes=1'
  Apr 18 01:47:23.028: INFO: stderr: ""
  Apr 18 01:47:23.029: INFO: stdout: "I"
  Apr 18 01:47:23.029: INFO: got output "I"
  STEP: exposing timestamps @ 04/18/23 01:47:23.029
  Apr 18 01:47:23.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-logs-4254 logs logs-generator logs-generator --tail=1 --timestamps'
  Apr 18 01:47:23.218: INFO: stderr: ""
  Apr 18 01:47:23.218: INFO: stdout: "2023-04-18T01:47:23.130742998Z I0418 01:47:23.130621       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/x5h 446\n"
  Apr 18 01:47:23.218: INFO: got output "2023-04-18T01:47:23.130742998Z I0418 01:47:23.130621       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/x5h 446\n"
  STEP: restricting to a time range @ 04/18/23 01:47:23.218
  E0418 01:47:23.585312      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:24.585463      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:25.585599      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:47:25.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-logs-4254 logs logs-generator logs-generator --since=1s'
  Apr 18 01:47:25.837: INFO: stderr: ""
  Apr 18 01:47:25.837: INFO: stdout: "I0418 01:47:24.931103       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/9kw 491\nI0418 01:47:25.131312       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/v68q 353\nI0418 01:47:25.330624       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/lp77 282\nI0418 01:47:25.530884       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/7zfn 568\nI0418 01:47:25.731235       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/s9jh 487\n"
  Apr 18 01:47:25.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-logs-4254 logs logs-generator logs-generator --since=24h'
  Apr 18 01:47:25.926: INFO: stderr: ""
  Apr 18 01:47:25.926: INFO: stdout: "I0418 01:47:22.330446       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/95b 294\nI0418 01:47:22.530998       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/qdnd 345\nI0418 01:47:22.731050       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/bghr 209\nI0418 01:47:22.931376       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/zkzh 370\nI0418 01:47:23.130621       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/x5h 446\nI0418 01:47:23.330932       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/wndc 258\nI0418 01:47:23.531223       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/gk5 213\nI0418 01:47:23.730468       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/xgk6 468\nI0418 01:47:23.930786       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/2drg 599\nI0418 01:47:24.131149       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/jv4 579\nI0418 01:47:24.331299       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/bdmn 249\nI0418 01:47:24.533507       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/llg 343\nI0418 01:47:24.730808       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/zxmj 458\nI0418 01:47:24.931103       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/9kw 491\nI0418 01:47:25.131312       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/v68q 353\nI0418 01:47:25.330624       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/lp77 282\nI0418 01:47:25.530884       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/7zfn 568\nI0418 01:47:25.731235       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/s9jh 487\n"
  Apr 18 01:47:25.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=kubectl-logs-4254 delete pod logs-generator'
  Apr 18 01:47:26.449: INFO: stderr: ""
  Apr 18 01:47:26.449: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Apr 18 01:47:26.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-4254" for this suite. @ 04/18/23 01:47:26.452
• [5.992 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 04/18/23 01:47:26.458
  Apr 18 01:47:26.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename replicaset @ 04/18/23 01:47:26.459
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:47:26.481
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:47:26.489
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 04/18/23 01:47:26.493
  Apr 18 01:47:26.502: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0418 01:47:26.585613      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:27.585721      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:28.585840      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:29.585997      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:30.586292      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:47:31.510: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/18/23 01:47:31.51
  STEP: getting scale subresource @ 04/18/23 01:47:31.51
  STEP: updating a scale subresource @ 04/18/23 01:47:31.517
  STEP: verifying the replicaset Spec.Replicas was modified @ 04/18/23 01:47:31.527
  STEP: Patch a scale subresource @ 04/18/23 01:47:31.543
  E0418 01:47:31.587438      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:47:31.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2037" for this suite. @ 04/18/23 01:47:31.616
• [5.187 seconds]
------------------------------
S
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 04/18/23 01:47:31.647
  Apr 18 01:47:31.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename proxy @ 04/18/23 01:47:31.648
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:47:31.69
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:47:31.695
  Apr 18 01:47:31.701: INFO: Creating pod...
  E0418 01:47:32.587987      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:33.589019      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:47:33.750: INFO: Creating service...
  Apr 18 01:47:33.761: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-6997/pods/agnhost/proxy/some/path/with/DELETE
  Apr 18 01:47:33.799: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 18 01:47:33.799: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-6997/pods/agnhost/proxy/some/path/with/GET
  Apr 18 01:47:33.803: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Apr 18 01:47:33.803: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-6997/pods/agnhost/proxy/some/path/with/HEAD
  Apr 18 01:47:33.817: INFO: http.Client request:HEAD | StatusCode:200
  Apr 18 01:47:33.819: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-6997/pods/agnhost/proxy/some/path/with/OPTIONS
  Apr 18 01:47:33.826: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 18 01:47:33.827: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-6997/pods/agnhost/proxy/some/path/with/PATCH
  Apr 18 01:47:33.833: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 18 01:47:33.834: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-6997/pods/agnhost/proxy/some/path/with/POST
  Apr 18 01:47:33.837: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 18 01:47:33.838: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-6997/pods/agnhost/proxy/some/path/with/PUT
  Apr 18 01:47:33.853: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 18 01:47:33.853: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-6997/services/test-service/proxy/some/path/with/DELETE
  Apr 18 01:47:33.858: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 18 01:47:33.859: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-6997/services/test-service/proxy/some/path/with/GET
  Apr 18 01:47:33.869: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Apr 18 01:47:33.870: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-6997/services/test-service/proxy/some/path/with/HEAD
  Apr 18 01:47:33.880: INFO: http.Client request:HEAD | StatusCode:200
  Apr 18 01:47:33.880: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-6997/services/test-service/proxy/some/path/with/OPTIONS
  Apr 18 01:47:33.890: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 18 01:47:33.890: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-6997/services/test-service/proxy/some/path/with/PATCH
  Apr 18 01:47:33.894: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 18 01:47:33.894: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-6997/services/test-service/proxy/some/path/with/POST
  Apr 18 01:47:33.898: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 18 01:47:33.899: INFO: Starting http.Client for https://10.3.0.1:443/api/v1/namespaces/proxy-6997/services/test-service/proxy/some/path/with/PUT
  Apr 18 01:47:33.923: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 18 01:47:33.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-6997" for this suite. @ 04/18/23 01:47:33.931
• [2.304 seconds]
------------------------------
S
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 04/18/23 01:47:33.952
  Apr 18 01:47:33.952: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename downward-api @ 04/18/23 01:47:33.952
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:47:33.987
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:47:34.006
  STEP: Creating a pod to test downward api env vars @ 04/18/23 01:47:34.015
  E0418 01:47:34.591876      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:35.592238      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:36.593178      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:37.593981      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:47:38.057
  Apr 18 01:47:38.060: INFO: Trying to get logs from node ip-10-0-14-154 pod downward-api-b7d11dfa-8831-48ad-ad22-30419e0b2de2 container dapi-container: <nil>
  STEP: delete the pod @ 04/18/23 01:47:38.064
  Apr 18 01:47:38.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6185" for this suite. @ 04/18/23 01:47:38.078
• [4.132 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 04/18/23 01:47:38.085
  Apr 18 01:47:38.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename daemonsets @ 04/18/23 01:47:38.086
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:47:38.103
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:47:38.106
  STEP: Creating simple DaemonSet "daemon-set" @ 04/18/23 01:47:38.124
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/18/23 01:47:38.13
  Apr 18 01:47:38.136: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 01:47:38.139: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 01:47:38.139: INFO: Node ip-10-0-14-154 is running 0 daemon pod, expected 1
  E0418 01:47:38.594532      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:47:39.143: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 01:47:39.150: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 18 01:47:39.150: INFO: Node ip-10-0-14-154 is running 0 daemon pod, expected 1
  E0418 01:47:39.594651      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:47:40.142: INFO: DaemonSet pods can't tolerate node ip-10-0-5-176 with taints [{Key:node-role.kubernetes.io/controller Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Apr 18 01:47:40.145: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 18 01:47:40.145: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: listing all DaemonSets @ 04/18/23 01:47:40.148
  STEP: DeleteCollection of the DaemonSets @ 04/18/23 01:47:40.15
  STEP: Verify that ReplicaSets have been deleted @ 04/18/23 01:47:40.156
  Apr 18 01:47:40.169: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"75726"},"items":null}

  Apr 18 01:47:40.180: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"75727"},"items":[{"metadata":{"name":"daemon-set-4c2wt","generateName":"daemon-set-","namespace":"daemonsets-251","uid":"bcfc6482-91f8-4884-8d4f-e0b7626eae7b","resourceVersion":"75727","creationTimestamp":"2023-04-18T01:47:38Z","deletionTimestamp":"2023-04-18T01:48:10Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"6274250eac4751d0d3d1c8a94069c04e0fcc8ebb484208e61e69f0735f32ad7a","cni.projectcalico.org/podIP":"10.2.129.251/32","cni.projectcalico.org/podIPs":"10.2.129.251/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d5b05b87-15ab-467b-9a95-936806a49aff","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-04-18T01:47:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-18T01:47:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5b05b87-15ab-467b-9a95-936806a49aff\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-18T01:47:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.129.251\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-8zjjx","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-8zjjx","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-27-81","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-27-81"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-18T01:47:38Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-18T01:47:39Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-18T01:47:39Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-18T01:47:38Z"}],"hostIP":"10.0.27.81","podIP":"10.2.129.251","podIPs":[{"ip":"10.2.129.251"}],"startTime":"2023-04-18T01:47:38Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-18T01:47:39Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://67f5fedeecb94ede3333c0109cfc23a9bd073e0db3adf6a144e593b067c2cfe7","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qbgk9","generateName":"daemon-set-","namespace":"daemonsets-251","uid":"4357a1a1-b8e0-4e96-9d02-d73d2b440434","resourceVersion":"75719","creationTimestamp":"2023-04-18T01:47:38Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"0a7214681537064394e8e58c85520b71a59ece4c24d8c9a9461eaca6582f4e9b","cni.projectcalico.org/podIP":"10.2.212.236/32","cni.projectcalico.org/podIPs":"10.2.212.236/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d5b05b87-15ab-467b-9a95-936806a49aff","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-04-18T01:47:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-18T01:47:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5b05b87-15ab-467b-9a95-936806a49aff\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-18T01:47:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.2.212.236\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-g9zsq","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-g9zsq","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-14-154","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-14-154"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-18T01:47:38Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-18T01:47:39Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-18T01:47:39Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-18T01:47:38Z"}],"hostIP":"10.0.14.154","podIP":"10.2.212.236","podIPs":[{"ip":"10.2.212.236"}],"startTime":"2023-04-18T01:47:38Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-18T01:47:38Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://c48b74b2da9244992274b5eb92f9107d7f333d17b0277c7d22e0c47635add869","started":true}],"qosClass":"BestEffort"}}]}

  Apr 18 01:47:40.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-251" for this suite. @ 04/18/23 01:47:40.196
• [2.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 04/18/23 01:47:40.207
  Apr 18 01:47:40.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename job @ 04/18/23 01:47:40.208
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:47:40.226
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:47:40.23
  STEP: Creating a job @ 04/18/23 01:47:40.234
  STEP: Ensuring job reaches completions @ 04/18/23 01:47:40.24
  E0418 01:47:40.595150      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:41.595268      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:42.596319      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:43.596467      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:44.597040      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:45.597469      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:46.597979      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:47.598069      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:48.598840      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:49.598903      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:50.599975      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:51.600235      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:47:52.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8287" for this suite. @ 04/18/23 01:47:52.246
• [12.047 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 04/18/23 01:47:52.255
  Apr 18 01:47:52.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename services @ 04/18/23 01:47:52.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:47:52.333
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:47:52.352
  STEP: creating service endpoint-test2 in namespace services-4494 @ 04/18/23 01:47:52.382
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4494 to expose endpoints map[] @ 04/18/23 01:47:52.416
  Apr 18 01:47:52.440: INFO: successfully validated that service endpoint-test2 in namespace services-4494 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-4494 @ 04/18/23 01:47:52.44
  E0418 01:47:52.601731      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:53.602034      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4494 to expose endpoints map[pod1:[80]] @ 04/18/23 01:47:54.477
  Apr 18 01:47:54.487: INFO: successfully validated that service endpoint-test2 in namespace services-4494 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 04/18/23 01:47:54.487
  Apr 18 01:47:54.487: INFO: Creating new exec pod
  E0418 01:47:54.602986      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:55.603495      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:56.604231      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:47:57.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-4494 exec execpodtbw8q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E0418 01:47:57.604598      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:47:57.770: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 18 01:47:57.770: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 18 01:47:57.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-4494 exec execpodtbw8q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.3.156.151 80'
  Apr 18 01:47:57.982: INFO: stderr: "+ nc -v -t -w 2 10.3.156.151 80\n+ echo hostName\nConnection to 10.3.156.151 80 port [tcp/http] succeeded!\n"
  Apr 18 01:47:57.982: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-4494 @ 04/18/23 01:47:57.982
  E0418 01:47:58.607923      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:47:59.608977      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4494 to expose endpoints map[pod1:[80] pod2:[80]] @ 04/18/23 01:48:00.046
  Apr 18 01:48:00.064: INFO: successfully validated that service endpoint-test2 in namespace services-4494 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 04/18/23 01:48:00.064
  E0418 01:48:00.610044      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:48:01.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-4494 exec execpodtbw8q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 18 01:48:01.325: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 18 01:48:01.325: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 18 01:48:01.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-4494 exec execpodtbw8q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.3.156.151 80'
  Apr 18 01:48:01.588: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.3.156.151 80\nConnection to 10.3.156.151 80 port [tcp/http] succeeded!\n"
  Apr 18 01:48:01.588: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-4494 @ 04/18/23 01:48:01.588
  E0418 01:48:01.610559      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4494 to expose endpoints map[pod2:[80]] @ 04/18/23 01:48:01.636
  E0418 01:48:02.610989      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:48:02.710: INFO: successfully validated that service endpoint-test2 in namespace services-4494 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 04/18/23 01:48:02.71
  E0418 01:48:03.611579      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:48:03.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-4494 exec execpodtbw8q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 18 01:48:03.920: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 18 01:48:03.920: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 18 01:48:03.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2348559409 --namespace=services-4494 exec execpodtbw8q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.3.156.151 80'
  Apr 18 01:48:04.118: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.3.156.151 80\nConnection to 10.3.156.151 80 port [tcp/http] succeeded!\n"
  Apr 18 01:48:04.119: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-4494 @ 04/18/23 01:48:04.119
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4494 to expose endpoints map[] @ 04/18/23 01:48:04.161
  E0418 01:48:04.611893      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:48:05.304: INFO: successfully validated that service endpoint-test2 in namespace services-4494 exposes endpoints map[]
  Apr 18 01:48:05.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4494" for this suite. @ 04/18/23 01:48:05.33
• [13.082 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 04/18/23 01:48:05.338
  Apr 18 01:48:05.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename replication-controller @ 04/18/23 01:48:05.339
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:48:05.361
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:48:05.365
  STEP: Given a ReplicationController is created @ 04/18/23 01:48:05.386
  STEP: When the matched label of one of its pods change @ 04/18/23 01:48:05.399
  Apr 18 01:48:05.414: INFO: Pod name pod-release: Found 0 pods out of 1
  E0418 01:48:05.612936      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:48:06.613075      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:48:07.613326      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:48:08.613539      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:48:09.613810      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:48:10.420: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 04/18/23 01:48:10.436
  E0418 01:48:10.614923      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 18 01:48:11.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-783" for this suite. @ 04/18/23 01:48:11.452
• [6.119 seconds]
------------------------------
S
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 04/18/23 01:48:11.457
  Apr 18 01:48:11.457: INFO: >>> kubeConfig: /tmp/kubeconfig-2348559409
  STEP: Building a namespace api object, basename downward-api @ 04/18/23 01:48:11.459
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/18/23 01:48:11.476
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/18/23 01:48:11.481
  STEP: Creating a pod to test downward API volume plugin @ 04/18/23 01:48:11.484
  E0418 01:48:11.615304      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:48:12.615804      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:48:13.616083      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0418 01:48:14.616254      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/18/23 01:48:15.508
  Apr 18 01:48:15.511: INFO: Trying to get logs from node ip-10-0-14-154 pod downwardapi-volume-a2561eb3-1623-4848-bbb2-521017694116 container client-container: <nil>
  STEP: delete the pod @ 04/18/23 01:48:15.519
  Apr 18 01:48:15.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8147" for this suite. @ 04/18/23 01:48:15.545
• [4.095 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Apr 18 01:48:15.561: INFO: Running AfterSuite actions on node 1
  Apr 18 01:48:15.562: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.007 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.064 seconds]
------------------------------

  E0418 01:48:15.636327      21 retrywatcher.go:130] "Watch failed" err="context canceled"
Ran 378 of 7207 Specs in 5580.056 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h33m0.549843674s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

